Fold 0
Command line
python chemprop/meta_train.py --meta_learning --ANIL --data_path /home/apappu/thesis/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv --dataset_type classification --split_type scaffold_balanced --chembl_assay_metadata_pickle_path filtered_chembl/ --save_dir checkpoints/anil/ --results_save_dir results/anil/ --experiment_name anil
Args
{'ANIL': True,
 'FO_MAML': False,
 'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 32,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'chembl_assay_metadata_pickle_path': 'filtered_chembl/',
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'data_path': '/home/apappu/thesis/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'dropout': 0.2,
 'dummy': False,
 'ensemble_size': 1,
 'epochs': 1000,
 'experiment_name': 'anil',
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_size': None,
 'ffn_hidden_size': 400,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'inner_loop_lr': 0.05,
 'kaiming': True,
 'log_frequency': 10,
 'loss_queue_window': 500,
 'max_data_size': None,
 'max_lr': 0.001,
 'meta_batch_size': 32,
 'meta_learning': True,
 'meta_test_epochs': 30,
 'meta_test_lr': 0.0001,
 'meta_test_split_sizes': (0.8, 0.1, 0.1),
 'meta_train_split_sizes': (0.8, 0.2, 0),
 'metric': 'prc-auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 1,
 'num_inner_gradient_steps': 2,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'outer_loop_lr': 0.0015,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results/anil/',
 'save_dir': 'checkpoints/anil/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': None,
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
Number of tasks = 645
Class sizes
CHEMBL1033994 0: 41.98%, 1: 58.02%
CHEMBL1119333 0: 9.59%, 1: 90.41%
CHEMBL1217000 0: 52.41%, 1: 47.59%
CHEMBL1243965 0: 17.11%, 1: 82.89%
CHEMBL1243966 0: 41.96%, 1: 58.04%
CHEMBL1243967 0: 10.17%, 1: 89.83%
CHEMBL1243968 0: 15.06%, 1: 84.94%
CHEMBL1243970 0: 6.91%, 1: 93.09%
CHEMBL1243972 0: 9.92%, 1: 90.08%
CHEMBL1243976 0: 28.99%, 1: 71.01%
CHEMBL1246087 0: 33.88%, 1: 66.12%
CHEMBL1246088 0: 48.37%, 1: 51.63%
CHEMBL1613762 0: 2.45%, 1: 97.55%
CHEMBL1613779 0: 58.47%, 1: 41.53%
CHEMBL1613785 0: 43.22%, 1: 56.78%
CHEMBL1613787 0: 69.08%, 1: 30.92%
CHEMBL1613807 0: 95.99%, 1: 4.01%
CHEMBL1613813 0: 44.20%, 1: 55.80%
CHEMBL1613814 0: 53.85%, 1: 46.15%
CHEMBL1613817 0: 84.86%, 1: 15.14%
CHEMBL1613853 0: 41.86%, 1: 58.14%
CHEMBL1613861 0: 14.47%, 1: 85.53%
CHEMBL1613864 0: 5.19%, 1: 94.81%
CHEMBL1613867 0: 97.13%, 1: 2.87%
CHEMBL1613870 0: 15.80%, 1: 84.20%
CHEMBL1613871 0: 6.15%, 1: 93.85%
CHEMBL1613874 0: 65.78%, 1: 34.22%
CHEMBL1613876 0: 54.35%, 1: 45.65%
CHEMBL1613884 0: 21.51%, 1: 78.49%
CHEMBL1613890 0: 34.85%, 1: 65.15%
CHEMBL1613897 0: 46.64%, 1: 53.36%
CHEMBL1613898 0: 75.45%, 1: 24.55%
CHEMBL1613904 0: 50.77%, 1: 49.23%
CHEMBL1613907 0: 5.34%, 1: 94.66%
CHEMBL1613926 0: 30.15%, 1: 69.85%
CHEMBL1613928 0: 45.90%, 1: 54.10%
CHEMBL1613929 0: 81.25%, 1: 18.75%
CHEMBL1613941 0: 32.58%, 1: 67.42%
CHEMBL1613942 0: 41.43%, 1: 58.57%
CHEMBL1613947 0: 46.01%, 1: 53.99%
CHEMBL1613949 0: 60.13%, 1: 39.87%
CHEMBL1613950 0: 43.05%, 1: 56.95%
CHEMBL1613955 0: 54.04%, 1: 45.96%
CHEMBL1613962 0: 24.26%, 1: 75.74%
CHEMBL1613967 0: 44.59%, 1: 55.41%
CHEMBL1613981 0: 58.85%, 1: 41.15%
CHEMBL1613991 0: 14.98%, 1: 85.02%
CHEMBL1613997 0: 7.73%, 1: 92.27%
CHEMBL1614001 0: 98.28%, 1: 1.72%
CHEMBL1614004 0: 14.66%, 1: 85.34%
CHEMBL1614016 0: 12.22%, 1: 87.78%
CHEMBL1614030 0: 19.96%, 1: 80.04%
CHEMBL1614034 0: 2.63%, 1: 97.37%
CHEMBL1614035 0: 77.14%, 1: 22.86%
CHEMBL1614049 0: 39.63%, 1: 60.37%
CHEMBL1614053 0: 42.48%, 1: 57.52%
CHEMBL1614063 0: 95.18%, 1: 4.82%
CHEMBL1614065 0: 9.71%, 1: 90.29%
CHEMBL1614066 0: 94.74%, 1: 5.26%
CHEMBL1614069 0: 56.82%, 1: 43.18%
CHEMBL1614072 0: 50.36%, 1: 49.64%
CHEMBL1614084 0: 13.26%, 1: 86.74%
CHEMBL1614091 0: 4.43%, 1: 95.57%
CHEMBL1614092 0: 47.50%, 1: 52.50%
CHEMBL1614097 0: 46.99%, 1: 53.01%
CHEMBL1614098 0: 97.25%, 1: 2.75%
CHEMBL1614104 0: 7.74%, 1: 92.26%
CHEMBL1614105 0: 6.12%, 1: 93.88%
CHEMBL1614109 0: 59.87%, 1: 40.13%
CHEMBL1614128 0: 10.35%, 1: 89.65%
CHEMBL1614131 0: 97.14%, 1: 2.86%
CHEMBL1614132 0: 42.07%, 1: 57.93%
CHEMBL1614138 0: 67.83%, 1: 32.17%
CHEMBL1614155 0: 25.40%, 1: 74.60%
CHEMBL1614158 0: 74.52%, 1: 25.48%
CHEMBL1614167 0: 16.17%, 1: 83.83%
CHEMBL1614170 0: 68.86%, 1: 31.14%
CHEMBL1614171 0: 93.77%, 1: 6.23%
CHEMBL1614175 0: 80.66%, 1: 19.34%
CHEMBL1614185 0: 26.17%, 1: 73.83%
CHEMBL1614197 0: 5.26%, 1: 94.74%
CHEMBL1614199 0: 49.66%, 1: 50.34%
CHEMBL1614202 0: 7.32%, 1: 92.68%
CHEMBL1614215 0: 52.76%, 1: 47.24%
CHEMBL1614216 0: 48.52%, 1: 51.48%
CHEMBL1614218 0: 22.36%, 1: 77.64%
CHEMBL1614225 0: 62.50%, 1: 37.50%
CHEMBL1614244 0: 38.85%, 1: 61.15%
CHEMBL1614247 0: 44.34%, 1: 55.66%
CHEMBL1614252 0: 60.11%, 1: 39.89%
CHEMBL1614255 0: 61.26%, 1: 38.74%
CHEMBL1614259 0: 32.82%, 1: 67.18%
CHEMBL1614272 0: 67.71%, 1: 32.29%
CHEMBL1614276 0: 51.74%, 1: 48.26%
CHEMBL1614287 0: 85.53%, 1: 14.47%
CHEMBL1614288 0: 72.51%, 1: 27.49%
CHEMBL1614290 0: 4.51%, 1: 95.49%
CHEMBL1614295 0: 2.29%, 1: 97.71%
CHEMBL1614301 0: 19.74%, 1: 80.26%
CHEMBL1614304 0: 63.76%, 1: 36.24%
CHEMBL1614309 0: 27.66%, 1: 72.34%
CHEMBL1614311 0: 53.76%, 1: 46.24%
CHEMBL1614314 0: 94.70%, 1: 5.30%
CHEMBL1614319 0: 72.16%, 1: 27.84%
CHEMBL1614320 0: 29.79%, 1: 70.21%
CHEMBL1614321 0: 60.16%, 1: 39.84%
CHEMBL1614328 0: 46.64%, 1: 53.36%
CHEMBL1614329 0: 40.69%, 1: 59.31%
CHEMBL1614336 0: 47.01%, 1: 52.99%
CHEMBL1614344 0: 67.72%, 1: 32.28%
CHEMBL1614356 0: 43.90%, 1: 56.10%
CHEMBL1614359 0: 50.26%, 1: 49.74%
CHEMBL1614363 0: 53.19%, 1: 46.81%
CHEMBL1614385 0: 93.27%, 1: 6.73%
CHEMBL1614388 0: 56.19%, 1: 43.81%
CHEMBL1614393 0: 45.00%, 1: 55.00%
CHEMBL1614395 0: 11.32%, 1: 88.68%
CHEMBL1614403 0: 88.15%, 1: 11.85%
CHEMBL1614423 0: 57.84%, 1: 42.16%
CHEMBL1614425 0: 38.24%, 1: 61.76%
CHEMBL1614433 0: 60.77%, 1: 39.23%
CHEMBL1614434 0: 80.25%, 1: 19.75%
CHEMBL1614456 0: 3.36%, 1: 96.64%
CHEMBL1614466 0: 10.32%, 1: 89.68%
CHEMBL1614469 0: 24.90%, 1: 75.10%
CHEMBL1614477 0: 43.75%, 1: 56.25%
CHEMBL1614478 0: 12.50%, 1: 87.50%
CHEMBL1614480 0: 85.71%, 1: 14.29%
CHEMBL1614484 0: 9.01%, 1: 90.99%
CHEMBL1614492 0: 40.54%, 1: 59.46%
CHEMBL1614499 0: 44.47%, 1: 55.53%
CHEMBL1614503 0: 40.30%, 1: 59.70%
CHEMBL1614504 0: 17.80%, 1: 82.20%
CHEMBL1614509 0: 79.43%, 1: 20.57%
CHEMBL1614512 0: 62.79%, 1: 37.21%
CHEMBL1614514 0: 83.07%, 1: 16.93%
CHEMBL1614515 0: 19.61%, 1: 80.39%
CHEMBL1614516 0: 96.81%, 1: 3.19%
CHEMBL1614522 0: 28.55%, 1: 71.45%
CHEMBL1614524 0: 12.98%, 1: 87.02%
CHEMBL1614528 0: 40.91%, 1: 59.09%
CHEMBL1614547 0: 72.51%, 1: 27.49%
CHEMBL1614548 0: 17.95%, 1: 82.05%
CHEMBL1614549 0: 84.25%, 1: 15.75%
CHEMBL1614550 0: 9.30%, 1: 90.70%
CHEMBL1614554 0: 71.64%, 1: 28.36%
CHEMBL1676103 0: 39.85%, 1: 60.15%
CHEMBL1737860 0: 3.60%, 1: 96.40%
CHEMBL1737863 0: 36.27%, 1: 63.73%
CHEMBL1737865 0: 61.01%, 1: 38.99%
CHEMBL1737868 0: 79.41%, 1: 20.59%
CHEMBL1737910 0: 45.71%, 1: 54.29%
CHEMBL1737912 0: 26.51%, 1: 73.49%
CHEMBL1737942 0: 44.52%, 1: 55.48%
CHEMBL1737951 0: 51.83%, 1: 48.17%
CHEMBL1737961 0: 93.34%, 1: 6.66%
CHEMBL1737966 0: 21.36%, 1: 78.64%
CHEMBL1737967 0: 89.17%, 1: 10.83%
CHEMBL1737977 0: 13.87%, 1: 86.13%
CHEMBL1737978 0: 18.18%, 1: 81.82%
CHEMBL1737979 0: 8.94%, 1: 91.06%
CHEMBL1738019 0: 24.85%, 1: 75.15%
CHEMBL1738021 0: 10.87%, 1: 89.13%
CHEMBL1738025 0: 33.80%, 1: 66.20%
CHEMBL1738040 0: 73.48%, 1: 26.52%
CHEMBL1738043 0: 46.43%, 1: 53.57%
CHEMBL1738079 0: 3.88%, 1: 96.12%
CHEMBL1738080 0: 76.43%, 1: 23.57%
CHEMBL1738091 0: 89.77%, 1: 10.23%
CHEMBL1738097 0: 72.41%, 1: 27.59%
CHEMBL1738131 0: 68.59%, 1: 31.41%
CHEMBL1738164 0: 79.79%, 1: 20.21%
CHEMBL1738171 0: 85.45%, 1: 14.55%
CHEMBL1738183 0: 59.62%, 1: 40.38%
CHEMBL1738197 0: 36.90%, 1: 63.10%
CHEMBL1738202 0: 3.47%, 1: 96.53%
CHEMBL1738242 0: 16.55%, 1: 83.45%
CHEMBL1738249 0: 86.53%, 1: 13.47%
CHEMBL1738253 0: 7.33%, 1: 92.67%
CHEMBL1738319 0: 59.17%, 1: 40.83%
CHEMBL1738325 0: 96.86%, 1: 3.14%
CHEMBL1738362 0: 21.43%, 1: 78.57%
CHEMBL1738369 0: 56.34%, 1: 43.66%
CHEMBL1738371 0: 81.97%, 1: 18.03%
CHEMBL1738391 0: 68.38%, 1: 31.62%
CHEMBL1738400 0: 77.86%, 1: 22.14%
CHEMBL1738402 0: 54.19%, 1: 45.81%
CHEMBL1738407 0: 85.58%, 1: 14.42%
CHEMBL1738408 0: 2.18%, 1: 97.82%
CHEMBL1738414 0: 5.11%, 1: 94.89%
CHEMBL1738418 0: 68.85%, 1: 31.15%
CHEMBL1738422 0: 4.29%, 1: 95.71%
CHEMBL1738424 0: 77.50%, 1: 22.50%
CHEMBL1738430 0: 25.54%, 1: 74.46%
CHEMBL1738438 0: 2.76%, 1: 97.24%
CHEMBL1738482 0: 7.69%, 1: 92.31%
CHEMBL1738485 0: 17.65%, 1: 82.35%
CHEMBL1738494 0: 97.26%, 1: 2.74%
CHEMBL1738495 0: 1.70%, 1: 98.30%
CHEMBL1738497 0: 64.50%, 1: 35.50%
CHEMBL1738502 0: 15.20%, 1: 84.80%
CHEMBL1738510 0: 54.41%, 1: 45.59%
CHEMBL1738512 0: 3.05%, 1: 96.95%
CHEMBL1738513 0: 28.64%, 1: 71.36%
CHEMBL1738552 0: 64.70%, 1: 35.30%
CHEMBL1738575 0: 65.65%, 1: 34.35%
CHEMBL1738578 0: 23.30%, 1: 76.70%
CHEMBL1738579 0: 22.79%, 1: 77.21%
CHEMBL1738593 0: 17.65%, 1: 82.35%
CHEMBL1738599 0: 16.07%, 1: 83.93%
CHEMBL1738602 0: 42.70%, 1: 57.30%
CHEMBL1738610 0: 42.07%, 1: 57.93%
CHEMBL1738611 0: 2.23%, 1: 97.77%
CHEMBL1738632 0: 86.32%, 1: 13.68%
CHEMBL1738633 0: 4.05%, 1: 95.95%
CHEMBL1738639 0: 64.47%, 1: 35.53%
CHEMBL1738642 0: 34.82%, 1: 65.18%
CHEMBL1738670 0: 9.18%, 1: 90.82%
CHEMBL1738673 0: 12.98%, 1: 87.02%
CHEMBL1738679 0: 63.95%, 1: 36.05%
CHEMBL1738682 0: 62.45%, 1: 37.55%
CHEMBL1794296 0: 44.02%, 1: 55.98%
CHEMBL1794303 0: 2.71%, 1: 97.29%
CHEMBL1794320 0: 96.68%, 1: 3.32%
CHEMBL1794327 0: 91.59%, 1: 8.41%
CHEMBL1794336 0: 53.07%, 1: 46.93%
CHEMBL1794350 0: 3.44%, 1: 96.56%
CHEMBL1794355 0: 5.26%, 1: 94.74%
CHEMBL1794356 0: 13.92%, 1: 86.08%
CHEMBL1794358 0: 94.14%, 1: 5.86%
CHEMBL1794365 0: 7.12%, 1: 92.88%
CHEMBL1794383 0: 6.90%, 1: 93.10%
CHEMBL1794387 0: 65.04%, 1: 34.96%
CHEMBL1794393 0: 67.55%, 1: 32.45%
CHEMBL1794396 0: 3.96%, 1: 96.04%
CHEMBL1794410 0: 23.98%, 1: 76.02%
CHEMBL1794413 0: 6.02%, 1: 93.98%
CHEMBL1794438 0: 40.17%, 1: 59.83%
CHEMBL1794445 0: 87.84%, 1: 12.16%
CHEMBL1794452 0: 94.30%, 1: 5.70%
CHEMBL1794457 0: 92.98%, 1: 7.02%
CHEMBL1794460 0: 28.63%, 1: 71.37%
CHEMBL1794467 0: 18.39%, 1: 81.61%
CHEMBL1794475 0: 56.02%, 1: 43.98%
CHEMBL1794484 0: 44.93%, 1: 55.07%
CHEMBL1794494 0: 5.33%, 1: 94.67%
CHEMBL1794497 0: 8.47%, 1: 91.53%
CHEMBL1794499 0: 73.90%, 1: 26.10%
CHEMBL1794508 0: 94.63%, 1: 5.37%
CHEMBL1794516 0: 3.21%, 1: 96.79%
CHEMBL1794522 0: 58.65%, 1: 41.35%
CHEMBL1794528 0: 79.86%, 1: 20.14%
CHEMBL1794531 0: 92.03%, 1: 7.97%
CHEMBL1794548 0: 88.74%, 1: 11.26%
CHEMBL1794566 0: 6.05%, 1: 93.95%
CHEMBL1794567 0: 9.61%, 1: 90.39%
CHEMBL1794570 0: 92.79%, 1: 7.21%
CHEMBL1794571 0: 53.55%, 1: 46.45%
CHEMBL1794573 0: 10.54%, 1: 89.46%
CHEMBL1794574 0: 1.09%, 1: 98.91%
CHEMBL1794578 0: 46.99%, 1: 53.01%
CHEMBL1794581 0: 19.65%, 1: 80.35%
CHEMBL1863510 0: 2.14%, 1: 97.86%
CHEMBL1863512 0: 2.79%, 1: 97.21%
CHEMBL1909084 0: 99.28%, 1: 0.72%
CHEMBL1909085 0: 92.10%, 1: 7.90%
CHEMBL1909086 0: 91.52%, 1: 8.48%
CHEMBL1909087 0: 91.86%, 1: 8.14%
CHEMBL1909088 0: 87.97%, 1: 12.03%
CHEMBL1909089 0: 88.69%, 1: 11.31%
CHEMBL1909090 0: 90.81%, 1: 9.19%
CHEMBL1909091 0: 97.24%, 1: 2.76%
CHEMBL1909092 0: 97.14%, 1: 2.86%
CHEMBL1909093 0: 98.42%, 1: 1.58%
CHEMBL1909094 0: 90.85%, 1: 9.15%
CHEMBL1909095 0: 98.56%, 1: 1.44%
CHEMBL1909097 0: 99.28%, 1: 0.72%
CHEMBL1909102 0: 92.94%, 1: 7.06%
CHEMBL1909103 0: 97.23%, 1: 2.77%
CHEMBL1909104 0: 87.06%, 1: 12.94%
CHEMBL1909105 0: 88.70%, 1: 11.30%
CHEMBL1909106 0: 98.57%, 1: 1.43%
CHEMBL1909107 0: 97.86%, 1: 2.14%
CHEMBL1909108 0: 93.04%, 1: 6.96%
CHEMBL1909109 0: 90.35%, 1: 9.65%
CHEMBL1909110 0: 92.47%, 1: 7.53%
CHEMBL1909111 0: 94.03%, 1: 5.97%
CHEMBL1909112 0: 90.91%, 1: 9.09%
CHEMBL1909114 0: 96.50%, 1: 3.50%
CHEMBL1909115 0: 96.00%, 1: 4.00%
CHEMBL1909116 0: 96.98%, 1: 3.02%
CHEMBL1909121 0: 92.80%, 1: 7.20%
CHEMBL1909123 0: 99.16%, 1: 0.84%
CHEMBL1909124 0: 99.40%, 1: 0.60%
CHEMBL1909130 0: 95.56%, 1: 4.44%
CHEMBL1909131 0: 97.01%, 1: 2.99%
CHEMBL1909132 0: 96.15%, 1: 3.85%
CHEMBL1909134 0: 95.19%, 1: 4.81%
CHEMBL1909135 0: 95.76%, 1: 4.24%
CHEMBL1909136 0: 93.20%, 1: 6.80%
CHEMBL1909138 0: 96.16%, 1: 3.84%
CHEMBL1909139 0: 93.84%, 1: 6.16%
CHEMBL1909140 0: 93.87%, 1: 6.13%
CHEMBL1909141 0: 89.73%, 1: 10.27%
CHEMBL1909142 0: 99.04%, 1: 0.96%
CHEMBL1909143 0: 93.01%, 1: 6.99%
CHEMBL1909145 0: 97.97%, 1: 2.03%
CHEMBL1909148 0: 98.57%, 1: 1.43%
CHEMBL1909150 0: 95.10%, 1: 4.90%
CHEMBL1909156 0: 94.28%, 1: 5.72%
CHEMBL1909157 0: 96.38%, 1: 3.62%
CHEMBL1909158 0: 99.05%, 1: 0.95%
CHEMBL1909159 0: 93.18%, 1: 6.82%
CHEMBL1909165 0: 97.79%, 1: 2.21%
CHEMBL1909169 0: 98.32%, 1: 1.68%
CHEMBL1909170 0: 92.35%, 1: 7.65%
CHEMBL1909171 0: 93.29%, 1: 6.71%
CHEMBL1909172 0: 92.82%, 1: 7.18%
CHEMBL1909173 0: 92.38%, 1: 7.62%
CHEMBL1909174 0: 92.71%, 1: 7.29%
CHEMBL1909180 0: 98.20%, 1: 1.80%
CHEMBL1909181 0: 97.71%, 1: 2.29%
CHEMBL1909182 0: 96.99%, 1: 3.01%
CHEMBL1909184 0: 98.93%, 1: 1.07%
CHEMBL1909186 0: 99.52%, 1: 0.48%
CHEMBL1909190 0: 98.66%, 1: 1.34%
CHEMBL1909191 0: 94.99%, 1: 5.01%
CHEMBL1909192 0: 99.52%, 1: 0.48%
CHEMBL1909200 0: 97.97%, 1: 2.03%
CHEMBL1909201 0: 97.72%, 1: 2.28%
CHEMBL1909203 0: 98.05%, 1: 1.95%
CHEMBL1909204 0: 97.17%, 1: 2.83%
CHEMBL1909205 0: 98.67%, 1: 1.33%
CHEMBL1909206 0: 99.16%, 1: 0.84%
CHEMBL1909209 0: 92.34%, 1: 7.66%
CHEMBL1909210 0: 96.05%, 1: 3.95%
CHEMBL1909211 0: 88.80%, 1: 11.20%
CHEMBL1909212 0: 98.30%, 1: 1.70%
CHEMBL1909213 0: 99.28%, 1: 0.72%
CHEMBL1909214 0: 98.79%, 1: 1.21%
CHEMBL1909215 0: 95.61%, 1: 4.39%
CHEMBL1963686 0: 61.95%, 1: 38.05%
CHEMBL1963687 0: 64.42%, 1: 35.58%
CHEMBL1963688 0: 62.52%, 1: 37.48%
CHEMBL1963689 0: 89.69%, 1: 10.31%
CHEMBL1963690 0: 57.36%, 1: 42.64%
CHEMBL1963691 0: 49.33%, 1: 50.67%
CHEMBL1963692 0: 53.73%, 1: 46.27%
CHEMBL1963693 0: 71.00%, 1: 29.00%
CHEMBL1963694 0: 91.05%, 1: 8.95%
CHEMBL1963695 0: 74.90%, 1: 25.10%
CHEMBL1963696 0: 77.85%, 1: 22.15%
CHEMBL1963697 0: 74.06%, 1: 25.94%
CHEMBL1963698 0: 74.60%, 1: 25.40%
CHEMBL1963699 0: 83.18%, 1: 16.82%
CHEMBL1963701 0: 67.32%, 1: 32.68%
CHEMBL1963702 0: 83.82%, 1: 16.18%
CHEMBL1963703 0: 68.65%, 1: 31.35%
CHEMBL1963704 0: 65.28%, 1: 34.72%
CHEMBL1963705 0: 55.92%, 1: 44.08%
CHEMBL1963706 0: 53.38%, 1: 46.62%
CHEMBL1963707 0: 53.25%, 1: 46.75%
CHEMBL1963708 0: 48.77%, 1: 51.23%
CHEMBL1963710 0: 63.21%, 1: 36.79%
CHEMBL1963711 0: 94.72%, 1: 5.28%
CHEMBL1963712 0: 75.76%, 1: 24.24%
CHEMBL1963714 0: 74.21%, 1: 25.79%
CHEMBL1963715 0: 50.28%, 1: 49.72%
CHEMBL1963716 0: 91.91%, 1: 8.09%
CHEMBL1963717 0: 55.50%, 1: 44.50%
CHEMBL1963718 0: 65.45%, 1: 34.55%
CHEMBL1963719 0: 63.39%, 1: 36.61%
CHEMBL1963720 0: 69.73%, 1: 30.27%
CHEMBL1963721 0: 56.94%, 1: 43.06%
CHEMBL1963722 0: 49.40%, 1: 50.60%
CHEMBL1963723 0: 53.82%, 1: 46.18%
CHEMBL1963724 0: 67.32%, 1: 32.68%
CHEMBL1963725 0: 69.97%, 1: 30.03%
CHEMBL1963727 0: 56.59%, 1: 43.41%
CHEMBL1963728 0: 92.02%, 1: 7.98%
CHEMBL1963729 0: 84.64%, 1: 15.36%
CHEMBL1963731 0: 61.35%, 1: 38.65%
CHEMBL1963733 0: 79.77%, 1: 20.23%
CHEMBL1963734 0: 79.51%, 1: 20.49%
CHEMBL1963735 0: 71.66%, 1: 28.34%
CHEMBL1963736 0: 78.32%, 1: 21.68%
CHEMBL1963737 0: 86.79%, 1: 13.21%
CHEMBL1963738 0: 61.18%, 1: 38.82%
CHEMBL1963739 0: 84.32%, 1: 15.68%
CHEMBL1963740 0: 64.30%, 1: 35.70%
CHEMBL1963741 0: 67.03%, 1: 32.97%
CHEMBL1963742 0: 73.47%, 1: 26.53%
CHEMBL1963743 0: 66.24%, 1: 33.76%
CHEMBL1963744 0: 71.97%, 1: 28.03%
CHEMBL1963745 0: 67.16%, 1: 32.84%
CHEMBL1963746 0: 66.34%, 1: 33.66%
CHEMBL1963747 0: 81.33%, 1: 18.67%
CHEMBL1963748 0: 58.75%, 1: 41.25%
CHEMBL1963749 0: 55.02%, 1: 44.98%
CHEMBL1963750 0: 58.61%, 1: 41.39%
CHEMBL1963751 0: 70.90%, 1: 29.10%
CHEMBL1963752 0: 64.78%, 1: 35.22%
CHEMBL1963753 0: 88.02%, 1: 11.98%
CHEMBL1963754 0: 59.12%, 1: 40.88%
CHEMBL1963756 0: 61.66%, 1: 38.34%
CHEMBL1963757 0: 73.28%, 1: 26.72%
CHEMBL1963758 0: 82.56%, 1: 17.44%
CHEMBL1963759 0: 81.38%, 1: 18.62%
CHEMBL1963760 0: 86.36%, 1: 13.64%
CHEMBL1963761 0: 83.63%, 1: 16.37%
CHEMBL1963763 0: 72.14%, 1: 27.86%
CHEMBL1963764 0: 62.50%, 1: 37.50%
CHEMBL1963765 0: 86.41%, 1: 13.59%
CHEMBL1963766 0: 81.38%, 1: 18.62%
CHEMBL1963767 0: 77.76%, 1: 22.24%
CHEMBL1963768 0: 77.63%, 1: 22.37%
CHEMBL1963770 0: 89.44%, 1: 10.56%
CHEMBL1963771 0: 53.53%, 1: 46.47%
CHEMBL1963772 0: 48.99%, 1: 51.01%
CHEMBL1963773 0: 59.00%, 1: 41.00%
CHEMBL1963775 0: 76.92%, 1: 23.08%
CHEMBL1963776 0: 87.71%, 1: 12.29%
CHEMBL1963777 0: 63.92%, 1: 36.08%
CHEMBL1963778 0: 51.70%, 1: 48.30%
CHEMBL1963779 0: 54.69%, 1: 45.31%
CHEMBL1963780 0: 89.06%, 1: 10.94%
CHEMBL1963781 0: 94.31%, 1: 5.69%
CHEMBL1963782 0: 75.86%, 1: 24.14%
CHEMBL1963783 0: 57.36%, 1: 42.64%
CHEMBL1963785 0: 70.10%, 1: 29.90%
CHEMBL1963786 0: 50.08%, 1: 49.92%
CHEMBL1963787 0: 66.39%, 1: 33.61%
CHEMBL1963789 0: 67.34%, 1: 32.66%
CHEMBL1963790 0: 57.93%, 1: 42.07%
CHEMBL1963791 0: 82.08%, 1: 17.92%
CHEMBL1963792 0: 85.73%, 1: 14.27%
CHEMBL1963793 0: 65.48%, 1: 34.52%
CHEMBL1963794 0: 77.53%, 1: 22.47%
CHEMBL1963795 0: 46.98%, 1: 53.02%
CHEMBL1963796 0: 69.76%, 1: 30.24%
CHEMBL1963797 0: 83.64%, 1: 16.36%
CHEMBL1963798 0: 89.45%, 1: 10.55%
CHEMBL1963799 0: 43.54%, 1: 56.46%
CHEMBL1963800 0: 64.88%, 1: 35.12%
CHEMBL1963801 0: 75.90%, 1: 24.10%
CHEMBL1963802 0: 61.41%, 1: 38.59%
CHEMBL1963803 0: 87.44%, 1: 12.56%
CHEMBL1963804 0: 65.90%, 1: 34.10%
CHEMBL1963805 0: 53.77%, 1: 46.23%
CHEMBL1963806 0: 41.19%, 1: 58.81%
CHEMBL1963807 0: 36.83%, 1: 63.17%
CHEMBL1963808 0: 68.82%, 1: 31.18%
CHEMBL1963809 0: 79.97%, 1: 20.03%
CHEMBL1963810 0: 43.69%, 1: 56.31%
CHEMBL1963811 0: 73.98%, 1: 26.02%
CHEMBL1963812 0: 53.08%, 1: 46.92%
CHEMBL1963813 0: 79.19%, 1: 20.81%
CHEMBL1963814 0: 51.10%, 1: 48.90%
CHEMBL1963815 0: 71.80%, 1: 28.20%
CHEMBL1963816 0: 89.64%, 1: 10.36%
CHEMBL1963817 0: 71.90%, 1: 28.10%
CHEMBL1963818 0: 55.13%, 1: 44.87%
CHEMBL1963819 0: 62.64%, 1: 37.36%
CHEMBL1963820 0: 84.46%, 1: 15.54%
CHEMBL1963821 0: 79.31%, 1: 20.69%
CHEMBL1963822 0: 90.91%, 1: 9.09%
CHEMBL1963823 0: 76.61%, 1: 23.39%
CHEMBL1963824 0: 39.57%, 1: 60.43%
CHEMBL1963825 0: 51.12%, 1: 48.88%
CHEMBL1963826 0: 59.25%, 1: 40.75%
CHEMBL1963827 0: 50.74%, 1: 49.26%
CHEMBL1963828 0: 80.86%, 1: 19.14%
CHEMBL1963829 0: 86.41%, 1: 13.59%
CHEMBL1963831 0: 52.56%, 1: 47.44%
CHEMBL1963832 0: 72.98%, 1: 27.02%
CHEMBL1963833 0: 87.13%, 1: 12.87%
CHEMBL1963834 0: 59.69%, 1: 40.31%
CHEMBL1963836 0: 85.18%, 1: 14.82%
CHEMBL1963837 0: 26.01%, 1: 73.99%
CHEMBL1963838 0: 75.83%, 1: 24.17%
CHEMBL1963846 0: 85.07%, 1: 14.93%
CHEMBL1963867 0: 73.13%, 1: 26.87%
CHEMBL1963893 0: 85.61%, 1: 14.39%
CHEMBL1963898 0: 75.62%, 1: 24.38%
CHEMBL1963907 0: 64.35%, 1: 35.65%
CHEMBL1963910 0: 10.50%, 1: 89.50%
CHEMBL1963915 0: 15.00%, 1: 85.00%
CHEMBL1963916 0: 40.48%, 1: 59.52%
CHEMBL1963918 0: 73.35%, 1: 26.65%
CHEMBL1963930 0: 3.64%, 1: 96.36%
CHEMBL1963933 0: 93.91%, 1: 6.09%
CHEMBL1963934 0: 3.64%, 1: 96.36%
CHEMBL1963937 0: 39.27%, 1: 60.73%
CHEMBL1963938 0: 65.85%, 1: 34.15%
CHEMBL1963940 0: 33.79%, 1: 66.21%
CHEMBL1963947 0: 6.25%, 1: 93.75%
CHEMBL1963966 0: 3.29%, 1: 96.71%
CHEMBL1963968 0: 51.52%, 1: 48.48%
CHEMBL1963969 0: 25.83%, 1: 74.17%
CHEMBL1963971 0: 10.20%, 1: 89.80%
CHEMBL1963974 0: 88.26%, 1: 11.74%
CHEMBL1963983 0: 76.39%, 1: 23.61%
CHEMBL1964000 0: 76.77%, 1: 23.23%
CHEMBL1964005 0: 6.67%, 1: 93.33%
CHEMBL1964010 0: 76.06%, 1: 23.94%
CHEMBL1964015 0: 73.77%, 1: 26.23%
CHEMBL1964022 0: 76.61%, 1: 23.39%
CHEMBL1964023 0: 73.75%, 1: 26.25%
CHEMBL1964081 0: 47.98%, 1: 52.02%
CHEMBL1964095 0: 65.41%, 1: 34.59%
CHEMBL1964096 0: 11.36%, 1: 88.64%
CHEMBL1964100 0: 79.62%, 1: 20.38%
CHEMBL1964101 0: 34.62%, 1: 65.38%
CHEMBL1964102 0: 71.54%, 1: 28.46%
CHEMBL1964103 0: 51.58%, 1: 48.42%
CHEMBL1964104 0: 63.88%, 1: 36.12%
CHEMBL1964105 0: 64.53%, 1: 35.47%
CHEMBL1964106 0: 57.96%, 1: 42.04%
CHEMBL1964108 0: 69.62%, 1: 30.38%
CHEMBL1964111 0: 73.41%, 1: 26.59%
CHEMBL1964112 0: 87.59%, 1: 12.41%
CHEMBL1964114 0: 61.11%, 1: 38.89%
CHEMBL1964115 0: 43.36%, 1: 56.64%
CHEMBL1964116 0: 70.55%, 1: 29.45%
CHEMBL1964117 0: 63.66%, 1: 36.34%
CHEMBL1964118 0: 60.33%, 1: 39.67%
CHEMBL1964119 0: 57.23%, 1: 42.77%
CHEMBL2028073 0: 76.99%, 1: 23.01%
CHEMBL2028074 0: 74.88%, 1: 25.12%
CHEMBL2028075 0: 56.46%, 1: 43.54%
CHEMBL2028076 0: 85.04%, 1: 14.96%
CHEMBL2028077 0: 96.19%, 1: 3.81%
CHEMBL2095143 0: 93.77%, 1: 6.23%
CHEMBL2098499 0: 74.45%, 1: 25.55%
CHEMBL2114715 0: 60.31%, 1: 39.69%
CHEMBL2114716 0: 12.27%, 1: 87.73%
CHEMBL2114719 0: 77.41%, 1: 22.59%
CHEMBL2114725 0: 51.74%, 1: 48.26%
CHEMBL2114727 0: 16.96%, 1: 83.04%
CHEMBL2114728 0: 9.82%, 1: 90.18%
CHEMBL2114737 0: 87.93%, 1: 12.07%
CHEMBL2114742 0: 33.90%, 1: 66.10%
CHEMBL2114748 0: 59.44%, 1: 40.56%
CHEMBL2114752 0: 15.02%, 1: 84.98%
CHEMBL2114753 0: 8.36%, 1: 91.64%
CHEMBL2114761 0: 78.40%, 1: 21.60%
CHEMBL2114764 0: 42.92%, 1: 57.08%
CHEMBL2114771 0: 64.02%, 1: 35.98%
CHEMBL2114791 0: 1.99%, 1: 98.01%
CHEMBL2114797 0: 42.41%, 1: 57.59%
CHEMBL2114811 0: 10.16%, 1: 89.84%
CHEMBL2114814 0: 44.98%, 1: 55.02%
CHEMBL2114816 0: 37.94%, 1: 62.06%
CHEMBL2114818 0: 4.15%, 1: 95.85%
CHEMBL2114820 0: 16.31%, 1: 83.69%
CHEMBL2114821 0: 18.77%, 1: 81.23%
CHEMBL2114823 0: 20.90%, 1: 79.10%
CHEMBL2114825 0: 18.62%, 1: 81.38%
CHEMBL2114827 0: 93.80%, 1: 6.20%
CHEMBL2114829 0: 25.17%, 1: 74.83%
CHEMBL2114830 0: 20.17%, 1: 79.83%
CHEMBL2114839 0: 28.69%, 1: 71.31%
CHEMBL2114842 0: 62.00%, 1: 38.00%
CHEMBL2114844 0: 32.59%, 1: 67.41%
CHEMBL2114847 0: 13.84%, 1: 86.16%
CHEMBL2114850 0: 8.62%, 1: 91.38%
CHEMBL2114852 0: 3.84%, 1: 96.16%
CHEMBL2114857 0: 50.00%, 1: 50.00%
CHEMBL2114858 0: 32.36%, 1: 67.64%
CHEMBL2114863 0: 33.55%, 1: 66.45%
CHEMBL2114865 0: 3.43%, 1: 96.57%
CHEMBL2114872 0: 9.51%, 1: 90.49%
CHEMBL2114874 0: 28.03%, 1: 71.97%
CHEMBL2114882 0: 92.25%, 1: 7.75%
CHEMBL2114896 0: 51.22%, 1: 48.78%
CHEMBL2114899 0: 87.76%, 1: 12.24%
CHEMBL2114909 0: 1.94%, 1: 98.06%
CHEMBL2114916 0: 42.50%, 1: 57.50%
CHEMBL2114926 0: 41.57%, 1: 58.43%
CHEMBL2114928 0: 86.06%, 1: 13.94%
CHEMBL2114930 0: 4.68%, 1: 95.32%
CHEMBL2114931 0: 97.44%, 1: 2.56%
CHEMBL2114932 0: 4.35%, 1: 95.65%
CHEMBL2354206 0: 93.86%, 1: 6.14%
CHEMBL2354207 0: 15.46%, 1: 84.54%
CHEMBL2354217 0: 20.69%, 1: 79.31%
CHEMBL2354227 0: 21.70%, 1: 78.30%
CHEMBL2354228 0: 59.88%, 1: 40.12%
CHEMBL2354248 0: 96.55%, 1: 3.45%
CHEMBL2354256 0: 7.85%, 1: 92.15%
CHEMBL2354269 0: 9.42%, 1: 90.58%
CHEMBL2354274 0: 5.26%, 1: 94.74%
CHEMBL2354276 0: 75.30%, 1: 24.70%
CHEMBL2354289 0: 34.38%, 1: 65.62%
CHEMBL2354292 0: 77.91%, 1: 22.09%
CHEMBL2354303 0: 86.76%, 1: 13.24%
CHEMBL2354305 0: 14.37%, 1: 85.63%
CHEMBL2354308 0: 9.09%, 1: 90.91%
CHEMBL2378059 0: 2.84%, 1: 97.16%
CHEMBL2449559 0: 17.37%, 1: 82.63%
CHEMBL3214794 0: 64.60%, 1: 35.40%
CHEMBL3214801 0: 28.46%, 1: 71.54%
CHEMBL3214812 0: 33.19%, 1: 66.81%
CHEMBL3214816 0: 25.43%, 1: 74.57%
CHEMBL3214851 0: 77.02%, 1: 22.98%
CHEMBL3214906 0: 61.72%, 1: 38.28%
CHEMBL3214907 0: 48.12%, 1: 51.88%
CHEMBL3214929 0: 44.25%, 1: 55.75%
CHEMBL3214930 0: 79.15%, 1: 20.85%
CHEMBL3214944 0: 32.70%, 1: 67.30%
CHEMBL3214958 0: 69.31%, 1: 30.69%
CHEMBL3214959 0: 31.09%, 1: 68.91%
CHEMBL3214970 0: 20.09%, 1: 79.91%
CHEMBL3214992 0: 27.76%, 1: 72.24%
CHEMBL3214993 0: 32.48%, 1: 67.52%
CHEMBL3214997 0: 27.27%, 1: 72.73%
CHEMBL3215006 0: 73.18%, 1: 26.82%
CHEMBL3215013 0: 10.05%, 1: 89.95%
CHEMBL3215025 0: 7.49%, 1: 92.51%
CHEMBL3215034 0: 39.87%, 1: 60.13%
CHEMBL3215078 0: 41.84%, 1: 58.16%
CHEMBL3215092 0: 57.98%, 1: 42.02%
CHEMBL3215096 0: 30.30%, 1: 69.70%
CHEMBL3215112 0: 7.48%, 1: 92.52%
CHEMBL3215116 0: 83.87%, 1: 16.13%
CHEMBL3215128 0: 14.05%, 1: 85.95%
CHEMBL3215154 0: 44.44%, 1: 55.56%
CHEMBL3215157 0: 5.96%, 1: 94.04%
CHEMBL3215158 0: 12.59%, 1: 87.41%
CHEMBL3215171 0: 97.98%, 1: 2.02%
CHEMBL3215176 0: 91.72%, 1: 8.28%
CHEMBL3215185 0: 91.95%, 1: 8.05%
CHEMBL3215187 0: 7.98%, 1: 92.02%
CHEMBL3215216 0: 15.79%, 1: 84.21%
CHEMBL3215220 0: 91.18%, 1: 8.82%
CHEMBL3215227 0: 32.66%, 1: 67.34%
CHEMBL3215228 0: 88.71%, 1: 11.29%
CHEMBL3215276 0: 7.46%, 1: 92.54%
CHEMBL3215277 0: 90.53%, 1: 9.47%
CHEMBL3215288 0: 27.05%, 1: 72.95%
CHEMBL829401 0: 10.53%, 1: 89.47%
CHEMBL830839 0: 3.07%, 1: 96.93%
CHEMBL830842 0: 10.53%, 1: 89.47%
CHEMBL914418 0: 14.93%, 1: 85.07%
CHEMBL918058 0: 93.33%, 1: 6.67%
Total scaffolds = 77 | train scaffolds = 66 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 133 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 99 | train scaffolds = 64 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan, 0.32608696,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan, 0.33333333,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 60 | train scaffolds = 48 | val scaffolds = 12 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.90566038, 0.78787879,
       0.94117647, 0.91304348, 0.95918367, 0.8974359 , 0.65853659,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 53, 33, 51, 46, 49, 39, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 1. , 1. , 1. , 1. , 0.6, 0.6, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 7, 4, 7, 6, 7, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 61 | train scaffolds = 42 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.84848485, 0.5625    ,
       0.87179487, 0.83333333, 0.97297297, 0.96153846, 0.71428571,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 33, 48, 39, 36, 37, 26, 28,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan,  0.,  1.,  0.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0.,  1., nan,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.66666667, 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 64 | train scaffolds = 42 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.92156863, 0.66666667,
       0.92307692, 0.89285714, 0.98305085, 0.9047619 , 0.625     ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 51, 39, 65, 56, 59, 42, 48,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 0. , 1. , 0.5, 1. , 1. , 0. , nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 65 | train scaffolds = 48 | val scaffolds = 17 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.91304348, 0.72222222,
       0.92857143, 0.9       , 0.98113208, 0.95      , 0.67391304,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 46, 36, 56, 60, 53, 40, 46,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0., nan,  0.,  0., nan,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 71 | train scaffolds = 49 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.95918367, 0.72972973,
       0.94915254, 0.96226415, 0.96969697, 0.93023256, 0.65306122,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 49, 37, 59, 53, 66, 43, 49,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 48 | train scaffolds = 26 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97435897, 0.80769231,
       1.        , 0.975     , 0.97674419, 0.91304348, 0.80487805,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 39, 26, 42, 40, 43, 46, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.6       , 0.33333333,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 5, 3, 5, 4, 5, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 51 | train scaffolds = 30 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97560976, 0.78571429,
       1.        , 0.95652174, 0.97959184, 0.95121951, 0.65384615,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 41, 28, 48, 46, 49, 41, 52,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 32 | train scaffolds = 21 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.63492063, 0.44444444,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 63, 45,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.8     , 0.640625,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 105,  64,   0,
         0,   0,   0,   0,   0,   0,   0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53846154, 0.47058824,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 39, 34,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.8 ,
       0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 31 | train scaffolds = 15 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53333333, 0.42      ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 45, 50,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.703125, 0.625   ,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 64, 72,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.47058824, 0.44736842,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34, 38,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.75,
       0.6 ,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0. , 0.5, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 245 | train scaffolds = 196 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 409 | train scaffolds = 339 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 98 | train scaffolds = 67 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.52173913,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 46,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.66666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.33333333,
              nan,        nan, 1.        ,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 94 | train scaffolds = 64 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.64705882,
              nan,        nan, 0.56410256,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       39,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
       1.        ,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       29,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 183 | train scaffolds = 146 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
Total scaffolds = 71 | train scaffolds = 46 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86363636,
              nan,        nan, 0.81818182,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.89473684,
       1.        ,        nan, 0.85      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 626 | train scaffolds = 501 | val scaffolds = 125 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 142 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 56 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
              nan,        nan, 0.77272727,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.9047619 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 446 | train scaffolds = 358 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 67 | train scaffolds = 44 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95      ,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 115 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 74 | train scaffolds = 52 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.9 ,  nan,  nan, 0.75,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.94736842,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0]))]
Total scaffolds = 31 | train scaffolds = 16 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 262 | train scaffolds = 209 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 119 | train scaffolds = 92 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 76 | train scaffolds = 50 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan, 0.  , 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 52 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 116 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 50 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 161 | train scaffolds = 128 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 82 | train scaffolds = 54 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 208 | train scaffolds = 164 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 160 | train scaffolds = 123 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 257 | train scaffolds = 189 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
Total scaffolds = 744 | train scaffolds = 602 | val scaffolds = 142 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 145 | train scaffolds = 110 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 85 | train scaffolds = 59 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.63636364,
              nan,        nan, 0.625     ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 53 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.74193548,
              nan,        nan, 0.68965517,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 232 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 175 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 240 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 88 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       0.5, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))]
Total scaffolds = 91 | train scaffolds = 59 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.63888889,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 36,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.78571429,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 222 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 283 | train scaffolds = 223 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 81 | train scaffolds = 53 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.67647059,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0]))]
Total scaffolds = 122 | train scaffolds = 97 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 752 | train scaffolds = 591 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 656 | train scaffolds = 506 | val scaffolds = 150 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 297 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 506 | train scaffolds = 408 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 97 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 404 | train scaffolds = 318 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 78 | train scaffolds = 52 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.79166667,
              nan,        nan, 0.73913043,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 24,  0,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , 1. , nan, 0.9, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
Total scaffolds = 87 | train scaffolds = 58 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.79166667,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       24,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 81 | train scaffolds = 50 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.9       ,
              nan,        nan, 0.77777778,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0,  0,
       11,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 115 | train scaffolds = 90 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 87 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 276 | train scaffolds = 220 | val scaffolds = 56 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 685 | train scaffolds = 555 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 159 | train scaffolds = 127 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 126 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 447 | train scaffolds = 355 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 208 | train scaffolds = 165 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 63 | train scaffolds = 47 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 272 | train scaffolds = 218 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 163 | train scaffolds = 129 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 95 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 86 | train scaffolds = 65 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.73913043,
              nan,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 23,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.92857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       14,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 83 | train scaffolds = 59 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([    nan,     nan,     nan,     nan,     nan,     nan,     nan,
           nan,     nan,     nan,     nan,     nan,     nan,     nan,
       0.6875 ,     nan,     nan, 0.65625,     nan,     nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 32,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 37 | train scaffolds = 24 | val scaffolds = 13 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 146 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 58 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.76923077,
              nan,        nan, 0.7037037 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  0,  0,
       27,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.84, 1.  ,  nan, 0.76,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 156 | train scaffolds = 119 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 426 | train scaffolds = 335 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 73 | train scaffolds = 44 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
       1.        ,        nan, 0.92307692,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       13,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76470588,
              nan,        nan, 0.82352941,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 17,  0,  0,
       17,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 111 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, nan, nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 644 | train scaffolds = 506 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 346 | train scaffolds = 268 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 206 | train scaffolds = 159 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.77419355,
              nan,        nan, 0.72413793,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 98 | train scaffolds = 76 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 233 | train scaffolds = 186 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 77 | train scaffolds = 51 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76666667,
              nan,        nan, 0.7       ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30,  0,  0,
       30,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.82608696,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 693 | train scaffolds = 558 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 98 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 177 | train scaffolds = 138 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 112 | train scaffolds = 90 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 76 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 262 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 761 | train scaffolds = 597 | val scaffolds = 164 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 82 | train scaffolds = 57 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 344 | train scaffolds = 272 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
Total scaffolds = 297 | train scaffolds = 232 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 124 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 726 | train scaffolds = 579 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 103 | train scaffolds = 79 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 577 | train scaffolds = 469 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 70 | train scaffolds = 48 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 151 | train scaffolds = 120 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 136 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 57 | train scaffolds = 29 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 717 | train scaffolds = 571 | val scaffolds = 146 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 89 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 142 | train scaffolds = 112 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 130 | train scaffolds = 103 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 103 | train scaffolds = 84 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 151 | train scaffolds = 118 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 63 | train scaffolds = 41 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 223 | train scaffolds = 177 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 214 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 155 | train scaffolds = 125 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 169 | train scaffolds = 131 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 120 | train scaffolds = 98 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 236 | train scaffolds = 181 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan,        nan,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 464 | train scaffolds = 373 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 293 | train scaffolds = 238 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 163 | train scaffolds = 127 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 545 | train scaffolds = 441 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 156 | train scaffolds = 126 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 118 | train scaffolds = 98 | val scaffolds = 20 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 150 | train scaffolds = 121 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 196 | train scaffolds = 170 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 209 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 80 | train scaffolds = 65 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 152 | train scaffolds = 124 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 452 | train scaffolds = 366 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 102 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 219 | train scaffolds = 164 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 132 | train scaffolds = 105 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 399 | train scaffolds = 320 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 111 | train scaffolds = 90 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 550 | train scaffolds = 434 | val scaffolds = 116 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 216 | train scaffolds = 172 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 257 | train scaffolds = 200 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 318 | train scaffolds = 263 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 84 | train scaffolds = 61 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 407 | train scaffolds = 320 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 338 | train scaffolds = 268 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 279 | train scaffolds = 218 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 258 | train scaffolds = 196 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 154 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 346 | train scaffolds = 279 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 174 | train scaffolds = 141 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 327 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 229 | train scaffolds = 181 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 224 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 196 | train scaffolds = 156 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 214 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 159 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 780 | train scaffolds = 619 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 195 | train scaffolds = 158 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 191 | train scaffolds = 148 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 137 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 413 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 405 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 400 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 404 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 406 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 481 | train scaffolds = 397 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 473 | train scaffolds = 392 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 402 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.66666667,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 481 | train scaffolds = 395 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 405 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 484 | train scaffolds = 402 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 414 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 398 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 401 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 394 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 405 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 406 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 365 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 409 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 360 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 404 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 400 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 401 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 407 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 402 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 403 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 485 | train scaffolds = 373 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 405 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 402 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 400 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 408 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 411 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 409 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 405 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 409 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 403 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 479 | train scaffolds = 362 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 408 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 405 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 375 | val scaffolds = 113 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 396 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 391 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 399 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 401 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 396 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 359 | train scaffolds = 282 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 253 | train scaffolds = 192 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 440 | train scaffolds = 371 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 294 | train scaffolds = 237 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 588 | train scaffolds = 455 | val scaffolds = 133 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 526 | train scaffolds = 421 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 252 | train scaffolds = 202 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 502 | train scaffolds = 385 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 424 | train scaffolds = 332 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 311 | train scaffolds = 249 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 349 | train scaffolds = 273 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 451 | train scaffolds = 369 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 542 | train scaffolds = 445 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 309 | train scaffolds = 246 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 437 | train scaffolds = 345 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 467 | train scaffolds = 374 | val scaffolds = 93 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 465 | train scaffolds = 374 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 433 | train scaffolds = 363 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 415 | train scaffolds = 328 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 353 | train scaffolds = 277 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 199 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 301 | train scaffolds = 234 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 477 | train scaffolds = 388 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 522 | train scaffolds = 436 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 303 | train scaffolds = 228 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 337 | train scaffolds = 272 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 532 | train scaffolds = 428 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 536 | train scaffolds = 434 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 400 | train scaffolds = 327 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 254 | train scaffolds = 202 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 461 | train scaffolds = 384 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 465 | train scaffolds = 368 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 551 | train scaffolds = 448 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 564 | train scaffolds = 454 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 308 | train scaffolds = 230 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 349 | train scaffolds = 276 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 432 | train scaffolds = 340 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 301 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 432 | train scaffolds = 348 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 569 | train scaffolds = 445 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 434 | train scaffolds = 357 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 364 | train scaffolds = 298 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 445 | train scaffolds = 370 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 582 | train scaffolds = 460 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 508 | train scaffolds = 382 | val scaffolds = 126 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 340 | train scaffolds = 251 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 227 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 278 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 539 | train scaffolds = 439 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 359 | train scaffolds = 293 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 388 | train scaffolds = 316 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 495 | train scaffolds = 392 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 524 | train scaffolds = 429 | val scaffolds = 95 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 302 | train scaffolds = 224 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 356 | train scaffolds = 278 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 376 | train scaffolds = 296 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 441 | train scaffolds = 352 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 529 | train scaffolds = 427 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 214 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 428 | train scaffolds = 355 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 249 | train scaffolds = 196 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 422 | train scaffolds = 326 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 362 | train scaffolds = 287 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 302 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 278 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 439 | train scaffolds = 343 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 379 | train scaffolds = 293 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 287 | train scaffolds = 233 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 568 | train scaffolds = 453 | val scaffolds = 115 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 523 | train scaffolds = 401 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 314 | train scaffolds = 232 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 543 | train scaffolds = 444 | val scaffolds = 99 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 209 | train scaffolds = 163 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 234 | train scaffolds = 201 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 230 | train scaffolds = 182 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 538 | train scaffolds = 433 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 347 | train scaffolds = 277 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 594 | train scaffolds = 466 | val scaffolds = 128 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 429 | train scaffolds = 353 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 439 | train scaffolds = 352 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 362 | train scaffolds = 282 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 256 | train scaffolds = 205 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 589 | train scaffolds = 471 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 404 | train scaffolds = 333 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 379 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 356 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 339 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 370 | train scaffolds = 287 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 358 | train scaffolds = 281 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 331 | train scaffolds = 261 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 293 | train scaffolds = 239 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 216 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 454 | train scaffolds = 372 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 436 | train scaffolds = 361 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 351 | train scaffolds = 261 | val scaffolds = 90 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 217 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 330 | train scaffolds = 264 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 186 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 250 | train scaffolds = 196 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 594 | train scaffolds = 474 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 353 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 382 | train scaffolds = 310 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 259 | train scaffolds = 207 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 398 | train scaffolds = 315 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 132 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 588 | train scaffolds = 457 | val scaffolds = 131 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 285 | train scaffolds = 237 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 410 | train scaffolds = 325 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 460 | train scaffolds = 350 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 367 | train scaffolds = 289 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 426 | train scaffolds = 355 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 369 | train scaffolds = 288 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 408 | train scaffolds = 343 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 550 | train scaffolds = 439 | val scaffolds = 111 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 288 | train scaffolds = 242 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 273 | train scaffolds = 220 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 356 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 369 | train scaffolds = 283 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 317 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 442 | train scaffolds = 365 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 267 | train scaffolds = 209 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 544 | train scaffolds = 442 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 395 | train scaffolds = 322 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 376 | train scaffolds = 301 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 422 | train scaffolds = 328 | val scaffolds = 94 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 603 | train scaffolds = 491 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 442 | train scaffolds = 360 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 155 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 171 | train scaffolds = 136 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 154 | train scaffolds = 127 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 571 | train scaffolds = 423 | val scaffolds = 148 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 162 | train scaffolds = 124 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 242 | train scaffolds = 188 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 535 | train scaffolds = 428 | val scaffolds = 107 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 110 | train scaffolds = 85 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 289 | train scaffolds = 228 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 630 | train scaffolds = 498 | val scaffolds = 132 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 596 | train scaffolds = 466 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 358 | train scaffolds = 276 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 586 | train scaffolds = 464 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 279 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 560 | train scaffolds = 451 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 436 | train scaffolds = 369 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 189 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 562 | train scaffolds = 426 | val scaffolds = 136 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 368 | train scaffolds = 283 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 241 | train scaffolds = 203 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 233 | train scaffolds = 191 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 242 | train scaffolds = 189 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 198 | train scaffolds = 152 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 265 | train scaffolds = 218 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 194 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 324 | train scaffolds = 247 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 231 | train scaffolds = 174 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 300 | train scaffolds = 229 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 450 | train scaffolds = 359 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 141 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 169 | train scaffolds = 135 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 172 | train scaffolds = 139 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 184 | train scaffolds = 145 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 98 | train scaffolds = 79 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 780 | train scaffolds = 623 | val scaffolds = 157 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 199 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 318 | train scaffolds = 255 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 157 | train scaffolds = 124 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 294 | train scaffolds = 236 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 214 | train scaffolds = 175 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 143 | train scaffolds = 111 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 302 | train scaffolds = 253 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 408 | train scaffolds = 319 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 108 | train scaffolds = 85 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 152 | train scaffolds = 127 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 135 | train scaffolds = 80 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.65517241,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 29,  0,  0,
       27,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.82352941,
       1.        ,        nan, 0.8125    ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17,  1,  0,
       16,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 1.  ,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 115 | train scaffolds = 87 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 314 | train scaffolds = 255 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 121 | train scaffolds = 93 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 260 | train scaffolds = 208 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 351 | train scaffolds = 278 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 333 | train scaffolds = 270 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 306 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 625 | train scaffolds = 487 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 128 | train scaffolds = 96 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 789 | train scaffolds = 628 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 194 | train scaffolds = 154 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 144 | train scaffolds = 110 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 116 | train scaffolds = 86 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 168 | train scaffolds = 137 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 298 | train scaffolds = 228 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 214 | train scaffolds = 168 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 179 | train scaffolds = 143 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 570 | train scaffolds = 452 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 56 | train scaffolds = 50 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 80 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 104 | train scaffolds = 83 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
Total scaffolds = 197 | train scaffolds = 158 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 612 | train scaffolds = 492 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 113 | train scaffolds = 91 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 498 | train scaffolds = 397 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 203 | train scaffolds = 160 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 176 | train scaffolds = 143 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 353 | train scaffolds = 274 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 443 | train scaffolds = 352 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 396 | train scaffolds = 310 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 193 | train scaffolds = 153 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 149 | train scaffolds = 122 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 280 | train scaffolds = 221 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 417 | train scaffolds = 336 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 104 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 237 | train scaffolds = 184 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 130 | train scaffolds = 94 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 6, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 162 | train scaffolds = 132 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
Total scaffolds = 193 | train scaffolds = 154 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 384 | train scaffolds = 309 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 124 | train scaffolds = 99 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 249 | train scaffolds = 184 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 1. , nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 282 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 105 | train scaffolds = 81 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 207 | train scaffolds = 165 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 174 | train scaffolds = 129 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 91 | train scaffolds = 73 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 377 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 350 | train scaffolds = 276 | val scaffolds = 74 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 41 | train scaffolds = 30 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 75 | train scaffolds = 57 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 109 | train scaffolds = 85 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 622 | train scaffolds = 493 | val scaffolds = 129 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 487 | train scaffolds = 377 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 356 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 486 | train scaffolds = 403 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 406 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 480 | train scaffolds = 398 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 406 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 488 | train scaffolds = 404 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 435 | train scaffolds = 355 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 431 | train scaffolds = 334 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 428 | train scaffolds = 342 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 365 | train scaffolds = 279 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 228 | train scaffolds = 183 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 40 | test scaffolds = 51
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 147 | train scaffolds = 125 | val scaffolds = 5 | test scaffolds = 17
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 364 | train scaffolds = 293 | val scaffolds = 32 | test scaffolds = 39
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 117 | train scaffolds = 91 | val scaffolds = 15 | test scaffolds = 11
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 104 | train scaffolds = 80 | val scaffolds = 12 | test scaffolds = 12
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 32 | test scaffolds = 40
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 117 | train scaffolds = 94 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 251 | train scaffolds = 198 | val scaffolds = 28 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 20 | test scaffolds = 20
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 366 | train scaffolds = 294 | val scaffolds = 34 | test scaffolds = 38
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 490 | train scaffolds = 364 | val scaffolds = 59 | test scaffolds = 67
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
        1.,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 376 | val scaffolds = 53 | test scaffolds = 62
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 491 | train scaffolds = 375 | val scaffolds = 57 | test scaffolds = 59
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
Total scaffolds = 489 | train scaffolds = 362 | val scaffolds = 63 | test scaffolds = 64
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 491 | train scaffolds = 366 | val scaffolds = 68 | test scaffolds = 57
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 482 | train scaffolds = 354 | val scaffolds = 59 | test scaffolds = 69
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 410 | train scaffolds = 346 | val scaffolds = 31 | test scaffolds = 33
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 546 | train scaffolds = 443 | val scaffolds = 45 | test scaffolds = 58
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 134 | train scaffolds = 111 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 25 | test scaffolds = 24
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 240 | train scaffolds = 197 | val scaffolds = 18 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 122 | train scaffolds = 95 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 139 | train scaffolds = 105 | val scaffolds = 19 | test scaffolds = 15
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1.,  1., nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 6, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.6       ,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 21 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Total scaffolds = 69 | train scaffolds = 53 | val scaffolds = 7 | test scaffolds = 9
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Building model 
Number of parameters = 385,101
ANILMoleculeModel(
  (molecule_model): MoleculeModel(
    (sigmoid): Sigmoid()
    (encoder): MPN(
      (encoder): MPNEncoder(
        (dropout_layer): Dropout(p=0.2, inplace=False)
        (act_func): ReLU()
        (W_i): Linear(in_features=147, out_features=300, bias=False)
        (W_h): Linear(in_features=300, out_features=300, bias=False)
        (W_o): Linear(in_features=433, out_features=300, bias=True)
      )
    )
    (ffn): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=300, out_features=400, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=400, out_features=1, bias=True)
    )
  )
  (gnn_featurizer): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.2, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (classifier): MAML(
    (module): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=300, out_features=400, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.2, inplace=False)
      (4): Linear(in_features=400, out_features=1, bias=True)
    )
  )
)
Moving maml model to cuda
Epoch 0
Meta loss on this task batch = 1.4667e+00, Meta loss averaged over last 500 steps = 1.4667e+00, PNorm = 51.0122, GNorm = 1.0000
Meta loss on this task batch = 1.0987e+00, Meta loss averaged over last 500 steps = 1.2827e+00, PNorm = 51.0157, GNorm = 1.0000
Meta loss on this task batch = 5.7957e-01, Meta loss averaged over last 500 steps = 1.0483e+00, PNorm = 51.0217, GNorm = 0.4557
Meta loss on this task batch = 5.7033e-01, Meta loss averaged over last 500 steps = 9.2884e-01, PNorm = 51.0295, GNorm = 0.9950
Meta loss on this task batch = 5.7706e-01, Meta loss averaged over last 500 steps = 8.5848e-01, PNorm = 51.0381, GNorm = 0.3731
Meta loss on this task batch = 5.3099e-01, Meta loss averaged over last 500 steps = 8.0390e-01, PNorm = 51.0466, GNorm = 0.3507
Meta loss on this task batch = 4.6550e-01, Meta loss averaged over last 500 steps = 7.5556e-01, PNorm = 51.0552, GNorm = 0.2806
Meta loss on this task batch = 5.9146e-01, Meta loss averaged over last 500 steps = 7.3505e-01, PNorm = 51.0629, GNorm = 0.7242
Meta loss on this task batch = 4.8425e-01, Meta loss averaged over last 500 steps = 7.0718e-01, PNorm = 51.0715, GNorm = 0.1661
Meta loss on this task batch = 4.9720e-01, Meta loss averaged over last 500 steps = 6.8618e-01, PNorm = 51.0810, GNorm = 0.2606
Meta loss on this task batch = 4.6832e-01, Meta loss averaged over last 500 steps = 6.6638e-01, PNorm = 51.0910, GNorm = 0.2443
Meta loss on this task batch = 4.8374e-01, Meta loss averaged over last 500 steps = 6.5116e-01, PNorm = 51.1014, GNorm = 0.2141
Meta loss on this task batch = 4.6731e-01, Meta loss averaged over last 500 steps = 6.3701e-01, PNorm = 51.1122, GNorm = 0.2052
Meta loss on this task batch = 3.9920e-01, Meta loss averaged over last 500 steps = 6.2003e-01, PNorm = 51.1232, GNorm = 0.1890
Meta loss on this task batch = 4.5700e-01, Meta loss averaged over last 500 steps = 6.0916e-01, PNorm = 51.1327, GNorm = 0.4120
Meta loss on this task batch = 4.5964e-01, Meta loss averaged over last 500 steps = 5.9981e-01, PNorm = 51.1419, GNorm = 0.1880
Meta loss on this task batch = 5.9636e-01, Meta loss averaged over last 500 steps = 5.9961e-01, PNorm = 51.1485, GNorm = 0.7546
Meta loss on this task batch = 5.1776e-01, Meta loss averaged over last 500 steps = 5.9506e-01, PNorm = 51.1546, GNorm = 0.2693
Meta loss on this task batch = 5.3612e-01, Meta loss averaged over last 500 steps = 5.9196e-01, PNorm = 51.1604, GNorm = 0.1687
Took 143.01885318756104 seconds to complete one epoch of meta training
Took 152.6210482120514 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.302460
Found better MAML checkpoint after meta validation, saving now
Epoch 1
Meta loss on this task batch = 5.0859e-01, Meta loss averaged over last 500 steps = 5.8779e-01, PNorm = 51.1664, GNorm = 0.1844
Meta loss on this task batch = 5.1496e-01, Meta loss averaged over last 500 steps = 5.8432e-01, PNorm = 51.1729, GNorm = 0.1899
Meta loss on this task batch = 5.0865e-01, Meta loss averaged over last 500 steps = 5.8088e-01, PNorm = 51.1796, GNorm = 0.2217
Meta loss on this task batch = 4.6020e-01, Meta loss averaged over last 500 steps = 5.7564e-01, PNorm = 51.1868, GNorm = 0.2843
Meta loss on this task batch = 5.0427e-01, Meta loss averaged over last 500 steps = 5.7266e-01, PNorm = 51.1943, GNorm = 0.1524
Meta loss on this task batch = 5.0751e-01, Meta loss averaged over last 500 steps = 5.7006e-01, PNorm = 51.2017, GNorm = 0.1199
Meta loss on this task batch = 5.0634e-01, Meta loss averaged over last 500 steps = 5.6761e-01, PNorm = 51.2087, GNorm = 0.1388
Meta loss on this task batch = 5.1493e-01, Meta loss averaged over last 500 steps = 5.6566e-01, PNorm = 51.2153, GNorm = 0.1438
Meta loss on this task batch = 4.3393e-01, Meta loss averaged over last 500 steps = 5.6095e-01, PNorm = 51.2214, GNorm = 0.1578
Meta loss on this task batch = 4.9214e-01, Meta loss averaged over last 500 steps = 5.5858e-01, PNorm = 51.2270, GNorm = 0.1924
Meta loss on this task batch = 4.3252e-01, Meta loss averaged over last 500 steps = 5.5438e-01, PNorm = 51.2326, GNorm = 0.1070
Meta loss on this task batch = 4.8351e-01, Meta loss averaged over last 500 steps = 5.5209e-01, PNorm = 51.2378, GNorm = 0.1462
Meta loss on this task batch = 5.2242e-01, Meta loss averaged over last 500 steps = 5.5116e-01, PNorm = 51.2422, GNorm = 0.1907
Meta loss on this task batch = 4.5635e-01, Meta loss averaged over last 500 steps = 5.4829e-01, PNorm = 51.2471, GNorm = 0.1181
Meta loss on this task batch = 4.8158e-01, Meta loss averaged over last 500 steps = 5.4633e-01, PNorm = 51.2516, GNorm = 0.1271
Meta loss on this task batch = 5.1936e-01, Meta loss averaged over last 500 steps = 5.4556e-01, PNorm = 51.2560, GNorm = 0.1186
Meta loss on this task batch = 4.9531e-01, Meta loss averaged over last 500 steps = 5.4416e-01, PNorm = 51.2604, GNorm = 0.1131
Meta loss on this task batch = 4.2773e-01, Meta loss averaged over last 500 steps = 5.4102e-01, PNorm = 51.2653, GNorm = 0.1495
Meta loss on this task batch = 5.1107e-01, Meta loss averaged over last 500 steps = 5.4023e-01, PNorm = 51.2697, GNorm = 0.1331
Took 129.80234050750732 seconds to complete one epoch of meta training
Took 139.91844058036804 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.307584
Found better MAML checkpoint after meta validation, saving now
Epoch 2
Meta loss on this task batch = 4.3920e-01, Meta loss averaged over last 500 steps = 5.3764e-01, PNorm = 51.2744, GNorm = 0.1022
Meta loss on this task batch = 4.7291e-01, Meta loss averaged over last 500 steps = 5.3602e-01, PNorm = 51.2794, GNorm = 0.1168
Meta loss on this task batch = 4.7137e-01, Meta loss averaged over last 500 steps = 5.3444e-01, PNorm = 51.2843, GNorm = 0.1285
Meta loss on this task batch = 5.0046e-01, Meta loss averaged over last 500 steps = 5.3363e-01, PNorm = 51.2882, GNorm = 0.1671
Meta loss on this task batch = 5.1203e-01, Meta loss averaged over last 500 steps = 5.3313e-01, PNorm = 51.2911, GNorm = 0.1565
Meta loss on this task batch = 4.3838e-01, Meta loss averaged over last 500 steps = 5.3098e-01, PNorm = 51.2942, GNorm = 0.1084
Meta loss on this task batch = 4.3465e-01, Meta loss averaged over last 500 steps = 5.2884e-01, PNorm = 51.2971, GNorm = 0.0983
Meta loss on this task batch = 4.9689e-01, Meta loss averaged over last 500 steps = 5.2814e-01, PNorm = 51.2997, GNorm = 0.1384
Meta loss on this task batch = 4.8086e-01, Meta loss averaged over last 500 steps = 5.2714e-01, PNorm = 51.3022, GNorm = 0.1104
Meta loss on this task batch = 5.2835e-01, Meta loss averaged over last 500 steps = 5.2716e-01, PNorm = 51.3047, GNorm = 0.1124
Meta loss on this task batch = 5.4181e-01, Meta loss averaged over last 500 steps = 5.2746e-01, PNorm = 51.3072, GNorm = 0.1092
Meta loss on this task batch = 4.9347e-01, Meta loss averaged over last 500 steps = 5.2678e-01, PNorm = 51.3097, GNorm = 0.1193
Meta loss on this task batch = 4.9987e-01, Meta loss averaged over last 500 steps = 5.2625e-01, PNorm = 51.3123, GNorm = 0.1111
Meta loss on this task batch = 4.7912e-01, Meta loss averaged over last 500 steps = 5.2535e-01, PNorm = 51.3148, GNorm = 0.1011
Meta loss on this task batch = 5.0781e-01, Meta loss averaged over last 500 steps = 5.2502e-01, PNorm = 51.3166, GNorm = 0.1029
Meta loss on this task batch = 4.9723e-01, Meta loss averaged over last 500 steps = 5.2450e-01, PNorm = 51.3186, GNorm = 0.0921
Meta loss on this task batch = 4.5556e-01, Meta loss averaged over last 500 steps = 5.2325e-01, PNorm = 51.3209, GNorm = 0.0890
Meta loss on this task batch = 4.7550e-01, Meta loss averaged over last 500 steps = 5.2239e-01, PNorm = 51.3238, GNorm = 0.0990
Meta loss on this task batch = 5.2766e-01, Meta loss averaged over last 500 steps = 5.2249e-01, PNorm = 51.3267, GNorm = 0.1097
Took 134.76122069358826 seconds to complete one epoch of meta training
Took 144.45053553581238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.326074
Found better MAML checkpoint after meta validation, saving now
Epoch 3
Meta loss on this task batch = 4.9445e-01, Meta loss averaged over last 500 steps = 5.2200e-01, PNorm = 51.3298, GNorm = 0.0972
Meta loss on this task batch = 4.8959e-01, Meta loss averaged over last 500 steps = 5.2145e-01, PNorm = 51.3328, GNorm = 0.0942
Meta loss on this task batch = 4.9082e-01, Meta loss averaged over last 500 steps = 5.2094e-01, PNorm = 51.3358, GNorm = 0.0951
Meta loss on this task batch = 4.7836e-01, Meta loss averaged over last 500 steps = 5.2025e-01, PNorm = 51.3390, GNorm = 0.0910
Meta loss on this task batch = 4.8393e-01, Meta loss averaged over last 500 steps = 5.1966e-01, PNorm = 51.3428, GNorm = 0.1011
Meta loss on this task batch = 4.9035e-01, Meta loss averaged over last 500 steps = 5.1919e-01, PNorm = 51.3464, GNorm = 0.0984
Meta loss on this task batch = 5.4213e-01, Meta loss averaged over last 500 steps = 5.1955e-01, PNorm = 51.3494, GNorm = 0.1438
Meta loss on this task batch = 4.7842e-01, Meta loss averaged over last 500 steps = 5.1892e-01, PNorm = 51.3528, GNorm = 0.1094
Meta loss on this task batch = 4.1426e-01, Meta loss averaged over last 500 steps = 5.1733e-01, PNorm = 51.3573, GNorm = 0.1177
Meta loss on this task batch = 4.6336e-01, Meta loss averaged over last 500 steps = 5.1653e-01, PNorm = 51.3610, GNorm = 0.1124
Meta loss on this task batch = 4.4608e-01, Meta loss averaged over last 500 steps = 5.1549e-01, PNorm = 51.3646, GNorm = 0.0933
Meta loss on this task batch = 5.7267e-01, Meta loss averaged over last 500 steps = 5.1632e-01, PNorm = 51.3673, GNorm = 0.1460
Meta loss on this task batch = 4.2579e-01, Meta loss averaged over last 500 steps = 5.1503e-01, PNorm = 51.3698, GNorm = 0.0960
Meta loss on this task batch = 4.8167e-01, Meta loss averaged over last 500 steps = 5.1456e-01, PNorm = 51.3723, GNorm = 0.1180
Meta loss on this task batch = 3.8085e-01, Meta loss averaged over last 500 steps = 5.1270e-01, PNorm = 51.3755, GNorm = 0.1232
Meta loss on this task batch = 4.4304e-01, Meta loss averaged over last 500 steps = 5.1175e-01, PNorm = 51.3797, GNorm = 0.1162
Meta loss on this task batch = 5.1163e-01, Meta loss averaged over last 500 steps = 5.1175e-01, PNorm = 51.3837, GNorm = 0.0881
Meta loss on this task batch = 5.6237e-01, Meta loss averaged over last 500 steps = 5.1242e-01, PNorm = 51.3874, GNorm = 0.1212
Meta loss on this task batch = 6.1131e-01, Meta loss averaged over last 500 steps = 5.1372e-01, PNorm = 51.3894, GNorm = 0.2015
Took 128.270033121109 seconds to complete one epoch of meta training
Took 137.69122076034546 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.312305
Epoch 4
Meta loss on this task batch = 4.6861e-01, Meta loss averaged over last 500 steps = 5.1314e-01, PNorm = 51.3920, GNorm = 0.0966
Meta loss on this task batch = 4.8804e-01, Meta loss averaged over last 500 steps = 5.1281e-01, PNorm = 51.3955, GNorm = 0.1025
Meta loss on this task batch = 4.8124e-01, Meta loss averaged over last 500 steps = 5.1241e-01, PNorm = 51.3996, GNorm = 0.0992
Meta loss on this task batch = 4.6270e-01, Meta loss averaged over last 500 steps = 5.1179e-01, PNorm = 51.4041, GNorm = 0.1037
Meta loss on this task batch = 4.9462e-01, Meta loss averaged over last 500 steps = 5.1158e-01, PNorm = 51.4080, GNorm = 0.1034
Meta loss on this task batch = 4.4326e-01, Meta loss averaged over last 500 steps = 5.1075e-01, PNorm = 51.4115, GNorm = 0.1125
Meta loss on this task batch = 4.4352e-01, Meta loss averaged over last 500 steps = 5.0994e-01, PNorm = 51.4150, GNorm = 0.0882
Meta loss on this task batch = 4.6956e-01, Meta loss averaged over last 500 steps = 5.0946e-01, PNorm = 51.4183, GNorm = 0.0966
Meta loss on this task batch = 4.1009e-01, Meta loss averaged over last 500 steps = 5.0829e-01, PNorm = 51.4224, GNorm = 0.0901
Meta loss on this task batch = 4.3889e-01, Meta loss averaged over last 500 steps = 5.0748e-01, PNorm = 51.4256, GNorm = 0.0934
Meta loss on this task batch = 4.5911e-01, Meta loss averaged over last 500 steps = 5.0693e-01, PNorm = 51.4289, GNorm = 0.1020
Meta loss on this task batch = 4.6919e-01, Meta loss averaged over last 500 steps = 5.0650e-01, PNorm = 51.4311, GNorm = 0.1309
Meta loss on this task batch = 4.3410e-01, Meta loss averaged over last 500 steps = 5.0568e-01, PNorm = 51.4332, GNorm = 0.0958
Meta loss on this task batch = 5.5113e-01, Meta loss averaged over last 500 steps = 5.0619e-01, PNorm = 51.4348, GNorm = 0.1355
Meta loss on this task batch = 5.5797e-01, Meta loss averaged over last 500 steps = 5.0676e-01, PNorm = 51.4357, GNorm = 0.1499
Meta loss on this task batch = 5.6134e-01, Meta loss averaged over last 500 steps = 5.0735e-01, PNorm = 51.4369, GNorm = 0.1142
Meta loss on this task batch = 4.8094e-01, Meta loss averaged over last 500 steps = 5.0707e-01, PNorm = 51.4384, GNorm = 0.0927
Meta loss on this task batch = 5.1826e-01, Meta loss averaged over last 500 steps = 5.0719e-01, PNorm = 51.4412, GNorm = 0.1487
Meta loss on this task batch = 5.3832e-01, Meta loss averaged over last 500 steps = 5.0751e-01, PNorm = 51.4445, GNorm = 0.0951
Took 129.54787874221802 seconds to complete one epoch of meta training
Took 137.922589302063 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.314012
Epoch 5
Meta loss on this task batch = 4.8033e-01, Meta loss averaged over last 500 steps = 5.0723e-01, PNorm = 51.4489, GNorm = 0.0973
Meta loss on this task batch = 5.0955e-01, Meta loss averaged over last 500 steps = 5.0725e-01, PNorm = 51.4542, GNorm = 0.1043
Meta loss on this task batch = 5.1164e-01, Meta loss averaged over last 500 steps = 5.0730e-01, PNorm = 51.4595, GNorm = 0.0883
Meta loss on this task batch = 4.9934e-01, Meta loss averaged over last 500 steps = 5.0722e-01, PNorm = 51.4647, GNorm = 0.0904
Meta loss on this task batch = 4.7314e-01, Meta loss averaged over last 500 steps = 5.0688e-01, PNorm = 51.4701, GNorm = 0.0900
Meta loss on this task batch = 5.0621e-01, Meta loss averaged over last 500 steps = 5.0687e-01, PNorm = 51.4754, GNorm = 0.0844
Meta loss on this task batch = 4.6157e-01, Meta loss averaged over last 500 steps = 5.0643e-01, PNorm = 51.4800, GNorm = 0.0875
Meta loss on this task batch = 4.6755e-01, Meta loss averaged over last 500 steps = 5.0605e-01, PNorm = 51.4831, GNorm = 0.1219
Meta loss on this task batch = 5.0205e-01, Meta loss averaged over last 500 steps = 5.0601e-01, PNorm = 51.4853, GNorm = 0.1324
Meta loss on this task batch = 4.6933e-01, Meta loss averaged over last 500 steps = 5.0566e-01, PNorm = 51.4863, GNorm = 0.1109
Meta loss on this task batch = 5.5033e-01, Meta loss averaged over last 500 steps = 5.0608e-01, PNorm = 51.4863, GNorm = 0.1662
Meta loss on this task batch = 5.2596e-01, Meta loss averaged over last 500 steps = 5.0627e-01, PNorm = 51.4858, GNorm = 0.1273
Meta loss on this task batch = 5.0354e-01, Meta loss averaged over last 500 steps = 5.0624e-01, PNorm = 51.4865, GNorm = 0.0936
Meta loss on this task batch = 4.1015e-01, Meta loss averaged over last 500 steps = 5.0536e-01, PNorm = 51.4888, GNorm = 0.1668
Meta loss on this task batch = 4.9976e-01, Meta loss averaged over last 500 steps = 5.0531e-01, PNorm = 51.4921, GNorm = 0.1298
Meta loss on this task batch = 3.8054e-01, Meta loss averaged over last 500 steps = 5.0419e-01, PNorm = 51.4975, GNorm = 0.1876
Meta loss on this task batch = 4.7845e-01, Meta loss averaged over last 500 steps = 5.0396e-01, PNorm = 51.5028, GNorm = 0.0775
Meta loss on this task batch = 5.1320e-01, Meta loss averaged over last 500 steps = 5.0404e-01, PNorm = 51.5078, GNorm = 0.1061
Meta loss on this task batch = 4.2755e-01, Meta loss averaged over last 500 steps = 5.0337e-01, PNorm = 51.5132, GNorm = 0.1141
Took 131.36914539337158 seconds to complete one epoch of meta training
Took 139.79994344711304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.332266
Found better MAML checkpoint after meta validation, saving now
Epoch 6
Meta loss on this task batch = 4.7869e-01, Meta loss averaged over last 500 steps = 5.0315e-01, PNorm = 51.5179, GNorm = 0.1343
Meta loss on this task batch = 4.9700e-01, Meta loss averaged over last 500 steps = 5.0310e-01, PNorm = 51.5221, GNorm = 0.1163
Meta loss on this task batch = 4.7958e-01, Meta loss averaged over last 500 steps = 5.0290e-01, PNorm = 51.5232, GNorm = 0.2828
Meta loss on this task batch = 4.7897e-01, Meta loss averaged over last 500 steps = 5.0270e-01, PNorm = 51.5234, GNorm = 0.1400
Meta loss on this task batch = 5.4259e-01, Meta loss averaged over last 500 steps = 5.0303e-01, PNorm = 51.5236, GNorm = 0.1399
Meta loss on this task batch = 4.5798e-01, Meta loss averaged over last 500 steps = 5.0266e-01, PNorm = 51.5243, GNorm = 0.0875
Meta loss on this task batch = 4.5514e-01, Meta loss averaged over last 500 steps = 5.0226e-01, PNorm = 51.5263, GNorm = 0.0983
Meta loss on this task batch = 4.8833e-01, Meta loss averaged over last 500 steps = 5.0215e-01, PNorm = 51.5299, GNorm = 0.1330
Meta loss on this task batch = 5.3109e-01, Meta loss averaged over last 500 steps = 5.0238e-01, PNorm = 51.5351, GNorm = 0.1139
Meta loss on this task batch = 4.8346e-01, Meta loss averaged over last 500 steps = 5.0223e-01, PNorm = 51.5411, GNorm = 0.0980
Meta loss on this task batch = 4.5738e-01, Meta loss averaged over last 500 steps = 5.0187e-01, PNorm = 51.5480, GNorm = 0.1137
Meta loss on this task batch = 4.8947e-01, Meta loss averaged over last 500 steps = 5.0177e-01, PNorm = 51.5551, GNorm = 0.0910
Meta loss on this task batch = 4.4901e-01, Meta loss averaged over last 500 steps = 5.0136e-01, PNorm = 51.5630, GNorm = 0.1076
Meta loss on this task batch = 5.2658e-01, Meta loss averaged over last 500 steps = 5.0156e-01, PNorm = 51.5686, GNorm = 0.1906
Meta loss on this task batch = 4.5357e-01, Meta loss averaged over last 500 steps = 5.0118e-01, PNorm = 51.5717, GNorm = 0.2578
Meta loss on this task batch = 4.4847e-01, Meta loss averaged over last 500 steps = 5.0078e-01, PNorm = 51.5739, GNorm = 0.1460
Meta loss on this task batch = 4.8217e-01, Meta loss averaged over last 500 steps = 5.0064e-01, PNorm = 51.5769, GNorm = 0.1024
Meta loss on this task batch = 5.1985e-01, Meta loss averaged over last 500 steps = 5.0078e-01, PNorm = 51.5803, GNorm = 0.0893
Meta loss on this task batch = 4.9007e-01, Meta loss averaged over last 500 steps = 5.0070e-01, PNorm = 51.5848, GNorm = 0.1193
Took 126.48467063903809 seconds to complete one epoch of meta training
Took 135.62105345726013 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.326570
Epoch 7
Meta loss on this task batch = 4.6867e-01, Meta loss averaged over last 500 steps = 5.0046e-01, PNorm = 51.5900, GNorm = 0.1135
Meta loss on this task batch = 4.6693e-01, Meta loss averaged over last 500 steps = 5.0021e-01, PNorm = 51.5966, GNorm = 0.1271
Meta loss on this task batch = 5.0660e-01, Meta loss averaged over last 500 steps = 5.0026e-01, PNorm = 51.6033, GNorm = 0.0805
Meta loss on this task batch = 4.6715e-01, Meta loss averaged over last 500 steps = 5.0002e-01, PNorm = 51.6100, GNorm = 0.0975
Meta loss on this task batch = 4.4506e-01, Meta loss averaged over last 500 steps = 4.9962e-01, PNorm = 51.6163, GNorm = 0.1031
Meta loss on this task batch = 4.7255e-01, Meta loss averaged over last 500 steps = 4.9943e-01, PNorm = 51.6211, GNorm = 0.1200
Meta loss on this task batch = 5.0626e-01, Meta loss averaged over last 500 steps = 4.9948e-01, PNorm = 51.6250, GNorm = 0.1406
Meta loss on this task batch = 4.1503e-01, Meta loss averaged over last 500 steps = 4.9888e-01, PNorm = 51.6289, GNorm = 0.0804
Meta loss on this task batch = 4.2529e-01, Meta loss averaged over last 500 steps = 4.9836e-01, PNorm = 51.6323, GNorm = 0.0773
Meta loss on this task batch = 5.6650e-01, Meta loss averaged over last 500 steps = 4.9883e-01, PNorm = 51.6356, GNorm = 0.0909
Meta loss on this task batch = 4.8318e-01, Meta loss averaged over last 500 steps = 4.9873e-01, PNorm = 51.6396, GNorm = 0.0839
Meta loss on this task batch = 4.6388e-01, Meta loss averaged over last 500 steps = 4.9849e-01, PNorm = 51.6445, GNorm = 0.0874
Meta loss on this task batch = 5.4003e-01, Meta loss averaged over last 500 steps = 4.9877e-01, PNorm = 51.6491, GNorm = 0.0978
Meta loss on this task batch = 3.8833e-01, Meta loss averaged over last 500 steps = 4.9802e-01, PNorm = 51.6549, GNorm = 0.1005
Meta loss on this task batch = 5.6835e-01, Meta loss averaged over last 500 steps = 4.9849e-01, PNorm = 51.6602, GNorm = 0.0994
Meta loss on this task batch = 4.7002e-01, Meta loss averaged over last 500 steps = 4.9830e-01, PNorm = 51.6651, GNorm = 0.0880
Meta loss on this task batch = 5.2965e-01, Meta loss averaged over last 500 steps = 4.9851e-01, PNorm = 51.6704, GNorm = 0.0825
Meta loss on this task batch = 4.4209e-01, Meta loss averaged over last 500 steps = 4.9814e-01, PNorm = 51.6755, GNorm = 0.0829
Meta loss on this task batch = 5.1667e-01, Meta loss averaged over last 500 steps = 4.9826e-01, PNorm = 51.6806, GNorm = 0.1193
Took 126.89445424079895 seconds to complete one epoch of meta training
Took 136.46091961860657 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.344694
Found better MAML checkpoint after meta validation, saving now
Epoch 8
Meta loss on this task batch = 4.5946e-01, Meta loss averaged over last 500 steps = 4.9801e-01, PNorm = 51.6853, GNorm = 0.0976
Meta loss on this task batch = 5.3926e-01, Meta loss averaged over last 500 steps = 4.9827e-01, PNorm = 51.6894, GNorm = 0.0997
Meta loss on this task batch = 4.7687e-01, Meta loss averaged over last 500 steps = 4.9814e-01, PNorm = 51.6925, GNorm = 0.0913
Meta loss on this task batch = 4.4116e-01, Meta loss averaged over last 500 steps = 4.9777e-01, PNorm = 51.6965, GNorm = 0.0938
Meta loss on this task batch = 5.1963e-01, Meta loss averaged over last 500 steps = 4.9791e-01, PNorm = 51.7005, GNorm = 0.0986
Meta loss on this task batch = 4.6849e-01, Meta loss averaged over last 500 steps = 4.9772e-01, PNorm = 51.7056, GNorm = 0.0933
Meta loss on this task batch = 4.4132e-01, Meta loss averaged over last 500 steps = 4.9737e-01, PNorm = 51.7104, GNorm = 0.0878
Meta loss on this task batch = 5.4983e-01, Meta loss averaged over last 500 steps = 4.9770e-01, PNorm = 51.7151, GNorm = 0.1097
Meta loss on this task batch = 4.9583e-01, Meta loss averaged over last 500 steps = 4.9769e-01, PNorm = 51.7199, GNorm = 0.0829
Meta loss on this task batch = 4.9549e-01, Meta loss averaged over last 500 steps = 4.9767e-01, PNorm = 51.7249, GNorm = 0.0863
Meta loss on this task batch = 5.0606e-01, Meta loss averaged over last 500 steps = 4.9772e-01, PNorm = 51.7301, GNorm = 0.1062
Meta loss on this task batch = 4.1650e-01, Meta loss averaged over last 500 steps = 4.9723e-01, PNorm = 51.7365, GNorm = 0.1051
Meta loss on this task batch = 5.1103e-01, Meta loss averaged over last 500 steps = 4.9731e-01, PNorm = 51.7419, GNorm = 0.1063
Meta loss on this task batch = 4.1206e-01, Meta loss averaged over last 500 steps = 4.9680e-01, PNorm = 51.7482, GNorm = 0.1072
Meta loss on this task batch = 5.0999e-01, Meta loss averaged over last 500 steps = 4.9688e-01, PNorm = 51.7531, GNorm = 0.1125
Meta loss on this task batch = 4.4711e-01, Meta loss averaged over last 500 steps = 4.9658e-01, PNorm = 51.7567, GNorm = 0.1641
Meta loss on this task batch = 4.6980e-01, Meta loss averaged over last 500 steps = 4.9642e-01, PNorm = 51.7586, GNorm = 0.1968
Meta loss on this task batch = 5.2382e-01, Meta loss averaged over last 500 steps = 4.9658e-01, PNorm = 51.7615, GNorm = 0.0853
Meta loss on this task batch = 4.4706e-01, Meta loss averaged over last 500 steps = 4.9629e-01, PNorm = 51.7656, GNorm = 0.1215
Took 125.66216158866882 seconds to complete one epoch of meta training
Took 134.4999873638153 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.338123
Epoch 9
Meta loss on this task batch = 4.7160e-01, Meta loss averaged over last 500 steps = 4.9615e-01, PNorm = 51.7702, GNorm = 0.0918
Meta loss on this task batch = 5.0896e-01, Meta loss averaged over last 500 steps = 4.9622e-01, PNorm = 51.7757, GNorm = 0.0987
Meta loss on this task batch = 5.0589e-01, Meta loss averaged over last 500 steps = 4.9628e-01, PNorm = 51.7825, GNorm = 0.1134
Meta loss on this task batch = 4.7924e-01, Meta loss averaged over last 500 steps = 4.9618e-01, PNorm = 51.7896, GNorm = 0.1260
Meta loss on this task batch = 5.0205e-01, Meta loss averaged over last 500 steps = 4.9622e-01, PNorm = 51.7968, GNorm = 0.0784
Meta loss on this task batch = 4.9997e-01, Meta loss averaged over last 500 steps = 4.9624e-01, PNorm = 51.8029, GNorm = 0.1401
Meta loss on this task batch = 4.5191e-01, Meta loss averaged over last 500 steps = 4.9599e-01, PNorm = 51.8088, GNorm = 0.0941
Meta loss on this task batch = 5.1317e-01, Meta loss averaged over last 500 steps = 4.9608e-01, PNorm = 51.8145, GNorm = 0.1305
Meta loss on this task batch = 4.1576e-01, Meta loss averaged over last 500 steps = 4.9564e-01, PNorm = 51.8201, GNorm = 0.0907
Meta loss on this task batch = 5.2524e-01, Meta loss averaged over last 500 steps = 4.9580e-01, PNorm = 51.8249, GNorm = 0.1359
Meta loss on this task batch = 5.2556e-01, Meta loss averaged over last 500 steps = 4.9596e-01, PNorm = 51.8278, GNorm = 0.1563
Meta loss on this task batch = 4.6193e-01, Meta loss averaged over last 500 steps = 4.9578e-01, PNorm = 51.8306, GNorm = 0.0999
Meta loss on this task batch = 4.7498e-01, Meta loss averaged over last 500 steps = 4.9567e-01, PNorm = 51.8333, GNorm = 0.0941
Meta loss on this task batch = 4.3330e-01, Meta loss averaged over last 500 steps = 4.9533e-01, PNorm = 51.8373, GNorm = 0.1321
Meta loss on this task batch = 4.4350e-01, Meta loss averaged over last 500 steps = 4.9505e-01, PNorm = 51.8428, GNorm = 0.0994
Meta loss on this task batch = 4.8745e-01, Meta loss averaged over last 500 steps = 4.9501e-01, PNorm = 51.8489, GNorm = 0.0862
Meta loss on this task batch = 4.8080e-01, Meta loss averaged over last 500 steps = 4.9493e-01, PNorm = 51.8558, GNorm = 0.0960
Meta loss on this task batch = 4.4310e-01, Meta loss averaged over last 500 steps = 4.9466e-01, PNorm = 51.8641, GNorm = 0.1001
Meta loss on this task batch = 4.4190e-01, Meta loss averaged over last 500 steps = 4.9438e-01, PNorm = 51.8717, GNorm = 0.1009
Took 128.60006141662598 seconds to complete one epoch of meta training
Took 136.9768328666687 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.337800
Epoch 10
Meta loss on this task batch = 4.6166e-01, Meta loss averaged over last 500 steps = 4.9421e-01, PNorm = 51.8786, GNorm = 0.1195
Meta loss on this task batch = 5.2386e-01, Meta loss averaged over last 500 steps = 4.9437e-01, PNorm = 51.8847, GNorm = 0.1209
Meta loss on this task batch = 4.4168e-01, Meta loss averaged over last 500 steps = 4.9409e-01, PNorm = 51.8914, GNorm = 0.0894
Meta loss on this task batch = 4.4830e-01, Meta loss averaged over last 500 steps = 4.9386e-01, PNorm = 51.8989, GNorm = 0.1041
Meta loss on this task batch = 5.0230e-01, Meta loss averaged over last 500 steps = 4.9390e-01, PNorm = 51.9047, GNorm = 0.1299
Meta loss on this task batch = 4.7395e-01, Meta loss averaged over last 500 steps = 4.9380e-01, PNorm = 51.9106, GNorm = 0.0928
Meta loss on this task batch = 4.0610e-01, Meta loss averaged over last 500 steps = 4.9335e-01, PNorm = 51.9170, GNorm = 0.0921
Meta loss on this task batch = 4.8899e-01, Meta loss averaged over last 500 steps = 4.9333e-01, PNorm = 51.9246, GNorm = 0.0936
Meta loss on this task batch = 4.6326e-01, Meta loss averaged over last 500 steps = 4.9318e-01, PNorm = 51.9323, GNorm = 0.0882
Meta loss on this task batch = 4.5500e-01, Meta loss averaged over last 500 steps = 4.9299e-01, PNorm = 51.9400, GNorm = 0.0847
Meta loss on this task batch = 5.1465e-01, Meta loss averaged over last 500 steps = 4.9310e-01, PNorm = 51.9467, GNorm = 0.1053
Meta loss on this task batch = 4.1208e-01, Meta loss averaged over last 500 steps = 4.9270e-01, PNorm = 51.9532, GNorm = 0.1072
Meta loss on this task batch = 5.1483e-01, Meta loss averaged over last 500 steps = 4.9280e-01, PNorm = 51.9582, GNorm = 0.1405
Meta loss on this task batch = 5.0800e-01, Meta loss averaged over last 500 steps = 4.9288e-01, PNorm = 51.9622, GNorm = 0.1312
Meta loss on this task batch = 4.2265e-01, Meta loss averaged over last 500 steps = 4.9254e-01, PNorm = 51.9658, GNorm = 0.0981
Meta loss on this task batch = 5.4603e-01, Meta loss averaged over last 500 steps = 4.9280e-01, PNorm = 51.9685, GNorm = 0.1325
Meta loss on this task batch = 4.8165e-01, Meta loss averaged over last 500 steps = 4.9274e-01, PNorm = 51.9728, GNorm = 0.1063
Meta loss on this task batch = 4.9189e-01, Meta loss averaged over last 500 steps = 4.9274e-01, PNorm = 51.9783, GNorm = 0.0963
Meta loss on this task batch = 5.1610e-01, Meta loss averaged over last 500 steps = 4.9285e-01, PNorm = 51.9857, GNorm = 0.1480
Took 130.7052390575409 seconds to complete one epoch of meta training
Took 139.52600502967834 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.340779
Epoch 11
Meta loss on this task batch = 4.7050e-01, Meta loss averaged over last 500 steps = 4.9274e-01, PNorm = 51.9947, GNorm = 0.1109
Meta loss on this task batch = 4.7796e-01, Meta loss averaged over last 500 steps = 4.9267e-01, PNorm = 52.0052, GNorm = 0.1108
Meta loss on this task batch = 4.5388e-01, Meta loss averaged over last 500 steps = 4.9249e-01, PNorm = 52.0165, GNorm = 0.0817
Meta loss on this task batch = 5.0296e-01, Meta loss averaged over last 500 steps = 4.9254e-01, PNorm = 52.0271, GNorm = 0.1072
Meta loss on this task batch = 5.2630e-01, Meta loss averaged over last 500 steps = 4.9270e-01, PNorm = 52.0337, GNorm = 0.2683
Meta loss on this task batch = 4.2455e-01, Meta loss averaged over last 500 steps = 4.9238e-01, PNorm = 52.0406, GNorm = 0.0907
Meta loss on this task batch = 4.4570e-01, Meta loss averaged over last 500 steps = 4.9216e-01, PNorm = 52.0487, GNorm = 0.1021
Meta loss on this task batch = 5.1404e-01, Meta loss averaged over last 500 steps = 4.9226e-01, PNorm = 52.0545, GNorm = 0.1790
Meta loss on this task batch = 4.3985e-01, Meta loss averaged over last 500 steps = 4.9202e-01, PNorm = 52.0609, GNorm = 0.0838
Meta loss on this task batch = 5.0832e-01, Meta loss averaged over last 500 steps = 4.9210e-01, PNorm = 52.0677, GNorm = 0.1342
Meta loss on this task batch = 4.4307e-01, Meta loss averaged over last 500 steps = 4.9188e-01, PNorm = 52.0751, GNorm = 0.0847
Meta loss on this task batch = 5.1556e-01, Meta loss averaged over last 500 steps = 4.9198e-01, PNorm = 52.0818, GNorm = 0.0945
Meta loss on this task batch = 4.6771e-01, Meta loss averaged over last 500 steps = 4.9187e-01, PNorm = 52.0886, GNorm = 0.1113
Meta loss on this task batch = 5.0158e-01, Meta loss averaged over last 500 steps = 4.9192e-01, PNorm = 52.0959, GNorm = 0.0984
Meta loss on this task batch = 4.6992e-01, Meta loss averaged over last 500 steps = 4.9182e-01, PNorm = 52.1029, GNorm = 0.0928
Meta loss on this task batch = 4.4455e-01, Meta loss averaged over last 500 steps = 4.9161e-01, PNorm = 52.1104, GNorm = 0.0893
Meta loss on this task batch = 4.7355e-01, Meta loss averaged over last 500 steps = 4.9153e-01, PNorm = 52.1177, GNorm = 0.0900
Meta loss on this task batch = 4.4974e-01, Meta loss averaged over last 500 steps = 4.9134e-01, PNorm = 52.1243, GNorm = 0.0947
Meta loss on this task batch = 4.6868e-01, Meta loss averaged over last 500 steps = 4.9125e-01, PNorm = 52.1314, GNorm = 0.1181
Took 135.2581388950348 seconds to complete one epoch of meta training
Took 143.17999935150146 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.344166
Epoch 12
Meta loss on this task batch = 4.9776e-01, Meta loss averaged over last 500 steps = 4.9127e-01, PNorm = 52.1370, GNorm = 0.1196
Meta loss on this task batch = 5.0705e-01, Meta loss averaged over last 500 steps = 4.9134e-01, PNorm = 52.1417, GNorm = 0.1190
Meta loss on this task batch = 4.8662e-01, Meta loss averaged over last 500 steps = 4.9132e-01, PNorm = 52.1471, GNorm = 0.1003
Meta loss on this task batch = 4.7957e-01, Meta loss averaged over last 500 steps = 4.9127e-01, PNorm = 52.1523, GNorm = 0.1052
Meta loss on this task batch = 4.2156e-01, Meta loss averaged over last 500 steps = 4.9097e-01, PNorm = 52.1582, GNorm = 0.1060
Meta loss on this task batch = 5.4330e-01, Meta loss averaged over last 500 steps = 4.9120e-01, PNorm = 52.1632, GNorm = 0.0835
Meta loss on this task batch = 5.4382e-01, Meta loss averaged over last 500 steps = 4.9142e-01, PNorm = 52.1683, GNorm = 0.1025
Meta loss on this task batch = 5.3027e-01, Meta loss averaged over last 500 steps = 4.9158e-01, PNorm = 52.1736, GNorm = 0.0910
Meta loss on this task batch = 4.1578e-01, Meta loss averaged over last 500 steps = 4.9126e-01, PNorm = 52.1804, GNorm = 0.1321
Meta loss on this task batch = 4.1207e-01, Meta loss averaged over last 500 steps = 4.9093e-01, PNorm = 52.1886, GNorm = 0.0908
Meta loss on this task batch = 4.5855e-01, Meta loss averaged over last 500 steps = 4.9080e-01, PNorm = 52.1987, GNorm = 0.1245
Meta loss on this task batch = 4.5955e-01, Meta loss averaged over last 500 steps = 4.9067e-01, PNorm = 52.2082, GNorm = 0.0891
Meta loss on this task batch = 4.7964e-01, Meta loss averaged over last 500 steps = 4.9062e-01, PNorm = 52.2177, GNorm = 0.0926
Meta loss on this task batch = 4.6915e-01, Meta loss averaged over last 500 steps = 4.9053e-01, PNorm = 52.2264, GNorm = 0.1248
Meta loss on this task batch = 3.5612e-01, Meta loss averaged over last 500 steps = 4.8998e-01, PNorm = 52.2344, GNorm = 0.1047
Meta loss on this task batch = 5.4891e-01, Meta loss averaged over last 500 steps = 4.9022e-01, PNorm = 52.2387, GNorm = 0.2546
Meta loss on this task batch = 4.9920e-01, Meta loss averaged over last 500 steps = 4.9026e-01, PNorm = 52.2415, GNorm = 0.2315
Meta loss on this task batch = 4.6222e-01, Meta loss averaged over last 500 steps = 4.9014e-01, PNorm = 52.2455, GNorm = 0.0996
Meta loss on this task batch = 4.0633e-01, Meta loss averaged over last 500 steps = 4.8980e-01, PNorm = 52.2515, GNorm = 0.1328
Took 127.57171106338501 seconds to complete one epoch of meta training
Took 136.1453800201416 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.348869
Found better MAML checkpoint after meta validation, saving now
Epoch 13
Meta loss on this task batch = 4.4540e-01, Meta loss averaged over last 500 steps = 4.8962e-01, PNorm = 52.2587, GNorm = 0.1120
Meta loss on this task batch = 4.9128e-01, Meta loss averaged over last 500 steps = 4.8963e-01, PNorm = 52.2660, GNorm = 0.0847
Meta loss on this task batch = 4.3357e-01, Meta loss averaged over last 500 steps = 4.8941e-01, PNorm = 52.2757, GNorm = 0.1933
Meta loss on this task batch = 4.7942e-01, Meta loss averaged over last 500 steps = 4.8937e-01, PNorm = 52.2860, GNorm = 0.0986
Meta loss on this task batch = 4.7063e-01, Meta loss averaged over last 500 steps = 4.8929e-01, PNorm = 52.2971, GNorm = 0.0869
Meta loss on this task batch = 5.0005e-01, Meta loss averaged over last 500 steps = 4.8934e-01, PNorm = 52.3065, GNorm = 0.1308
Meta loss on this task batch = 4.3545e-01, Meta loss averaged over last 500 steps = 4.8912e-01, PNorm = 52.3156, GNorm = 0.0941
Meta loss on this task batch = 4.5494e-01, Meta loss averaged over last 500 steps = 4.8899e-01, PNorm = 52.3244, GNorm = 0.0999
Meta loss on this task batch = 4.7071e-01, Meta loss averaged over last 500 steps = 4.8892e-01, PNorm = 52.3323, GNorm = 0.1142
Meta loss on this task batch = 4.9835e-01, Meta loss averaged over last 500 steps = 4.8895e-01, PNorm = 52.3368, GNorm = 0.2220
Meta loss on this task batch = 5.1796e-01, Meta loss averaged over last 500 steps = 4.8907e-01, PNorm = 52.3406, GNorm = 0.1334
Meta loss on this task batch = 4.9155e-01, Meta loss averaged over last 500 steps = 4.8908e-01, PNorm = 52.3445, GNorm = 0.1094
Meta loss on this task batch = 4.7444e-01, Meta loss averaged over last 500 steps = 4.8902e-01, PNorm = 52.3500, GNorm = 0.1073
Meta loss on this task batch = 5.0527e-01, Meta loss averaged over last 500 steps = 4.8908e-01, PNorm = 52.3561, GNorm = 0.0989
Meta loss on this task batch = 4.5454e-01, Meta loss averaged over last 500 steps = 4.8895e-01, PNorm = 52.3639, GNorm = 0.1032
Meta loss on this task batch = 4.4416e-01, Meta loss averaged over last 500 steps = 4.8878e-01, PNorm = 52.3728, GNorm = 0.1007
Meta loss on this task batch = 5.2523e-01, Meta loss averaged over last 500 steps = 4.8892e-01, PNorm = 52.3819, GNorm = 0.1076
Meta loss on this task batch = 5.0488e-01, Meta loss averaged over last 500 steps = 4.8898e-01, PNorm = 52.3907, GNorm = 0.1215
Meta loss on this task batch = 4.4189e-01, Meta loss averaged over last 500 steps = 4.8880e-01, PNorm = 52.4008, GNorm = 0.1140
Took 131.66231656074524 seconds to complete one epoch of meta training
Took 139.4230785369873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.349893
Found better MAML checkpoint after meta validation, saving now
Epoch 14
Meta loss on this task batch = 4.8980e-01, Meta loss averaged over last 500 steps = 4.8881e-01, PNorm = 52.4114, GNorm = 0.0877
Meta loss on this task batch = 5.1078e-01, Meta loss averaged over last 500 steps = 4.8889e-01, PNorm = 52.4207, GNorm = 0.1892
Meta loss on this task batch = 4.3660e-01, Meta loss averaged over last 500 steps = 4.8869e-01, PNorm = 52.4309, GNorm = 0.1147
Meta loss on this task batch = 4.1843e-01, Meta loss averaged over last 500 steps = 4.8843e-01, PNorm = 52.4407, GNorm = 0.0960
Meta loss on this task batch = 4.2049e-01, Meta loss averaged over last 500 steps = 4.8818e-01, PNorm = 52.4501, GNorm = 0.1050
Meta loss on this task batch = 4.5620e-01, Meta loss averaged over last 500 steps = 4.8806e-01, PNorm = 52.4586, GNorm = 0.1202
Meta loss on this task batch = 4.9373e-01, Meta loss averaged over last 500 steps = 4.8808e-01, PNorm = 52.4667, GNorm = 0.1434
Meta loss on this task batch = 4.9342e-01, Meta loss averaged over last 500 steps = 4.8810e-01, PNorm = 52.4749, GNorm = 0.1027
Meta loss on this task batch = 4.4424e-01, Meta loss averaged over last 500 steps = 4.8794e-01, PNorm = 52.4841, GNorm = 0.0918
Meta loss on this task batch = 5.2252e-01, Meta loss averaged over last 500 steps = 4.8807e-01, PNorm = 52.4925, GNorm = 0.0942
Meta loss on this task batch = 4.5447e-01, Meta loss averaged over last 500 steps = 4.8795e-01, PNorm = 52.5021, GNorm = 0.0988
Meta loss on this task batch = 4.2402e-01, Meta loss averaged over last 500 steps = 4.8772e-01, PNorm = 52.5131, GNorm = 0.1005
Meta loss on this task batch = 4.5710e-01, Meta loss averaged over last 500 steps = 4.8761e-01, PNorm = 52.5244, GNorm = 0.0985
Meta loss on this task batch = 4.8928e-01, Meta loss averaged over last 500 steps = 4.8762e-01, PNorm = 52.5350, GNorm = 0.1141
Meta loss on this task batch = 4.8321e-01, Meta loss averaged over last 500 steps = 4.8760e-01, PNorm = 52.5459, GNorm = 0.1163
Meta loss on this task batch = 5.2229e-01, Meta loss averaged over last 500 steps = 4.8772e-01, PNorm = 52.5564, GNorm = 0.1103
Meta loss on this task batch = 4.5893e-01, Meta loss averaged over last 500 steps = 4.8762e-01, PNorm = 52.5668, GNorm = 0.1059
Meta loss on this task batch = 4.4606e-01, Meta loss averaged over last 500 steps = 4.8747e-01, PNorm = 52.5773, GNorm = 0.0990
Meta loss on this task batch = 5.0746e-01, Meta loss averaged over last 500 steps = 4.8754e-01, PNorm = 52.5856, GNorm = 0.1741
Took 136.00277519226074 seconds to complete one epoch of meta training
Took 145.72378420829773 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.367925
Found better MAML checkpoint after meta validation, saving now
Epoch 15
Meta loss on this task batch = 4.6023e-01, Meta loss averaged over last 500 steps = 4.8745e-01, PNorm = 52.5918, GNorm = 0.1354
Meta loss on this task batch = 4.8866e-01, Meta loss averaged over last 500 steps = 4.8745e-01, PNorm = 52.5975, GNorm = 0.1308
Meta loss on this task batch = 4.4866e-01, Meta loss averaged over last 500 steps = 4.8732e-01, PNorm = 52.6037, GNorm = 0.1024
Meta loss on this task batch = 4.2551e-01, Meta loss averaged over last 500 steps = 4.8710e-01, PNorm = 52.6112, GNorm = 0.0997
Meta loss on this task batch = 4.9672e-01, Meta loss averaged over last 500 steps = 4.8714e-01, PNorm = 52.6185, GNorm = 0.1228
Meta loss on this task batch = 4.8020e-01, Meta loss averaged over last 500 steps = 4.8711e-01, PNorm = 52.6257, GNorm = 0.1180
Meta loss on this task batch = 5.1102e-01, Meta loss averaged over last 500 steps = 4.8720e-01, PNorm = 52.6326, GNorm = 0.0918
Meta loss on this task batch = 4.9883e-01, Meta loss averaged over last 500 steps = 4.8724e-01, PNorm = 52.6404, GNorm = 0.0847
Meta loss on this task batch = 4.8968e-01, Meta loss averaged over last 500 steps = 4.8724e-01, PNorm = 52.6488, GNorm = 0.0917
Meta loss on this task batch = 4.4194e-01, Meta loss averaged over last 500 steps = 4.8709e-01, PNorm = 52.6583, GNorm = 0.0958
Meta loss on this task batch = 4.5100e-01, Meta loss averaged over last 500 steps = 4.8697e-01, PNorm = 52.6679, GNorm = 0.0970
Meta loss on this task batch = 4.8891e-01, Meta loss averaged over last 500 steps = 4.8698e-01, PNorm = 52.6782, GNorm = 0.0945
Meta loss on this task batch = 4.0414e-01, Meta loss averaged over last 500 steps = 4.8670e-01, PNorm = 52.6890, GNorm = 0.1234
Meta loss on this task batch = 4.7941e-01, Meta loss averaged over last 500 steps = 4.8667e-01, PNorm = 52.7007, GNorm = 0.1088
Meta loss on this task batch = 4.4083e-01, Meta loss averaged over last 500 steps = 4.8652e-01, PNorm = 52.7122, GNorm = 0.0898
Meta loss on this task batch = 4.3469e-01, Meta loss averaged over last 500 steps = 4.8635e-01, PNorm = 52.7219, GNorm = 0.1300
Meta loss on this task batch = 4.5119e-01, Meta loss averaged over last 500 steps = 4.8623e-01, PNorm = 52.7304, GNorm = 0.1588
Meta loss on this task batch = 4.9724e-01, Meta loss averaged over last 500 steps = 4.8627e-01, PNorm = 52.7375, GNorm = 0.1367
Meta loss on this task batch = 5.6580e-01, Meta loss averaged over last 500 steps = 4.8653e-01, PNorm = 52.7419, GNorm = 0.2177
Took 142.09735465049744 seconds to complete one epoch of meta training
Took 150.4678373336792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.357278
Epoch 16
Meta loss on this task batch = 4.5866e-01, Meta loss averaged over last 500 steps = 4.8644e-01, PNorm = 52.7476, GNorm = 0.0868
Meta loss on this task batch = 4.2151e-01, Meta loss averaged over last 500 steps = 4.8623e-01, PNorm = 52.7561, GNorm = 0.1027
Meta loss on this task batch = 4.3717e-01, Meta loss averaged over last 500 steps = 4.8607e-01, PNorm = 52.7667, GNorm = 0.1312
Meta loss on this task batch = 4.5332e-01, Meta loss averaged over last 500 steps = 4.8596e-01, PNorm = 52.7785, GNorm = 0.1222
Meta loss on this task batch = 4.6863e-01, Meta loss averaged over last 500 steps = 4.8590e-01, PNorm = 52.7904, GNorm = 0.0970
Meta loss on this task batch = 4.6006e-01, Meta loss averaged over last 500 steps = 4.8582e-01, PNorm = 52.8027, GNorm = 0.0947
Meta loss on this task batch = 5.1352e-01, Meta loss averaged over last 500 steps = 4.8591e-01, PNorm = 52.8148, GNorm = 0.1269
Meta loss on this task batch = 4.7692e-01, Meta loss averaged over last 500 steps = 4.8588e-01, PNorm = 52.8269, GNorm = 0.1158
Meta loss on this task batch = 4.7370e-01, Meta loss averaged over last 500 steps = 4.8584e-01, PNorm = 52.8390, GNorm = 0.1126
Meta loss on this task batch = 5.0962e-01, Meta loss averaged over last 500 steps = 4.8592e-01, PNorm = 52.8487, GNorm = 0.1618
Meta loss on this task batch = 4.9520e-01, Meta loss averaged over last 500 steps = 4.8595e-01, PNorm = 52.8586, GNorm = 0.1349
Meta loss on this task batch = 4.4218e-01, Meta loss averaged over last 500 steps = 4.8581e-01, PNorm = 52.8677, GNorm = 0.1208
Meta loss on this task batch = 5.0423e-01, Meta loss averaged over last 500 steps = 4.8587e-01, PNorm = 52.8753, GNorm = 0.1400
Meta loss on this task batch = 4.8429e-01, Meta loss averaged over last 500 steps = 4.8586e-01, PNorm = 52.8836, GNorm = 0.1208
Meta loss on this task batch = 5.0525e-01, Meta loss averaged over last 500 steps = 4.8592e-01, PNorm = 52.8929, GNorm = 0.0971
Meta loss on this task batch = 3.8345e-01, Meta loss averaged over last 500 steps = 4.8560e-01, PNorm = 52.9034, GNorm = 0.0913
Meta loss on this task batch = 5.0885e-01, Meta loss averaged over last 500 steps = 4.8567e-01, PNorm = 52.9134, GNorm = 0.1162
Meta loss on this task batch = 4.7298e-01, Meta loss averaged over last 500 steps = 4.8563e-01, PNorm = 52.9248, GNorm = 0.1181
Meta loss on this task batch = 4.6997e-01, Meta loss averaged over last 500 steps = 4.8559e-01, PNorm = 52.9363, GNorm = 0.1504
Took 123.4761118888855 seconds to complete one epoch of meta training
Took 131.04562067985535 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.380293
Found better MAML checkpoint after meta validation, saving now
Epoch 17
Meta loss on this task batch = 4.0573e-01, Meta loss averaged over last 500 steps = 4.8534e-01, PNorm = 52.9492, GNorm = 0.1024
Meta loss on this task batch = 4.6450e-01, Meta loss averaged over last 500 steps = 4.8528e-01, PNorm = 52.9607, GNorm = 0.0959
Meta loss on this task batch = 5.1058e-01, Meta loss averaged over last 500 steps = 4.8535e-01, PNorm = 52.9715, GNorm = 0.1380
Meta loss on this task batch = 4.5165e-01, Meta loss averaged over last 500 steps = 4.8525e-01, PNorm = 52.9807, GNorm = 0.1405
Meta loss on this task batch = 4.8143e-01, Meta loss averaged over last 500 steps = 4.8524e-01, PNorm = 52.9895, GNorm = 0.1022
Meta loss on this task batch = 4.3386e-01, Meta loss averaged over last 500 steps = 4.8508e-01, PNorm = 52.9969, GNorm = 0.1117
Meta loss on this task batch = 5.1877e-01, Meta loss averaged over last 500 steps = 4.8518e-01, PNorm = 53.0018, GNorm = 0.1239
Meta loss on this task batch = 4.8499e-01, Meta loss averaged over last 500 steps = 4.8518e-01, PNorm = 53.0074, GNorm = 0.0989
Meta loss on this task batch = 4.9425e-01, Meta loss averaged over last 500 steps = 4.8521e-01, PNorm = 53.0147, GNorm = 0.1388
Meta loss on this task batch = 5.3606e-01, Meta loss averaged over last 500 steps = 4.8536e-01, PNorm = 53.0236, GNorm = 0.1083
Meta loss on this task batch = 4.1137e-01, Meta loss averaged over last 500 steps = 4.8514e-01, PNorm = 53.0347, GNorm = 0.1143
Meta loss on this task batch = 4.6899e-01, Meta loss averaged over last 500 steps = 4.8509e-01, PNorm = 53.0460, GNorm = 0.1376
Meta loss on this task batch = 3.8074e-01, Meta loss averaged over last 500 steps = 4.8478e-01, PNorm = 53.0591, GNorm = 0.1394
Meta loss on this task batch = 4.8101e-01, Meta loss averaged over last 500 steps = 4.8477e-01, PNorm = 53.0729, GNorm = 0.1371
Meta loss on this task batch = 4.9261e-01, Meta loss averaged over last 500 steps = 4.8480e-01, PNorm = 53.0843, GNorm = 0.1500
Meta loss on this task batch = 3.9526e-01, Meta loss averaged over last 500 steps = 4.8453e-01, PNorm = 53.0951, GNorm = 0.1173
Meta loss on this task batch = 5.0875e-01, Meta loss averaged over last 500 steps = 4.8460e-01, PNorm = 53.1056, GNorm = 0.1364
Meta loss on this task batch = 5.0330e-01, Meta loss averaged over last 500 steps = 4.8466e-01, PNorm = 53.1150, GNorm = 0.1827
Meta loss on this task batch = 5.1010e-01, Meta loss averaged over last 500 steps = 4.8473e-01, PNorm = 53.1224, GNorm = 0.2335
Took 210.60616970062256 seconds to complete one epoch of meta training
Took 218.4288477897644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.361192
Epoch 18
Meta loss on this task batch = 4.7314e-01, Meta loss averaged over last 500 steps = 4.8470e-01, PNorm = 53.1302, GNorm = 0.1694
Meta loss on this task batch = 4.6618e-01, Meta loss averaged over last 500 steps = 4.8464e-01, PNorm = 53.1380, GNorm = 0.1246
Meta loss on this task batch = 4.7944e-01, Meta loss averaged over last 500 steps = 4.8463e-01, PNorm = 53.1445, GNorm = 0.1688
Meta loss on this task batch = 4.4747e-01, Meta loss averaged over last 500 steps = 4.8452e-01, PNorm = 53.1514, GNorm = 0.1081
Meta loss on this task batch = 5.3051e-01, Meta loss averaged over last 500 steps = 4.8465e-01, PNorm = 53.1588, GNorm = 0.1053
Meta loss on this task batch = 4.8097e-01, Meta loss averaged over last 500 steps = 4.8464e-01, PNorm = 53.1679, GNorm = 0.1048
Meta loss on this task batch = 5.1059e-01, Meta loss averaged over last 500 steps = 4.8472e-01, PNorm = 53.1774, GNorm = 0.0897
Meta loss on this task batch = 4.1369e-01, Meta loss averaged over last 500 steps = 4.8452e-01, PNorm = 53.1892, GNorm = 0.1487
Meta loss on this task batch = 4.8070e-01, Meta loss averaged over last 500 steps = 4.8450e-01, PNorm = 53.2023, GNorm = 0.1124
Meta loss on this task batch = 5.5204e-01, Meta loss averaged over last 500 steps = 4.8470e-01, PNorm = 53.2152, GNorm = 0.1052
Meta loss on this task batch = 4.1208e-01, Meta loss averaged over last 500 steps = 4.8449e-01, PNorm = 53.2292, GNorm = 0.1117
Meta loss on this task batch = 4.7507e-01, Meta loss averaged over last 500 steps = 4.8446e-01, PNorm = 53.2423, GNorm = 0.0990
Meta loss on this task batch = 4.6097e-01, Meta loss averaged over last 500 steps = 4.8440e-01, PNorm = 53.2543, GNorm = 0.1008
Meta loss on this task batch = 5.0008e-01, Meta loss averaged over last 500 steps = 4.8444e-01, PNorm = 53.2660, GNorm = 0.1156
Meta loss on this task batch = 4.9104e-01, Meta loss averaged over last 500 steps = 4.8446e-01, PNorm = 53.2760, GNorm = 0.1166
Meta loss on this task batch = 4.2136e-01, Meta loss averaged over last 500 steps = 4.8428e-01, PNorm = 53.2860, GNorm = 0.1028
Meta loss on this task batch = 4.4306e-01, Meta loss averaged over last 500 steps = 4.8417e-01, PNorm = 53.2953, GNorm = 0.1104
Meta loss on this task batch = 4.0831e-01, Meta loss averaged over last 500 steps = 4.8396e-01, PNorm = 53.3051, GNorm = 0.1125
Meta loss on this task batch = 4.7556e-01, Meta loss averaged over last 500 steps = 4.8394e-01, PNorm = 53.3142, GNorm = 0.1495
Took 127.30142498016357 seconds to complete one epoch of meta training
Took 135.45447397232056 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.379434
Epoch 19
Meta loss on this task batch = 5.2653e-01, Meta loss averaged over last 500 steps = 4.8405e-01, PNorm = 53.3206, GNorm = 0.1400
Meta loss on this task batch = 4.4091e-01, Meta loss averaged over last 500 steps = 4.8393e-01, PNorm = 53.3270, GNorm = 0.1332
Meta loss on this task batch = 5.6816e-01, Meta loss averaged over last 500 steps = 4.8417e-01, PNorm = 53.3340, GNorm = 0.1463
Meta loss on this task batch = 4.2948e-01, Meta loss averaged over last 500 steps = 4.8402e-01, PNorm = 53.3417, GNorm = 0.1393
Meta loss on this task batch = 5.2754e-01, Meta loss averaged over last 500 steps = 4.8413e-01, PNorm = 53.3511, GNorm = 0.1096
Meta loss on this task batch = 3.9538e-01, Meta loss averaged over last 500 steps = 4.8389e-01, PNorm = 53.3621, GNorm = 0.1097
Meta loss on this task batch = 5.2594e-01, Meta loss averaged over last 500 steps = 4.8401e-01, PNorm = 53.3730, GNorm = 0.1137
Meta loss on this task batch = 5.0769e-01, Meta loss averaged over last 500 steps = 4.8407e-01, PNorm = 53.3840, GNorm = 0.1056
Meta loss on this task batch = 4.1527e-01, Meta loss averaged over last 500 steps = 4.8389e-01, PNorm = 53.3953, GNorm = 0.0956
Meta loss on this task batch = 4.5853e-01, Meta loss averaged over last 500 steps = 4.8382e-01, PNorm = 53.4062, GNorm = 0.0907
Meta loss on this task batch = 5.2177e-01, Meta loss averaged over last 500 steps = 4.8392e-01, PNorm = 53.4150, GNorm = 0.1364
Meta loss on this task batch = 4.8251e-01, Meta loss averaged over last 500 steps = 4.8391e-01, PNorm = 53.4243, GNorm = 0.1212
Meta loss on this task batch = 4.6696e-01, Meta loss averaged over last 500 steps = 4.8387e-01, PNorm = 53.4334, GNorm = 0.1394
Meta loss on this task batch = 5.0672e-01, Meta loss averaged over last 500 steps = 4.8393e-01, PNorm = 53.4426, GNorm = 0.0960
Meta loss on this task batch = 4.2823e-01, Meta loss averaged over last 500 steps = 4.8378e-01, PNorm = 53.4536, GNorm = 0.1080
Meta loss on this task batch = 4.8780e-01, Meta loss averaged over last 500 steps = 4.8379e-01, PNorm = 53.4629, GNorm = 0.1171
Meta loss on this task batch = 4.0027e-01, Meta loss averaged over last 500 steps = 4.8357e-01, PNorm = 53.4724, GNorm = 0.0962
Meta loss on this task batch = 3.8706e-01, Meta loss averaged over last 500 steps = 4.8332e-01, PNorm = 53.4825, GNorm = 0.0939
Meta loss on this task batch = 3.9819e-01, Meta loss averaged over last 500 steps = 4.8309e-01, PNorm = 53.4922, GNorm = 0.1244
Took 148.1542067527771 seconds to complete one epoch of meta training
Took 157.1389217376709 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.399046
Found better MAML checkpoint after meta validation, saving now
Epoch 20
Meta loss on this task batch = 4.1067e-01, Meta loss averaged over last 500 steps = 4.8290e-01, PNorm = 53.5032, GNorm = 0.1037
Meta loss on this task batch = 4.9603e-01, Meta loss averaged over last 500 steps = 4.8294e-01, PNorm = 53.5148, GNorm = 0.1128
Meta loss on this task batch = 4.4906e-01, Meta loss averaged over last 500 steps = 4.8285e-01, PNorm = 53.5269, GNorm = 0.1006
Meta loss on this task batch = 4.2457e-01, Meta loss averaged over last 500 steps = 4.8270e-01, PNorm = 53.5398, GNorm = 0.0931
Meta loss on this task batch = 4.9868e-01, Meta loss averaged over last 500 steps = 4.8274e-01, PNorm = 53.5515, GNorm = 0.1474
Meta loss on this task batch = 4.7837e-01, Meta loss averaged over last 500 steps = 4.8273e-01, PNorm = 53.5618, GNorm = 0.1286
Meta loss on this task batch = 4.7840e-01, Meta loss averaged over last 500 steps = 4.8272e-01, PNorm = 53.5712, GNorm = 0.1344
Meta loss on this task batch = 4.6028e-01, Meta loss averaged over last 500 steps = 4.8266e-01, PNorm = 53.5784, GNorm = 0.1894
Meta loss on this task batch = 4.8291e-01, Meta loss averaged over last 500 steps = 4.8266e-01, PNorm = 53.5848, GNorm = 0.1655
Meta loss on this task batch = 4.7925e-01, Meta loss averaged over last 500 steps = 4.8265e-01, PNorm = 53.5939, GNorm = 0.1319
Meta loss on this task batch = 4.0576e-01, Meta loss averaged over last 500 steps = 4.8245e-01, PNorm = 53.6055, GNorm = 0.1094
Meta loss on this task batch = 4.9873e-01, Meta loss averaged over last 500 steps = 4.8250e-01, PNorm = 53.6170, GNorm = 0.1715
Meta loss on this task batch = 4.7776e-01, Meta loss averaged over last 500 steps = 4.8248e-01, PNorm = 53.6274, GNorm = 0.1236
Meta loss on this task batch = 4.6808e-01, Meta loss averaged over last 500 steps = 4.8245e-01, PNorm = 53.6363, GNorm = 0.1373
Meta loss on this task batch = 4.1947e-01, Meta loss averaged over last 500 steps = 4.8229e-01, PNorm = 53.6464, GNorm = 0.1098
Meta loss on this task batch = 4.9975e-01, Meta loss averaged over last 500 steps = 4.8233e-01, PNorm = 53.6562, GNorm = 0.1030
Meta loss on this task batch = 4.4041e-01, Meta loss averaged over last 500 steps = 4.8223e-01, PNorm = 53.6671, GNorm = 0.1136
Meta loss on this task batch = 5.1254e-01, Meta loss averaged over last 500 steps = 4.8230e-01, PNorm = 53.6775, GNorm = 0.1224
Meta loss on this task batch = 4.1950e-01, Meta loss averaged over last 500 steps = 4.8214e-01, PNorm = 53.6887, GNorm = 0.1199
Took 226.1149230003357 seconds to complete one epoch of meta training
Took 233.52864909172058 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.353034
Epoch 21
Meta loss on this task batch = 4.6379e-01, Meta loss averaged over last 500 steps = 4.8210e-01, PNorm = 53.6985, GNorm = 0.1754
Meta loss on this task batch = 4.5145e-01, Meta loss averaged over last 500 steps = 4.8202e-01, PNorm = 53.7084, GNorm = 0.0948
Meta loss on this task batch = 5.0049e-01, Meta loss averaged over last 500 steps = 4.8207e-01, PNorm = 53.7181, GNorm = 0.1220
Meta loss on this task batch = 4.7350e-01, Meta loss averaged over last 500 steps = 4.8205e-01, PNorm = 53.7274, GNorm = 0.1032
Meta loss on this task batch = 5.1007e-01, Meta loss averaged over last 500 steps = 4.8212e-01, PNorm = 53.7359, GNorm = 0.1202
Meta loss on this task batch = 4.8400e-01, Meta loss averaged over last 500 steps = 4.8212e-01, PNorm = 53.7444, GNorm = 0.1106
Meta loss on this task batch = 5.1202e-01, Meta loss averaged over last 500 steps = 4.8219e-01, PNorm = 53.7544, GNorm = 0.1023
Meta loss on this task batch = 4.1025e-01, Meta loss averaged over last 500 steps = 4.8202e-01, PNorm = 53.7662, GNorm = 0.1192
Meta loss on this task batch = 4.7779e-01, Meta loss averaged over last 500 steps = 4.8201e-01, PNorm = 53.7795, GNorm = 0.1117
Meta loss on this task batch = 4.7218e-01, Meta loss averaged over last 500 steps = 4.8198e-01, PNorm = 53.7938, GNorm = 0.1018
Meta loss on this task batch = 3.9676e-01, Meta loss averaged over last 500 steps = 4.8178e-01, PNorm = 53.8094, GNorm = 0.1123
Meta loss on this task batch = 4.6481e-01, Meta loss averaged over last 500 steps = 4.8173e-01, PNorm = 53.8259, GNorm = 0.1179
Meta loss on this task batch = 5.4860e-01, Meta loss averaged over last 500 steps = 4.8190e-01, PNorm = 53.8367, GNorm = 0.2083
Meta loss on this task batch = 3.9483e-01, Meta loss averaged over last 500 steps = 4.8169e-01, PNorm = 53.8484, GNorm = 0.1441
Meta loss on this task batch = 4.6975e-01, Meta loss averaged over last 500 steps = 4.8166e-01, PNorm = 53.8591, GNorm = 0.1380
Meta loss on this task batch = 5.0446e-01, Meta loss averaged over last 500 steps = 4.8171e-01, PNorm = 53.8675, GNorm = 0.1774
Meta loss on this task batch = 4.4395e-01, Meta loss averaged over last 500 steps = 4.8162e-01, PNorm = 53.8766, GNorm = 0.1244
Meta loss on this task batch = 4.4867e-01, Meta loss averaged over last 500 steps = 4.8154e-01, PNorm = 53.8859, GNorm = 0.1028
Meta loss on this task batch = 4.1221e-01, Meta loss averaged over last 500 steps = 4.8138e-01, PNorm = 53.8962, GNorm = 0.1580
Took 163.01369881629944 seconds to complete one epoch of meta training
Took 171.19090032577515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.396202
Epoch 22
Meta loss on this task batch = 4.1316e-01, Meta loss averaged over last 500 steps = 4.8121e-01, PNorm = 53.9066, GNorm = 0.0876
Meta loss on this task batch = 5.5102e-01, Meta loss averaged over last 500 steps = 4.8138e-01, PNorm = 53.9154, GNorm = 0.1287
Meta loss on this task batch = 4.7272e-01, Meta loss averaged over last 500 steps = 4.8136e-01, PNorm = 53.9244, GNorm = 0.1025
Meta loss on this task batch = 5.2524e-01, Meta loss averaged over last 500 steps = 4.8146e-01, PNorm = 53.9315, GNorm = 0.1289
Meta loss on this task batch = 4.3753e-01, Meta loss averaged over last 500 steps = 4.8136e-01, PNorm = 53.9404, GNorm = 0.1001
Meta loss on this task batch = 4.5764e-01, Meta loss averaged over last 500 steps = 4.8130e-01, PNorm = 53.9508, GNorm = 0.1105
Meta loss on this task batch = 4.4521e-01, Meta loss averaged over last 500 steps = 4.8122e-01, PNorm = 53.9628, GNorm = 0.1050
Meta loss on this task batch = 4.3496e-01, Meta loss averaged over last 500 steps = 4.8111e-01, PNorm = 53.9758, GNorm = 0.1125
Meta loss on this task batch = 4.4221e-01, Meta loss averaged over last 500 steps = 4.8102e-01, PNorm = 53.9891, GNorm = 0.0997
Meta loss on this task batch = 3.9569e-01, Meta loss averaged over last 500 steps = 4.8082e-01, PNorm = 54.0019, GNorm = 0.1069
Meta loss on this task batch = 4.9840e-01, Meta loss averaged over last 500 steps = 4.8086e-01, PNorm = 54.0135, GNorm = 0.0988
Meta loss on this task batch = 5.1253e-01, Meta loss averaged over last 500 steps = 4.8093e-01, PNorm = 54.0229, GNorm = 0.2204
Meta loss on this task batch = 5.2374e-01, Meta loss averaged over last 500 steps = 4.8103e-01, PNorm = 54.0304, GNorm = 0.1680
Meta loss on this task batch = 4.5219e-01, Meta loss averaged over last 500 steps = 4.8097e-01, PNorm = 54.0378, GNorm = 0.1197
Meta loss on this task batch = 4.3190e-01, Meta loss averaged over last 500 steps = 4.8085e-01, PNorm = 54.0465, GNorm = 0.1268
Meta loss on this task batch = 4.4973e-01, Meta loss averaged over last 500 steps = 4.8078e-01, PNorm = 54.0547, GNorm = 0.1685
Meta loss on this task batch = 4.3790e-01, Meta loss averaged over last 500 steps = 4.8068e-01, PNorm = 54.0645, GNorm = 0.1450
Meta loss on this task batch = 4.6814e-01, Meta loss averaged over last 500 steps = 4.8065e-01, PNorm = 54.0761, GNorm = 0.1127
Meta loss on this task batch = 5.0589e-01, Meta loss averaged over last 500 steps = 4.8071e-01, PNorm = 54.0879, GNorm = 0.1528
Took 120.39732956886292 seconds to complete one epoch of meta training
Took 128.52887725830078 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.389809
Epoch 23
Meta loss on this task batch = 4.2284e-01, Meta loss averaged over last 500 steps = 4.8058e-01, PNorm = 54.1004, GNorm = 0.1232
Meta loss on this task batch = 4.4965e-01, Meta loss averaged over last 500 steps = 4.8051e-01, PNorm = 54.1113, GNorm = 0.1507
Meta loss on this task batch = 4.9206e-01, Meta loss averaged over last 500 steps = 4.8054e-01, PNorm = 54.1204, GNorm = 0.2038
Meta loss on this task batch = 4.6645e-01, Meta loss averaged over last 500 steps = 4.8050e-01, PNorm = 54.1298, GNorm = 0.1520
Meta loss on this task batch = 4.4709e-01, Meta loss averaged over last 500 steps = 4.8043e-01, PNorm = 54.1397, GNorm = 0.0994
Meta loss on this task batch = 4.9050e-01, Meta loss averaged over last 500 steps = 4.8045e-01, PNorm = 54.1487, GNorm = 0.1436
Meta loss on this task batch = 4.0218e-01, Meta loss averaged over last 500 steps = 4.8027e-01, PNorm = 54.1587, GNorm = 0.0962
Meta loss on this task batch = 4.5293e-01, Meta loss averaged over last 500 steps = 4.8021e-01, PNorm = 54.1695, GNorm = 0.1236
Meta loss on this task batch = 4.4205e-01, Meta loss averaged over last 500 steps = 4.8013e-01, PNorm = 54.1800, GNorm = 0.1511
Meta loss on this task batch = 4.4863e-01, Meta loss averaged over last 500 steps = 4.8006e-01, PNorm = 54.1908, GNorm = 0.1016
Meta loss on this task batch = 5.4916e-01, Meta loss averaged over last 500 steps = 4.8021e-01, PNorm = 54.2006, GNorm = 0.1630
Meta loss on this task batch = 4.4074e-01, Meta loss averaged over last 500 steps = 4.8012e-01, PNorm = 54.2120, GNorm = 0.1239
Meta loss on this task batch = 4.3155e-01, Meta loss averaged over last 500 steps = 4.8002e-01, PNorm = 54.2239, GNorm = 0.0924
Meta loss on this task batch = 4.6671e-01, Meta loss averaged over last 500 steps = 4.7999e-01, PNorm = 54.2351, GNorm = 0.1013
Meta loss on this task batch = 5.0783e-01, Meta loss averaged over last 500 steps = 4.8005e-01, PNorm = 54.2459, GNorm = 0.1213
Meta loss on this task batch = 4.8571e-01, Meta loss averaged over last 500 steps = 4.8006e-01, PNorm = 54.2576, GNorm = 0.0996
Meta loss on this task batch = 4.1830e-01, Meta loss averaged over last 500 steps = 4.7992e-01, PNorm = 54.2693, GNorm = 0.1061
Meta loss on this task batch = 4.8071e-01, Meta loss averaged over last 500 steps = 4.7993e-01, PNorm = 54.2817, GNorm = 0.1187
Meta loss on this task batch = 4.6130e-01, Meta loss averaged over last 500 steps = 4.7988e-01, PNorm = 54.2937, GNorm = 0.1208
Took 118.942227602005 seconds to complete one epoch of meta training
Took 126.8194842338562 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.387783
Epoch 24
Meta loss on this task batch = 4.2930e-01, Meta loss averaged over last 500 steps = 4.7977e-01, PNorm = 54.3064, GNorm = 0.1429
Meta loss on this task batch = 4.4703e-01, Meta loss averaged over last 500 steps = 4.7970e-01, PNorm = 54.3187, GNorm = 0.1048
Meta loss on this task batch = 4.3092e-01, Meta loss averaged over last 500 steps = 4.7960e-01, PNorm = 54.3307, GNorm = 0.1326
Meta loss on this task batch = 4.8289e-01, Meta loss averaged over last 500 steps = 4.7960e-01, PNorm = 54.3437, GNorm = 0.1079
Meta loss on this task batch = 4.8280e-01, Meta loss averaged over last 500 steps = 4.7961e-01, PNorm = 54.3563, GNorm = 0.1691
Meta loss on this task batch = 4.6861e-01, Meta loss averaged over last 500 steps = 4.7959e-01, PNorm = 54.3679, GNorm = 0.1291
Meta loss on this task batch = 3.8571e-01, Meta loss averaged over last 500 steps = 4.7938e-01, PNorm = 54.3787, GNorm = 0.1012
Meta loss on this task batch = 5.3623e-01, Meta loss averaged over last 500 steps = 4.7951e-01, PNorm = 54.3872, GNorm = 0.1800
Meta loss on this task batch = 4.8435e-01, Meta loss averaged over last 500 steps = 4.7952e-01, PNorm = 54.3941, GNorm = 0.1381
Meta loss on this task batch = 4.8253e-01, Meta loss averaged over last 500 steps = 4.7952e-01, PNorm = 54.4021, GNorm = 0.1321
Meta loss on this task batch = 4.5347e-01, Meta loss averaged over last 500 steps = 4.7947e-01, PNorm = 54.4117, GNorm = 0.1357
Meta loss on this task batch = 4.8864e-01, Meta loss averaged over last 500 steps = 4.7949e-01, PNorm = 54.4221, GNorm = 0.1208
Meta loss on this task batch = 4.5291e-01, Meta loss averaged over last 500 steps = 4.7943e-01, PNorm = 54.4335, GNorm = 0.0916
Meta loss on this task batch = 4.9927e-01, Meta loss averaged over last 500 steps = 4.7947e-01, PNorm = 54.4463, GNorm = 0.1307
Meta loss on this task batch = 4.8610e-01, Meta loss averaged over last 500 steps = 4.7949e-01, PNorm = 54.4592, GNorm = 0.1021
Meta loss on this task batch = 4.2902e-01, Meta loss averaged over last 500 steps = 4.7938e-01, PNorm = 54.4740, GNorm = 0.1085
Meta loss on this task batch = 4.4078e-01, Meta loss averaged over last 500 steps = 4.7930e-01, PNorm = 54.4886, GNorm = 0.1123
Meta loss on this task batch = 4.7408e-01, Meta loss averaged over last 500 steps = 4.7929e-01, PNorm = 54.5007, GNorm = 0.1760
Meta loss on this task batch = 4.8217e-01, Meta loss averaged over last 500 steps = 4.7929e-01, PNorm = 54.5125, GNorm = 0.1804
Took 124.18241548538208 seconds to complete one epoch of meta training
Took 132.94221758842468 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.391542
Epoch 25
Meta loss on this task batch = 4.8765e-01, Meta loss averaged over last 500 steps = 4.7931e-01, PNorm = 54.5214, GNorm = 0.1675
Meta loss on this task batch = 4.3333e-01, Meta loss averaged over last 500 steps = 4.7921e-01, PNorm = 54.5307, GNorm = 0.1208
Meta loss on this task batch = 4.4442e-01, Meta loss averaged over last 500 steps = 4.7914e-01, PNorm = 54.5412, GNorm = 0.1297
Meta loss on this task batch = 4.6625e-01, Meta loss averaged over last 500 steps = 4.7911e-01, PNorm = 54.5525, GNorm = 0.1199
Meta loss on this task batch = 4.5755e-01, Meta loss averaged over last 500 steps = 4.7907e-01, PNorm = 54.5650, GNorm = 0.1398
Meta loss on this task batch = 4.3534e-01, Meta loss averaged over last 500 steps = 4.7898e-01, PNorm = 54.5779, GNorm = 0.1322
Meta loss on this task batch = 4.6933e-01, Meta loss averaged over last 500 steps = 4.7896e-01, PNorm = 54.5898, GNorm = 0.1220
Meta loss on this task batch = 4.3965e-01, Meta loss averaged over last 500 steps = 4.7888e-01, PNorm = 54.6047, GNorm = 0.1290
Meta loss on this task batch = 4.2991e-01, Meta loss averaged over last 500 steps = 4.7878e-01, PNorm = 54.6193, GNorm = 0.1194
Meta loss on this task batch = 4.0379e-01, Meta loss averaged over last 500 steps = 4.7862e-01, PNorm = 54.6340, GNorm = 0.1095
Meta loss on this task batch = 4.9372e-01, Meta loss averaged over last 500 steps = 4.7865e-01, PNorm = 54.6475, GNorm = 0.1420
Meta loss on this task batch = 4.4745e-01, Meta loss averaged over last 500 steps = 4.7859e-01, PNorm = 54.6607, GNorm = 0.1064
Meta loss on this task batch = 5.3186e-01, Meta loss averaged over last 500 steps = 4.7870e-01, PNorm = 54.6707, GNorm = 0.3193
Meta loss on this task batch = 4.4576e-01, Meta loss averaged over last 500 steps = 4.7863e-01, PNorm = 54.6804, GNorm = 0.1244
Meta loss on this task batch = 4.9455e-01, Meta loss averaged over last 500 steps = 4.7866e-01, PNorm = 54.6896, GNorm = 0.1213
Meta loss on this task batch = 4.6030e-01, Meta loss averaged over last 500 steps = 4.7863e-01, PNorm = 54.6987, GNorm = 0.1399
Meta loss on this task batch = 5.2573e-01, Meta loss averaged over last 500 steps = 4.7872e-01, PNorm = 54.7060, GNorm = 0.1850
Meta loss on this task batch = 4.6023e-01, Meta loss averaged over last 500 steps = 4.7868e-01, PNorm = 54.7143, GNorm = 0.1108
Meta loss on this task batch = 4.3163e-01, Meta loss averaged over last 500 steps = 4.7859e-01, PNorm = 54.7231, GNorm = 0.1382
Took 123.12540245056152 seconds to complete one epoch of meta training
Took 131.57795357704163 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.410877
Found better MAML checkpoint after meta validation, saving now
Epoch 26
Meta loss on this task batch = 4.0189e-01, Meta loss averaged over last 500 steps = 4.7843e-01, PNorm = 54.7337, GNorm = 0.1346
Meta loss on this task batch = 4.7891e-01, Meta loss averaged over last 500 steps = 4.7843e-01, PNorm = 54.7463, GNorm = 0.1310
Meta loss on this task batch = 4.1689e-01, Meta loss averaged over last 500 steps = 4.7831e-01, PNorm = 54.7605, GNorm = 0.1022
Meta loss on this task batch = 4.4921e-01, Meta loss averaged over last 500 steps = 4.7825e-01, PNorm = 54.7743, GNorm = 0.1045
Meta loss on this task batch = 4.7187e-01, Meta loss averaged over last 500 steps = 4.7824e-01, PNorm = 54.7863, GNorm = 0.1892
Meta loss on this task batch = 4.1285e-01, Meta loss averaged over last 500 steps = 4.7811e-01, PNorm = 54.7981, GNorm = 0.0945
Meta loss on this task batch = 4.8781e-01, Meta loss averaged over last 500 steps = 4.7615e-01, PNorm = 54.8091, GNorm = 0.1273
Meta loss on this task batch = 4.1324e-01, Meta loss averaged over last 500 steps = 4.7478e-01, PNorm = 54.8202, GNorm = 0.1082
Meta loss on this task batch = 4.7332e-01, Meta loss averaged over last 500 steps = 4.7457e-01, PNorm = 54.8314, GNorm = 0.1149
Meta loss on this task batch = 5.0587e-01, Meta loss averaged over last 500 steps = 4.7444e-01, PNorm = 54.8428, GNorm = 0.1143
Meta loss on this task batch = 4.2428e-01, Meta loss averaged over last 500 steps = 4.7413e-01, PNorm = 54.8538, GNorm = 0.1122
Meta loss on this task batch = 4.4778e-01, Meta loss averaged over last 500 steps = 4.7397e-01, PNorm = 54.8645, GNorm = 0.1104
Meta loss on this task batch = 5.1834e-01, Meta loss averaged over last 500 steps = 4.7407e-01, PNorm = 54.8758, GNorm = 0.1271
Meta loss on this task batch = 4.4623e-01, Meta loss averaged over last 500 steps = 4.7378e-01, PNorm = 54.8887, GNorm = 0.1086
Meta loss on this task batch = 4.3864e-01, Meta loss averaged over last 500 steps = 4.7369e-01, PNorm = 54.9015, GNorm = 0.1129
Meta loss on this task batch = 5.4735e-01, Meta loss averaged over last 500 steps = 4.7379e-01, PNorm = 54.9128, GNorm = 0.1940
Meta loss on this task batch = 4.7288e-01, Meta loss averaged over last 500 steps = 4.7380e-01, PNorm = 54.9237, GNorm = 0.1004
Meta loss on this task batch = 4.0971e-01, Meta loss averaged over last 500 steps = 4.7365e-01, PNorm = 54.9370, GNorm = 0.1294
Meta loss on this task batch = 4.5704e-01, Meta loss averaged over last 500 steps = 4.7363e-01, PNorm = 54.9504, GNorm = 0.1660
Took 120.19727563858032 seconds to complete one epoch of meta training
Took 128.91968607902527 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.408530
Epoch 27
Meta loss on this task batch = 4.7452e-01, Meta loss averaged over last 500 steps = 4.7378e-01, PNorm = 54.9649, GNorm = 0.1122
Meta loss on this task batch = 3.8990e-01, Meta loss averaged over last 500 steps = 4.7365e-01, PNorm = 54.9784, GNorm = 0.1413
Meta loss on this task batch = 5.2569e-01, Meta loss averaged over last 500 steps = 4.7378e-01, PNorm = 54.9885, GNorm = 0.1579
Meta loss on this task batch = 4.0465e-01, Meta loss averaged over last 500 steps = 4.7340e-01, PNorm = 54.9995, GNorm = 0.1210
Meta loss on this task batch = 4.1193e-01, Meta loss averaged over last 500 steps = 4.7318e-01, PNorm = 55.0107, GNorm = 0.1176
Meta loss on this task batch = 5.1386e-01, Meta loss averaged over last 500 steps = 4.7314e-01, PNorm = 55.0200, GNorm = 0.1702
Meta loss on this task batch = 4.8024e-01, Meta loss averaged over last 500 steps = 4.7308e-01, PNorm = 55.0289, GNorm = 0.1221
Meta loss on this task batch = 5.1288e-01, Meta loss averaged over last 500 steps = 4.7308e-01, PNorm = 55.0382, GNorm = 0.2072
Meta loss on this task batch = 4.5042e-01, Meta loss averaged over last 500 steps = 4.7296e-01, PNorm = 55.0489, GNorm = 0.1500
Meta loss on this task batch = 4.8333e-01, Meta loss averaged over last 500 steps = 4.7301e-01, PNorm = 55.0583, GNorm = 0.1621
Meta loss on this task batch = 4.2955e-01, Meta loss averaged over last 500 steps = 4.7286e-01, PNorm = 55.0689, GNorm = 0.1013
Meta loss on this task batch = 4.2692e-01, Meta loss averaged over last 500 steps = 4.7270e-01, PNorm = 55.0801, GNorm = 0.1423
Meta loss on this task batch = 4.1520e-01, Meta loss averaged over last 500 steps = 4.7252e-01, PNorm = 55.0907, GNorm = 0.1025
Meta loss on this task batch = 4.7521e-01, Meta loss averaged over last 500 steps = 4.7244e-01, PNorm = 55.1005, GNorm = 0.1102
Meta loss on this task batch = 4.7122e-01, Meta loss averaged over last 500 steps = 4.7251e-01, PNorm = 55.1116, GNorm = 0.1417
Meta loss on this task batch = 4.8761e-01, Meta loss averaged over last 500 steps = 4.7250e-01, PNorm = 55.1228, GNorm = 0.1089
Meta loss on this task batch = 4.8937e-01, Meta loss averaged over last 500 steps = 4.7262e-01, PNorm = 55.1349, GNorm = 0.1456
Meta loss on this task batch = 4.6466e-01, Meta loss averaged over last 500 steps = 4.7258e-01, PNorm = 55.1477, GNorm = 0.1609
Meta loss on this task batch = 4.7273e-01, Meta loss averaged over last 500 steps = 4.7248e-01, PNorm = 55.1591, GNorm = 0.1246
Took 122.27547955513 seconds to complete one epoch of meta training
Took 130.0741250514984 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.428728
Found better MAML checkpoint after meta validation, saving now
Epoch 28
Meta loss on this task batch = 4.6873e-01, Meta loss averaged over last 500 steps = 4.7250e-01, PNorm = 55.1722, GNorm = 0.1450
Meta loss on this task batch = 4.6476e-01, Meta loss averaged over last 500 steps = 4.7247e-01, PNorm = 55.1854, GNorm = 0.1303
Meta loss on this task batch = 4.6896e-01, Meta loss averaged over last 500 steps = 4.7237e-01, PNorm = 55.1996, GNorm = 0.1049
Meta loss on this task batch = 4.8396e-01, Meta loss averaged over last 500 steps = 4.7235e-01, PNorm = 55.2141, GNorm = 0.1255
Meta loss on this task batch = 4.0200e-01, Meta loss averaged over last 500 steps = 4.7230e-01, PNorm = 55.2288, GNorm = 0.1085
Meta loss on this task batch = 4.8600e-01, Meta loss averaged over last 500 steps = 4.7225e-01, PNorm = 55.2420, GNorm = 0.1741
Meta loss on this task batch = 4.2749e-01, Meta loss averaged over last 500 steps = 4.7222e-01, PNorm = 55.2546, GNorm = 0.1140
Meta loss on this task batch = 4.2682e-01, Meta loss averaged over last 500 steps = 4.7213e-01, PNorm = 55.2673, GNorm = 0.1079
Meta loss on this task batch = 5.1775e-01, Meta loss averaged over last 500 steps = 4.7222e-01, PNorm = 55.2768, GNorm = 0.2199
Meta loss on this task batch = 3.9283e-01, Meta loss averaged over last 500 steps = 4.7201e-01, PNorm = 55.2871, GNorm = 0.1103
Meta loss on this task batch = 4.3762e-01, Meta loss averaged over last 500 steps = 4.7186e-01, PNorm = 55.2987, GNorm = 0.1283
Meta loss on this task batch = 4.5440e-01, Meta loss averaged over last 500 steps = 4.7189e-01, PNorm = 55.3097, GNorm = 0.1147
Meta loss on this task batch = 4.6795e-01, Meta loss averaged over last 500 steps = 4.7196e-01, PNorm = 55.3224, GNorm = 0.1230
Meta loss on this task batch = 4.6886e-01, Meta loss averaged over last 500 steps = 4.7190e-01, PNorm = 55.3345, GNorm = 0.1459
Meta loss on this task batch = 4.6979e-01, Meta loss averaged over last 500 steps = 4.7188e-01, PNorm = 55.3474, GNorm = 0.1136
Meta loss on this task batch = 4.4029e-01, Meta loss averaged over last 500 steps = 4.7170e-01, PNorm = 55.3610, GNorm = 0.1236
Meta loss on this task batch = 4.5609e-01, Meta loss averaged over last 500 steps = 4.7153e-01, PNorm = 55.3721, GNorm = 0.1396
Meta loss on this task batch = 5.2252e-01, Meta loss averaged over last 500 steps = 4.7159e-01, PNorm = 55.3826, GNorm = 0.1633
Meta loss on this task batch = 4.8872e-01, Meta loss averaged over last 500 steps = 4.7157e-01, PNorm = 55.3935, GNorm = 0.1720
Took 121.14846301078796 seconds to complete one epoch of meta training
Took 129.57627773284912 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.420903
Epoch 29
Meta loss on this task batch = 4.5091e-01, Meta loss averaged over last 500 steps = 4.7151e-01, PNorm = 55.4061, GNorm = 0.1371
Meta loss on this task batch = 5.3613e-01, Meta loss averaged over last 500 steps = 4.7157e-01, PNorm = 55.4187, GNorm = 0.1268
Meta loss on this task batch = 4.3583e-01, Meta loss averaged over last 500 steps = 4.7144e-01, PNorm = 55.4343, GNorm = 0.1160
Meta loss on this task batch = 4.9039e-01, Meta loss averaged over last 500 steps = 4.7151e-01, PNorm = 55.4489, GNorm = 0.1501
Meta loss on this task batch = 4.7886e-01, Meta loss averaged over last 500 steps = 4.7152e-01, PNorm = 55.4629, GNorm = 0.1138
Meta loss on this task batch = 4.1704e-01, Meta loss averaged over last 500 steps = 4.7130e-01, PNorm = 55.4780, GNorm = 0.1078
Meta loss on this task batch = 4.6284e-01, Meta loss averaged over last 500 steps = 4.7124e-01, PNorm = 55.4934, GNorm = 0.1475
Meta loss on this task batch = 4.6237e-01, Meta loss averaged over last 500 steps = 4.7118e-01, PNorm = 55.5092, GNorm = 0.1451
Meta loss on this task batch = 4.9506e-01, Meta loss averaged over last 500 steps = 4.7119e-01, PNorm = 55.5248, GNorm = 0.1497
Meta loss on this task batch = 4.3929e-01, Meta loss averaged over last 500 steps = 4.7111e-01, PNorm = 55.5398, GNorm = 0.1541
Meta loss on this task batch = 4.1180e-01, Meta loss averaged over last 500 steps = 4.7097e-01, PNorm = 55.5550, GNorm = 0.1416
Meta loss on this task batch = 4.3109e-01, Meta loss averaged over last 500 steps = 4.7085e-01, PNorm = 55.5692, GNorm = 0.1365
Meta loss on this task batch = 4.3802e-01, Meta loss averaged over last 500 steps = 4.7064e-01, PNorm = 55.5819, GNorm = 0.1514
Meta loss on this task batch = 4.7190e-01, Meta loss averaged over last 500 steps = 4.7063e-01, PNorm = 55.5941, GNorm = 0.2042
Meta loss on this task batch = 4.4062e-01, Meta loss averaged over last 500 steps = 4.7068e-01, PNorm = 55.6062, GNorm = 0.1634
Meta loss on this task batch = 4.2034e-01, Meta loss averaged over last 500 steps = 4.7059e-01, PNorm = 55.6168, GNorm = 0.1617
Meta loss on this task batch = 4.2251e-01, Meta loss averaged over last 500 steps = 4.7055e-01, PNorm = 55.6281, GNorm = 0.1068
Meta loss on this task batch = 4.6971e-01, Meta loss averaged over last 500 steps = 4.7034e-01, PNorm = 55.6389, GNorm = 0.1336
Meta loss on this task batch = 3.9238e-01, Meta loss averaged over last 500 steps = 4.7027e-01, PNorm = 55.6520, GNorm = 0.1274
Took 120.57800316810608 seconds to complete one epoch of meta training
Took 128.7181248664856 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.434394
Found better MAML checkpoint after meta validation, saving now
Epoch 30
Meta loss on this task batch = 4.3789e-01, Meta loss averaged over last 500 steps = 4.7019e-01, PNorm = 55.6651, GNorm = 0.1027
Meta loss on this task batch = 4.7619e-01, Meta loss averaged over last 500 steps = 4.7038e-01, PNorm = 55.6786, GNorm = 0.1234
Meta loss on this task batch = 5.0488e-01, Meta loss averaged over last 500 steps = 4.7050e-01, PNorm = 55.6920, GNorm = 0.1808
Meta loss on this task batch = 3.8280e-01, Meta loss averaged over last 500 steps = 4.7024e-01, PNorm = 55.7058, GNorm = 0.0974
Meta loss on this task batch = 4.9178e-01, Meta loss averaged over last 500 steps = 4.7010e-01, PNorm = 55.7185, GNorm = 0.1508
Meta loss on this task batch = 5.1620e-01, Meta loss averaged over last 500 steps = 4.6991e-01, PNorm = 55.7310, GNorm = 0.1260
Meta loss on this task batch = 4.1191e-01, Meta loss averaged over last 500 steps = 4.6980e-01, PNorm = 55.7447, GNorm = 0.1112
Meta loss on this task batch = 4.8893e-01, Meta loss averaged over last 500 steps = 4.6980e-01, PNorm = 55.7590, GNorm = 0.1224
Meta loss on this task batch = 4.4690e-01, Meta loss averaged over last 500 steps = 4.6973e-01, PNorm = 55.7728, GNorm = 0.1130
Meta loss on this task batch = 4.6177e-01, Meta loss averaged over last 500 steps = 4.6973e-01, PNorm = 55.7879, GNorm = 0.1166
Meta loss on this task batch = 3.2852e-01, Meta loss averaged over last 500 steps = 4.6940e-01, PNorm = 55.8050, GNorm = 0.1084
Meta loss on this task batch = 5.0000e-01, Meta loss averaged over last 500 steps = 4.6951e-01, PNorm = 55.8221, GNorm = 0.1164
Meta loss on this task batch = 4.9260e-01, Meta loss averaged over last 500 steps = 4.6961e-01, PNorm = 55.8354, GNorm = 0.1673
Meta loss on this task batch = 3.9187e-01, Meta loss averaged over last 500 steps = 4.6945e-01, PNorm = 55.8500, GNorm = 0.1111
Meta loss on this task batch = 4.0835e-01, Meta loss averaged over last 500 steps = 4.6945e-01, PNorm = 55.8641, GNorm = 0.1287
Meta loss on this task batch = 4.5941e-01, Meta loss averaged over last 500 steps = 4.6949e-01, PNorm = 55.8767, GNorm = 0.1106
Meta loss on this task batch = 4.5847e-01, Meta loss averaged over last 500 steps = 4.6949e-01, PNorm = 55.8876, GNorm = 0.1342
Meta loss on this task batch = 4.5613e-01, Meta loss averaged over last 500 steps = 4.6946e-01, PNorm = 55.8982, GNorm = 0.1568
Meta loss on this task batch = 4.7344e-01, Meta loss averaged over last 500 steps = 4.6954e-01, PNorm = 55.9077, GNorm = 0.1639
Took 121.52670931816101 seconds to complete one epoch of meta training
Took 129.50329160690308 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.426653
Epoch 31
Meta loss on this task batch = 4.5402e-01, Meta loss averaged over last 500 steps = 4.6935e-01, PNorm = 55.9168, GNorm = 0.1238
Meta loss on this task batch = 4.1977e-01, Meta loss averaged over last 500 steps = 4.6907e-01, PNorm = 55.9266, GNorm = 0.1261
Meta loss on this task batch = 4.9456e-01, Meta loss averaged over last 500 steps = 4.6894e-01, PNorm = 55.9377, GNorm = 0.1365
Meta loss on this task batch = 4.1619e-01, Meta loss averaged over last 500 steps = 4.6881e-01, PNorm = 55.9492, GNorm = 0.1289
Meta loss on this task batch = 4.8066e-01, Meta loss averaged over last 500 steps = 4.6873e-01, PNorm = 55.9610, GNorm = 0.1417
Meta loss on this task batch = 4.6256e-01, Meta loss averaged over last 500 steps = 4.6858e-01, PNorm = 55.9745, GNorm = 0.1203
Meta loss on this task batch = 4.7525e-01, Meta loss averaged over last 500 steps = 4.6857e-01, PNorm = 55.9870, GNorm = 0.1232
Meta loss on this task batch = 3.8369e-01, Meta loss averaged over last 500 steps = 4.6832e-01, PNorm = 56.0001, GNorm = 0.1426
Meta loss on this task batch = 4.4736e-01, Meta loss averaged over last 500 steps = 4.6819e-01, PNorm = 56.0134, GNorm = 0.1138
Meta loss on this task batch = 4.5417e-01, Meta loss averaged over last 500 steps = 4.6810e-01, PNorm = 56.0257, GNorm = 0.1410
Meta loss on this task batch = 4.8768e-01, Meta loss averaged over last 500 steps = 4.6813e-01, PNorm = 56.0359, GNorm = 0.2424
Meta loss on this task batch = 4.8956e-01, Meta loss averaged over last 500 steps = 4.6810e-01, PNorm = 56.0440, GNorm = 0.2621
Meta loss on this task batch = 4.2535e-01, Meta loss averaged over last 500 steps = 4.6803e-01, PNorm = 56.0540, GNorm = 0.1114
Meta loss on this task batch = 5.3585e-01, Meta loss averaged over last 500 steps = 4.6816e-01, PNorm = 56.0642, GNorm = 0.1419
Meta loss on this task batch = 3.7567e-01, Meta loss averaged over last 500 steps = 4.6791e-01, PNorm = 56.0755, GNorm = 0.0959
Meta loss on this task batch = 5.1355e-01, Meta loss averaged over last 500 steps = 4.6800e-01, PNorm = 56.0873, GNorm = 0.1493
Meta loss on this task batch = 4.4912e-01, Meta loss averaged over last 500 steps = 4.6780e-01, PNorm = 56.0991, GNorm = 0.1661
Meta loss on this task batch = 5.2548e-01, Meta loss averaged over last 500 steps = 4.6779e-01, PNorm = 56.1086, GNorm = 0.2266
Meta loss on this task batch = 4.1915e-01, Meta loss averaged over last 500 steps = 4.6763e-01, PNorm = 56.1188, GNorm = 0.1261
Took 121.66844439506531 seconds to complete one epoch of meta training
Took 129.65585780143738 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.443370
Found better MAML checkpoint after meta validation, saving now
Epoch 32
Meta loss on this task batch = 4.1860e-01, Meta loss averaged over last 500 steps = 4.6764e-01, PNorm = 56.1315, GNorm = 0.1196
Meta loss on this task batch = 4.8967e-01, Meta loss averaged over last 500 steps = 4.6762e-01, PNorm = 56.1461, GNorm = 0.1482
Meta loss on this task batch = 4.8113e-01, Meta loss averaged over last 500 steps = 4.6782e-01, PNorm = 56.1613, GNorm = 0.1160
Meta loss on this task batch = 4.8145e-01, Meta loss averaged over last 500 steps = 4.6783e-01, PNorm = 56.1766, GNorm = 0.1413
Meta loss on this task batch = 4.9128e-01, Meta loss averaged over last 500 steps = 4.6779e-01, PNorm = 56.1914, GNorm = 0.1179
Meta loss on this task batch = 4.9372e-01, Meta loss averaged over last 500 steps = 4.6792e-01, PNorm = 56.2056, GNorm = 0.1449
Meta loss on this task batch = 4.8998e-01, Meta loss averaged over last 500 steps = 4.6794e-01, PNorm = 56.2186, GNorm = 0.1307
Meta loss on this task batch = 4.1682e-01, Meta loss averaged over last 500 steps = 4.6778e-01, PNorm = 56.2325, GNorm = 0.1189
Meta loss on this task batch = 4.1710e-01, Meta loss averaged over last 500 steps = 4.6766e-01, PNorm = 56.2468, GNorm = 0.1144
Meta loss on this task batch = 4.5901e-01, Meta loss averaged over last 500 steps = 4.6762e-01, PNorm = 56.2609, GNorm = 0.1360
Meta loss on this task batch = 4.2638e-01, Meta loss averaged over last 500 steps = 4.6738e-01, PNorm = 56.2741, GNorm = 0.1608
Meta loss on this task batch = 4.5601e-01, Meta loss averaged over last 500 steps = 4.6738e-01, PNorm = 56.2880, GNorm = 0.1253
Meta loss on this task batch = 4.6775e-01, Meta loss averaged over last 500 steps = 4.6740e-01, PNorm = 56.3014, GNorm = 0.1756
Meta loss on this task batch = 4.2579e-01, Meta loss averaged over last 500 steps = 4.6728e-01, PNorm = 56.3146, GNorm = 0.1654
Meta loss on this task batch = 4.6922e-01, Meta loss averaged over last 500 steps = 4.6716e-01, PNorm = 56.3259, GNorm = 0.1346
Meta loss on this task batch = 4.8305e-01, Meta loss averaged over last 500 steps = 4.6715e-01, PNorm = 56.3381, GNorm = 0.1492
Meta loss on this task batch = 4.1439e-01, Meta loss averaged over last 500 steps = 4.6707e-01, PNorm = 56.3511, GNorm = 0.1227
Meta loss on this task batch = 4.4219e-01, Meta loss averaged over last 500 steps = 4.6697e-01, PNorm = 56.3647, GNorm = 0.1130
Meta loss on this task batch = 4.5825e-01, Meta loss averaged over last 500 steps = 4.6699e-01, PNorm = 56.3772, GNorm = 0.1385
Took 122.28365969657898 seconds to complete one epoch of meta training
Took 131.42236185073853 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.450132
Found better MAML checkpoint after meta validation, saving now
Epoch 33
Meta loss on this task batch = 4.9135e-01, Meta loss averaged over last 500 steps = 4.6692e-01, PNorm = 56.3890, GNorm = 0.1270
Meta loss on this task batch = 4.7048e-01, Meta loss averaged over last 500 steps = 4.6696e-01, PNorm = 56.4020, GNorm = 0.1245
Meta loss on this task batch = 4.5781e-01, Meta loss averaged over last 500 steps = 4.6697e-01, PNorm = 56.4155, GNorm = 0.1209
Meta loss on this task batch = 3.8833e-01, Meta loss averaged over last 500 steps = 4.6679e-01, PNorm = 56.4300, GNorm = 0.1151
Meta loss on this task batch = 4.7072e-01, Meta loss averaged over last 500 steps = 4.6669e-01, PNorm = 56.4425, GNorm = 0.1695
Meta loss on this task batch = 4.3402e-01, Meta loss averaged over last 500 steps = 4.6658e-01, PNorm = 56.4526, GNorm = 0.1389
Meta loss on this task batch = 5.1501e-01, Meta loss averaged over last 500 steps = 4.6667e-01, PNorm = 56.4618, GNorm = 0.1576
Meta loss on this task batch = 4.4017e-01, Meta loss averaged over last 500 steps = 4.6662e-01, PNorm = 56.4722, GNorm = 0.1381
Meta loss on this task batch = 3.8999e-01, Meta loss averaged over last 500 steps = 4.6638e-01, PNorm = 56.4853, GNorm = 0.1198
Meta loss on this task batch = 4.4340e-01, Meta loss averaged over last 500 steps = 4.6634e-01, PNorm = 56.4986, GNorm = 0.1202
Meta loss on this task batch = 4.7371e-01, Meta loss averaged over last 500 steps = 4.6639e-01, PNorm = 56.5107, GNorm = 0.1425
Meta loss on this task batch = 4.4045e-01, Meta loss averaged over last 500 steps = 4.6633e-01, PNorm = 56.5242, GNorm = 0.1526
Meta loss on this task batch = 4.7131e-01, Meta loss averaged over last 500 steps = 4.6626e-01, PNorm = 56.5378, GNorm = 0.1438
Meta loss on this task batch = 4.1170e-01, Meta loss averaged over last 500 steps = 4.6625e-01, PNorm = 56.5517, GNorm = 0.1002
Meta loss on this task batch = 4.4038e-01, Meta loss averaged over last 500 steps = 4.6628e-01, PNorm = 56.5663, GNorm = 0.1161
Meta loss on this task batch = 4.0290e-01, Meta loss averaged over last 500 steps = 4.6595e-01, PNorm = 56.5803, GNorm = 0.0998
Meta loss on this task batch = 5.3313e-01, Meta loss averaged over last 500 steps = 4.6605e-01, PNorm = 56.5924, GNorm = 0.2241
Meta loss on this task batch = 4.8260e-01, Meta loss averaged over last 500 steps = 4.6609e-01, PNorm = 56.6036, GNorm = 0.1622
Meta loss on this task batch = 4.3542e-01, Meta loss averaged over last 500 steps = 4.6588e-01, PNorm = 56.6142, GNorm = 0.1375
Took 118.9378273487091 seconds to complete one epoch of meta training
Took 128.57136011123657 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.413619
Epoch 34
Meta loss on this task batch = 4.3135e-01, Meta loss averaged over last 500 steps = 4.6597e-01, PNorm = 56.6256, GNorm = 0.1494
Meta loss on this task batch = 4.8407e-01, Meta loss averaged over last 500 steps = 4.6580e-01, PNorm = 56.6379, GNorm = 0.1485
Meta loss on this task batch = 5.2516e-01, Meta loss averaged over last 500 steps = 4.6591e-01, PNorm = 56.6503, GNorm = 0.1147
Meta loss on this task batch = 4.5369e-01, Meta loss averaged over last 500 steps = 4.6576e-01, PNorm = 56.6630, GNorm = 0.1256
Meta loss on this task batch = 4.6100e-01, Meta loss averaged over last 500 steps = 4.6580e-01, PNorm = 56.6758, GNorm = 0.1270
Meta loss on this task batch = 4.7704e-01, Meta loss averaged over last 500 steps = 4.6572e-01, PNorm = 56.6880, GNorm = 0.1433
Meta loss on this task batch = 4.5231e-01, Meta loss averaged over last 500 steps = 4.6570e-01, PNorm = 56.7004, GNorm = 0.1257
Meta loss on this task batch = 4.3955e-01, Meta loss averaged over last 500 steps = 4.6550e-01, PNorm = 56.7146, GNorm = 0.1235
Meta loss on this task batch = 4.7060e-01, Meta loss averaged over last 500 steps = 4.6549e-01, PNorm = 56.7283, GNorm = 0.1194
Meta loss on this task batch = 4.2804e-01, Meta loss averaged over last 500 steps = 4.6546e-01, PNorm = 56.7437, GNorm = 0.1466
Meta loss on this task batch = 4.8562e-01, Meta loss averaged over last 500 steps = 4.6540e-01, PNorm = 56.7569, GNorm = 0.1723
Meta loss on this task batch = 4.4600e-01, Meta loss averaged over last 500 steps = 4.6535e-01, PNorm = 56.7709, GNorm = 0.1359
Meta loss on this task batch = 4.6926e-01, Meta loss averaged over last 500 steps = 4.6541e-01, PNorm = 56.7831, GNorm = 0.1516
Meta loss on this task batch = 4.0860e-01, Meta loss averaged over last 500 steps = 4.6513e-01, PNorm = 56.7950, GNorm = 0.1158
Meta loss on this task batch = 4.1265e-01, Meta loss averaged over last 500 steps = 4.6496e-01, PNorm = 56.8077, GNorm = 0.1335
Meta loss on this task batch = 3.7344e-01, Meta loss averaged over last 500 steps = 4.6471e-01, PNorm = 56.8216, GNorm = 0.1269
Meta loss on this task batch = 4.6226e-01, Meta loss averaged over last 500 steps = 4.6463e-01, PNorm = 56.8340, GNorm = 0.1149
Meta loss on this task batch = 4.2595e-01, Meta loss averaged over last 500 steps = 4.6465e-01, PNorm = 56.8464, GNorm = 0.1199
Meta loss on this task batch = 4.6648e-01, Meta loss averaged over last 500 steps = 4.6456e-01, PNorm = 56.8584, GNorm = 0.1614
Took 122.55597758293152 seconds to complete one epoch of meta training
Took 129.56100296974182 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.440222
Epoch 35
Meta loss on this task batch = 4.4417e-01, Meta loss averaged over last 500 steps = 4.6462e-01, PNorm = 56.8700, GNorm = 0.1507
Meta loss on this task batch = 4.9133e-01, Meta loss averaged over last 500 steps = 4.6458e-01, PNorm = 56.8826, GNorm = 0.1573
Meta loss on this task batch = 4.5642e-01, Meta loss averaged over last 500 steps = 4.6460e-01, PNorm = 56.8933, GNorm = 0.1424
Meta loss on this task batch = 4.6190e-01, Meta loss averaged over last 500 steps = 4.6459e-01, PNorm = 56.9035, GNorm = 0.1415
Meta loss on this task batch = 4.0176e-01, Meta loss averaged over last 500 steps = 4.6434e-01, PNorm = 56.9149, GNorm = 0.1021
Meta loss on this task batch = 4.2506e-01, Meta loss averaged over last 500 steps = 4.6430e-01, PNorm = 56.9275, GNorm = 0.1188
Meta loss on this task batch = 4.4002e-01, Meta loss averaged over last 500 steps = 4.6424e-01, PNorm = 56.9418, GNorm = 0.1358
Meta loss on this task batch = 4.2318e-01, Meta loss averaged over last 500 steps = 4.6406e-01, PNorm = 56.9572, GNorm = 0.1356
Meta loss on this task batch = 4.7674e-01, Meta loss averaged over last 500 steps = 4.6401e-01, PNorm = 56.9727, GNorm = 0.1297
Meta loss on this task batch = 4.3568e-01, Meta loss averaged over last 500 steps = 4.6392e-01, PNorm = 56.9883, GNorm = 0.1197
Meta loss on this task batch = 4.7629e-01, Meta loss averaged over last 500 steps = 4.6387e-01, PNorm = 57.0033, GNorm = 0.1122
Meta loss on this task batch = 4.1676e-01, Meta loss averaged over last 500 steps = 4.6370e-01, PNorm = 57.0169, GNorm = 0.1732
Meta loss on this task batch = 4.7097e-01, Meta loss averaged over last 500 steps = 4.6374e-01, PNorm = 57.0292, GNorm = 0.1589
Meta loss on this task batch = 4.0937e-01, Meta loss averaged over last 500 steps = 4.6353e-01, PNorm = 57.0407, GNorm = 0.1256
Meta loss on this task batch = 4.4911e-01, Meta loss averaged over last 500 steps = 4.6360e-01, PNorm = 57.0514, GNorm = 0.1320
Meta loss on this task batch = 4.2314e-01, Meta loss averaged over last 500 steps = 4.6339e-01, PNorm = 57.0628, GNorm = 0.1192
Meta loss on this task batch = 4.6267e-01, Meta loss averaged over last 500 steps = 4.6327e-01, PNorm = 57.0747, GNorm = 0.1272
Meta loss on this task batch = 4.2374e-01, Meta loss averaged over last 500 steps = 4.6319e-01, PNorm = 57.0859, GNorm = 0.1160
Meta loss on this task batch = 4.7999e-01, Meta loss averaged over last 500 steps = 4.6320e-01, PNorm = 57.0968, GNorm = 0.1465
Took 124.29959726333618 seconds to complete one epoch of meta training
Took 131.04799962043762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.428531
Epoch 36
Meta loss on this task batch = 4.4151e-01, Meta loss averaged over last 500 steps = 4.6322e-01, PNorm = 57.1086, GNorm = 0.1512
Meta loss on this task batch = 4.4291e-01, Meta loss averaged over last 500 steps = 4.6322e-01, PNorm = 57.1219, GNorm = 0.1160
Meta loss on this task batch = 4.0391e-01, Meta loss averaged over last 500 steps = 4.6305e-01, PNorm = 57.1355, GNorm = 0.1182
Meta loss on this task batch = 4.4729e-01, Meta loss averaged over last 500 steps = 4.6298e-01, PNorm = 57.1487, GNorm = 0.1109
Meta loss on this task batch = 4.2456e-01, Meta loss averaged over last 500 steps = 4.6295e-01, PNorm = 57.1632, GNorm = 0.1629
Meta loss on this task batch = 4.6401e-01, Meta loss averaged over last 500 steps = 4.6299e-01, PNorm = 57.1784, GNorm = 0.1630
Meta loss on this task batch = 4.6112e-01, Meta loss averaged over last 500 steps = 4.6299e-01, PNorm = 57.1940, GNorm = 0.1468
Meta loss on this task batch = 4.2058e-01, Meta loss averaged over last 500 steps = 4.6278e-01, PNorm = 57.2088, GNorm = 0.1788
Meta loss on this task batch = 4.5034e-01, Meta loss averaged over last 500 steps = 4.6280e-01, PNorm = 57.2235, GNorm = 0.1471
Meta loss on this task batch = 5.1768e-01, Meta loss averaged over last 500 steps = 4.6294e-01, PNorm = 57.2330, GNorm = 0.2755
Meta loss on this task batch = 4.9191e-01, Meta loss averaged over last 500 steps = 4.6292e-01, PNorm = 57.2415, GNorm = 0.1615
Meta loss on this task batch = 4.9883e-01, Meta loss averaged over last 500 steps = 4.6297e-01, PNorm = 57.2494, GNorm = 0.1364
Meta loss on this task batch = 3.8245e-01, Meta loss averaged over last 500 steps = 4.6292e-01, PNorm = 57.2596, GNorm = 0.1138
Meta loss on this task batch = 4.6892e-01, Meta loss averaged over last 500 steps = 4.6288e-01, PNorm = 57.2715, GNorm = 0.1336
Meta loss on this task batch = 4.1245e-01, Meta loss averaged over last 500 steps = 4.6278e-01, PNorm = 57.2852, GNorm = 0.1515
Meta loss on this task batch = 4.6309e-01, Meta loss averaged over last 500 steps = 4.6279e-01, PNorm = 57.2999, GNorm = 0.1061
Meta loss on this task batch = 4.6388e-01, Meta loss averaged over last 500 steps = 4.6269e-01, PNorm = 57.3157, GNorm = 0.1534
Meta loss on this task batch = 4.4937e-01, Meta loss averaged over last 500 steps = 4.6277e-01, PNorm = 57.3307, GNorm = 0.1133
Meta loss on this task batch = 3.6154e-01, Meta loss averaged over last 500 steps = 4.6246e-01, PNorm = 57.3448, GNorm = 0.1145
Took 123.13123345375061 seconds to complete one epoch of meta training
Took 131.19398307800293 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455484
Found better MAML checkpoint after meta validation, saving now
Epoch 37
Meta loss on this task batch = 4.1276e-01, Meta loss averaged over last 500 steps = 4.6227e-01, PNorm = 57.3602, GNorm = 0.1283
Meta loss on this task batch = 4.1958e-01, Meta loss averaged over last 500 steps = 4.6226e-01, PNorm = 57.3754, GNorm = 0.1092
Meta loss on this task batch = 4.5567e-01, Meta loss averaged over last 500 steps = 4.6208e-01, PNorm = 57.3896, GNorm = 0.1491
Meta loss on this task batch = 4.4278e-01, Meta loss averaged over last 500 steps = 4.6201e-01, PNorm = 57.4001, GNorm = 0.2219
Meta loss on this task batch = 4.3085e-01, Meta loss averaged over last 500 steps = 4.6188e-01, PNorm = 57.4106, GNorm = 0.1377
Meta loss on this task batch = 4.0021e-01, Meta loss averaged over last 500 steps = 4.6165e-01, PNorm = 57.4205, GNorm = 0.1446
Meta loss on this task batch = 4.7297e-01, Meta loss averaged over last 500 steps = 4.6166e-01, PNorm = 57.4307, GNorm = 0.1339
Meta loss on this task batch = 4.2615e-01, Meta loss averaged over last 500 steps = 4.6155e-01, PNorm = 57.4409, GNorm = 0.1327
Meta loss on this task batch = 3.9983e-01, Meta loss averaged over last 500 steps = 4.6144e-01, PNorm = 57.4529, GNorm = 0.1236
Meta loss on this task batch = 4.3662e-01, Meta loss averaged over last 500 steps = 4.6131e-01, PNorm = 57.4649, GNorm = 0.1318
Meta loss on this task batch = 3.5971e-01, Meta loss averaged over last 500 steps = 4.6098e-01, PNorm = 57.4777, GNorm = 0.1396
Meta loss on this task batch = 5.1785e-01, Meta loss averaged over last 500 steps = 4.6117e-01, PNorm = 57.4899, GNorm = 0.1482
Meta loss on this task batch = 5.3093e-01, Meta loss averaged over last 500 steps = 4.6134e-01, PNorm = 57.5017, GNorm = 0.1910
Meta loss on this task batch = 4.7903e-01, Meta loss averaged over last 500 steps = 4.6127e-01, PNorm = 57.5116, GNorm = 0.1498
Meta loss on this task batch = 4.5791e-01, Meta loss averaged over last 500 steps = 4.6130e-01, PNorm = 57.5217, GNorm = 0.1234
Meta loss on this task batch = 4.2165e-01, Meta loss averaged over last 500 steps = 4.6113e-01, PNorm = 57.5325, GNorm = 0.1197
Meta loss on this task batch = 4.8881e-01, Meta loss averaged over last 500 steps = 4.6122e-01, PNorm = 57.5434, GNorm = 0.1384
Meta loss on this task batch = 4.5257e-01, Meta loss averaged over last 500 steps = 4.6109e-01, PNorm = 57.5519, GNorm = 0.2153
Meta loss on this task batch = 5.0143e-01, Meta loss averaged over last 500 steps = 4.6116e-01, PNorm = 57.5622, GNorm = 0.1451
Took 121.71635913848877 seconds to complete one epoch of meta training
Took 129.4146523475647 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.444264
Epoch 38
Meta loss on this task batch = 5.0981e-01, Meta loss averaged over last 500 steps = 4.6118e-01, PNorm = 57.5728, GNorm = 0.1377
Meta loss on this task batch = 4.2612e-01, Meta loss averaged over last 500 steps = 4.6109e-01, PNorm = 57.5846, GNorm = 0.1102
Meta loss on this task batch = 4.5556e-01, Meta loss averaged over last 500 steps = 4.6111e-01, PNorm = 57.5969, GNorm = 0.1580
Meta loss on this task batch = 4.6552e-01, Meta loss averaged over last 500 steps = 4.6110e-01, PNorm = 57.6099, GNorm = 0.1218
Meta loss on this task batch = 4.2453e-01, Meta loss averaged over last 500 steps = 4.6105e-01, PNorm = 57.6241, GNorm = 0.1290
Meta loss on this task batch = 4.4953e-01, Meta loss averaged over last 500 steps = 4.6101e-01, PNorm = 57.6394, GNorm = 0.1433
Meta loss on this task batch = 4.3965e-01, Meta loss averaged over last 500 steps = 4.6089e-01, PNorm = 57.6535, GNorm = 0.1389
Meta loss on this task batch = 4.7368e-01, Meta loss averaged over last 500 steps = 4.6082e-01, PNorm = 57.6679, GNorm = 0.1556
Meta loss on this task batch = 5.0162e-01, Meta loss averaged over last 500 steps = 4.6085e-01, PNorm = 57.6810, GNorm = 0.1826
Meta loss on this task batch = 5.0216e-01, Meta loss averaged over last 500 steps = 4.6090e-01, PNorm = 57.6917, GNorm = 0.1843
Meta loss on this task batch = 3.9911e-01, Meta loss averaged over last 500 steps = 4.6086e-01, PNorm = 57.7027, GNorm = 0.1369
Meta loss on this task batch = 4.2632e-01, Meta loss averaged over last 500 steps = 4.6062e-01, PNorm = 57.7145, GNorm = 0.1330
Meta loss on this task batch = 4.5440e-01, Meta loss averaged over last 500 steps = 4.6044e-01, PNorm = 57.7263, GNorm = 0.1005
Meta loss on this task batch = 4.0332e-01, Meta loss averaged over last 500 steps = 4.6019e-01, PNorm = 57.7412, GNorm = 0.1383
Meta loss on this task batch = 4.8488e-01, Meta loss averaged over last 500 steps = 4.6033e-01, PNorm = 57.7570, GNorm = 0.1230
Meta loss on this task batch = 4.4990e-01, Meta loss averaged over last 500 steps = 4.6040e-01, PNorm = 57.7721, GNorm = 0.1648
Meta loss on this task batch = 4.4393e-01, Meta loss averaged over last 500 steps = 4.6037e-01, PNorm = 57.7875, GNorm = 0.1405
Meta loss on this task batch = 4.4915e-01, Meta loss averaged over last 500 steps = 4.6035e-01, PNorm = 57.8037, GNorm = 0.1350
Meta loss on this task batch = 4.4904e-01, Meta loss averaged over last 500 steps = 4.6029e-01, PNorm = 57.8191, GNorm = 0.1493
Took 121.75831437110901 seconds to complete one epoch of meta training
Took 128.61873722076416 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.434937
Epoch 39
Meta loss on this task batch = 4.2213e-01, Meta loss averaged over last 500 steps = 4.6020e-01, PNorm = 57.8336, GNorm = 0.1492
Meta loss on this task batch = 4.8554e-01, Meta loss averaged over last 500 steps = 4.6046e-01, PNorm = 57.8450, GNorm = 0.2349
Meta loss on this task batch = 4.2098e-01, Meta loss averaged over last 500 steps = 4.6020e-01, PNorm = 57.8549, GNorm = 0.2044
Meta loss on this task batch = 4.7020e-01, Meta loss averaged over last 500 steps = 4.6014e-01, PNorm = 57.8638, GNorm = 0.1514
Meta loss on this task batch = 4.4780e-01, Meta loss averaged over last 500 steps = 4.6011e-01, PNorm = 57.8715, GNorm = 0.1661
Meta loss on this task batch = 4.9642e-01, Meta loss averaged over last 500 steps = 4.6029e-01, PNorm = 57.8794, GNorm = 0.1725
Meta loss on this task batch = 4.3965e-01, Meta loss averaged over last 500 steps = 4.6028e-01, PNorm = 57.8891, GNorm = 0.1313
Meta loss on this task batch = 4.6224e-01, Meta loss averaged over last 500 steps = 4.6022e-01, PNorm = 57.8988, GNorm = 0.1438
Meta loss on this task batch = 4.3916e-01, Meta loss averaged over last 500 steps = 4.6023e-01, PNorm = 57.9113, GNorm = 0.1457
Meta loss on this task batch = 5.1746e-01, Meta loss averaged over last 500 steps = 4.6031e-01, PNorm = 57.9233, GNorm = 0.1819
Meta loss on this task batch = 3.5084e-01, Meta loss averaged over last 500 steps = 4.6007e-01, PNorm = 57.9377, GNorm = 0.1280
Meta loss on this task batch = 4.8746e-01, Meta loss averaged over last 500 steps = 4.6005e-01, PNorm = 57.9496, GNorm = 0.1851
Meta loss on this task batch = 4.2689e-01, Meta loss averaged over last 500 steps = 4.6003e-01, PNorm = 57.9612, GNorm = 0.1206
Meta loss on this task batch = 4.0812e-01, Meta loss averaged over last 500 steps = 4.5994e-01, PNorm = 57.9739, GNorm = 0.1195
Meta loss on this task batch = 4.5818e-01, Meta loss averaged over last 500 steps = 4.5991e-01, PNorm = 57.9878, GNorm = 0.1267
Meta loss on this task batch = 4.6553e-01, Meta loss averaged over last 500 steps = 4.5984e-01, PNorm = 58.0023, GNorm = 0.1373
Meta loss on this task batch = 4.8958e-01, Meta loss averaged over last 500 steps = 4.5979e-01, PNorm = 58.0134, GNorm = 0.2024
Meta loss on this task batch = 4.3608e-01, Meta loss averaged over last 500 steps = 4.5968e-01, PNorm = 58.0228, GNorm = 0.1931
Meta loss on this task batch = 4.5843e-01, Meta loss averaged over last 500 steps = 4.5965e-01, PNorm = 58.0333, GNorm = 0.1691
Took 121.69312000274658 seconds to complete one epoch of meta training
Took 129.14583826065063 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.446046
Epoch 40
Meta loss on this task batch = 4.2134e-01, Meta loss averaged over last 500 steps = 4.5948e-01, PNorm = 58.0445, GNorm = 0.1127
Meta loss on this task batch = 4.5117e-01, Meta loss averaged over last 500 steps = 4.5947e-01, PNorm = 58.0569, GNorm = 0.1316
Meta loss on this task batch = 4.6241e-01, Meta loss averaged over last 500 steps = 4.5951e-01, PNorm = 58.0703, GNorm = 0.1161
Meta loss on this task batch = 4.6487e-01, Meta loss averaged over last 500 steps = 4.5939e-01, PNorm = 58.0837, GNorm = 0.1191
Meta loss on this task batch = 4.2605e-01, Meta loss averaged over last 500 steps = 4.5923e-01, PNorm = 58.0976, GNorm = 0.1244
Meta loss on this task batch = 4.7994e-01, Meta loss averaged over last 500 steps = 4.5930e-01, PNorm = 58.1129, GNorm = 0.1424
Meta loss on this task batch = 4.3787e-01, Meta loss averaged over last 500 steps = 4.5920e-01, PNorm = 58.1291, GNorm = 0.1194
Meta loss on this task batch = 4.1469e-01, Meta loss averaged over last 500 steps = 4.5901e-01, PNorm = 58.1463, GNorm = 0.1211
Meta loss on this task batch = 4.6790e-01, Meta loss averaged over last 500 steps = 4.5907e-01, PNorm = 58.1634, GNorm = 0.1255
Meta loss on this task batch = 4.8541e-01, Meta loss averaged over last 500 steps = 4.5921e-01, PNorm = 58.1796, GNorm = 0.1725
Meta loss on this task batch = 4.3529e-01, Meta loss averaged over last 500 steps = 4.5923e-01, PNorm = 58.1957, GNorm = 0.1599
Meta loss on this task batch = 4.4863e-01, Meta loss averaged over last 500 steps = 4.5922e-01, PNorm = 58.2092, GNorm = 0.1545
Meta loss on this task batch = 4.4086e-01, Meta loss averaged over last 500 steps = 4.5911e-01, PNorm = 58.2206, GNorm = 0.1639
Meta loss on this task batch = 4.0577e-01, Meta loss averaged over last 500 steps = 4.5894e-01, PNorm = 58.2317, GNorm = 0.1566
Meta loss on this task batch = 4.5325e-01, Meta loss averaged over last 500 steps = 4.5896e-01, PNorm = 58.2416, GNorm = 0.1376
Meta loss on this task batch = 4.1324e-01, Meta loss averaged over last 500 steps = 4.5874e-01, PNorm = 58.2524, GNorm = 0.1553
Meta loss on this task batch = 4.4690e-01, Meta loss averaged over last 500 steps = 4.5872e-01, PNorm = 58.2626, GNorm = 0.1432
Meta loss on this task batch = 4.3904e-01, Meta loss averaged over last 500 steps = 4.5875e-01, PNorm = 58.2730, GNorm = 0.1805
Meta loss on this task batch = 4.5570e-01, Meta loss averaged over last 500 steps = 4.5875e-01, PNorm = 58.2828, GNorm = 0.2110
Took 119.28266072273254 seconds to complete one epoch of meta training
Took 127.59559941291809 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447927
Epoch 41
Meta loss on this task batch = 4.5130e-01, Meta loss averaged over last 500 steps = 4.5867e-01, PNorm = 58.2936, GNorm = 0.1389
Meta loss on this task batch = 4.6735e-01, Meta loss averaged over last 500 steps = 4.5864e-01, PNorm = 58.3050, GNorm = 0.1321
Meta loss on this task batch = 4.6150e-01, Meta loss averaged over last 500 steps = 4.5852e-01, PNorm = 58.3187, GNorm = 0.1446
Meta loss on this task batch = 3.6861e-01, Meta loss averaged over last 500 steps = 4.5834e-01, PNorm = 58.3336, GNorm = 0.1169
Meta loss on this task batch = 4.4075e-01, Meta loss averaged over last 500 steps = 4.5833e-01, PNorm = 58.3491, GNorm = 0.1610
Meta loss on this task batch = 4.1593e-01, Meta loss averaged over last 500 steps = 4.5815e-01, PNorm = 58.3645, GNorm = 0.1313
Meta loss on this task batch = 4.5592e-01, Meta loss averaged over last 500 steps = 4.5814e-01, PNorm = 58.3787, GNorm = 0.1397
Meta loss on this task batch = 4.9317e-01, Meta loss averaged over last 500 steps = 4.5815e-01, PNorm = 58.3930, GNorm = 0.1566
Meta loss on this task batch = 4.4217e-01, Meta loss averaged over last 500 steps = 4.5813e-01, PNorm = 58.4046, GNorm = 0.2220
Meta loss on this task batch = 3.9819e-01, Meta loss averaged over last 500 steps = 4.5808e-01, PNorm = 58.4168, GNorm = 0.1155
Meta loss on this task batch = 4.5649e-01, Meta loss averaged over last 500 steps = 4.5800e-01, PNorm = 58.4296, GNorm = 0.1516
Meta loss on this task batch = 4.7042e-01, Meta loss averaged over last 500 steps = 4.5798e-01, PNorm = 58.4420, GNorm = 0.1631
Meta loss on this task batch = 4.1760e-01, Meta loss averaged over last 500 steps = 4.5779e-01, PNorm = 58.4545, GNorm = 0.1357
Meta loss on this task batch = 4.7429e-01, Meta loss averaged over last 500 steps = 4.5774e-01, PNorm = 58.4669, GNorm = 0.1407
Meta loss on this task batch = 5.0287e-01, Meta loss averaged over last 500 steps = 4.5777e-01, PNorm = 58.4805, GNorm = 0.1368
Meta loss on this task batch = 4.2299e-01, Meta loss averaged over last 500 steps = 4.5773e-01, PNorm = 58.4934, GNorm = 0.1639
Meta loss on this task batch = 4.0560e-01, Meta loss averaged over last 500 steps = 4.5764e-01, PNorm = 58.5073, GNorm = 0.1136
Meta loss on this task batch = 3.9823e-01, Meta loss averaged over last 500 steps = 4.5746e-01, PNorm = 58.5211, GNorm = 0.1505
Meta loss on this task batch = 5.2842e-01, Meta loss averaged over last 500 steps = 4.5771e-01, PNorm = 58.5332, GNorm = 0.2047
Took 128.12926316261292 seconds to complete one epoch of meta training
Took 136.52393984794617 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463152
Found better MAML checkpoint after meta validation, saving now
Epoch 42
Meta loss on this task batch = 4.0586e-01, Meta loss averaged over last 500 steps = 4.5756e-01, PNorm = 58.5473, GNorm = 0.1347
Meta loss on this task batch = 4.8488e-01, Meta loss averaged over last 500 steps = 4.5765e-01, PNorm = 58.5612, GNorm = 0.1604
Meta loss on this task batch = 4.8107e-01, Meta loss averaged over last 500 steps = 4.5774e-01, PNorm = 58.5749, GNorm = 0.1478
Meta loss on this task batch = 3.6455e-01, Meta loss averaged over last 500 steps = 4.5757e-01, PNorm = 58.5884, GNorm = 0.1314
Meta loss on this task batch = 3.6724e-01, Meta loss averaged over last 500 steps = 4.5731e-01, PNorm = 58.6027, GNorm = 0.1196
Meta loss on this task batch = 4.4802e-01, Meta loss averaged over last 500 steps = 4.5707e-01, PNorm = 58.6163, GNorm = 0.1284
Meta loss on this task batch = 3.7226e-01, Meta loss averaged over last 500 steps = 4.5690e-01, PNorm = 58.6295, GNorm = 0.1161
Meta loss on this task batch = 4.5319e-01, Meta loss averaged over last 500 steps = 4.5696e-01, PNorm = 58.6425, GNorm = 0.1403
Meta loss on this task batch = 4.7062e-01, Meta loss averaged over last 500 steps = 4.5703e-01, PNorm = 58.6536, GNorm = 0.1817
Meta loss on this task batch = 4.5830e-01, Meta loss averaged over last 500 steps = 4.5704e-01, PNorm = 58.6641, GNorm = 0.1610
Meta loss on this task batch = 4.1897e-01, Meta loss averaged over last 500 steps = 4.5694e-01, PNorm = 58.6744, GNorm = 0.1288
Meta loss on this task batch = 4.9097e-01, Meta loss averaged over last 500 steps = 4.5700e-01, PNorm = 58.6842, GNorm = 0.1596
Meta loss on this task batch = 4.2847e-01, Meta loss averaged over last 500 steps = 4.5683e-01, PNorm = 58.6946, GNorm = 0.1230
Meta loss on this task batch = 3.6507e-01, Meta loss averaged over last 500 steps = 4.5661e-01, PNorm = 58.7063, GNorm = 0.1039
Meta loss on this task batch = 4.6183e-01, Meta loss averaged over last 500 steps = 4.5659e-01, PNorm = 58.7179, GNorm = 0.1493
Meta loss on this task batch = 4.7864e-01, Meta loss averaged over last 500 steps = 4.5652e-01, PNorm = 58.7294, GNorm = 0.1232
Meta loss on this task batch = 4.6725e-01, Meta loss averaged over last 500 steps = 4.5647e-01, PNorm = 58.7414, GNorm = 0.1340
Meta loss on this task batch = 5.3345e-01, Meta loss averaged over last 500 steps = 4.5665e-01, PNorm = 58.7541, GNorm = 0.1815
Meta loss on this task batch = 4.3472e-01, Meta loss averaged over last 500 steps = 4.5651e-01, PNorm = 58.7675, GNorm = 0.1202
Took 121.95216369628906 seconds to complete one epoch of meta training
Took 130.05511283874512 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467099
Found better MAML checkpoint after meta validation, saving now
Epoch 43
Meta loss on this task batch = 4.1849e-01, Meta loss averaged over last 500 steps = 4.5638e-01, PNorm = 58.7811, GNorm = 0.1209
Meta loss on this task batch = 4.3506e-01, Meta loss averaged over last 500 steps = 4.5624e-01, PNorm = 58.7960, GNorm = 0.1353
Meta loss on this task batch = 4.5948e-01, Meta loss averaged over last 500 steps = 4.5639e-01, PNorm = 58.8114, GNorm = 0.1642
Meta loss on this task batch = 4.3112e-01, Meta loss averaged over last 500 steps = 4.5624e-01, PNorm = 58.8272, GNorm = 0.1523
Meta loss on this task batch = 4.7882e-01, Meta loss averaged over last 500 steps = 4.5625e-01, PNorm = 58.8416, GNorm = 0.1687
Meta loss on this task batch = 4.4161e-01, Meta loss averaged over last 500 steps = 4.5619e-01, PNorm = 58.8556, GNorm = 0.1638
Meta loss on this task batch = 3.8699e-01, Meta loss averaged over last 500 steps = 4.5615e-01, PNorm = 58.8689, GNorm = 0.1703
Meta loss on this task batch = 4.8667e-01, Meta loss averaged over last 500 steps = 4.5620e-01, PNorm = 58.8798, GNorm = 0.1785
Meta loss on this task batch = 4.4989e-01, Meta loss averaged over last 500 steps = 4.5608e-01, PNorm = 58.8898, GNorm = 0.2139
Meta loss on this task batch = 4.2660e-01, Meta loss averaged over last 500 steps = 4.5603e-01, PNorm = 58.9004, GNorm = 0.1244
Meta loss on this task batch = 4.4516e-01, Meta loss averaged over last 500 steps = 4.5595e-01, PNorm = 58.9101, GNorm = 0.1377
Meta loss on this task batch = 4.0924e-01, Meta loss averaged over last 500 steps = 4.5590e-01, PNorm = 58.9181, GNorm = 0.1454
Meta loss on this task batch = 5.0013e-01, Meta loss averaged over last 500 steps = 4.5587e-01, PNorm = 58.9232, GNorm = 0.2545
Meta loss on this task batch = 4.2963e-01, Meta loss averaged over last 500 steps = 4.5576e-01, PNorm = 58.9304, GNorm = 0.1507
Meta loss on this task batch = 4.1436e-01, Meta loss averaged over last 500 steps = 4.5560e-01, PNorm = 58.9406, GNorm = 0.1486
Meta loss on this task batch = 4.3933e-01, Meta loss averaged over last 500 steps = 4.5540e-01, PNorm = 58.9517, GNorm = 0.1501
Meta loss on this task batch = 4.7500e-01, Meta loss averaged over last 500 steps = 4.5553e-01, PNorm = 58.9645, GNorm = 0.2044
Meta loss on this task batch = 4.1944e-01, Meta loss averaged over last 500 steps = 4.5543e-01, PNorm = 58.9788, GNorm = 0.1562
Meta loss on this task batch = 4.8267e-01, Meta loss averaged over last 500 steps = 4.5564e-01, PNorm = 58.9934, GNorm = 0.1551
Took 123.38384485244751 seconds to complete one epoch of meta training
Took 131.1694529056549 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467426
Found better MAML checkpoint after meta validation, saving now
Epoch 44
Meta loss on this task batch = 4.0457e-01, Meta loss averaged over last 500 steps = 4.5548e-01, PNorm = 59.0096, GNorm = 0.1663
Meta loss on this task batch = 4.4759e-01, Meta loss averaged over last 500 steps = 4.5539e-01, PNorm = 59.0241, GNorm = 0.1525
Meta loss on this task batch = 3.6087e-01, Meta loss averaged over last 500 steps = 4.5532e-01, PNorm = 59.0377, GNorm = 0.1188
Meta loss on this task batch = 4.4679e-01, Meta loss averaged over last 500 steps = 4.5520e-01, PNorm = 59.0487, GNorm = 0.1853
Meta loss on this task batch = 4.1177e-01, Meta loss averaged over last 500 steps = 4.5502e-01, PNorm = 59.0592, GNorm = 0.1406
Meta loss on this task batch = 3.9189e-01, Meta loss averaged over last 500 steps = 4.5478e-01, PNorm = 59.0706, GNorm = 0.1328
Meta loss on this task batch = 4.1835e-01, Meta loss averaged over last 500 steps = 4.5467e-01, PNorm = 59.0809, GNorm = 0.1385
Meta loss on this task batch = 4.5016e-01, Meta loss averaged over last 500 steps = 4.5464e-01, PNorm = 59.0904, GNorm = 0.1338
Meta loss on this task batch = 3.9305e-01, Meta loss averaged over last 500 steps = 4.5447e-01, PNorm = 59.1002, GNorm = 0.1212
Meta loss on this task batch = 4.6257e-01, Meta loss averaged over last 500 steps = 4.5450e-01, PNorm = 59.1104, GNorm = 0.1672
Meta loss on this task batch = 4.8726e-01, Meta loss averaged over last 500 steps = 4.5441e-01, PNorm = 59.1202, GNorm = 0.1657
Meta loss on this task batch = 4.6146e-01, Meta loss averaged over last 500 steps = 4.5437e-01, PNorm = 59.1318, GNorm = 0.1701
Meta loss on this task batch = 4.0548e-01, Meta loss averaged over last 500 steps = 4.5416e-01, PNorm = 59.1455, GNorm = 0.1736
Meta loss on this task batch = 4.5309e-01, Meta loss averaged over last 500 steps = 4.5424e-01, PNorm = 59.1595, GNorm = 0.1562
Meta loss on this task batch = 4.9855e-01, Meta loss averaged over last 500 steps = 4.5427e-01, PNorm = 59.1725, GNorm = 0.1281
Meta loss on this task batch = 4.4634e-01, Meta loss averaged over last 500 steps = 4.5406e-01, PNorm = 59.1857, GNorm = 0.1693
Meta loss on this task batch = 5.1233e-01, Meta loss averaged over last 500 steps = 4.5426e-01, PNorm = 59.1974, GNorm = 0.1889
Meta loss on this task batch = 5.0507e-01, Meta loss averaged over last 500 steps = 4.5432e-01, PNorm = 59.2083, GNorm = 0.1777
Meta loss on this task batch = 4.3871e-01, Meta loss averaged over last 500 steps = 4.5428e-01, PNorm = 59.2194, GNorm = 0.1474
Took 121.82726311683655 seconds to complete one epoch of meta training
Took 130.02123713493347 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.433489
Epoch 45
Meta loss on this task batch = 4.8876e-01, Meta loss averaged over last 500 steps = 4.5426e-01, PNorm = 59.2321, GNorm = 0.1632
Meta loss on this task batch = 4.6270e-01, Meta loss averaged over last 500 steps = 4.5420e-01, PNorm = 59.2449, GNorm = 0.1374
Meta loss on this task batch = 4.5861e-01, Meta loss averaged over last 500 steps = 4.5427e-01, PNorm = 59.2581, GNorm = 0.1640
Meta loss on this task batch = 4.9744e-01, Meta loss averaged over last 500 steps = 4.5438e-01, PNorm = 59.2708, GNorm = 0.1544
Meta loss on this task batch = 4.0710e-01, Meta loss averaged over last 500 steps = 4.5438e-01, PNorm = 59.2834, GNorm = 0.1460
Meta loss on this task batch = 4.1246e-01, Meta loss averaged over last 500 steps = 4.5425e-01, PNorm = 59.2946, GNorm = 0.1298
Meta loss on this task batch = 4.5298e-01, Meta loss averaged over last 500 steps = 4.5411e-01, PNorm = 59.3060, GNorm = 0.1419
Meta loss on this task batch = 4.7811e-01, Meta loss averaged over last 500 steps = 4.5418e-01, PNorm = 59.3180, GNorm = 0.1534
Meta loss on this task batch = 3.8923e-01, Meta loss averaged over last 500 steps = 4.5382e-01, PNorm = 59.3318, GNorm = 0.1401
Meta loss on this task batch = 4.6542e-01, Meta loss averaged over last 500 steps = 4.5390e-01, PNorm = 59.3451, GNorm = 0.1430
Meta loss on this task batch = 4.8717e-01, Meta loss averaged over last 500 steps = 4.5382e-01, PNorm = 59.3580, GNorm = 0.1694
Meta loss on this task batch = 3.3520e-01, Meta loss averaged over last 500 steps = 4.5369e-01, PNorm = 59.3722, GNorm = 0.1430
Meta loss on this task batch = 4.1115e-01, Meta loss averaged over last 500 steps = 4.5347e-01, PNorm = 59.3866, GNorm = 0.1423
Meta loss on this task batch = 4.6248e-01, Meta loss averaged over last 500 steps = 4.5337e-01, PNorm = 59.3996, GNorm = 0.1572
Meta loss on this task batch = 4.0001e-01, Meta loss averaged over last 500 steps = 4.5334e-01, PNorm = 59.4121, GNorm = 0.1207
Meta loss on this task batch = 4.6968e-01, Meta loss averaged over last 500 steps = 4.5337e-01, PNorm = 59.4213, GNorm = 0.1940
Meta loss on this task batch = 4.8271e-01, Meta loss averaged over last 500 steps = 4.5329e-01, PNorm = 59.4275, GNorm = 0.1966
Meta loss on this task batch = 4.5795e-01, Meta loss averaged over last 500 steps = 4.5324e-01, PNorm = 59.4336, GNorm = 0.1512
Meta loss on this task batch = 4.1215e-01, Meta loss averaged over last 500 steps = 4.5313e-01, PNorm = 59.4414, GNorm = 0.1878
Took 117.5024802684784 seconds to complete one epoch of meta training
Took 126.33964467048645 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465853
Epoch 46
Meta loss on this task batch = 4.5517e-01, Meta loss averaged over last 500 steps = 4.5303e-01, PNorm = 59.4514, GNorm = 0.1398
Meta loss on this task batch = 4.0670e-01, Meta loss averaged over last 500 steps = 4.5298e-01, PNorm = 59.4631, GNorm = 0.1543
Meta loss on this task batch = 4.3986e-01, Meta loss averaged over last 500 steps = 4.5289e-01, PNorm = 59.4774, GNorm = 0.1586
Meta loss on this task batch = 3.9116e-01, Meta loss averaged over last 500 steps = 4.5287e-01, PNorm = 59.4927, GNorm = 0.1369
Meta loss on this task batch = 4.3546e-01, Meta loss averaged over last 500 steps = 4.5297e-01, PNorm = 59.5068, GNorm = 0.1294
Meta loss on this task batch = 4.3697e-01, Meta loss averaged over last 500 steps = 4.5304e-01, PNorm = 59.5215, GNorm = 0.1635
Meta loss on this task batch = 4.2585e-01, Meta loss averaged over last 500 steps = 4.5307e-01, PNorm = 59.5358, GNorm = 0.1435
Meta loss on this task batch = 4.8979e-01, Meta loss averaged over last 500 steps = 4.5306e-01, PNorm = 59.5504, GNorm = 0.1428
Meta loss on this task batch = 4.2972e-01, Meta loss averaged over last 500 steps = 4.5302e-01, PNorm = 59.5645, GNorm = 0.1531
Meta loss on this task batch = 4.5366e-01, Meta loss averaged over last 500 steps = 4.5308e-01, PNorm = 59.5771, GNorm = 0.1535
Meta loss on this task batch = 4.9470e-01, Meta loss averaged over last 500 steps = 4.5307e-01, PNorm = 59.5876, GNorm = 0.1570
Meta loss on this task batch = 4.3352e-01, Meta loss averaged over last 500 steps = 4.5298e-01, PNorm = 59.5983, GNorm = 0.1652
Meta loss on this task batch = 4.4018e-01, Meta loss averaged over last 500 steps = 4.5291e-01, PNorm = 59.6091, GNorm = 0.1939
Meta loss on this task batch = 4.2065e-01, Meta loss averaged over last 500 steps = 4.5283e-01, PNorm = 59.6197, GNorm = 0.1473
Meta loss on this task batch = 4.5164e-01, Meta loss averaged over last 500 steps = 4.5277e-01, PNorm = 59.6307, GNorm = 0.1734
Meta loss on this task batch = 4.7975e-01, Meta loss averaged over last 500 steps = 4.5277e-01, PNorm = 59.6418, GNorm = 0.1462
Meta loss on this task batch = 4.7846e-01, Meta loss averaged over last 500 steps = 4.5291e-01, PNorm = 59.6509, GNorm = 0.1712
Meta loss on this task batch = 4.2228e-01, Meta loss averaged over last 500 steps = 4.5276e-01, PNorm = 59.6614, GNorm = 0.1635
Meta loss on this task batch = 3.5949e-01, Meta loss averaged over last 500 steps = 4.5252e-01, PNorm = 59.6728, GNorm = 0.1542
Took 120.97770166397095 seconds to complete one epoch of meta training
Took 128.38899207115173 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462822
Epoch 47
Meta loss on this task batch = 4.3818e-01, Meta loss averaged over last 500 steps = 4.5246e-01, PNorm = 59.6860, GNorm = 0.1548
Meta loss on this task batch = 4.1317e-01, Meta loss averaged over last 500 steps = 4.5245e-01, PNorm = 59.6998, GNorm = 0.1284
Meta loss on this task batch = 3.8908e-01, Meta loss averaged over last 500 steps = 4.5223e-01, PNorm = 59.7143, GNorm = 0.1435
Meta loss on this task batch = 4.2288e-01, Meta loss averaged over last 500 steps = 4.5219e-01, PNorm = 59.7288, GNorm = 0.1309
Meta loss on this task batch = 4.4437e-01, Meta loss averaged over last 500 steps = 4.5206e-01, PNorm = 59.7421, GNorm = 0.1483
Meta loss on this task batch = 4.6250e-01, Meta loss averaged over last 500 steps = 4.5214e-01, PNorm = 59.7538, GNorm = 0.1431
Meta loss on this task batch = 4.4894e-01, Meta loss averaged over last 500 steps = 4.5211e-01, PNorm = 59.7644, GNorm = 0.1503
Meta loss on this task batch = 4.6181e-01, Meta loss averaged over last 500 steps = 4.5213e-01, PNorm = 59.7749, GNorm = 0.1302
Meta loss on this task batch = 4.0671e-01, Meta loss averaged over last 500 steps = 4.5195e-01, PNorm = 59.7872, GNorm = 0.1371
Meta loss on this task batch = 4.9712e-01, Meta loss averaged over last 500 steps = 4.5199e-01, PNorm = 59.7990, GNorm = 0.1505
Meta loss on this task batch = 4.0153e-01, Meta loss averaged over last 500 steps = 4.5178e-01, PNorm = 59.8114, GNorm = 0.1475
Meta loss on this task batch = 3.9227e-01, Meta loss averaged over last 500 steps = 4.5159e-01, PNorm = 59.8249, GNorm = 0.1346
Meta loss on this task batch = 4.0576e-01, Meta loss averaged over last 500 steps = 4.5138e-01, PNorm = 59.8376, GNorm = 0.1408
Meta loss on this task batch = 4.3470e-01, Meta loss averaged over last 500 steps = 4.5143e-01, PNorm = 59.8499, GNorm = 0.1460
Meta loss on this task batch = 4.1443e-01, Meta loss averaged over last 500 steps = 4.5130e-01, PNorm = 59.8621, GNorm = 0.1374
Meta loss on this task batch = 5.1146e-01, Meta loss averaged over last 500 steps = 4.5138e-01, PNorm = 59.8728, GNorm = 0.2051
Meta loss on this task batch = 4.7940e-01, Meta loss averaged over last 500 steps = 4.5155e-01, PNorm = 59.8832, GNorm = 0.1640
Meta loss on this task batch = 4.2394e-01, Meta loss averaged over last 500 steps = 4.5146e-01, PNorm = 59.8923, GNorm = 0.1743
Meta loss on this task batch = 5.0897e-01, Meta loss averaged over last 500 steps = 4.5139e-01, PNorm = 59.9019, GNorm = 0.1762
Took 120.33335828781128 seconds to complete one epoch of meta training
Took 127.9705765247345 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447178
Epoch 48
Meta loss on this task batch = 3.8716e-01, Meta loss averaged over last 500 steps = 4.5137e-01, PNorm = 59.9134, GNorm = 0.1334
Meta loss on this task batch = 4.4205e-01, Meta loss averaged over last 500 steps = 4.5131e-01, PNorm = 59.9261, GNorm = 0.1207
Meta loss on this task batch = 4.0917e-01, Meta loss averaged over last 500 steps = 4.5112e-01, PNorm = 59.9403, GNorm = 0.1254
Meta loss on this task batch = 4.5073e-01, Meta loss averaged over last 500 steps = 4.5114e-01, PNorm = 59.9544, GNorm = 0.1260
Meta loss on this task batch = 4.0749e-01, Meta loss averaged over last 500 steps = 4.5106e-01, PNorm = 59.9687, GNorm = 0.1572
Meta loss on this task batch = 4.1692e-01, Meta loss averaged over last 500 steps = 4.5106e-01, PNorm = 59.9844, GNorm = 0.1378
Meta loss on this task batch = 4.3260e-01, Meta loss averaged over last 500 steps = 4.5110e-01, PNorm = 60.0001, GNorm = 0.1290
Meta loss on this task batch = 4.4774e-01, Meta loss averaged over last 500 steps = 4.5090e-01, PNorm = 60.0143, GNorm = 0.1562
Meta loss on this task batch = 4.5373e-01, Meta loss averaged over last 500 steps = 4.5086e-01, PNorm = 60.0271, GNorm = 0.1604
Meta loss on this task batch = 4.5291e-01, Meta loss averaged over last 500 steps = 4.5071e-01, PNorm = 60.0390, GNorm = 0.1608
Meta loss on this task batch = 4.3481e-01, Meta loss averaged over last 500 steps = 4.5071e-01, PNorm = 60.0491, GNorm = 0.1520
Meta loss on this task batch = 4.7503e-01, Meta loss averaged over last 500 steps = 4.5074e-01, PNorm = 60.0577, GNorm = 0.1685
Meta loss on this task batch = 4.9443e-01, Meta loss averaged over last 500 steps = 4.5084e-01, PNorm = 60.0665, GNorm = 0.1666
Meta loss on this task batch = 4.1377e-01, Meta loss averaged over last 500 steps = 4.5080e-01, PNorm = 60.0773, GNorm = 0.1327
Meta loss on this task batch = 3.5701e-01, Meta loss averaged over last 500 steps = 4.5063e-01, PNorm = 60.0899, GNorm = 0.1192
Meta loss on this task batch = 4.3421e-01, Meta loss averaged over last 500 steps = 4.5071e-01, PNorm = 60.1035, GNorm = 0.1460
Meta loss on this task batch = 4.4497e-01, Meta loss averaged over last 500 steps = 4.5060e-01, PNorm = 60.1171, GNorm = 0.1454
Meta loss on this task batch = 5.3963e-01, Meta loss averaged over last 500 steps = 4.5065e-01, PNorm = 60.1303, GNorm = 0.1944
Meta loss on this task batch = 4.6579e-01, Meta loss averaged over last 500 steps = 4.5054e-01, PNorm = 60.1423, GNorm = 0.1456
Took 124.35692310333252 seconds to complete one epoch of meta training
Took 132.52052998542786 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455550
Epoch 49
Meta loss on this task batch = 4.3418e-01, Meta loss averaged over last 500 steps = 4.5050e-01, PNorm = 60.1543, GNorm = 0.1502
Meta loss on this task batch = 4.3453e-01, Meta loss averaged over last 500 steps = 4.5051e-01, PNorm = 60.1667, GNorm = 0.1235
Meta loss on this task batch = 4.5768e-01, Meta loss averaged over last 500 steps = 4.5052e-01, PNorm = 60.1795, GNorm = 0.1239
Meta loss on this task batch = 3.9637e-01, Meta loss averaged over last 500 steps = 4.5044e-01, PNorm = 60.1939, GNorm = 0.1290
Meta loss on this task batch = 4.5568e-01, Meta loss averaged over last 500 steps = 4.5042e-01, PNorm = 60.2080, GNorm = 0.1452
Meta loss on this task batch = 4.8069e-01, Meta loss averaged over last 500 steps = 4.5036e-01, PNorm = 60.2225, GNorm = 0.1268
Meta loss on this task batch = 4.2294e-01, Meta loss averaged over last 500 steps = 4.5037e-01, PNorm = 60.2380, GNorm = 0.1592
Meta loss on this task batch = 4.7670e-01, Meta loss averaged over last 500 steps = 4.5042e-01, PNorm = 60.2526, GNorm = 0.2026
Meta loss on this task batch = 3.9472e-01, Meta loss averaged over last 500 steps = 4.5022e-01, PNorm = 60.2656, GNorm = 0.1298
Meta loss on this task batch = 4.2293e-01, Meta loss averaged over last 500 steps = 4.5014e-01, PNorm = 60.2798, GNorm = 0.1477
Meta loss on this task batch = 4.5001e-01, Meta loss averaged over last 500 steps = 4.5014e-01, PNorm = 60.2937, GNorm = 0.1603
Meta loss on this task batch = 3.9631e-01, Meta loss averaged over last 500 steps = 4.4995e-01, PNorm = 60.3056, GNorm = 0.1461
Meta loss on this task batch = 4.6676e-01, Meta loss averaged over last 500 steps = 4.5008e-01, PNorm = 60.3179, GNorm = 0.1869
Meta loss on this task batch = 4.8242e-01, Meta loss averaged over last 500 steps = 4.5014e-01, PNorm = 60.3308, GNorm = 0.1562
Meta loss on this task batch = 4.7415e-01, Meta loss averaged over last 500 steps = 4.5021e-01, PNorm = 60.3438, GNorm = 0.1463
Meta loss on this task batch = 3.9817e-01, Meta loss averaged over last 500 steps = 4.5011e-01, PNorm = 60.3577, GNorm = 0.1636
Meta loss on this task batch = 4.1564e-01, Meta loss averaged over last 500 steps = 4.4984e-01, PNorm = 60.3707, GNorm = 0.1959
Meta loss on this task batch = 4.2000e-01, Meta loss averaged over last 500 steps = 4.4980e-01, PNorm = 60.3853, GNorm = 0.1569
Meta loss on this task batch = 4.3820e-01, Meta loss averaged over last 500 steps = 4.4981e-01, PNorm = 60.4009, GNorm = 0.1779
Took 124.1602783203125 seconds to complete one epoch of meta training
Took 132.72461128234863 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493857
Found better MAML checkpoint after meta validation, saving now
Epoch 50
Meta loss on this task batch = 4.3743e-01, Meta loss averaged over last 500 steps = 4.4975e-01, PNorm = 60.4162, GNorm = 0.1605
Meta loss on this task batch = 4.5267e-01, Meta loss averaged over last 500 steps = 4.4964e-01, PNorm = 60.4316, GNorm = 0.1809
Meta loss on this task batch = 4.4818e-01, Meta loss averaged over last 500 steps = 4.4957e-01, PNorm = 60.4473, GNorm = 0.1628
Meta loss on this task batch = 5.0345e-01, Meta loss averaged over last 500 steps = 4.4974e-01, PNorm = 60.4622, GNorm = 0.1343
Meta loss on this task batch = 4.7583e-01, Meta loss averaged over last 500 steps = 4.4973e-01, PNorm = 60.4768, GNorm = 0.1440
Meta loss on this task batch = 4.0806e-01, Meta loss averaged over last 500 steps = 4.4962e-01, PNorm = 60.4924, GNorm = 0.1696
Meta loss on this task batch = 4.1107e-01, Meta loss averaged over last 500 steps = 4.4958e-01, PNorm = 60.5066, GNorm = 0.1490
Meta loss on this task batch = 4.4979e-01, Meta loss averaged over last 500 steps = 4.4959e-01, PNorm = 60.5219, GNorm = 0.2013
Meta loss on this task batch = 4.0437e-01, Meta loss averaged over last 500 steps = 4.4954e-01, PNorm = 60.5374, GNorm = 0.1623
Meta loss on this task batch = 4.4601e-01, Meta loss averaged over last 500 steps = 4.4946e-01, PNorm = 60.5518, GNorm = 0.1729
Meta loss on this task batch = 4.2528e-01, Meta loss averaged over last 500 steps = 4.4935e-01, PNorm = 60.5662, GNorm = 0.1321
Meta loss on this task batch = 4.9561e-01, Meta loss averaged over last 500 steps = 4.4940e-01, PNorm = 60.5788, GNorm = 0.2002
Meta loss on this task batch = 3.8098e-01, Meta loss averaged over last 500 steps = 4.4939e-01, PNorm = 60.5924, GNorm = 0.1672
Meta loss on this task batch = 3.6752e-01, Meta loss averaged over last 500 steps = 4.4906e-01, PNorm = 60.6064, GNorm = 0.1319
Meta loss on this task batch = 4.1951e-01, Meta loss averaged over last 500 steps = 4.4893e-01, PNorm = 60.6216, GNorm = 0.1367
Meta loss on this task batch = 4.6622e-01, Meta loss averaged over last 500 steps = 4.4889e-01, PNorm = 60.6363, GNorm = 0.1659
Meta loss on this task batch = 3.9093e-01, Meta loss averaged over last 500 steps = 4.4877e-01, PNorm = 60.6506, GNorm = 0.1618
Meta loss on this task batch = 4.0006e-01, Meta loss averaged over last 500 steps = 4.4859e-01, PNorm = 60.6630, GNorm = 0.1896
Meta loss on this task batch = 4.3295e-01, Meta loss averaged over last 500 steps = 4.4855e-01, PNorm = 60.6765, GNorm = 0.1883
Took 124.29087853431702 seconds to complete one epoch of meta training
Took 133.22740268707275 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465030
Epoch 51
Meta loss on this task batch = 4.1573e-01, Meta loss averaged over last 500 steps = 4.4838e-01, PNorm = 60.6907, GNorm = 0.1267
Meta loss on this task batch = 5.0141e-01, Meta loss averaged over last 500 steps = 4.4841e-01, PNorm = 60.7036, GNorm = 0.1524
Meta loss on this task batch = 4.1399e-01, Meta loss averaged over last 500 steps = 4.4838e-01, PNorm = 60.7150, GNorm = 0.1467
Meta loss on this task batch = 3.9893e-01, Meta loss averaged over last 500 steps = 4.4830e-01, PNorm = 60.7271, GNorm = 0.1114
Meta loss on this task batch = 4.2623e-01, Meta loss averaged over last 500 steps = 4.4821e-01, PNorm = 60.7397, GNorm = 0.1166
Meta loss on this task batch = 4.6210e-01, Meta loss averaged over last 500 steps = 4.4817e-01, PNorm = 60.7515, GNorm = 0.1402
Meta loss on this task batch = 4.3639e-01, Meta loss averaged over last 500 steps = 4.4806e-01, PNorm = 60.7641, GNorm = 0.1297
Meta loss on this task batch = 4.6663e-01, Meta loss averaged over last 500 steps = 4.4813e-01, PNorm = 60.7786, GNorm = 0.1388
Meta loss on this task batch = 4.2058e-01, Meta loss averaged over last 500 steps = 4.4808e-01, PNorm = 60.7952, GNorm = 0.1619
Meta loss on this task batch = 3.8607e-01, Meta loss averaged over last 500 steps = 4.4792e-01, PNorm = 60.8117, GNorm = 0.1299
Meta loss on this task batch = 4.1374e-01, Meta loss averaged over last 500 steps = 4.4783e-01, PNorm = 60.8292, GNorm = 0.1620
Meta loss on this task batch = 4.6964e-01, Meta loss averaged over last 500 steps = 4.4790e-01, PNorm = 60.8446, GNorm = 0.1576
Meta loss on this task batch = 4.1631e-01, Meta loss averaged over last 500 steps = 4.4780e-01, PNorm = 60.8597, GNorm = 0.1390
Meta loss on this task batch = 4.9806e-01, Meta loss averaged over last 500 steps = 4.4791e-01, PNorm = 60.8746, GNorm = 0.1606
Meta loss on this task batch = 4.5231e-01, Meta loss averaged over last 500 steps = 4.4796e-01, PNorm = 60.8888, GNorm = 0.1408
Meta loss on this task batch = 4.8470e-01, Meta loss averaged over last 500 steps = 4.4812e-01, PNorm = 60.9004, GNorm = 0.1826
Meta loss on this task batch = 4.4306e-01, Meta loss averaged over last 500 steps = 4.4802e-01, PNorm = 60.9118, GNorm = 0.1686
Meta loss on this task batch = 5.1141e-01, Meta loss averaged over last 500 steps = 4.4815e-01, PNorm = 60.9226, GNorm = 0.1716
Meta loss on this task batch = 3.4492e-01, Meta loss averaged over last 500 steps = 4.4777e-01, PNorm = 60.9364, GNorm = 0.1327
Took 121.23872590065002 seconds to complete one epoch of meta training
Took 129.29578733444214 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471164
Epoch 52
Meta loss on this task batch = 5.1176e-01, Meta loss averaged over last 500 steps = 4.4790e-01, PNorm = 60.9503, GNorm = 0.1276
Meta loss on this task batch = 3.7887e-01, Meta loss averaged over last 500 steps = 4.4767e-01, PNorm = 60.9649, GNorm = 0.1101
Meta loss on this task batch = 4.4689e-01, Meta loss averaged over last 500 steps = 4.4765e-01, PNorm = 60.9806, GNorm = 0.1948
Meta loss on this task batch = 4.5142e-01, Meta loss averaged over last 500 steps = 4.4750e-01, PNorm = 60.9964, GNorm = 0.1441
Meta loss on this task batch = 5.0108e-01, Meta loss averaged over last 500 steps = 4.4758e-01, PNorm = 61.0096, GNorm = 0.1746
Meta loss on this task batch = 4.4346e-01, Meta loss averaged over last 500 steps = 4.4760e-01, PNorm = 61.0220, GNorm = 0.1429
Meta loss on this task batch = 3.8372e-01, Meta loss averaged over last 500 steps = 4.4757e-01, PNorm = 61.0355, GNorm = 0.1231
Meta loss on this task batch = 4.7680e-01, Meta loss averaged over last 500 steps = 4.4756e-01, PNorm = 61.0500, GNorm = 0.1503
Meta loss on this task batch = 3.9516e-01, Meta loss averaged over last 500 steps = 4.4752e-01, PNorm = 61.0654, GNorm = 0.1401
Meta loss on this task batch = 3.8100e-01, Meta loss averaged over last 500 steps = 4.4738e-01, PNorm = 61.0809, GNorm = 0.1322
Meta loss on this task batch = 4.7437e-01, Meta loss averaged over last 500 steps = 4.4739e-01, PNorm = 61.0941, GNorm = 0.1762
Meta loss on this task batch = 4.3953e-01, Meta loss averaged over last 500 steps = 4.4744e-01, PNorm = 61.1086, GNorm = 0.1510
Meta loss on this task batch = 4.3635e-01, Meta loss averaged over last 500 steps = 4.4734e-01, PNorm = 61.1221, GNorm = 0.1689
Meta loss on this task batch = 3.7533e-01, Meta loss averaged over last 500 steps = 4.4726e-01, PNorm = 61.1364, GNorm = 0.1284
Meta loss on this task batch = 3.9540e-01, Meta loss averaged over last 500 steps = 4.4711e-01, PNorm = 61.1500, GNorm = 0.1537
Meta loss on this task batch = 3.8145e-01, Meta loss averaged over last 500 steps = 4.4686e-01, PNorm = 61.1643, GNorm = 0.1342
Meta loss on this task batch = 4.7406e-01, Meta loss averaged over last 500 steps = 4.4696e-01, PNorm = 61.1779, GNorm = 0.1533
Meta loss on this task batch = 4.0948e-01, Meta loss averaged over last 500 steps = 4.4688e-01, PNorm = 61.1905, GNorm = 0.1752
Meta loss on this task batch = 4.6415e-01, Meta loss averaged over last 500 steps = 4.4677e-01, PNorm = 61.2038, GNorm = 0.1913
Took 121.31265020370483 seconds to complete one epoch of meta training
Took 130.25895190238953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480453
Epoch 53
Meta loss on this task batch = 4.3931e-01, Meta loss averaged over last 500 steps = 4.4676e-01, PNorm = 61.2171, GNorm = 0.1643
Meta loss on this task batch = 3.9476e-01, Meta loss averaged over last 500 steps = 4.4667e-01, PNorm = 61.2312, GNorm = 0.1277
Meta loss on this task batch = 3.6415e-01, Meta loss averaged over last 500 steps = 4.4630e-01, PNorm = 61.2465, GNorm = 0.1214
Meta loss on this task batch = 4.6674e-01, Meta loss averaged over last 500 steps = 4.4629e-01, PNorm = 61.2610, GNorm = 0.1439
Meta loss on this task batch = 4.0008e-01, Meta loss averaged over last 500 steps = 4.4627e-01, PNorm = 61.2754, GNorm = 0.1931
Meta loss on this task batch = 5.0219e-01, Meta loss averaged over last 500 steps = 4.4636e-01, PNorm = 61.2885, GNorm = 0.1750
Meta loss on this task batch = 4.1361e-01, Meta loss averaged over last 500 steps = 4.4624e-01, PNorm = 61.3011, GNorm = 0.2005
Meta loss on this task batch = 4.4984e-01, Meta loss averaged over last 500 steps = 4.4636e-01, PNorm = 61.3148, GNorm = 0.1515
Meta loss on this task batch = 3.9315e-01, Meta loss averaged over last 500 steps = 4.4610e-01, PNorm = 61.3293, GNorm = 0.1503
Meta loss on this task batch = 4.7784e-01, Meta loss averaged over last 500 steps = 4.4624e-01, PNorm = 61.3444, GNorm = 0.1483
Meta loss on this task batch = 4.5112e-01, Meta loss averaged over last 500 steps = 4.4632e-01, PNorm = 61.3599, GNorm = 0.1994
Meta loss on this task batch = 3.8377e-01, Meta loss averaged over last 500 steps = 4.4606e-01, PNorm = 61.3772, GNorm = 0.1480
Meta loss on this task batch = 4.5612e-01, Meta loss averaged over last 500 steps = 4.4601e-01, PNorm = 61.3930, GNorm = 0.1540
Meta loss on this task batch = 5.0796e-01, Meta loss averaged over last 500 steps = 4.4600e-01, PNorm = 61.4074, GNorm = 0.1690
Meta loss on this task batch = 4.6398e-01, Meta loss averaged over last 500 steps = 4.4603e-01, PNorm = 61.4225, GNorm = 0.1395
Meta loss on this task batch = 4.2907e-01, Meta loss averaged over last 500 steps = 4.4592e-01, PNorm = 61.4372, GNorm = 0.1670
Meta loss on this task batch = 4.3240e-01, Meta loss averaged over last 500 steps = 4.4593e-01, PNorm = 61.4524, GNorm = 0.1718
Meta loss on this task batch = 3.4488e-01, Meta loss averaged over last 500 steps = 4.4576e-01, PNorm = 61.4690, GNorm = 0.1276
Meta loss on this task batch = 4.5358e-01, Meta loss averaged over last 500 steps = 4.4584e-01, PNorm = 61.4858, GNorm = 0.1462
Took 117.45017099380493 seconds to complete one epoch of meta training
Took 125.15458512306213 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454373
Epoch 54
Meta loss on this task batch = 3.7306e-01, Meta loss averaged over last 500 steps = 4.4563e-01, PNorm = 61.5029, GNorm = 0.1673
Meta loss on this task batch = 4.3187e-01, Meta loss averaged over last 500 steps = 4.4556e-01, PNorm = 61.5199, GNorm = 0.1463
Meta loss on this task batch = 4.2137e-01, Meta loss averaged over last 500 steps = 4.4542e-01, PNorm = 61.5382, GNorm = 0.1912
Meta loss on this task batch = 4.3058e-01, Meta loss averaged over last 500 steps = 4.4531e-01, PNorm = 61.5561, GNorm = 0.1781
Meta loss on this task batch = 4.7063e-01, Meta loss averaged over last 500 steps = 4.4532e-01, PNorm = 61.5717, GNorm = 0.1726
Meta loss on this task batch = 4.6201e-01, Meta loss averaged over last 500 steps = 4.4530e-01, PNorm = 61.5871, GNorm = 0.1718
Meta loss on this task batch = 4.4479e-01, Meta loss averaged over last 500 steps = 4.4525e-01, PNorm = 61.6018, GNorm = 0.1738
Meta loss on this task batch = 4.1866e-01, Meta loss averaged over last 500 steps = 4.4516e-01, PNorm = 61.6160, GNorm = 0.1823
Meta loss on this task batch = 3.9795e-01, Meta loss averaged over last 500 steps = 4.4501e-01, PNorm = 61.6293, GNorm = 0.1610
Meta loss on this task batch = 4.4829e-01, Meta loss averaged over last 500 steps = 4.4494e-01, PNorm = 61.6434, GNorm = 0.1454
Meta loss on this task batch = 3.6862e-01, Meta loss averaged over last 500 steps = 4.4488e-01, PNorm = 61.6587, GNorm = 0.1244
Meta loss on this task batch = 4.0357e-01, Meta loss averaged over last 500 steps = 4.4471e-01, PNorm = 61.6733, GNorm = 0.1302
Meta loss on this task batch = 4.5804e-01, Meta loss averaged over last 500 steps = 4.4477e-01, PNorm = 61.6880, GNorm = 0.1579
Meta loss on this task batch = 4.3571e-01, Meta loss averaged over last 500 steps = 4.4479e-01, PNorm = 61.7009, GNorm = 0.1724
Meta loss on this task batch = 4.3017e-01, Meta loss averaged over last 500 steps = 4.4462e-01, PNorm = 61.7136, GNorm = 0.1562
Meta loss on this task batch = 4.4543e-01, Meta loss averaged over last 500 steps = 4.4472e-01, PNorm = 61.7255, GNorm = 0.1608
Meta loss on this task batch = 4.4608e-01, Meta loss averaged over last 500 steps = 4.4474e-01, PNorm = 61.7376, GNorm = 0.1346
Meta loss on this task batch = 3.9287e-01, Meta loss averaged over last 500 steps = 4.4461e-01, PNorm = 61.7498, GNorm = 0.1622
Meta loss on this task batch = 4.3576e-01, Meta loss averaged over last 500 steps = 4.4455e-01, PNorm = 61.7637, GNorm = 0.1621
Took 124.91936111450195 seconds to complete one epoch of meta training
Took 133.09528636932373 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463593
Epoch 55
Meta loss on this task batch = 4.3706e-01, Meta loss averaged over last 500 steps = 4.4449e-01, PNorm = 61.7776, GNorm = 0.1534
Meta loss on this task batch = 4.7248e-01, Meta loss averaged over last 500 steps = 4.4449e-01, PNorm = 61.7899, GNorm = 0.1488
Meta loss on this task batch = 4.3200e-01, Meta loss averaged over last 500 steps = 4.4448e-01, PNorm = 61.8030, GNorm = 0.1491
Meta loss on this task batch = 3.9035e-01, Meta loss averaged over last 500 steps = 4.4434e-01, PNorm = 61.8163, GNorm = 0.1523
Meta loss on this task batch = 4.4901e-01, Meta loss averaged over last 500 steps = 4.4420e-01, PNorm = 61.8289, GNorm = 0.1683
Meta loss on this task batch = 4.6386e-01, Meta loss averaged over last 500 steps = 4.4415e-01, PNorm = 61.8408, GNorm = 0.1358
Meta loss on this task batch = 4.2199e-01, Meta loss averaged over last 500 steps = 4.4409e-01, PNorm = 61.8536, GNorm = 0.1723
Meta loss on this task batch = 4.2063e-01, Meta loss averaged over last 500 steps = 4.4386e-01, PNorm = 61.8650, GNorm = 0.1360
Meta loss on this task batch = 4.1101e-01, Meta loss averaged over last 500 steps = 4.4381e-01, PNorm = 61.8769, GNorm = 0.1271
Meta loss on this task batch = 4.1463e-01, Meta loss averaged over last 500 steps = 4.4366e-01, PNorm = 61.8899, GNorm = 0.1449
Meta loss on this task batch = 4.0992e-01, Meta loss averaged over last 500 steps = 4.4352e-01, PNorm = 61.9028, GNorm = 0.1667
Meta loss on this task batch = 4.2750e-01, Meta loss averaged over last 500 steps = 4.4354e-01, PNorm = 61.9156, GNorm = 0.1649
Meta loss on this task batch = 5.2301e-01, Meta loss averaged over last 500 steps = 4.4366e-01, PNorm = 61.9269, GNorm = 0.1676
Meta loss on this task batch = 4.2696e-01, Meta loss averaged over last 500 steps = 4.4359e-01, PNorm = 61.9370, GNorm = 0.1478
Meta loss on this task batch = 4.0485e-01, Meta loss averaged over last 500 steps = 4.4341e-01, PNorm = 61.9477, GNorm = 0.1658
Meta loss on this task batch = 3.8914e-01, Meta loss averaged over last 500 steps = 4.4331e-01, PNorm = 61.9599, GNorm = 0.1553
Meta loss on this task batch = 4.5082e-01, Meta loss averaged over last 500 steps = 4.4339e-01, PNorm = 61.9734, GNorm = 0.1333
Meta loss on this task batch = 4.4017e-01, Meta loss averaged over last 500 steps = 4.4340e-01, PNorm = 61.9880, GNorm = 0.1315
Meta loss on this task batch = 4.1758e-01, Meta loss averaged over last 500 steps = 4.4336e-01, PNorm = 62.0031, GNorm = 0.1730
Took 116.81531310081482 seconds to complete one epoch of meta training
Took 124.52135562896729 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479700
Epoch 56
Meta loss on this task batch = 4.5763e-01, Meta loss averaged over last 500 steps = 4.4334e-01, PNorm = 62.0182, GNorm = 0.1613
Meta loss on this task batch = 3.8181e-01, Meta loss averaged over last 500 steps = 4.4322e-01, PNorm = 62.0331, GNorm = 0.1912
Meta loss on this task batch = 4.4003e-01, Meta loss averaged over last 500 steps = 4.4326e-01, PNorm = 62.0472, GNorm = 0.1680
Meta loss on this task batch = 3.6241e-01, Meta loss averaged over last 500 steps = 4.4314e-01, PNorm = 62.0618, GNorm = 0.1453
Meta loss on this task batch = 4.2114e-01, Meta loss averaged over last 500 steps = 4.4304e-01, PNorm = 62.0760, GNorm = 0.1531
Meta loss on this task batch = 4.4874e-01, Meta loss averaged over last 500 steps = 4.4315e-01, PNorm = 62.0909, GNorm = 0.1908
Meta loss on this task batch = 5.1549e-01, Meta loss averaged over last 500 steps = 4.4331e-01, PNorm = 62.1057, GNorm = 0.1944
Meta loss on this task batch = 4.3478e-01, Meta loss averaged over last 500 steps = 4.4322e-01, PNorm = 62.1198, GNorm = 0.1447
Meta loss on this task batch = 4.5775e-01, Meta loss averaged over last 500 steps = 4.4313e-01, PNorm = 62.1342, GNorm = 0.1398
Meta loss on this task batch = 3.9388e-01, Meta loss averaged over last 500 steps = 4.4315e-01, PNorm = 62.1475, GNorm = 0.1502
Meta loss on this task batch = 4.5863e-01, Meta loss averaged over last 500 steps = 4.4309e-01, PNorm = 62.1596, GNorm = 0.1410
Meta loss on this task batch = 4.2067e-01, Meta loss averaged over last 500 steps = 4.4290e-01, PNorm = 62.1732, GNorm = 0.1619
Meta loss on this task batch = 4.2150e-01, Meta loss averaged over last 500 steps = 4.4291e-01, PNorm = 62.1870, GNorm = 0.1395
Meta loss on this task batch = 4.2282e-01, Meta loss averaged over last 500 steps = 4.4278e-01, PNorm = 62.2019, GNorm = 0.1626
Meta loss on this task batch = 3.9178e-01, Meta loss averaged over last 500 steps = 4.4267e-01, PNorm = 62.2165, GNorm = 0.1554
Meta loss on this task batch = 3.8881e-01, Meta loss averaged over last 500 steps = 4.4253e-01, PNorm = 62.2294, GNorm = 0.1516
Meta loss on this task batch = 4.4032e-01, Meta loss averaged over last 500 steps = 4.4275e-01, PNorm = 62.2418, GNorm = 0.1596
Meta loss on this task batch = 4.4388e-01, Meta loss averaged over last 500 steps = 4.4264e-01, PNorm = 62.2549, GNorm = 0.1548
Meta loss on this task batch = 4.4046e-01, Meta loss averaged over last 500 steps = 4.4253e-01, PNorm = 62.2701, GNorm = 0.1794
Took 120.5681893825531 seconds to complete one epoch of meta training
Took 127.38505411148071 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474725
Epoch 57
Meta loss on this task batch = 4.1950e-01, Meta loss averaged over last 500 steps = 4.4259e-01, PNorm = 62.2851, GNorm = 0.1198
Meta loss on this task batch = 4.0510e-01, Meta loss averaged over last 500 steps = 4.4258e-01, PNorm = 62.2997, GNorm = 0.1664
Meta loss on this task batch = 4.2431e-01, Meta loss averaged over last 500 steps = 4.4251e-01, PNorm = 62.3139, GNorm = 0.1541
Meta loss on this task batch = 4.1056e-01, Meta loss averaged over last 500 steps = 4.4242e-01, PNorm = 62.3268, GNorm = 0.1145
Meta loss on this task batch = 4.0989e-01, Meta loss averaged over last 500 steps = 4.4232e-01, PNorm = 62.3392, GNorm = 0.1419
Meta loss on this task batch = 5.0819e-01, Meta loss averaged over last 500 steps = 4.4239e-01, PNorm = 62.3511, GNorm = 0.1396
Meta loss on this task batch = 4.3692e-01, Meta loss averaged over last 500 steps = 4.4236e-01, PNorm = 62.3626, GNorm = 0.1545
Meta loss on this task batch = 4.7479e-01, Meta loss averaged over last 500 steps = 4.4247e-01, PNorm = 62.3728, GNorm = 0.1853
Meta loss on this task batch = 4.2214e-01, Meta loss averaged over last 500 steps = 4.4232e-01, PNorm = 62.3839, GNorm = 0.1630
Meta loss on this task batch = 4.0862e-01, Meta loss averaged over last 500 steps = 4.4231e-01, PNorm = 62.3970, GNorm = 0.1726
Meta loss on this task batch = 4.3006e-01, Meta loss averaged over last 500 steps = 4.4221e-01, PNorm = 62.4117, GNorm = 0.1457
Meta loss on this task batch = 4.0542e-01, Meta loss averaged over last 500 steps = 4.4209e-01, PNorm = 62.4258, GNorm = 0.1563
Meta loss on this task batch = 3.4335e-01, Meta loss averaged over last 500 steps = 4.4183e-01, PNorm = 62.4408, GNorm = 0.1990
Meta loss on this task batch = 4.0416e-01, Meta loss averaged over last 500 steps = 4.4187e-01, PNorm = 62.4555, GNorm = 0.1501
Meta loss on this task batch = 4.3637e-01, Meta loss averaged over last 500 steps = 4.4185e-01, PNorm = 62.4698, GNorm = 0.1288
Meta loss on this task batch = 5.3409e-01, Meta loss averaged over last 500 steps = 4.4201e-01, PNorm = 62.4818, GNorm = 0.2131
Meta loss on this task batch = 3.9563e-01, Meta loss averaged over last 500 steps = 4.4182e-01, PNorm = 62.4946, GNorm = 0.1480
Meta loss on this task batch = 4.6808e-01, Meta loss averaged over last 500 steps = 4.4178e-01, PNorm = 62.5057, GNorm = 0.2165
Meta loss on this task batch = 4.5963e-01, Meta loss averaged over last 500 steps = 4.4185e-01, PNorm = 62.5179, GNorm = 0.1842
Took 119.6770830154419 seconds to complete one epoch of meta training
Took 127.96829342842102 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479640
Epoch 58
Meta loss on this task batch = 4.3217e-01, Meta loss averaged over last 500 steps = 4.4164e-01, PNorm = 62.5309, GNorm = 0.1430
Meta loss on this task batch = 3.8743e-01, Meta loss averaged over last 500 steps = 4.4167e-01, PNorm = 62.5464, GNorm = 0.1948
Meta loss on this task batch = 4.4363e-01, Meta loss averaged over last 500 steps = 4.4153e-01, PNorm = 62.5629, GNorm = 0.1444
Meta loss on this task batch = 4.4266e-01, Meta loss averaged over last 500 steps = 4.4151e-01, PNorm = 62.5800, GNorm = 0.1536
Meta loss on this task batch = 4.9340e-01, Meta loss averaged over last 500 steps = 4.4145e-01, PNorm = 62.5958, GNorm = 0.1636
Meta loss on this task batch = 4.1479e-01, Meta loss averaged over last 500 steps = 4.4144e-01, PNorm = 62.6117, GNorm = 0.1463
Meta loss on this task batch = 4.4567e-01, Meta loss averaged over last 500 steps = 4.4149e-01, PNorm = 62.6257, GNorm = 0.1822
Meta loss on this task batch = 5.4198e-01, Meta loss averaged over last 500 steps = 4.4160e-01, PNorm = 62.6344, GNorm = 0.2570
Meta loss on this task batch = 3.8565e-01, Meta loss averaged over last 500 steps = 4.4141e-01, PNorm = 62.6432, GNorm = 0.1507
Meta loss on this task batch = 3.7916e-01, Meta loss averaged over last 500 steps = 4.4120e-01, PNorm = 62.6548, GNorm = 0.1629
Meta loss on this task batch = 3.4527e-01, Meta loss averaged over last 500 steps = 4.4091e-01, PNorm = 62.6692, GNorm = 0.1504
Meta loss on this task batch = 3.9969e-01, Meta loss averaged over last 500 steps = 4.4072e-01, PNorm = 62.6835, GNorm = 0.1469
Meta loss on this task batch = 4.2716e-01, Meta loss averaged over last 500 steps = 4.4060e-01, PNorm = 62.6969, GNorm = 0.1423
Meta loss on this task batch = 4.4779e-01, Meta loss averaged over last 500 steps = 4.4066e-01, PNorm = 62.7107, GNorm = 0.1473
Meta loss on this task batch = 4.1276e-01, Meta loss averaged over last 500 steps = 4.4065e-01, PNorm = 62.7245, GNorm = 0.1589
Meta loss on this task batch = 4.7514e-01, Meta loss averaged over last 500 steps = 4.4068e-01, PNorm = 62.7375, GNorm = 0.1364
Meta loss on this task batch = 4.2577e-01, Meta loss averaged over last 500 steps = 4.4068e-01, PNorm = 62.7495, GNorm = 0.1483
Meta loss on this task batch = 3.7550e-01, Meta loss averaged over last 500 steps = 4.4052e-01, PNorm = 62.7624, GNorm = 0.1416
Meta loss on this task batch = 4.6374e-01, Meta loss averaged over last 500 steps = 4.4051e-01, PNorm = 62.7759, GNorm = 0.1915
Took 121.82202649116516 seconds to complete one epoch of meta training
Took 130.34677720069885 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481360
Epoch 59
Meta loss on this task batch = 4.0507e-01, Meta loss averaged over last 500 steps = 4.4047e-01, PNorm = 62.7906, GNorm = 0.1304
Meta loss on this task batch = 4.3876e-01, Meta loss averaged over last 500 steps = 4.4041e-01, PNorm = 62.8052, GNorm = 0.1383
Meta loss on this task batch = 4.9983e-01, Meta loss averaged over last 500 steps = 4.4044e-01, PNorm = 62.8191, GNorm = 0.1907
Meta loss on this task batch = 4.4575e-01, Meta loss averaged over last 500 steps = 4.4051e-01, PNorm = 62.8332, GNorm = 0.1551
Meta loss on this task batch = 4.0375e-01, Meta loss averaged over last 500 steps = 4.4043e-01, PNorm = 62.8487, GNorm = 0.1973
Meta loss on this task batch = 4.7002e-01, Meta loss averaged over last 500 steps = 4.4045e-01, PNorm = 62.8639, GNorm = 0.1761
Meta loss on this task batch = 4.0841e-01, Meta loss averaged over last 500 steps = 4.4029e-01, PNorm = 62.8796, GNorm = 0.2055
Meta loss on this task batch = 4.8754e-01, Meta loss averaged over last 500 steps = 4.4032e-01, PNorm = 62.8951, GNorm = 0.1678
Meta loss on this task batch = 4.2501e-01, Meta loss averaged over last 500 steps = 4.4026e-01, PNorm = 62.9098, GNorm = 0.1913
Meta loss on this task batch = 3.8089e-01, Meta loss averaged over last 500 steps = 4.4024e-01, PNorm = 62.9246, GNorm = 0.1555
Meta loss on this task batch = 4.2640e-01, Meta loss averaged over last 500 steps = 4.4015e-01, PNorm = 62.9381, GNorm = 0.1815
Meta loss on this task batch = 4.0600e-01, Meta loss averaged over last 500 steps = 4.4010e-01, PNorm = 62.9503, GNorm = 0.1521
Meta loss on this task batch = 3.9460e-01, Meta loss averaged over last 500 steps = 4.3986e-01, PNorm = 62.9621, GNorm = 0.1719
Meta loss on this task batch = 4.3689e-01, Meta loss averaged over last 500 steps = 4.3985e-01, PNorm = 62.9721, GNorm = 0.1742
Meta loss on this task batch = 4.3150e-01, Meta loss averaged over last 500 steps = 4.3993e-01, PNorm = 62.9812, GNorm = 0.1650
Meta loss on this task batch = 4.0114e-01, Meta loss averaged over last 500 steps = 4.3985e-01, PNorm = 62.9907, GNorm = 0.1632
Meta loss on this task batch = 4.4569e-01, Meta loss averaged over last 500 steps = 4.3979e-01, PNorm = 63.0000, GNorm = 0.1683
Meta loss on this task batch = 4.6916e-01, Meta loss averaged over last 500 steps = 4.3985e-01, PNorm = 63.0103, GNorm = 0.1779
Meta loss on this task batch = 4.0187e-01, Meta loss averaged over last 500 steps = 4.3971e-01, PNorm = 63.0223, GNorm = 0.2329
Took 119.87802505493164 seconds to complete one epoch of meta training
Took 127.91474604606628 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481265
Epoch 60
Meta loss on this task batch = 4.2760e-01, Meta loss averaged over last 500 steps = 4.3974e-01, PNorm = 63.0345, GNorm = 0.1698
Meta loss on this task batch = 4.2751e-01, Meta loss averaged over last 500 steps = 4.3972e-01, PNorm = 63.0471, GNorm = 0.1510
Meta loss on this task batch = 3.7069e-01, Meta loss averaged over last 500 steps = 4.3965e-01, PNorm = 63.0616, GNorm = 0.1301
Meta loss on this task batch = 3.9601e-01, Meta loss averaged over last 500 steps = 4.3938e-01, PNorm = 63.0767, GNorm = 0.1563
Meta loss on this task batch = 4.0877e-01, Meta loss averaged over last 500 steps = 4.3923e-01, PNorm = 63.0928, GNorm = 0.2045
Meta loss on this task batch = 4.4910e-01, Meta loss averaged over last 500 steps = 4.3926e-01, PNorm = 63.1086, GNorm = 0.1469
Meta loss on this task batch = 3.8150e-01, Meta loss averaged over last 500 steps = 4.3916e-01, PNorm = 63.1236, GNorm = 0.1535
Meta loss on this task batch = 4.2510e-01, Meta loss averaged over last 500 steps = 4.3904e-01, PNorm = 63.1384, GNorm = 0.1485
Meta loss on this task batch = 4.5388e-01, Meta loss averaged over last 500 steps = 4.3890e-01, PNorm = 63.1515, GNorm = 0.1656
Meta loss on this task batch = 4.5026e-01, Meta loss averaged over last 500 steps = 4.3889e-01, PNorm = 63.1634, GNorm = 0.2177
Meta loss on this task batch = 4.2460e-01, Meta loss averaged over last 500 steps = 4.3882e-01, PNorm = 63.1733, GNorm = 0.1566
Meta loss on this task batch = 4.3464e-01, Meta loss averaged over last 500 steps = 4.3873e-01, PNorm = 63.1835, GNorm = 0.1614
Meta loss on this task batch = 4.7627e-01, Meta loss averaged over last 500 steps = 4.3878e-01, PNorm = 63.1946, GNorm = 0.1454
Meta loss on this task batch = 5.0242e-01, Meta loss averaged over last 500 steps = 4.3891e-01, PNorm = 63.2057, GNorm = 0.1711
Meta loss on this task batch = 4.5578e-01, Meta loss averaged over last 500 steps = 4.3888e-01, PNorm = 63.2182, GNorm = 0.1414
Meta loss on this task batch = 4.3992e-01, Meta loss averaged over last 500 steps = 4.3890e-01, PNorm = 63.2329, GNorm = 0.1643
Meta loss on this task batch = 3.9107e-01, Meta loss averaged over last 500 steps = 4.3871e-01, PNorm = 63.2493, GNorm = 0.1445
Meta loss on this task batch = 4.1488e-01, Meta loss averaged over last 500 steps = 4.3865e-01, PNorm = 63.2681, GNorm = 0.1845
Meta loss on this task batch = 4.4752e-01, Meta loss averaged over last 500 steps = 4.3861e-01, PNorm = 63.2875, GNorm = 0.1799
Took 116.70572471618652 seconds to complete one epoch of meta training
Took 124.66740417480469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.441488
Epoch 61
Meta loss on this task batch = 4.5045e-01, Meta loss averaged over last 500 steps = 4.3869e-01, PNorm = 63.3066, GNorm = 0.1564
Meta loss on this task batch = 4.4793e-01, Meta loss averaged over last 500 steps = 4.3876e-01, PNorm = 63.3242, GNorm = 0.1749
Meta loss on this task batch = 4.0353e-01, Meta loss averaged over last 500 steps = 4.3882e-01, PNorm = 63.3397, GNorm = 0.1816
Meta loss on this task batch = 4.6674e-01, Meta loss averaged over last 500 steps = 4.3883e-01, PNorm = 63.3543, GNorm = 0.1585
Meta loss on this task batch = 4.1856e-01, Meta loss averaged over last 500 steps = 4.3881e-01, PNorm = 63.3682, GNorm = 0.1515
Meta loss on this task batch = 4.2512e-01, Meta loss averaged over last 500 steps = 4.3873e-01, PNorm = 63.3807, GNorm = 0.1527
Meta loss on this task batch = 4.4973e-01, Meta loss averaged over last 500 steps = 4.3874e-01, PNorm = 63.3924, GNorm = 0.1477
Meta loss on this task batch = 4.2924e-01, Meta loss averaged over last 500 steps = 4.3862e-01, PNorm = 63.4040, GNorm = 0.1622
Meta loss on this task batch = 4.2355e-01, Meta loss averaged over last 500 steps = 4.3855e-01, PNorm = 63.4156, GNorm = 0.1704
Meta loss on this task batch = 4.1353e-01, Meta loss averaged over last 500 steps = 4.3846e-01, PNorm = 63.4284, GNorm = 0.1710
Meta loss on this task batch = 4.3976e-01, Meta loss averaged over last 500 steps = 4.3853e-01, PNorm = 63.4427, GNorm = 0.1398
Meta loss on this task batch = 3.7023e-01, Meta loss averaged over last 500 steps = 4.3842e-01, PNorm = 63.4582, GNorm = 0.1366
Meta loss on this task batch = 4.5430e-01, Meta loss averaged over last 500 steps = 4.3845e-01, PNorm = 63.4740, GNorm = 0.1711
Meta loss on this task batch = 4.8452e-01, Meta loss averaged over last 500 steps = 4.3857e-01, PNorm = 63.4876, GNorm = 0.1791
Meta loss on this task batch = 4.0542e-01, Meta loss averaged over last 500 steps = 4.3843e-01, PNorm = 63.5016, GNorm = 0.1416
Meta loss on this task batch = 4.2714e-01, Meta loss averaged over last 500 steps = 4.3841e-01, PNorm = 63.5147, GNorm = 0.1714
Meta loss on this task batch = 4.1709e-01, Meta loss averaged over last 500 steps = 4.3830e-01, PNorm = 63.5257, GNorm = 0.1346
Meta loss on this task batch = 4.1195e-01, Meta loss averaged over last 500 steps = 4.3829e-01, PNorm = 63.5353, GNorm = 0.1522
Meta loss on this task batch = 4.6020e-01, Meta loss averaged over last 500 steps = 4.3826e-01, PNorm = 63.5446, GNorm = 0.1669
Took 117.17379546165466 seconds to complete one epoch of meta training
Took 125.10796761512756 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475336
Epoch 62
Meta loss on this task batch = 4.1844e-01, Meta loss averaged over last 500 steps = 4.3828e-01, PNorm = 63.5553, GNorm = 0.1539
Meta loss on this task batch = 4.4766e-01, Meta loss averaged over last 500 steps = 4.3828e-01, PNorm = 63.5665, GNorm = 0.1632
Meta loss on this task batch = 3.9112e-01, Meta loss averaged over last 500 steps = 4.3822e-01, PNorm = 63.5787, GNorm = 0.1274
Meta loss on this task batch = 3.9118e-01, Meta loss averaged over last 500 steps = 4.3807e-01, PNorm = 63.5918, GNorm = 0.1626
Meta loss on this task batch = 4.4866e-01, Meta loss averaged over last 500 steps = 4.3812e-01, PNorm = 63.6047, GNorm = 0.1504
Meta loss on this task batch = 4.4627e-01, Meta loss averaged over last 500 steps = 4.3806e-01, PNorm = 63.6185, GNorm = 0.1588
Meta loss on this task batch = 4.0840e-01, Meta loss averaged over last 500 steps = 4.3799e-01, PNorm = 63.6331, GNorm = 0.1424
Meta loss on this task batch = 4.0448e-01, Meta loss averaged over last 500 steps = 4.3791e-01, PNorm = 63.6485, GNorm = 0.1411
Meta loss on this task batch = 3.8681e-01, Meta loss averaged over last 500 steps = 4.3788e-01, PNorm = 63.6632, GNorm = 0.1198
Meta loss on this task batch = 4.5300e-01, Meta loss averaged over last 500 steps = 4.3789e-01, PNorm = 63.6776, GNorm = 0.1429
Meta loss on this task batch = 4.3926e-01, Meta loss averaged over last 500 steps = 4.3792e-01, PNorm = 63.6903, GNorm = 0.1840
Meta loss on this task batch = 4.5080e-01, Meta loss averaged over last 500 steps = 4.3789e-01, PNorm = 63.7045, GNorm = 0.1703
Meta loss on this task batch = 4.1802e-01, Meta loss averaged over last 500 steps = 4.3781e-01, PNorm = 63.7182, GNorm = 0.1340
Meta loss on this task batch = 4.3020e-01, Meta loss averaged over last 500 steps = 4.3783e-01, PNorm = 63.7318, GNorm = 0.1956
Meta loss on this task batch = 4.9033e-01, Meta loss averaged over last 500 steps = 4.3791e-01, PNorm = 63.7441, GNorm = 0.2523
Meta loss on this task batch = 4.1118e-01, Meta loss averaged over last 500 steps = 4.3769e-01, PNorm = 63.7577, GNorm = 0.1767
Meta loss on this task batch = 3.6409e-01, Meta loss averaged over last 500 steps = 4.3744e-01, PNorm = 63.7724, GNorm = 0.1296
Meta loss on this task batch = 4.9434e-01, Meta loss averaged over last 500 steps = 4.3743e-01, PNorm = 63.7847, GNorm = 0.1727
Meta loss on this task batch = 4.4762e-01, Meta loss averaged over last 500 steps = 4.3756e-01, PNorm = 63.7980, GNorm = 0.1717
Took 116.25611901283264 seconds to complete one epoch of meta training
Took 124.09441614151001 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480157
Epoch 63
Meta loss on this task batch = 4.0976e-01, Meta loss averaged over last 500 steps = 4.3744e-01, PNorm = 63.8114, GNorm = 0.1393
Meta loss on this task batch = 3.5103e-01, Meta loss averaged over last 500 steps = 4.3732e-01, PNorm = 63.8277, GNorm = 0.1507
Meta loss on this task batch = 4.1520e-01, Meta loss averaged over last 500 steps = 4.3722e-01, PNorm = 63.8429, GNorm = 0.1511
Meta loss on this task batch = 4.0620e-01, Meta loss averaged over last 500 steps = 4.3711e-01, PNorm = 63.8581, GNorm = 0.1426
Meta loss on this task batch = 4.0247e-01, Meta loss averaged over last 500 steps = 4.3701e-01, PNorm = 63.8723, GNorm = 0.1588
Meta loss on this task batch = 4.3574e-01, Meta loss averaged over last 500 steps = 4.3716e-01, PNorm = 63.8848, GNorm = 0.2047
Meta loss on this task batch = 4.3511e-01, Meta loss averaged over last 500 steps = 4.3721e-01, PNorm = 63.8960, GNorm = 0.1586
Meta loss on this task batch = 4.7007e-01, Meta loss averaged over last 500 steps = 4.3731e-01, PNorm = 63.9076, GNorm = 0.1588
Meta loss on this task batch = 4.6781e-01, Meta loss averaged over last 500 steps = 4.3733e-01, PNorm = 63.9171, GNorm = 0.2068
Meta loss on this task batch = 3.9005e-01, Meta loss averaged over last 500 steps = 4.3723e-01, PNorm = 63.9255, GNorm = 0.1421
Meta loss on this task batch = 4.1653e-01, Meta loss averaged over last 500 steps = 4.3720e-01, PNorm = 63.9356, GNorm = 0.1769
Meta loss on this task batch = 3.9887e-01, Meta loss averaged over last 500 steps = 4.3719e-01, PNorm = 63.9471, GNorm = 0.1298
Meta loss on this task batch = 4.1458e-01, Meta loss averaged over last 500 steps = 4.3708e-01, PNorm = 63.9596, GNorm = 0.1497
Meta loss on this task batch = 4.1940e-01, Meta loss averaged over last 500 steps = 4.3706e-01, PNorm = 63.9735, GNorm = 0.1315
Meta loss on this task batch = 4.7516e-01, Meta loss averaged over last 500 steps = 4.3721e-01, PNorm = 63.9876, GNorm = 0.1484
Meta loss on this task batch = 4.4058e-01, Meta loss averaged over last 500 steps = 4.3722e-01, PNorm = 64.0008, GNorm = 0.1731
Meta loss on this task batch = 4.4148e-01, Meta loss averaged over last 500 steps = 4.3739e-01, PNorm = 64.0145, GNorm = 0.1488
Meta loss on this task batch = 4.4460e-01, Meta loss averaged over last 500 steps = 4.3724e-01, PNorm = 64.0270, GNorm = 0.1500
Meta loss on this task batch = 4.3879e-01, Meta loss averaged over last 500 steps = 4.3705e-01, PNorm = 64.0411, GNorm = 0.2101
Took 115.96782183647156 seconds to complete one epoch of meta training
Took 123.37483763694763 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463386
Epoch 64
Meta loss on this task batch = 4.0499e-01, Meta loss averaged over last 500 steps = 4.3691e-01, PNorm = 64.0569, GNorm = 0.1456
Meta loss on this task batch = 4.3443e-01, Meta loss averaged over last 500 steps = 4.3686e-01, PNorm = 64.0720, GNorm = 0.1448
Meta loss on this task batch = 4.2046e-01, Meta loss averaged over last 500 steps = 4.3686e-01, PNorm = 64.0871, GNorm = 0.1309
Meta loss on this task batch = 3.9470e-01, Meta loss averaged over last 500 steps = 4.3667e-01, PNorm = 64.1021, GNorm = 0.1597
Meta loss on this task batch = 4.0011e-01, Meta loss averaged over last 500 steps = 4.3656e-01, PNorm = 64.1165, GNorm = 0.1765
Meta loss on this task batch = 3.9846e-01, Meta loss averaged over last 500 steps = 4.3636e-01, PNorm = 64.1292, GNorm = 0.1624
Meta loss on this task batch = 4.4829e-01, Meta loss averaged over last 500 steps = 4.3624e-01, PNorm = 64.1424, GNorm = 0.1543
Meta loss on this task batch = 3.9900e-01, Meta loss averaged over last 500 steps = 4.3618e-01, PNorm = 64.1556, GNorm = 0.1314
Meta loss on this task batch = 4.6286e-01, Meta loss averaged over last 500 steps = 4.3620e-01, PNorm = 64.1662, GNorm = 0.1882
Meta loss on this task batch = 3.6643e-01, Meta loss averaged over last 500 steps = 4.3600e-01, PNorm = 64.1776, GNorm = 0.1327
Meta loss on this task batch = 4.4453e-01, Meta loss averaged over last 500 steps = 4.3604e-01, PNorm = 64.1896, GNorm = 0.1491
Meta loss on this task batch = 3.9924e-01, Meta loss averaged over last 500 steps = 4.3594e-01, PNorm = 64.2030, GNorm = 0.1471
Meta loss on this task batch = 4.2000e-01, Meta loss averaged over last 500 steps = 4.3590e-01, PNorm = 64.2175, GNorm = 0.1677
Meta loss on this task batch = 4.5793e-01, Meta loss averaged over last 500 steps = 4.3587e-01, PNorm = 64.2304, GNorm = 0.1551
Meta loss on this task batch = 4.4754e-01, Meta loss averaged over last 500 steps = 4.3576e-01, PNorm = 64.2442, GNorm = 0.1597
Meta loss on this task batch = 3.5378e-01, Meta loss averaged over last 500 steps = 4.3546e-01, PNorm = 64.2588, GNorm = 0.1405
Meta loss on this task batch = 4.9493e-01, Meta loss averaged over last 500 steps = 4.3565e-01, PNorm = 64.2733, GNorm = 0.2313
Meta loss on this task batch = 4.6098e-01, Meta loss averaged over last 500 steps = 4.3572e-01, PNorm = 64.2854, GNorm = 0.1839
Meta loss on this task batch = 4.0378e-01, Meta loss averaged over last 500 steps = 4.3562e-01, PNorm = 64.2988, GNorm = 0.2076
Took 112.14832639694214 seconds to complete one epoch of meta training
Took 119.65127873420715 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472065
Epoch 65
Meta loss on this task batch = 4.3992e-01, Meta loss averaged over last 500 steps = 4.3569e-01, PNorm = 64.3126, GNorm = 0.1631
Meta loss on this task batch = 4.4912e-01, Meta loss averaged over last 500 steps = 4.3562e-01, PNorm = 64.3267, GNorm = 0.1732
Meta loss on this task batch = 3.8181e-01, Meta loss averaged over last 500 steps = 4.3549e-01, PNorm = 64.3405, GNorm = 0.1563
Meta loss on this task batch = 4.2481e-01, Meta loss averaged over last 500 steps = 4.3545e-01, PNorm = 64.3540, GNorm = 0.1508
Meta loss on this task batch = 4.3315e-01, Meta loss averaged over last 500 steps = 4.3542e-01, PNorm = 64.3656, GNorm = 0.1765
Meta loss on this task batch = 4.6596e-01, Meta loss averaged over last 500 steps = 4.3545e-01, PNorm = 64.3772, GNorm = 0.1610
Meta loss on this task batch = 3.8427e-01, Meta loss averaged over last 500 steps = 4.3537e-01, PNorm = 64.3911, GNorm = 0.1743
Meta loss on this task batch = 4.7450e-01, Meta loss averaged over last 500 steps = 4.3535e-01, PNorm = 64.4046, GNorm = 0.1628
Meta loss on this task batch = 3.7413e-01, Meta loss averaged over last 500 steps = 4.3526e-01, PNorm = 64.4184, GNorm = 0.1408
Meta loss on this task batch = 3.6322e-01, Meta loss averaged over last 500 steps = 4.3504e-01, PNorm = 64.4332, GNorm = 0.1334
Meta loss on this task batch = 4.6010e-01, Meta loss averaged over last 500 steps = 4.3507e-01, PNorm = 64.4473, GNorm = 0.1738
Meta loss on this task batch = 4.4920e-01, Meta loss averaged over last 500 steps = 4.3497e-01, PNorm = 64.4625, GNorm = 0.1499
Meta loss on this task batch = 4.1463e-01, Meta loss averaged over last 500 steps = 4.3492e-01, PNorm = 64.4775, GNorm = 0.1419
Meta loss on this task batch = 3.9133e-01, Meta loss averaged over last 500 steps = 4.3478e-01, PNorm = 64.4923, GNorm = 0.1382
Meta loss on this task batch = 4.0878e-01, Meta loss averaged over last 500 steps = 4.3472e-01, PNorm = 64.5073, GNorm = 0.1543
Meta loss on this task batch = 3.8523e-01, Meta loss averaged over last 500 steps = 4.3446e-01, PNorm = 64.5219, GNorm = 0.1745
Meta loss on this task batch = 4.7139e-01, Meta loss averaged over last 500 steps = 4.3470e-01, PNorm = 64.5350, GNorm = 0.1944
Meta loss on this task batch = 4.2514e-01, Meta loss averaged over last 500 steps = 4.3457e-01, PNorm = 64.5478, GNorm = 0.1396
Meta loss on this task batch = 3.5315e-01, Meta loss averaged over last 500 steps = 4.3443e-01, PNorm = 64.5608, GNorm = 0.2033
Took 116.05574321746826 seconds to complete one epoch of meta training
Took 124.46797251701355 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485137
Epoch 66
Meta loss on this task batch = 4.6881e-01, Meta loss averaged over last 500 steps = 4.3455e-01, PNorm = 64.5726, GNorm = 0.1847
Meta loss on this task batch = 4.2519e-01, Meta loss averaged over last 500 steps = 4.3448e-01, PNorm = 64.5856, GNorm = 0.1635
Meta loss on this task batch = 3.9694e-01, Meta loss averaged over last 500 steps = 4.3434e-01, PNorm = 64.5994, GNorm = 0.1699
Meta loss on this task batch = 4.2252e-01, Meta loss averaged over last 500 steps = 4.3421e-01, PNorm = 64.6149, GNorm = 0.1561
Meta loss on this task batch = 4.7790e-01, Meta loss averaged over last 500 steps = 4.3429e-01, PNorm = 64.6291, GNorm = 0.1657
Meta loss on this task batch = 4.5791e-01, Meta loss averaged over last 500 steps = 4.3429e-01, PNorm = 64.6437, GNorm = 0.1455
Meta loss on this task batch = 4.3376e-01, Meta loss averaged over last 500 steps = 4.3432e-01, PNorm = 64.6579, GNorm = 0.1795
Meta loss on this task batch = 4.0573e-01, Meta loss averaged over last 500 steps = 4.3423e-01, PNorm = 64.6718, GNorm = 0.1848
Meta loss on this task batch = 3.4742e-01, Meta loss averaged over last 500 steps = 4.3400e-01, PNorm = 64.6864, GNorm = 0.1457
Meta loss on this task batch = 3.7639e-01, Meta loss averaged over last 500 steps = 4.3382e-01, PNorm = 64.7013, GNorm = 0.1614
Meta loss on this task batch = 4.0093e-01, Meta loss averaged over last 500 steps = 4.3377e-01, PNorm = 64.7164, GNorm = 0.1674
Meta loss on this task batch = 4.2826e-01, Meta loss averaged over last 500 steps = 4.3367e-01, PNorm = 64.7308, GNorm = 0.1771
Meta loss on this task batch = 4.0465e-01, Meta loss averaged over last 500 steps = 4.3360e-01, PNorm = 64.7438, GNorm = 0.1769
Meta loss on this task batch = 4.2755e-01, Meta loss averaged over last 500 steps = 4.3363e-01, PNorm = 64.7566, GNorm = 0.1604
Meta loss on this task batch = 4.5471e-01, Meta loss averaged over last 500 steps = 4.3360e-01, PNorm = 64.7693, GNorm = 0.2003
Meta loss on this task batch = 4.1028e-01, Meta loss averaged over last 500 steps = 4.3345e-01, PNorm = 64.7815, GNorm = 0.1402
Meta loss on this task batch = 4.5476e-01, Meta loss averaged over last 500 steps = 4.3349e-01, PNorm = 64.7917, GNorm = 0.1936
Meta loss on this task batch = 4.7195e-01, Meta loss averaged over last 500 steps = 4.3353e-01, PNorm = 64.8028, GNorm = 0.1566
Meta loss on this task batch = 4.0873e-01, Meta loss averaged over last 500 steps = 4.3347e-01, PNorm = 64.8122, GNorm = 0.1876
Took 117.79974174499512 seconds to complete one epoch of meta training
Took 125.50889134407043 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485134
Epoch 67
Meta loss on this task batch = 3.8238e-01, Meta loss averaged over last 500 steps = 4.3342e-01, PNorm = 64.8227, GNorm = 0.1260
Meta loss on this task batch = 3.8414e-01, Meta loss averaged over last 500 steps = 4.3329e-01, PNorm = 64.8344, GNorm = 0.1282
Meta loss on this task batch = 3.9604e-01, Meta loss averaged over last 500 steps = 4.3325e-01, PNorm = 64.8475, GNorm = 0.1245
Meta loss on this task batch = 3.8576e-01, Meta loss averaged over last 500 steps = 4.3313e-01, PNorm = 64.8621, GNorm = 0.1391
Meta loss on this task batch = 4.0708e-01, Meta loss averaged over last 500 steps = 4.3306e-01, PNorm = 64.8777, GNorm = 0.1521
Meta loss on this task batch = 3.7100e-01, Meta loss averaged over last 500 steps = 4.3290e-01, PNorm = 64.8946, GNorm = 0.1444
Meta loss on this task batch = 4.8319e-01, Meta loss averaged over last 500 steps = 4.3296e-01, PNorm = 64.9107, GNorm = 0.1884
Meta loss on this task batch = 4.2943e-01, Meta loss averaged over last 500 steps = 4.3288e-01, PNorm = 64.9274, GNorm = 0.1649
Meta loss on this task batch = 4.1780e-01, Meta loss averaged over last 500 steps = 4.3280e-01, PNorm = 64.9415, GNorm = 0.1712
Meta loss on this task batch = 4.6368e-01, Meta loss averaged over last 500 steps = 4.3299e-01, PNorm = 64.9554, GNorm = 0.1589
Meta loss on this task batch = 4.2192e-01, Meta loss averaged over last 500 steps = 4.3295e-01, PNorm = 64.9692, GNorm = 0.1763
Meta loss on this task batch = 4.3951e-01, Meta loss averaged over last 500 steps = 4.3300e-01, PNorm = 64.9824, GNorm = 0.2292
Meta loss on this task batch = 4.3460e-01, Meta loss averaged over last 500 steps = 4.3295e-01, PNorm = 64.9963, GNorm = 0.1639
Meta loss on this task batch = 4.4874e-01, Meta loss averaged over last 500 steps = 4.3286e-01, PNorm = 65.0110, GNorm = 0.1359
Meta loss on this task batch = 4.6071e-01, Meta loss averaged over last 500 steps = 4.3290e-01, PNorm = 65.0248, GNorm = 0.1711
Meta loss on this task batch = 4.2242e-01, Meta loss averaged over last 500 steps = 4.3295e-01, PNorm = 65.0391, GNorm = 0.1454
Meta loss on this task batch = 4.7208e-01, Meta loss averaged over last 500 steps = 4.3298e-01, PNorm = 65.0530, GNorm = 0.1739
Meta loss on this task batch = 4.2855e-01, Meta loss averaged over last 500 steps = 4.3290e-01, PNorm = 65.0643, GNorm = 0.1932
Meta loss on this task batch = 4.6118e-01, Meta loss averaged over last 500 steps = 4.3298e-01, PNorm = 65.0764, GNorm = 0.1668
Took 117.40608930587769 seconds to complete one epoch of meta training
Took 125.3294882774353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462993
Epoch 68
Meta loss on this task batch = 4.3632e-01, Meta loss averaged over last 500 steps = 4.3291e-01, PNorm = 65.0897, GNorm = 0.1384
Meta loss on this task batch = 4.7767e-01, Meta loss averaged over last 500 steps = 4.3286e-01, PNorm = 65.1030, GNorm = 0.1841
Meta loss on this task batch = 4.3241e-01, Meta loss averaged over last 500 steps = 4.3288e-01, PNorm = 65.1176, GNorm = 0.1467
Meta loss on this task batch = 3.7130e-01, Meta loss averaged over last 500 steps = 4.3281e-01, PNorm = 65.1326, GNorm = 0.1431
Meta loss on this task batch = 4.1331e-01, Meta loss averaged over last 500 steps = 4.3284e-01, PNorm = 65.1493, GNorm = 0.1380
Meta loss on this task batch = 4.1460e-01, Meta loss averaged over last 500 steps = 4.3261e-01, PNorm = 65.1670, GNorm = 0.1486
Meta loss on this task batch = 4.2321e-01, Meta loss averaged over last 500 steps = 4.3265e-01, PNorm = 65.1840, GNorm = 0.1502
Meta loss on this task batch = 4.4135e-01, Meta loss averaged over last 500 steps = 4.3256e-01, PNorm = 65.2004, GNorm = 0.1378
Meta loss on this task batch = 3.9037e-01, Meta loss averaged over last 500 steps = 4.3238e-01, PNorm = 65.2156, GNorm = 0.1572
Meta loss on this task batch = 3.5852e-01, Meta loss averaged over last 500 steps = 4.3236e-01, PNorm = 65.2302, GNorm = 0.1538
Meta loss on this task batch = 3.8045e-01, Meta loss averaged over last 500 steps = 4.3239e-01, PNorm = 65.2445, GNorm = 0.1443
Meta loss on this task batch = 4.2205e-01, Meta loss averaged over last 500 steps = 4.3234e-01, PNorm = 65.2595, GNorm = 0.1895
Meta loss on this task batch = 4.5598e-01, Meta loss averaged over last 500 steps = 4.3251e-01, PNorm = 65.2733, GNorm = 0.1743
Meta loss on this task batch = 4.2300e-01, Meta loss averaged over last 500 steps = 4.3245e-01, PNorm = 65.2866, GNorm = 0.1775
Meta loss on this task batch = 4.1673e-01, Meta loss averaged over last 500 steps = 4.3234e-01, PNorm = 65.2984, GNorm = 0.1917
Meta loss on this task batch = 4.3025e-01, Meta loss averaged over last 500 steps = 4.3228e-01, PNorm = 65.3098, GNorm = 0.1803
Meta loss on this task batch = 4.4712e-01, Meta loss averaged over last 500 steps = 4.3234e-01, PNorm = 65.3208, GNorm = 0.1617
Meta loss on this task batch = 4.0616e-01, Meta loss averaged over last 500 steps = 4.3217e-01, PNorm = 65.3332, GNorm = 0.1621
Meta loss on this task batch = 4.7707e-01, Meta loss averaged over last 500 steps = 4.3227e-01, PNorm = 65.3456, GNorm = 0.1886
Took 115.29355025291443 seconds to complete one epoch of meta training
Took 122.74782681465149 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471407
Epoch 69
Meta loss on this task batch = 4.5417e-01, Meta loss averaged over last 500 steps = 4.3244e-01, PNorm = 65.3591, GNorm = 0.1894
Meta loss on this task batch = 3.7065e-01, Meta loss averaged over last 500 steps = 4.3226e-01, PNorm = 65.3732, GNorm = 0.1498
Meta loss on this task batch = 4.8652e-01, Meta loss averaged over last 500 steps = 4.3228e-01, PNorm = 65.3884, GNorm = 0.1532
Meta loss on this task batch = 4.0070e-01, Meta loss averaged over last 500 steps = 4.3215e-01, PNorm = 65.4049, GNorm = 0.2056
Meta loss on this task batch = 4.2247e-01, Meta loss averaged over last 500 steps = 4.3192e-01, PNorm = 65.4211, GNorm = 0.1950
Meta loss on this task batch = 4.3264e-01, Meta loss averaged over last 500 steps = 4.3192e-01, PNorm = 65.4369, GNorm = 0.1626
Meta loss on this task batch = 4.4321e-01, Meta loss averaged over last 500 steps = 4.3197e-01, PNorm = 65.4537, GNorm = 0.1460
Meta loss on this task batch = 4.5993e-01, Meta loss averaged over last 500 steps = 4.3202e-01, PNorm = 65.4685, GNorm = 0.1761
Meta loss on this task batch = 4.6990e-01, Meta loss averaged over last 500 steps = 4.3204e-01, PNorm = 65.4811, GNorm = 0.1722
Meta loss on this task batch = 4.2091e-01, Meta loss averaged over last 500 steps = 4.3202e-01, PNorm = 65.4927, GNorm = 0.1710
Meta loss on this task batch = 4.4981e-01, Meta loss averaged over last 500 steps = 4.3196e-01, PNorm = 65.5021, GNorm = 0.2575
Meta loss on this task batch = 3.9977e-01, Meta loss averaged over last 500 steps = 4.3188e-01, PNorm = 65.5123, GNorm = 0.1801
Meta loss on this task batch = 3.9034e-01, Meta loss averaged over last 500 steps = 4.3188e-01, PNorm = 65.5238, GNorm = 0.1739
Meta loss on this task batch = 3.8424e-01, Meta loss averaged over last 500 steps = 4.3168e-01, PNorm = 65.5369, GNorm = 0.1498
Meta loss on this task batch = 4.0105e-01, Meta loss averaged over last 500 steps = 4.3158e-01, PNorm = 65.5510, GNorm = 0.1437
Meta loss on this task batch = 4.0216e-01, Meta loss averaged over last 500 steps = 4.3153e-01, PNorm = 65.5648, GNorm = 0.1439
Meta loss on this task batch = 4.0226e-01, Meta loss averaged over last 500 steps = 4.3145e-01, PNorm = 65.5791, GNorm = 0.1307
Meta loss on this task batch = 4.5091e-01, Meta loss averaged over last 500 steps = 4.3153e-01, PNorm = 65.5915, GNorm = 0.2296
Meta loss on this task batch = 4.5249e-01, Meta loss averaged over last 500 steps = 4.3143e-01, PNorm = 65.6046, GNorm = 0.1546
Took 117.33491921424866 seconds to complete one epoch of meta training
Took 125.30217266082764 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.453866
Epoch 70
Meta loss on this task batch = 4.0024e-01, Meta loss averaged over last 500 steps = 4.3138e-01, PNorm = 65.6175, GNorm = 0.1687
Meta loss on this task batch = 4.2085e-01, Meta loss averaged over last 500 steps = 4.3139e-01, PNorm = 65.6316, GNorm = 0.1397
Meta loss on this task batch = 3.9103e-01, Meta loss averaged over last 500 steps = 4.3129e-01, PNorm = 65.6459, GNorm = 0.1212
Meta loss on this task batch = 4.0093e-01, Meta loss averaged over last 500 steps = 4.3114e-01, PNorm = 65.6614, GNorm = 0.1639
Meta loss on this task batch = 4.3386e-01, Meta loss averaged over last 500 steps = 4.3117e-01, PNorm = 65.6767, GNorm = 0.1642
Meta loss on this task batch = 4.4595e-01, Meta loss averaged over last 500 steps = 4.3110e-01, PNorm = 65.6929, GNorm = 0.1500
Meta loss on this task batch = 4.5482e-01, Meta loss averaged over last 500 steps = 4.3120e-01, PNorm = 65.7085, GNorm = 0.1815
Meta loss on this task batch = 4.5638e-01, Meta loss averaged over last 500 steps = 4.3122e-01, PNorm = 65.7252, GNorm = 0.1613
Meta loss on this task batch = 4.5471e-01, Meta loss averaged over last 500 steps = 4.3140e-01, PNorm = 65.7413, GNorm = 0.1734
Meta loss on this task batch = 4.0773e-01, Meta loss averaged over last 500 steps = 4.3133e-01, PNorm = 65.7589, GNorm = 0.1644
Meta loss on this task batch = 4.6895e-01, Meta loss averaged over last 500 steps = 4.3144e-01, PNorm = 65.7733, GNorm = 0.2000
Meta loss on this task batch = 3.9669e-01, Meta loss averaged over last 500 steps = 4.3145e-01, PNorm = 65.7857, GNorm = 0.2028
Meta loss on this task batch = 3.9460e-01, Meta loss averaged over last 500 steps = 4.3140e-01, PNorm = 65.7978, GNorm = 0.1455
Meta loss on this task batch = 3.9112e-01, Meta loss averaged over last 500 steps = 4.3129e-01, PNorm = 65.8100, GNorm = 0.1609
Meta loss on this task batch = 4.4863e-01, Meta loss averaged over last 500 steps = 4.3140e-01, PNorm = 65.8226, GNorm = 0.1533
Meta loss on this task batch = 3.6081e-01, Meta loss averaged over last 500 steps = 4.3119e-01, PNorm = 65.8366, GNorm = 0.1450
Meta loss on this task batch = 4.0926e-01, Meta loss averaged over last 500 steps = 4.3104e-01, PNorm = 65.8504, GNorm = 0.1570
Meta loss on this task batch = 4.2214e-01, Meta loss averaged over last 500 steps = 4.3096e-01, PNorm = 65.8641, GNorm = 0.1500
Meta loss on this task batch = 4.7279e-01, Meta loss averaged over last 500 steps = 4.3109e-01, PNorm = 65.8747, GNorm = 0.2041
Took 115.74216604232788 seconds to complete one epoch of meta training
Took 123.5631971359253 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482760
Epoch 71
Meta loss on this task batch = 3.6044e-01, Meta loss averaged over last 500 steps = 4.3091e-01, PNorm = 65.8872, GNorm = 0.1391
Meta loss on this task batch = 4.1957e-01, Meta loss averaged over last 500 steps = 4.3075e-01, PNorm = 65.9002, GNorm = 0.2244
Meta loss on this task batch = 4.5236e-01, Meta loss averaged over last 500 steps = 4.3076e-01, PNorm = 65.9142, GNorm = 0.1532
Meta loss on this task batch = 4.0809e-01, Meta loss averaged over last 500 steps = 4.3055e-01, PNorm = 65.9282, GNorm = 0.1433
Meta loss on this task batch = 3.9420e-01, Meta loss averaged over last 500 steps = 4.3033e-01, PNorm = 65.9430, GNorm = 0.1511
Meta loss on this task batch = 4.5695e-01, Meta loss averaged over last 500 steps = 4.3037e-01, PNorm = 65.9574, GNorm = 0.1459
Meta loss on this task batch = 4.0994e-01, Meta loss averaged over last 500 steps = 4.3021e-01, PNorm = 65.9716, GNorm = 0.1709
Meta loss on this task batch = 4.4334e-01, Meta loss averaged over last 500 steps = 4.3017e-01, PNorm = 65.9844, GNorm = 0.1657
Meta loss on this task batch = 4.0244e-01, Meta loss averaged over last 500 steps = 4.3006e-01, PNorm = 65.9975, GNorm = 0.1629
Meta loss on this task batch = 3.4294e-01, Meta loss averaged over last 500 steps = 4.2975e-01, PNorm = 66.0131, GNorm = 0.1639
Meta loss on this task batch = 4.7189e-01, Meta loss averaged over last 500 steps = 4.2988e-01, PNorm = 66.0287, GNorm = 0.1625
Meta loss on this task batch = 4.0030e-01, Meta loss averaged over last 500 steps = 4.2986e-01, PNorm = 66.0451, GNorm = 0.1622
Meta loss on this task batch = 3.6056e-01, Meta loss averaged over last 500 steps = 4.2967e-01, PNorm = 66.0612, GNorm = 0.1513
Meta loss on this task batch = 4.1429e-01, Meta loss averaged over last 500 steps = 4.2954e-01, PNorm = 66.0766, GNorm = 0.1617
Meta loss on this task batch = 4.1730e-01, Meta loss averaged over last 500 steps = 4.2960e-01, PNorm = 66.0912, GNorm = 0.1717
Meta loss on this task batch = 4.6528e-01, Meta loss averaged over last 500 steps = 4.2960e-01, PNorm = 66.1034, GNorm = 0.1808
Meta loss on this task batch = 4.4594e-01, Meta loss averaged over last 500 steps = 4.2952e-01, PNorm = 66.1155, GNorm = 0.1844
Meta loss on this task batch = 3.4765e-01, Meta loss averaged over last 500 steps = 4.2954e-01, PNorm = 66.1290, GNorm = 0.1434
Meta loss on this task batch = 4.5735e-01, Meta loss averaged over last 500 steps = 4.2963e-01, PNorm = 66.1409, GNorm = 0.1687
Took 117.67857265472412 seconds to complete one epoch of meta training
Took 125.03324723243713 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489672
Epoch 72
Meta loss on this task batch = 4.3589e-01, Meta loss averaged over last 500 steps = 4.2958e-01, PNorm = 66.1529, GNorm = 0.1752
Meta loss on this task batch = 4.2650e-01, Meta loss averaged over last 500 steps = 4.2963e-01, PNorm = 66.1660, GNorm = 0.1566
Meta loss on this task batch = 4.8230e-01, Meta loss averaged over last 500 steps = 4.2966e-01, PNorm = 66.1786, GNorm = 0.2063
Meta loss on this task batch = 3.6005e-01, Meta loss averaged over last 500 steps = 4.2941e-01, PNorm = 66.1915, GNorm = 0.1284
Meta loss on this task batch = 4.2734e-01, Meta loss averaged over last 500 steps = 4.2935e-01, PNorm = 66.2060, GNorm = 0.1999
Meta loss on this task batch = 4.6218e-01, Meta loss averaged over last 500 steps = 4.2945e-01, PNorm = 66.2197, GNorm = 0.1417
Meta loss on this task batch = 4.6248e-01, Meta loss averaged over last 500 steps = 4.2947e-01, PNorm = 66.2335, GNorm = 0.2091
Meta loss on this task batch = 3.9501e-01, Meta loss averaged over last 500 steps = 4.2944e-01, PNorm = 66.2488, GNorm = 0.1355
Meta loss on this task batch = 4.2525e-01, Meta loss averaged over last 500 steps = 4.2941e-01, PNorm = 66.2652, GNorm = 0.1451
Meta loss on this task batch = 4.0326e-01, Meta loss averaged over last 500 steps = 4.2944e-01, PNorm = 66.2805, GNorm = 0.1758
Meta loss on this task batch = 4.1410e-01, Meta loss averaged over last 500 steps = 4.2940e-01, PNorm = 66.2950, GNorm = 0.1673
Meta loss on this task batch = 4.4369e-01, Meta loss averaged over last 500 steps = 4.2941e-01, PNorm = 66.3082, GNorm = 0.1816
Meta loss on this task batch = 3.8212e-01, Meta loss averaged over last 500 steps = 4.2932e-01, PNorm = 66.3224, GNorm = 0.2041
Meta loss on this task batch = 4.0740e-01, Meta loss averaged over last 500 steps = 4.2916e-01, PNorm = 66.3367, GNorm = 0.1559
Meta loss on this task batch = 4.9353e-01, Meta loss averaged over last 500 steps = 4.2928e-01, PNorm = 66.3487, GNorm = 0.2121
Meta loss on this task batch = 4.4710e-01, Meta loss averaged over last 500 steps = 4.2927e-01, PNorm = 66.3603, GNorm = 0.1548
Meta loss on this task batch = 4.0941e-01, Meta loss averaged over last 500 steps = 4.2910e-01, PNorm = 66.3721, GNorm = 0.1620
Meta loss on this task batch = 3.6113e-01, Meta loss averaged over last 500 steps = 4.2896e-01, PNorm = 66.3854, GNorm = 0.1403
Meta loss on this task batch = 3.5856e-01, Meta loss averaged over last 500 steps = 4.2879e-01, PNorm = 66.4005, GNorm = 0.1489
Took 118.86190128326416 seconds to complete one epoch of meta training
Took 126.24969744682312 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473334
Epoch 73
Meta loss on this task batch = 4.1036e-01, Meta loss averaged over last 500 steps = 4.2877e-01, PNorm = 66.4161, GNorm = 0.1507
Meta loss on this task batch = 4.6254e-01, Meta loss averaged over last 500 steps = 4.2879e-01, PNorm = 66.4325, GNorm = 0.2193
Meta loss on this task batch = 4.7953e-01, Meta loss averaged over last 500 steps = 4.2879e-01, PNorm = 66.4477, GNorm = 0.1835
Meta loss on this task batch = 4.3816e-01, Meta loss averaged over last 500 steps = 4.2871e-01, PNorm = 66.4623, GNorm = 0.1872
Meta loss on this task batch = 3.8478e-01, Meta loss averaged over last 500 steps = 4.2864e-01, PNorm = 66.4761, GNorm = 0.1491
Meta loss on this task batch = 3.8300e-01, Meta loss averaged over last 500 steps = 4.2869e-01, PNorm = 66.4893, GNorm = 0.1514
Meta loss on this task batch = 3.8496e-01, Meta loss averaged over last 500 steps = 4.2858e-01, PNorm = 66.5030, GNorm = 0.1470
Meta loss on this task batch = 4.2996e-01, Meta loss averaged over last 500 steps = 4.2861e-01, PNorm = 66.5168, GNorm = 0.1738
Meta loss on this task batch = 3.9577e-01, Meta loss averaged over last 500 steps = 4.2863e-01, PNorm = 66.5307, GNorm = 0.1275
Meta loss on this task batch = 4.3041e-01, Meta loss averaged over last 500 steps = 4.2864e-01, PNorm = 66.5438, GNorm = 0.1940
Meta loss on this task batch = 4.3956e-01, Meta loss averaged over last 500 steps = 4.2863e-01, PNorm = 66.5560, GNorm = 0.2016
Meta loss on this task batch = 4.7815e-01, Meta loss averaged over last 500 steps = 4.2866e-01, PNorm = 66.5655, GNorm = 0.2331
Meta loss on this task batch = 3.9342e-01, Meta loss averaged over last 500 steps = 4.2855e-01, PNorm = 66.5770, GNorm = 0.1568
Meta loss on this task batch = 3.4818e-01, Meta loss averaged over last 500 steps = 4.2832e-01, PNorm = 66.5903, GNorm = 0.1467
Meta loss on this task batch = 4.3067e-01, Meta loss averaged over last 500 steps = 4.2837e-01, PNorm = 66.6040, GNorm = 0.1515
Meta loss on this task batch = 4.5926e-01, Meta loss averaged over last 500 steps = 4.2830e-01, PNorm = 66.6170, GNorm = 0.1736
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 4.2820e-01, PNorm = 66.6308, GNorm = 0.1433
Meta loss on this task batch = 4.2599e-01, Meta loss averaged over last 500 steps = 4.2827e-01, PNorm = 66.6444, GNorm = 0.1575
Meta loss on this task batch = 4.3375e-01, Meta loss averaged over last 500 steps = 4.2833e-01, PNorm = 66.6575, GNorm = 0.2169
Took 115.91983389854431 seconds to complete one epoch of meta training
Took 123.6709623336792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473079
Epoch 74
Meta loss on this task batch = 4.0201e-01, Meta loss averaged over last 500 steps = 4.2826e-01, PNorm = 66.6709, GNorm = 0.1757
Meta loss on this task batch = 4.3038e-01, Meta loss averaged over last 500 steps = 4.2829e-01, PNorm = 66.6844, GNorm = 0.1589
Meta loss on this task batch = 4.1341e-01, Meta loss averaged over last 500 steps = 4.2810e-01, PNorm = 66.6979, GNorm = 0.1731
Meta loss on this task batch = 4.3614e-01, Meta loss averaged over last 500 steps = 4.2801e-01, PNorm = 66.7114, GNorm = 0.1827
Meta loss on this task batch = 3.9230e-01, Meta loss averaged over last 500 steps = 4.2795e-01, PNorm = 66.7243, GNorm = 0.1684
Meta loss on this task batch = 4.5666e-01, Meta loss averaged over last 500 steps = 4.2784e-01, PNorm = 66.7376, GNorm = 0.1449
Meta loss on this task batch = 3.8949e-01, Meta loss averaged over last 500 steps = 4.2785e-01, PNorm = 66.7509, GNorm = 0.1682
Meta loss on this task batch = 4.2058e-01, Meta loss averaged over last 500 steps = 4.2780e-01, PNorm = 66.7650, GNorm = 0.1668
Meta loss on this task batch = 4.3199e-01, Meta loss averaged over last 500 steps = 4.2785e-01, PNorm = 66.7786, GNorm = 0.1716
Meta loss on this task batch = 4.2509e-01, Meta loss averaged over last 500 steps = 4.2780e-01, PNorm = 66.7926, GNorm = 0.1374
Meta loss on this task batch = 3.5655e-01, Meta loss averaged over last 500 steps = 4.2770e-01, PNorm = 66.8069, GNorm = 0.1470
Meta loss on this task batch = 3.6712e-01, Meta loss averaged over last 500 steps = 4.2760e-01, PNorm = 66.8207, GNorm = 0.1315
Meta loss on this task batch = 3.5523e-01, Meta loss averaged over last 500 steps = 4.2744e-01, PNorm = 66.8346, GNorm = 0.1497
Meta loss on this task batch = 4.6987e-01, Meta loss averaged over last 500 steps = 4.2749e-01, PNorm = 66.8438, GNorm = 0.2861
Meta loss on this task batch = 5.0980e-01, Meta loss averaged over last 500 steps = 4.2760e-01, PNorm = 66.8518, GNorm = 0.1799
Meta loss on this task batch = 3.8753e-01, Meta loss averaged over last 500 steps = 4.2747e-01, PNorm = 66.8596, GNorm = 0.1496
Meta loss on this task batch = 4.0866e-01, Meta loss averaged over last 500 steps = 4.2741e-01, PNorm = 66.8685, GNorm = 0.1628
Meta loss on this task batch = 4.3026e-01, Meta loss averaged over last 500 steps = 4.2733e-01, PNorm = 66.8792, GNorm = 0.1592
Meta loss on this task batch = 3.8259e-01, Meta loss averaged over last 500 steps = 4.2710e-01, PNorm = 66.8924, GNorm = 0.1887
Took 117.72744011878967 seconds to complete one epoch of meta training
Took 125.71686339378357 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464145
Epoch 75
Meta loss on this task batch = 4.1433e-01, Meta loss averaged over last 500 steps = 4.2710e-01, PNorm = 66.9057, GNorm = 0.1336
Meta loss on this task batch = 4.1018e-01, Meta loss averaged over last 500 steps = 4.2721e-01, PNorm = 66.9199, GNorm = 0.1579
Meta loss on this task batch = 4.2763e-01, Meta loss averaged over last 500 steps = 4.2720e-01, PNorm = 66.9346, GNorm = 0.1666
Meta loss on this task batch = 3.5671e-01, Meta loss averaged over last 500 steps = 4.2702e-01, PNorm = 66.9504, GNorm = 0.1356
Meta loss on this task batch = 4.3199e-01, Meta loss averaged over last 500 steps = 4.2680e-01, PNorm = 66.9640, GNorm = 0.1623
Meta loss on this task batch = 4.2652e-01, Meta loss averaged over last 500 steps = 4.2673e-01, PNorm = 66.9760, GNorm = 0.1608
Meta loss on this task batch = 4.3538e-01, Meta loss averaged over last 500 steps = 4.2673e-01, PNorm = 66.9884, GNorm = 0.1965
Meta loss on this task batch = 3.9825e-01, Meta loss averaged over last 500 steps = 4.2666e-01, PNorm = 66.9992, GNorm = 0.1731
Meta loss on this task batch = 4.9968e-01, Meta loss averaged over last 500 steps = 4.2674e-01, PNorm = 67.0090, GNorm = 0.1859
Meta loss on this task batch = 4.1838e-01, Meta loss averaged over last 500 steps = 4.2678e-01, PNorm = 67.0195, GNorm = 0.1830
Meta loss on this task batch = 4.4193e-01, Meta loss averaged over last 500 steps = 4.2676e-01, PNorm = 67.0300, GNorm = 0.1692
Meta loss on this task batch = 4.1902e-01, Meta loss averaged over last 500 steps = 4.2663e-01, PNorm = 67.0404, GNorm = 0.2174
Meta loss on this task batch = 4.1871e-01, Meta loss averaged over last 500 steps = 4.2662e-01, PNorm = 67.0524, GNorm = 0.1572
Meta loss on this task batch = 3.9807e-01, Meta loss averaged over last 500 steps = 4.2647e-01, PNorm = 67.0656, GNorm = 0.1762
Meta loss on this task batch = 3.9525e-01, Meta loss averaged over last 500 steps = 4.2647e-01, PNorm = 67.0800, GNorm = 0.1453
Meta loss on this task batch = 4.1432e-01, Meta loss averaged over last 500 steps = 4.2645e-01, PNorm = 67.0965, GNorm = 0.1790
Meta loss on this task batch = 3.6427e-01, Meta loss averaged over last 500 steps = 4.2628e-01, PNorm = 67.1138, GNorm = 0.1539
Meta loss on this task batch = 3.4942e-01, Meta loss averaged over last 500 steps = 4.2619e-01, PNorm = 67.1311, GNorm = 0.1473
Meta loss on this task batch = 4.0966e-01, Meta loss averaged over last 500 steps = 4.2607e-01, PNorm = 67.1463, GNorm = 0.2306
Took 116.62218022346497 seconds to complete one epoch of meta training
Took 124.477623462677 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463132
Epoch 76
Meta loss on this task batch = 4.2158e-01, Meta loss averaged over last 500 steps = 4.2595e-01, PNorm = 67.1589, GNorm = 0.1844
Meta loss on this task batch = 4.0512e-01, Meta loss averaged over last 500 steps = 4.2581e-01, PNorm = 67.1697, GNorm = 0.1931
Meta loss on this task batch = 4.4892e-01, Meta loss averaged over last 500 steps = 4.2591e-01, PNorm = 67.1807, GNorm = 0.1717
Meta loss on this task batch = 3.8419e-01, Meta loss averaged over last 500 steps = 4.2585e-01, PNorm = 67.1915, GNorm = 0.1465
Meta loss on this task batch = 4.1583e-01, Meta loss averaged over last 500 steps = 4.2584e-01, PNorm = 67.2027, GNorm = 0.1412
Meta loss on this task batch = 3.2584e-01, Meta loss averaged over last 500 steps = 4.2562e-01, PNorm = 67.2151, GNorm = 0.2027
Meta loss on this task batch = 4.0399e-01, Meta loss averaged over last 500 steps = 4.2555e-01, PNorm = 67.2295, GNorm = 0.1679
Meta loss on this task batch = 4.4944e-01, Meta loss averaged over last 500 steps = 4.2554e-01, PNorm = 67.2447, GNorm = 0.1642
Meta loss on this task batch = 4.2035e-01, Meta loss averaged over last 500 steps = 4.2549e-01, PNorm = 67.2597, GNorm = 0.1611
Meta loss on this task batch = 4.3237e-01, Meta loss averaged over last 500 steps = 4.2535e-01, PNorm = 67.2743, GNorm = 0.1510
Meta loss on this task batch = 4.7729e-01, Meta loss averaged over last 500 steps = 4.2535e-01, PNorm = 67.2868, GNorm = 0.1901
Meta loss on this task batch = 3.8176e-01, Meta loss averaged over last 500 steps = 4.2530e-01, PNorm = 67.2986, GNorm = 0.1898
Meta loss on this task batch = 4.0969e-01, Meta loss averaged over last 500 steps = 4.2529e-01, PNorm = 67.3101, GNorm = 0.1998
Meta loss on this task batch = 3.6252e-01, Meta loss averaged over last 500 steps = 4.2512e-01, PNorm = 67.3229, GNorm = 0.1652
Meta loss on this task batch = 4.1142e-01, Meta loss averaged over last 500 steps = 4.2513e-01, PNorm = 67.3352, GNorm = 0.1603
Meta loss on this task batch = 4.7069e-01, Meta loss averaged over last 500 steps = 4.2518e-01, PNorm = 67.3477, GNorm = 0.1702
Meta loss on this task batch = 3.8980e-01, Meta loss averaged over last 500 steps = 4.2511e-01, PNorm = 67.3614, GNorm = 0.1568
Meta loss on this task batch = 4.2325e-01, Meta loss averaged over last 500 steps = 4.2497e-01, PNorm = 67.3766, GNorm = 0.1658
Meta loss on this task batch = 4.3734e-01, Meta loss averaged over last 500 steps = 4.2508e-01, PNorm = 67.3910, GNorm = 0.1914
Took 115.84135246276855 seconds to complete one epoch of meta training
Took 123.54118609428406 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482442
Epoch 77
Meta loss on this task batch = 3.9757e-01, Meta loss averaged over last 500 steps = 4.2514e-01, PNorm = 67.4051, GNorm = 0.1448
Meta loss on this task batch = 3.5805e-01, Meta loss averaged over last 500 steps = 4.2502e-01, PNorm = 67.4203, GNorm = 0.1697
Meta loss on this task batch = 3.9121e-01, Meta loss averaged over last 500 steps = 4.2487e-01, PNorm = 67.4354, GNorm = 0.1278
Meta loss on this task batch = 4.1360e-01, Meta loss averaged over last 500 steps = 4.2491e-01, PNorm = 67.4496, GNorm = 0.1671
Meta loss on this task batch = 4.2364e-01, Meta loss averaged over last 500 steps = 4.2496e-01, PNorm = 67.4644, GNorm = 0.1611
Meta loss on this task batch = 4.4196e-01, Meta loss averaged over last 500 steps = 4.2498e-01, PNorm = 67.4795, GNorm = 0.1940
Meta loss on this task batch = 4.0621e-01, Meta loss averaged over last 500 steps = 4.2496e-01, PNorm = 67.4952, GNorm = 0.1562
Meta loss on this task batch = 4.2458e-01, Meta loss averaged over last 500 steps = 4.2480e-01, PNorm = 67.5108, GNorm = 0.1788
Meta loss on this task batch = 4.5357e-01, Meta loss averaged over last 500 steps = 4.2488e-01, PNorm = 67.5247, GNorm = 0.1504
Meta loss on this task batch = 3.9282e-01, Meta loss averaged over last 500 steps = 4.2487e-01, PNorm = 67.5370, GNorm = 0.2031
Meta loss on this task batch = 4.1178e-01, Meta loss averaged over last 500 steps = 4.2484e-01, PNorm = 67.5479, GNorm = 0.1591
Meta loss on this task batch = 4.9683e-01, Meta loss averaged over last 500 steps = 4.2491e-01, PNorm = 67.5591, GNorm = 0.1694
Meta loss on this task batch = 4.2770e-01, Meta loss averaged over last 500 steps = 4.2489e-01, PNorm = 67.5703, GNorm = 0.1590
Meta loss on this task batch = 4.1962e-01, Meta loss averaged over last 500 steps = 4.2480e-01, PNorm = 67.5814, GNorm = 0.1492
Meta loss on this task batch = 4.3734e-01, Meta loss averaged over last 500 steps = 4.2483e-01, PNorm = 67.5933, GNorm = 0.1757
Meta loss on this task batch = 4.5863e-01, Meta loss averaged over last 500 steps = 4.2498e-01, PNorm = 67.6050, GNorm = 0.1693
Meta loss on this task batch = 3.8627e-01, Meta loss averaged over last 500 steps = 4.2492e-01, PNorm = 67.6191, GNorm = 0.1791
Meta loss on this task batch = 3.8917e-01, Meta loss averaged over last 500 steps = 4.2476e-01, PNorm = 67.6340, GNorm = 0.1718
Meta loss on this task batch = 3.4916e-01, Meta loss averaged over last 500 steps = 4.2463e-01, PNorm = 67.6497, GNorm = 0.1762
Took 117.0431079864502 seconds to complete one epoch of meta training
Took 125.17185735702515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469421
Epoch 78
Meta loss on this task batch = 4.0377e-01, Meta loss averaged over last 500 steps = 4.2444e-01, PNorm = 67.6657, GNorm = 0.1765
Meta loss on this task batch = 4.3558e-01, Meta loss averaged over last 500 steps = 4.2441e-01, PNorm = 67.6817, GNorm = 0.1645
Meta loss on this task batch = 3.5976e-01, Meta loss averaged over last 500 steps = 4.2416e-01, PNorm = 67.6975, GNorm = 0.1574
Meta loss on this task batch = 4.8565e-01, Meta loss averaged over last 500 steps = 4.2424e-01, PNorm = 67.7115, GNorm = 0.1924
Meta loss on this task batch = 3.5781e-01, Meta loss averaged over last 500 steps = 4.2393e-01, PNorm = 67.7255, GNorm = 0.1283
Meta loss on this task batch = 4.3836e-01, Meta loss averaged over last 500 steps = 4.2412e-01, PNorm = 67.7409, GNorm = 0.1699
Meta loss on this task batch = 4.5145e-01, Meta loss averaged over last 500 steps = 4.2400e-01, PNorm = 67.7559, GNorm = 0.2205
Meta loss on this task batch = 3.1861e-01, Meta loss averaged over last 500 steps = 4.2388e-01, PNorm = 67.7719, GNorm = 0.1379
Meta loss on this task batch = 4.4944e-01, Meta loss averaged over last 500 steps = 4.2389e-01, PNorm = 67.7881, GNorm = 0.2315
Meta loss on this task batch = 4.7171e-01, Meta loss averaged over last 500 steps = 4.2393e-01, PNorm = 67.8013, GNorm = 0.2540
Meta loss on this task batch = 3.8550e-01, Meta loss averaged over last 500 steps = 4.2369e-01, PNorm = 67.8139, GNorm = 0.2006
Meta loss on this task batch = 4.3732e-01, Meta loss averaged over last 500 steps = 4.2368e-01, PNorm = 67.8256, GNorm = 0.2320
Meta loss on this task batch = 4.6703e-01, Meta loss averaged over last 500 steps = 4.2385e-01, PNorm = 67.8382, GNorm = 0.1744
Meta loss on this task batch = 4.0465e-01, Meta loss averaged over last 500 steps = 4.2371e-01, PNorm = 67.8506, GNorm = 0.1810
Meta loss on this task batch = 3.8554e-01, Meta loss averaged over last 500 steps = 4.2369e-01, PNorm = 67.8637, GNorm = 0.2037
Meta loss on this task batch = 3.9131e-01, Meta loss averaged over last 500 steps = 4.2371e-01, PNorm = 67.8775, GNorm = 0.1628
Meta loss on this task batch = 3.8173e-01, Meta loss averaged over last 500 steps = 4.2352e-01, PNorm = 67.8918, GNorm = 0.2102
Meta loss on this task batch = 4.4748e-01, Meta loss averaged over last 500 steps = 4.2354e-01, PNorm = 67.9058, GNorm = 0.1680
Meta loss on this task batch = 4.0109e-01, Meta loss averaged over last 500 steps = 4.2347e-01, PNorm = 67.9206, GNorm = 0.1736
Took 114.91309118270874 seconds to complete one epoch of meta training
Took 122.43357586860657 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481497
Epoch 79
Meta loss on this task batch = 4.0496e-01, Meta loss averaged over last 500 steps = 4.2353e-01, PNorm = 67.9356, GNorm = 0.1580
Meta loss on this task batch = 4.3988e-01, Meta loss averaged over last 500 steps = 4.2361e-01, PNorm = 67.9504, GNorm = 0.1466
Meta loss on this task batch = 4.2339e-01, Meta loss averaged over last 500 steps = 4.2370e-01, PNorm = 67.9661, GNorm = 0.1531
Meta loss on this task batch = 3.8962e-01, Meta loss averaged over last 500 steps = 4.2353e-01, PNorm = 67.9825, GNorm = 0.1411
Meta loss on this task batch = 3.7757e-01, Meta loss averaged over last 500 steps = 4.2347e-01, PNorm = 67.9997, GNorm = 0.1518
Meta loss on this task batch = 3.5557e-01, Meta loss averaged over last 500 steps = 4.2325e-01, PNorm = 68.0172, GNorm = 0.1390
Meta loss on this task batch = 4.5723e-01, Meta loss averaged over last 500 steps = 4.2328e-01, PNorm = 68.0330, GNorm = 0.1757
Meta loss on this task batch = 4.7007e-01, Meta loss averaged over last 500 steps = 4.2344e-01, PNorm = 68.0461, GNorm = 0.2061
Meta loss on this task batch = 4.0650e-01, Meta loss averaged over last 500 steps = 4.2352e-01, PNorm = 68.0586, GNorm = 0.1770
Meta loss on this task batch = 4.3929e-01, Meta loss averaged over last 500 steps = 4.2347e-01, PNorm = 68.0709, GNorm = 0.1963
Meta loss on this task batch = 4.2665e-01, Meta loss averaged over last 500 steps = 4.2352e-01, PNorm = 68.0835, GNorm = 0.1610
Meta loss on this task batch = 3.7827e-01, Meta loss averaged over last 500 steps = 4.2327e-01, PNorm = 68.0952, GNorm = 0.1460
Meta loss on this task batch = 4.0711e-01, Meta loss averaged over last 500 steps = 4.2326e-01, PNorm = 68.1069, GNorm = 0.1644
Meta loss on this task batch = 4.5253e-01, Meta loss averaged over last 500 steps = 4.2326e-01, PNorm = 68.1185, GNorm = 0.1664
Meta loss on this task batch = 4.0728e-01, Meta loss averaged over last 500 steps = 4.2329e-01, PNorm = 68.1312, GNorm = 0.1581
Meta loss on this task batch = 4.6509e-01, Meta loss averaged over last 500 steps = 4.2327e-01, PNorm = 68.1440, GNorm = 0.1846
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 4.2298e-01, PNorm = 68.1585, GNorm = 0.1611
Meta loss on this task batch = 4.7322e-01, Meta loss averaged over last 500 steps = 4.2316e-01, PNorm = 68.1735, GNorm = 0.1592
Meta loss on this task batch = 3.6604e-01, Meta loss averaged over last 500 steps = 4.2298e-01, PNorm = 68.1898, GNorm = 0.1945
Took 118.0860583782196 seconds to complete one epoch of meta training
Took 126.09342455863953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470291
Epoch 80
Meta loss on this task batch = 3.5197e-01, Meta loss averaged over last 500 steps = 4.2267e-01, PNorm = 68.2067, GNorm = 0.1542
Meta loss on this task batch = 4.2291e-01, Meta loss averaged over last 500 steps = 4.2258e-01, PNorm = 68.2244, GNorm = 0.1762
Meta loss on this task batch = 4.0243e-01, Meta loss averaged over last 500 steps = 4.2253e-01, PNorm = 68.2398, GNorm = 0.2513
Meta loss on this task batch = 3.8075e-01, Meta loss averaged over last 500 steps = 4.2243e-01, PNorm = 68.2540, GNorm = 0.1510
Meta loss on this task batch = 4.4016e-01, Meta loss averaged over last 500 steps = 4.2262e-01, PNorm = 68.2669, GNorm = 0.1913
Meta loss on this task batch = 4.5236e-01, Meta loss averaged over last 500 steps = 4.2262e-01, PNorm = 68.2792, GNorm = 0.2457
Meta loss on this task batch = 4.5548e-01, Meta loss averaged over last 500 steps = 4.2278e-01, PNorm = 68.2904, GNorm = 0.2116
Meta loss on this task batch = 4.5347e-01, Meta loss averaged over last 500 steps = 4.2282e-01, PNorm = 68.2993, GNorm = 0.1914
Meta loss on this task batch = 3.8905e-01, Meta loss averaged over last 500 steps = 4.2276e-01, PNorm = 68.3100, GNorm = 0.1868
Meta loss on this task batch = 4.0419e-01, Meta loss averaged over last 500 steps = 4.2271e-01, PNorm = 68.3212, GNorm = 0.1375
Meta loss on this task batch = 4.1454e-01, Meta loss averaged over last 500 steps = 4.2259e-01, PNorm = 68.3344, GNorm = 0.1904
Meta loss on this task batch = 4.0022e-01, Meta loss averaged over last 500 steps = 4.2247e-01, PNorm = 68.3487, GNorm = 0.1692
Meta loss on this task batch = 3.8582e-01, Meta loss averaged over last 500 steps = 4.2235e-01, PNorm = 68.3634, GNorm = 0.1363
Meta loss on this task batch = 3.9364e-01, Meta loss averaged over last 500 steps = 4.2230e-01, PNorm = 68.3789, GNorm = 0.1611
Meta loss on this task batch = 4.7450e-01, Meta loss averaged over last 500 steps = 4.2246e-01, PNorm = 68.3930, GNorm = 0.2198
Meta loss on this task batch = 4.4783e-01, Meta loss averaged over last 500 steps = 4.2245e-01, PNorm = 68.4044, GNorm = 0.1826
Meta loss on this task batch = 3.8890e-01, Meta loss averaged over last 500 steps = 4.2250e-01, PNorm = 68.4163, GNorm = 0.1358
Meta loss on this task batch = 3.8791e-01, Meta loss averaged over last 500 steps = 4.2246e-01, PNorm = 68.4286, GNorm = 0.1709
Meta loss on this task batch = 4.8866e-01, Meta loss averaged over last 500 steps = 4.2253e-01, PNorm = 68.4392, GNorm = 0.2871
Took 118.93157410621643 seconds to complete one epoch of meta training
Took 126.38813090324402 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478827
Epoch 81
Meta loss on this task batch = 4.5687e-01, Meta loss averaged over last 500 steps = 4.2257e-01, PNorm = 68.4505, GNorm = 0.1334
Meta loss on this task batch = 3.9426e-01, Meta loss averaged over last 500 steps = 4.2250e-01, PNorm = 68.4638, GNorm = 0.1722
Meta loss on this task batch = 4.2128e-01, Meta loss averaged over last 500 steps = 4.2245e-01, PNorm = 68.4786, GNorm = 0.1841
Meta loss on this task batch = 3.7874e-01, Meta loss averaged over last 500 steps = 4.2231e-01, PNorm = 68.4947, GNorm = 0.1513
Meta loss on this task batch = 4.4876e-01, Meta loss averaged over last 500 steps = 4.2242e-01, PNorm = 68.5101, GNorm = 0.1611
Meta loss on this task batch = 3.9251e-01, Meta loss averaged over last 500 steps = 4.2234e-01, PNorm = 68.5255, GNorm = 0.1376
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 4.2209e-01, PNorm = 68.5410, GNorm = 0.1210
Meta loss on this task batch = 3.9503e-01, Meta loss averaged over last 500 steps = 4.2194e-01, PNorm = 68.5557, GNorm = 0.1477
Meta loss on this task batch = 4.8790e-01, Meta loss averaged over last 500 steps = 4.2205e-01, PNorm = 68.5706, GNorm = 0.1752
Meta loss on this task batch = 4.2851e-01, Meta loss averaged over last 500 steps = 4.2212e-01, PNorm = 68.5849, GNorm = 0.1644
Meta loss on this task batch = 4.1177e-01, Meta loss averaged over last 500 steps = 4.2205e-01, PNorm = 68.5980, GNorm = 0.1889
Meta loss on this task batch = 4.3257e-01, Meta loss averaged over last 500 steps = 4.2199e-01, PNorm = 68.6095, GNorm = 0.1618
Meta loss on this task batch = 3.5925e-01, Meta loss averaged over last 500 steps = 4.2186e-01, PNorm = 68.6216, GNorm = 0.1470
Meta loss on this task batch = 4.1637e-01, Meta loss averaged over last 500 steps = 4.2185e-01, PNorm = 68.6330, GNorm = 0.1941
Meta loss on this task batch = 3.8484e-01, Meta loss averaged over last 500 steps = 4.2180e-01, PNorm = 68.6453, GNorm = 0.1641
Meta loss on this task batch = 4.4123e-01, Meta loss averaged over last 500 steps = 4.2185e-01, PNorm = 68.6570, GNorm = 0.1828
Meta loss on this task batch = 3.7840e-01, Meta loss averaged over last 500 steps = 4.2179e-01, PNorm = 68.6694, GNorm = 0.1587
Meta loss on this task batch = 4.6905e-01, Meta loss averaged over last 500 steps = 4.2187e-01, PNorm = 68.6824, GNorm = 0.1967
Meta loss on this task batch = 4.3042e-01, Meta loss averaged over last 500 steps = 4.2169e-01, PNorm = 68.6946, GNorm = 0.1899
Took 116.72485971450806 seconds to complete one epoch of meta training
Took 124.50971245765686 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485617
Epoch 82
Meta loss on this task batch = 4.1247e-01, Meta loss averaged over last 500 steps = 4.2166e-01, PNorm = 68.7079, GNorm = 0.1450
Meta loss on this task batch = 3.8911e-01, Meta loss averaged over last 500 steps = 4.2163e-01, PNorm = 68.7212, GNorm = 0.1497
Meta loss on this task batch = 4.2551e-01, Meta loss averaged over last 500 steps = 4.2170e-01, PNorm = 68.7352, GNorm = 0.1642
Meta loss on this task batch = 3.6895e-01, Meta loss averaged over last 500 steps = 4.2154e-01, PNorm = 68.7496, GNorm = 0.1492
Meta loss on this task batch = 3.7610e-01, Meta loss averaged over last 500 steps = 4.2141e-01, PNorm = 68.7648, GNorm = 0.1696
Meta loss on this task batch = 3.8036e-01, Meta loss averaged over last 500 steps = 4.2133e-01, PNorm = 68.7807, GNorm = 0.1820
Meta loss on this task batch = 4.2533e-01, Meta loss averaged over last 500 steps = 4.2127e-01, PNorm = 68.7955, GNorm = 0.1738
Meta loss on this task batch = 4.4699e-01, Meta loss averaged over last 500 steps = 4.2140e-01, PNorm = 68.8097, GNorm = 0.1696
Meta loss on this task batch = 4.4622e-01, Meta loss averaged over last 500 steps = 4.2141e-01, PNorm = 68.8223, GNorm = 0.1893
Meta loss on this task batch = 4.4442e-01, Meta loss averaged over last 500 steps = 4.2158e-01, PNorm = 68.8346, GNorm = 0.1659
Meta loss on this task batch = 4.0427e-01, Meta loss averaged over last 500 steps = 4.2154e-01, PNorm = 68.8445, GNorm = 0.2017
Meta loss on this task batch = 3.7117e-01, Meta loss averaged over last 500 steps = 4.2139e-01, PNorm = 68.8531, GNorm = 0.1915
Meta loss on this task batch = 3.7561e-01, Meta loss averaged over last 500 steps = 4.2111e-01, PNorm = 68.8626, GNorm = 0.1621
Meta loss on this task batch = 4.5871e-01, Meta loss averaged over last 500 steps = 4.2116e-01, PNorm = 68.8736, GNorm = 0.1881
Meta loss on this task batch = 3.6555e-01, Meta loss averaged over last 500 steps = 4.2097e-01, PNorm = 68.8854, GNorm = 0.1752
Meta loss on this task batch = 3.9079e-01, Meta loss averaged over last 500 steps = 4.2097e-01, PNorm = 68.8978, GNorm = 0.1546
Meta loss on this task batch = 3.7587e-01, Meta loss averaged over last 500 steps = 4.2080e-01, PNorm = 68.9112, GNorm = 0.1624
Meta loss on this task batch = 4.4721e-01, Meta loss averaged over last 500 steps = 4.2085e-01, PNorm = 68.9238, GNorm = 0.1963
Meta loss on this task batch = 3.4917e-01, Meta loss averaged over last 500 steps = 4.2071e-01, PNorm = 68.9373, GNorm = 0.1716
Took 117.44989585876465 seconds to complete one epoch of meta training
Took 124.97910499572754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472662
Epoch 83
Meta loss on this task batch = 4.3541e-01, Meta loss averaged over last 500 steps = 4.2073e-01, PNorm = 68.9496, GNorm = 0.1724
Meta loss on this task batch = 4.0146e-01, Meta loss averaged over last 500 steps = 4.2075e-01, PNorm = 68.9625, GNorm = 0.1314
Meta loss on this task batch = 3.0927e-01, Meta loss averaged over last 500 steps = 4.2059e-01, PNorm = 68.9780, GNorm = 0.1641
Meta loss on this task batch = 3.7110e-01, Meta loss averaged over last 500 steps = 4.2046e-01, PNorm = 68.9934, GNorm = 0.1707
Meta loss on this task batch = 4.3512e-01, Meta loss averaged over last 500 steps = 4.2044e-01, PNorm = 69.0074, GNorm = 0.2041
Meta loss on this task batch = 4.3202e-01, Meta loss averaged over last 500 steps = 4.2042e-01, PNorm = 69.0213, GNorm = 0.1697
Meta loss on this task batch = 4.0830e-01, Meta loss averaged over last 500 steps = 4.2040e-01, PNorm = 69.0352, GNorm = 0.1537
Meta loss on this task batch = 3.3982e-01, Meta loss averaged over last 500 steps = 4.2027e-01, PNorm = 69.0495, GNorm = 0.1613
Meta loss on this task batch = 4.4429e-01, Meta loss averaged over last 500 steps = 4.2031e-01, PNorm = 69.0627, GNorm = 0.1667
Meta loss on this task batch = 4.0148e-01, Meta loss averaged over last 500 steps = 4.2029e-01, PNorm = 69.0766, GNorm = 0.1706
Meta loss on this task batch = 4.7278e-01, Meta loss averaged over last 500 steps = 4.2042e-01, PNorm = 69.0912, GNorm = 0.1764
Meta loss on this task batch = 4.0304e-01, Meta loss averaged over last 500 steps = 4.2021e-01, PNorm = 69.1042, GNorm = 0.1678
Meta loss on this task batch = 3.5504e-01, Meta loss averaged over last 500 steps = 4.2004e-01, PNorm = 69.1170, GNorm = 0.1349
Meta loss on this task batch = 3.8616e-01, Meta loss averaged over last 500 steps = 4.1986e-01, PNorm = 69.1307, GNorm = 0.1495
Meta loss on this task batch = 3.5293e-01, Meta loss averaged over last 500 steps = 4.1973e-01, PNorm = 69.1451, GNorm = 0.1703
Meta loss on this task batch = 4.4488e-01, Meta loss averaged over last 500 steps = 4.1980e-01, PNorm = 69.1594, GNorm = 0.1688
Meta loss on this task batch = 4.9238e-01, Meta loss averaged over last 500 steps = 4.1992e-01, PNorm = 69.1727, GNorm = 0.2023
Meta loss on this task batch = 3.8470e-01, Meta loss averaged over last 500 steps = 4.1988e-01, PNorm = 69.1853, GNorm = 0.1512
Meta loss on this task batch = 3.9001e-01, Meta loss averaged over last 500 steps = 4.1997e-01, PNorm = 69.1966, GNorm = 0.1874
Took 113.93282675743103 seconds to complete one epoch of meta training
Took 120.20836877822876 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489890
Epoch 84
Meta loss on this task batch = 4.0035e-01, Meta loss averaged over last 500 steps = 4.1997e-01, PNorm = 69.2066, GNorm = 0.1683
Meta loss on this task batch = 4.4692e-01, Meta loss averaged over last 500 steps = 4.1999e-01, PNorm = 69.2174, GNorm = 0.1745
Meta loss on this task batch = 3.6590e-01, Meta loss averaged over last 500 steps = 4.1965e-01, PNorm = 69.2278, GNorm = 0.1499
Meta loss on this task batch = 4.0480e-01, Meta loss averaged over last 500 steps = 4.1967e-01, PNorm = 69.2387, GNorm = 0.1566
Meta loss on this task batch = 3.4421e-01, Meta loss averaged over last 500 steps = 4.1942e-01, PNorm = 69.2508, GNorm = 0.1507
Meta loss on this task batch = 3.9881e-01, Meta loss averaged over last 500 steps = 4.1930e-01, PNorm = 69.2631, GNorm = 0.1834
Meta loss on this task batch = 4.1919e-01, Meta loss averaged over last 500 steps = 4.1927e-01, PNorm = 69.2722, GNorm = 0.1855
Meta loss on this task batch = 3.1408e-01, Meta loss averaged over last 500 steps = 4.1913e-01, PNorm = 69.2837, GNorm = 0.1529
Meta loss on this task batch = 4.3026e-01, Meta loss averaged over last 500 steps = 4.1910e-01, PNorm = 69.2961, GNorm = 0.1617
Meta loss on this task batch = 4.4109e-01, Meta loss averaged over last 500 steps = 4.1910e-01, PNorm = 69.3079, GNorm = 0.1498
Meta loss on this task batch = 4.4481e-01, Meta loss averaged over last 500 steps = 4.1900e-01, PNorm = 69.3200, GNorm = 0.1925
Meta loss on this task batch = 4.1326e-01, Meta loss averaged over last 500 steps = 4.1900e-01, PNorm = 69.3327, GNorm = 0.1740
Meta loss on this task batch = 4.6120e-01, Meta loss averaged over last 500 steps = 4.1903e-01, PNorm = 69.3449, GNorm = 0.1531
Meta loss on this task batch = 3.8474e-01, Meta loss averaged over last 500 steps = 4.1871e-01, PNorm = 69.3572, GNorm = 0.1454
Meta loss on this task batch = 4.2306e-01, Meta loss averaged over last 500 steps = 4.1879e-01, PNorm = 69.3702, GNorm = 0.1699
Meta loss on this task batch = 4.0698e-01, Meta loss averaged over last 500 steps = 4.1885e-01, PNorm = 69.3831, GNorm = 0.1454
Meta loss on this task batch = 4.2914e-01, Meta loss averaged over last 500 steps = 4.1901e-01, PNorm = 69.3953, GNorm = 0.1864
Meta loss on this task batch = 4.4562e-01, Meta loss averaged over last 500 steps = 4.1910e-01, PNorm = 69.4072, GNorm = 0.2236
Meta loss on this task batch = 4.3895e-01, Meta loss averaged over last 500 steps = 4.1913e-01, PNorm = 69.4206, GNorm = 0.2128
Took 117.6800742149353 seconds to complete one epoch of meta training
Took 125.65176558494568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484339
Epoch 85
Meta loss on this task batch = 4.3650e-01, Meta loss averaged over last 500 steps = 4.1911e-01, PNorm = 69.4333, GNorm = 0.1641
Meta loss on this task batch = 4.0822e-01, Meta loss averaged over last 500 steps = 4.1910e-01, PNorm = 69.4461, GNorm = 0.2074
Meta loss on this task batch = 4.2013e-01, Meta loss averaged over last 500 steps = 4.1899e-01, PNorm = 69.4593, GNorm = 0.1866
Meta loss on this task batch = 3.6817e-01, Meta loss averaged over last 500 steps = 4.1887e-01, PNorm = 69.4736, GNorm = 0.1569
Meta loss on this task batch = 3.8685e-01, Meta loss averaged over last 500 steps = 4.1889e-01, PNorm = 69.4870, GNorm = 0.1972
Meta loss on this task batch = 3.9133e-01, Meta loss averaged over last 500 steps = 4.1875e-01, PNorm = 69.5000, GNorm = 0.1830
Meta loss on this task batch = 4.1527e-01, Meta loss averaged over last 500 steps = 4.1877e-01, PNorm = 69.5112, GNorm = 0.2040
Meta loss on this task batch = 3.8703e-01, Meta loss averaged over last 500 steps = 4.1867e-01, PNorm = 69.5230, GNorm = 0.1794
Meta loss on this task batch = 3.8875e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 69.5345, GNorm = 0.1854
Meta loss on this task batch = 4.2686e-01, Meta loss averaged over last 500 steps = 4.1841e-01, PNorm = 69.5455, GNorm = 0.2045
Meta loss on this task batch = 4.3211e-01, Meta loss averaged over last 500 steps = 4.1846e-01, PNorm = 69.5543, GNorm = 0.2133
Meta loss on this task batch = 4.3515e-01, Meta loss averaged over last 500 steps = 4.1839e-01, PNorm = 69.5637, GNorm = 0.1807
Meta loss on this task batch = 4.3264e-01, Meta loss averaged over last 500 steps = 4.1844e-01, PNorm = 69.5744, GNorm = 0.1590
Meta loss on this task batch = 4.2868e-01, Meta loss averaged over last 500 steps = 4.1832e-01, PNorm = 69.5858, GNorm = 0.1653
Meta loss on this task batch = 3.7216e-01, Meta loss averaged over last 500 steps = 4.1822e-01, PNorm = 69.5984, GNorm = 0.1334
Meta loss on this task batch = 3.8681e-01, Meta loss averaged over last 500 steps = 4.1823e-01, PNorm = 69.6115, GNorm = 0.1306
Meta loss on this task batch = 4.0167e-01, Meta loss averaged over last 500 steps = 4.1818e-01, PNorm = 69.6252, GNorm = 0.1525
Meta loss on this task batch = 4.1684e-01, Meta loss averaged over last 500 steps = 4.1820e-01, PNorm = 69.6392, GNorm = 0.1556
Meta loss on this task batch = 4.9132e-01, Meta loss averaged over last 500 steps = 4.1840e-01, PNorm = 69.6517, GNorm = 0.2395
Took 112.15709042549133 seconds to complete one epoch of meta training
Took 119.54645109176636 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484438
Epoch 86
Meta loss on this task batch = 3.7098e-01, Meta loss averaged over last 500 steps = 4.1826e-01, PNorm = 69.6655, GNorm = 0.1652
Meta loss on this task batch = 3.6518e-01, Meta loss averaged over last 500 steps = 4.1813e-01, PNorm = 69.6795, GNorm = 0.1513
Meta loss on this task batch = 3.8510e-01, Meta loss averaged over last 500 steps = 4.1810e-01, PNorm = 69.6941, GNorm = 0.1457
Meta loss on this task batch = 3.4651e-01, Meta loss averaged over last 500 steps = 4.1790e-01, PNorm = 69.7097, GNorm = 0.1352
Meta loss on this task batch = 4.0738e-01, Meta loss averaged over last 500 steps = 4.1778e-01, PNorm = 69.7247, GNorm = 0.1676
Meta loss on this task batch = 4.2411e-01, Meta loss averaged over last 500 steps = 4.1782e-01, PNorm = 69.7403, GNorm = 0.1794
Meta loss on this task batch = 4.3810e-01, Meta loss averaged over last 500 steps = 4.1784e-01, PNorm = 69.7545, GNorm = 0.1868
Meta loss on this task batch = 4.4267e-01, Meta loss averaged over last 500 steps = 4.1787e-01, PNorm = 69.7674, GNorm = 0.1663
Meta loss on this task batch = 4.5624e-01, Meta loss averaged over last 500 steps = 4.1804e-01, PNorm = 69.7793, GNorm = 0.1868
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 4.1799e-01, PNorm = 69.7910, GNorm = 0.1508
Meta loss on this task batch = 3.4879e-01, Meta loss averaged over last 500 steps = 4.1787e-01, PNorm = 69.8035, GNorm = 0.1794
Meta loss on this task batch = 4.3455e-01, Meta loss averaged over last 500 steps = 4.1784e-01, PNorm = 69.8164, GNorm = 0.1874
Meta loss on this task batch = 4.2563e-01, Meta loss averaged over last 500 steps = 4.1793e-01, PNorm = 69.8286, GNorm = 0.1728
Meta loss on this task batch = 3.7994e-01, Meta loss averaged over last 500 steps = 4.1784e-01, PNorm = 69.8413, GNorm = 0.1613
Meta loss on this task batch = 4.7452e-01, Meta loss averaged over last 500 steps = 4.1788e-01, PNorm = 69.8538, GNorm = 0.1853
Meta loss on this task batch = 4.5992e-01, Meta loss averaged over last 500 steps = 4.1790e-01, PNorm = 69.8657, GNorm = 0.2520
Meta loss on this task batch = 3.7913e-01, Meta loss averaged over last 500 steps = 4.1781e-01, PNorm = 69.8789, GNorm = 0.1460
Meta loss on this task batch = 3.6700e-01, Meta loss averaged over last 500 steps = 4.1767e-01, PNorm = 69.8922, GNorm = 0.1750
Meta loss on this task batch = 4.3439e-01, Meta loss averaged over last 500 steps = 4.1759e-01, PNorm = 69.9047, GNorm = 0.2143
Took 116.82218813896179 seconds to complete one epoch of meta training
Took 124.56668448448181 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470752
Epoch 87
Meta loss on this task batch = 3.9084e-01, Meta loss averaged over last 500 steps = 4.1737e-01, PNorm = 69.9180, GNorm = 0.1904
Meta loss on this task batch = 3.7521e-01, Meta loss averaged over last 500 steps = 4.1721e-01, PNorm = 69.9320, GNorm = 0.1766
Meta loss on this task batch = 4.4819e-01, Meta loss averaged over last 500 steps = 4.1722e-01, PNorm = 69.9466, GNorm = 0.1698
Meta loss on this task batch = 4.2960e-01, Meta loss averaged over last 500 steps = 4.1730e-01, PNorm = 69.9618, GNorm = 0.1799
Meta loss on this task batch = 3.8611e-01, Meta loss averaged over last 500 steps = 4.1724e-01, PNorm = 69.9758, GNorm = 0.1646
Meta loss on this task batch = 4.5984e-01, Meta loss averaged over last 500 steps = 4.1727e-01, PNorm = 69.9905, GNorm = 0.1749
Meta loss on this task batch = 4.0065e-01, Meta loss averaged over last 500 steps = 4.1717e-01, PNorm = 70.0062, GNorm = 0.1505
Meta loss on this task batch = 4.4990e-01, Meta loss averaged over last 500 steps = 4.1717e-01, PNorm = 70.0189, GNorm = 0.1842
Meta loss on this task batch = 4.0143e-01, Meta loss averaged over last 500 steps = 4.1717e-01, PNorm = 70.0324, GNorm = 0.1470
Meta loss on this task batch = 4.5529e-01, Meta loss averaged over last 500 steps = 4.1714e-01, PNorm = 70.0447, GNorm = 0.1812
Meta loss on this task batch = 3.7273e-01, Meta loss averaged over last 500 steps = 4.1705e-01, PNorm = 70.0574, GNorm = 0.1627
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 4.1696e-01, PNorm = 70.0711, GNorm = 0.1432
Meta loss on this task batch = 4.3527e-01, Meta loss averaged over last 500 steps = 4.1693e-01, PNorm = 70.0841, GNorm = 0.1413
Meta loss on this task batch = 4.0385e-01, Meta loss averaged over last 500 steps = 4.1688e-01, PNorm = 70.0968, GNorm = 0.1598
Meta loss on this task batch = 4.3061e-01, Meta loss averaged over last 500 steps = 4.1689e-01, PNorm = 70.1098, GNorm = 0.1950
Meta loss on this task batch = 4.0419e-01, Meta loss averaged over last 500 steps = 4.1688e-01, PNorm = 70.1238, GNorm = 0.1577
Meta loss on this task batch = 3.8628e-01, Meta loss averaged over last 500 steps = 4.1677e-01, PNorm = 70.1385, GNorm = 0.1435
Meta loss on this task batch = 3.9771e-01, Meta loss averaged over last 500 steps = 4.1682e-01, PNorm = 70.1536, GNorm = 0.1619
Meta loss on this task batch = 4.2602e-01, Meta loss averaged over last 500 steps = 4.1677e-01, PNorm = 70.1687, GNorm = 0.1788
Took 117.03597927093506 seconds to complete one epoch of meta training
Took 124.96432900428772 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489275
Epoch 88
Meta loss on this task batch = 4.0252e-01, Meta loss averaged over last 500 steps = 4.1660e-01, PNorm = 70.1832, GNorm = 0.1651
Meta loss on this task batch = 3.8191e-01, Meta loss averaged over last 500 steps = 4.1656e-01, PNorm = 70.1973, GNorm = 0.1512
Meta loss on this task batch = 3.4790e-01, Meta loss averaged over last 500 steps = 4.1640e-01, PNorm = 70.2115, GNorm = 0.2307
Meta loss on this task batch = 4.2846e-01, Meta loss averaged over last 500 steps = 4.1642e-01, PNorm = 70.2252, GNorm = 0.1984
Meta loss on this task batch = 3.5689e-01, Meta loss averaged over last 500 steps = 4.1631e-01, PNorm = 70.2388, GNorm = 0.1925
Meta loss on this task batch = 4.0488e-01, Meta loss averaged over last 500 steps = 4.1620e-01, PNorm = 70.2504, GNorm = 0.2112
Meta loss on this task batch = 4.4516e-01, Meta loss averaged over last 500 steps = 4.1625e-01, PNorm = 70.2613, GNorm = 0.1878
Meta loss on this task batch = 3.9911e-01, Meta loss averaged over last 500 steps = 4.1616e-01, PNorm = 70.2730, GNorm = 0.1808
Meta loss on this task batch = 4.3863e-01, Meta loss averaged over last 500 steps = 4.1625e-01, PNorm = 70.2837, GNorm = 0.1817
Meta loss on this task batch = 3.5447e-01, Meta loss averaged over last 500 steps = 4.1618e-01, PNorm = 70.2952, GNorm = 0.1932
Meta loss on this task batch = 4.1018e-01, Meta loss averaged over last 500 steps = 4.1610e-01, PNorm = 70.3076, GNorm = 0.1929
Meta loss on this task batch = 4.5284e-01, Meta loss averaged over last 500 steps = 4.1611e-01, PNorm = 70.3202, GNorm = 0.1782
Meta loss on this task batch = 3.8019e-01, Meta loss averaged over last 500 steps = 4.1606e-01, PNorm = 70.3340, GNorm = 0.1577
Meta loss on this task batch = 4.2740e-01, Meta loss averaged over last 500 steps = 4.1610e-01, PNorm = 70.3485, GNorm = 0.1622
Meta loss on this task batch = 3.5390e-01, Meta loss averaged over last 500 steps = 4.1604e-01, PNorm = 70.3648, GNorm = 0.1458
Meta loss on this task batch = 4.8966e-01, Meta loss averaged over last 500 steps = 4.1611e-01, PNorm = 70.3792, GNorm = 0.2220
Meta loss on this task batch = 4.7135e-01, Meta loss averaged over last 500 steps = 4.1617e-01, PNorm = 70.3936, GNorm = 0.1766
Meta loss on this task batch = 4.2925e-01, Meta loss averaged over last 500 steps = 4.1613e-01, PNorm = 70.4063, GNorm = 0.1679
Meta loss on this task batch = 4.0275e-01, Meta loss averaged over last 500 steps = 4.1610e-01, PNorm = 70.4177, GNorm = 0.2027
Took 115.5775203704834 seconds to complete one epoch of meta training
Took 123.068279504776 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481175
Epoch 89
Meta loss on this task batch = 4.9342e-01, Meta loss averaged over last 500 steps = 4.1623e-01, PNorm = 70.4263, GNorm = 0.2520
Meta loss on this task batch = 3.7402e-01, Meta loss averaged over last 500 steps = 4.1599e-01, PNorm = 70.4359, GNorm = 0.1288
Meta loss on this task batch = 3.7311e-01, Meta loss averaged over last 500 steps = 4.1592e-01, PNorm = 70.4461, GNorm = 0.1512
Meta loss on this task batch = 3.9897e-01, Meta loss averaged over last 500 steps = 4.1599e-01, PNorm = 70.4575, GNorm = 0.1698
Meta loss on this task batch = 3.9661e-01, Meta loss averaged over last 500 steps = 4.1579e-01, PNorm = 70.4703, GNorm = 0.1399
Meta loss on this task batch = 3.6121e-01, Meta loss averaged over last 500 steps = 4.1562e-01, PNorm = 70.4844, GNorm = 0.1450
Meta loss on this task batch = 4.4175e-01, Meta loss averaged over last 500 steps = 4.1568e-01, PNorm = 70.4982, GNorm = 0.1570
Meta loss on this task batch = 4.0111e-01, Meta loss averaged over last 500 steps = 4.1578e-01, PNorm = 70.5130, GNorm = 0.1757
Meta loss on this task batch = 3.2867e-01, Meta loss averaged over last 500 steps = 4.1561e-01, PNorm = 70.5287, GNorm = 0.1592
Meta loss on this task batch = 3.7938e-01, Meta loss averaged over last 500 steps = 4.1556e-01, PNorm = 70.5442, GNorm = 0.1664
Meta loss on this task batch = 4.1802e-01, Meta loss averaged over last 500 steps = 4.1559e-01, PNorm = 70.5593, GNorm = 0.1831
Meta loss on this task batch = 4.5064e-01, Meta loss averaged over last 500 steps = 4.1562e-01, PNorm = 70.5716, GNorm = 0.2157
Meta loss on this task batch = 3.7135e-01, Meta loss averaged over last 500 steps = 4.1549e-01, PNorm = 70.5839, GNorm = 0.1857
Meta loss on this task batch = 4.1811e-01, Meta loss averaged over last 500 steps = 4.1539e-01, PNorm = 70.5958, GNorm = 0.1810
Meta loss on this task batch = 4.1462e-01, Meta loss averaged over last 500 steps = 4.1528e-01, PNorm = 70.6079, GNorm = 0.1968
Meta loss on this task batch = 4.4097e-01, Meta loss averaged over last 500 steps = 4.1538e-01, PNorm = 70.6185, GNorm = 0.2100
Meta loss on this task batch = 4.6218e-01, Meta loss averaged over last 500 steps = 4.1547e-01, PNorm = 70.6281, GNorm = 0.1683
Meta loss on this task batch = 4.4648e-01, Meta loss averaged over last 500 steps = 4.1557e-01, PNorm = 70.6390, GNorm = 0.1836
Meta loss on this task batch = 3.7296e-01, Meta loss averaged over last 500 steps = 4.1549e-01, PNorm = 70.6512, GNorm = 0.1630
Took 112.37378215789795 seconds to complete one epoch of meta training
Took 119.78737783432007 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475583
Epoch 90
Meta loss on this task batch = 4.2869e-01, Meta loss averaged over last 500 steps = 4.1550e-01, PNorm = 70.6646, GNorm = 0.1841
Meta loss on this task batch = 4.0961e-01, Meta loss averaged over last 500 steps = 4.1537e-01, PNorm = 70.6790, GNorm = 0.1742
Meta loss on this task batch = 4.1973e-01, Meta loss averaged over last 500 steps = 4.1533e-01, PNorm = 70.6936, GNorm = 0.1832
Meta loss on this task batch = 3.5289e-01, Meta loss averaged over last 500 steps = 4.1515e-01, PNorm = 70.7089, GNorm = 0.1401
Meta loss on this task batch = 4.2597e-01, Meta loss averaged over last 500 steps = 4.1512e-01, PNorm = 70.7240, GNorm = 0.1757
Meta loss on this task batch = 4.1923e-01, Meta loss averaged over last 500 steps = 4.1508e-01, PNorm = 70.7367, GNorm = 0.1804
Meta loss on this task batch = 4.1097e-01, Meta loss averaged over last 500 steps = 4.1509e-01, PNorm = 70.7477, GNorm = 0.2728
Meta loss on this task batch = 4.4552e-01, Meta loss averaged over last 500 steps = 4.1511e-01, PNorm = 70.7590, GNorm = 0.1992
Meta loss on this task batch = 3.4099e-01, Meta loss averaged over last 500 steps = 4.1495e-01, PNorm = 70.7707, GNorm = 0.1607
Meta loss on this task batch = 3.9096e-01, Meta loss averaged over last 500 steps = 4.1495e-01, PNorm = 70.7836, GNorm = 0.1951
Meta loss on this task batch = 4.1931e-01, Meta loss averaged over last 500 steps = 4.1498e-01, PNorm = 70.7977, GNorm = 0.2209
Meta loss on this task batch = 4.5220e-01, Meta loss averaged over last 500 steps = 4.1509e-01, PNorm = 70.8110, GNorm = 0.1742
Meta loss on this task batch = 3.8899e-01, Meta loss averaged over last 500 steps = 4.1497e-01, PNorm = 70.8250, GNorm = 0.1800
Meta loss on this task batch = 3.9239e-01, Meta loss averaged over last 500 steps = 4.1496e-01, PNorm = 70.8384, GNorm = 0.1872
Meta loss on this task batch = 3.8507e-01, Meta loss averaged over last 500 steps = 4.1480e-01, PNorm = 70.8520, GNorm = 0.1650
Meta loss on this task batch = 4.8993e-01, Meta loss averaged over last 500 steps = 4.1505e-01, PNorm = 70.8641, GNorm = 0.1963
Meta loss on this task batch = 4.1398e-01, Meta loss averaged over last 500 steps = 4.1499e-01, PNorm = 70.8760, GNorm = 0.1676
Meta loss on this task batch = 3.6614e-01, Meta loss averaged over last 500 steps = 4.1492e-01, PNorm = 70.8874, GNorm = 0.1634
Meta loss on this task batch = 3.7891e-01, Meta loss averaged over last 500 steps = 4.1484e-01, PNorm = 70.8980, GNorm = 0.1865
Took 115.43050622940063 seconds to complete one epoch of meta training
Took 123.67363548278809 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479626
Epoch 91
Meta loss on this task batch = 4.2042e-01, Meta loss averaged over last 500 steps = 4.1477e-01, PNorm = 70.9080, GNorm = 0.1666
Meta loss on this task batch = 3.5741e-01, Meta loss averaged over last 500 steps = 4.1459e-01, PNorm = 70.9190, GNorm = 0.1494
Meta loss on this task batch = 3.6985e-01, Meta loss averaged over last 500 steps = 4.1462e-01, PNorm = 70.9315, GNorm = 0.1590
Meta loss on this task batch = 4.5923e-01, Meta loss averaged over last 500 steps = 4.1455e-01, PNorm = 70.9432, GNorm = 0.1932
Meta loss on this task batch = 3.5271e-01, Meta loss averaged over last 500 steps = 4.1433e-01, PNorm = 70.9552, GNorm = 0.1844
Meta loss on this task batch = 4.2572e-01, Meta loss averaged over last 500 steps = 4.1437e-01, PNorm = 70.9669, GNorm = 0.1559
Meta loss on this task batch = 4.1489e-01, Meta loss averaged over last 500 steps = 4.1432e-01, PNorm = 70.9789, GNorm = 0.1742
Meta loss on this task batch = 4.0189e-01, Meta loss averaged over last 500 steps = 4.1423e-01, PNorm = 70.9907, GNorm = 0.1524
Meta loss on this task batch = 4.1741e-01, Meta loss averaged over last 500 steps = 4.1430e-01, PNorm = 71.0030, GNorm = 0.1855
Meta loss on this task batch = 4.3236e-01, Meta loss averaged over last 500 steps = 4.1432e-01, PNorm = 71.0172, GNorm = 0.1581
Meta loss on this task batch = 4.2962e-01, Meta loss averaged over last 500 steps = 4.1431e-01, PNorm = 71.0309, GNorm = 0.1752
Meta loss on this task batch = 4.2775e-01, Meta loss averaged over last 500 steps = 4.1423e-01, PNorm = 71.0450, GNorm = 0.1685
Meta loss on this task batch = 3.8312e-01, Meta loss averaged over last 500 steps = 4.1423e-01, PNorm = 71.0594, GNorm = 0.1481
Meta loss on this task batch = 5.2956e-01, Meta loss averaged over last 500 steps = 4.1434e-01, PNorm = 71.0735, GNorm = 0.2010
Meta loss on this task batch = 3.8388e-01, Meta loss averaged over last 500 steps = 4.1436e-01, PNorm = 71.0869, GNorm = 0.1765
Meta loss on this task batch = 3.5710e-01, Meta loss averaged over last 500 steps = 4.1435e-01, PNorm = 71.1009, GNorm = 0.1541
Meta loss on this task batch = 3.9100e-01, Meta loss averaged over last 500 steps = 4.1421e-01, PNorm = 71.1152, GNorm = 0.1621
Meta loss on this task batch = 3.8134e-01, Meta loss averaged over last 500 steps = 4.1407e-01, PNorm = 71.1293, GNorm = 0.1997
Meta loss on this task batch = 4.1229e-01, Meta loss averaged over last 500 steps = 4.1407e-01, PNorm = 71.1428, GNorm = 0.2162
Took 115.78400087356567 seconds to complete one epoch of meta training
Took 122.60474920272827 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464934
Epoch 92
Meta loss on this task batch = 3.7233e-01, Meta loss averaged over last 500 steps = 4.1403e-01, PNorm = 71.1568, GNorm = 0.1561
Meta loss on this task batch = 3.5320e-01, Meta loss averaged over last 500 steps = 4.1392e-01, PNorm = 71.1711, GNorm = 0.1694
Meta loss on this task batch = 4.2049e-01, Meta loss averaged over last 500 steps = 4.1399e-01, PNorm = 71.1848, GNorm = 0.1977
Meta loss on this task batch = 4.4099e-01, Meta loss averaged over last 500 steps = 4.1393e-01, PNorm = 71.1963, GNorm = 0.2162
Meta loss on this task batch = 4.4933e-01, Meta loss averaged over last 500 steps = 4.1398e-01, PNorm = 71.2066, GNorm = 0.2182
Meta loss on this task batch = 4.0773e-01, Meta loss averaged over last 500 steps = 4.1409e-01, PNorm = 71.2164, GNorm = 0.1600
Meta loss on this task batch = 4.1607e-01, Meta loss averaged over last 500 steps = 4.1398e-01, PNorm = 71.2271, GNorm = 0.1667
Meta loss on this task batch = 3.9229e-01, Meta loss averaged over last 500 steps = 4.1392e-01, PNorm = 71.2394, GNorm = 0.1508
Meta loss on this task batch = 4.0337e-01, Meta loss averaged over last 500 steps = 4.1393e-01, PNorm = 71.2524, GNorm = 0.1931
Meta loss on this task batch = 4.4342e-01, Meta loss averaged over last 500 steps = 4.1397e-01, PNorm = 71.2648, GNorm = 0.1716
Meta loss on this task batch = 3.6712e-01, Meta loss averaged over last 500 steps = 4.1375e-01, PNorm = 71.2760, GNorm = 0.2242
Meta loss on this task batch = 3.8596e-01, Meta loss averaged over last 500 steps = 4.1361e-01, PNorm = 71.2885, GNorm = 0.1684
Meta loss on this task batch = 4.9813e-01, Meta loss averaged over last 500 steps = 4.1373e-01, PNorm = 71.2998, GNorm = 0.2102
Meta loss on this task batch = 4.1182e-01, Meta loss averaged over last 500 steps = 4.1375e-01, PNorm = 71.3129, GNorm = 0.1703
Meta loss on this task batch = 4.5234e-01, Meta loss averaged over last 500 steps = 4.1396e-01, PNorm = 71.3253, GNorm = 0.1740
Meta loss on this task batch = 3.6108e-01, Meta loss averaged over last 500 steps = 4.1393e-01, PNorm = 71.3384, GNorm = 0.1880
Meta loss on this task batch = 4.3138e-01, Meta loss averaged over last 500 steps = 4.1399e-01, PNorm = 71.3505, GNorm = 0.1569
Meta loss on this task batch = 3.9190e-01, Meta loss averaged over last 500 steps = 4.1391e-01, PNorm = 71.3632, GNorm = 0.1731
Meta loss on this task batch = 3.6007e-01, Meta loss averaged over last 500 steps = 4.1382e-01, PNorm = 71.3773, GNorm = 0.1871
Took 115.0704276561737 seconds to complete one epoch of meta training
Took 122.79857444763184 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493304
Epoch 93
Meta loss on this task batch = 3.0291e-01, Meta loss averaged over last 500 steps = 4.1358e-01, PNorm = 71.3920, GNorm = 0.1564
Meta loss on this task batch = 4.0729e-01, Meta loss averaged over last 500 steps = 4.1348e-01, PNorm = 71.4069, GNorm = 0.1852
Meta loss on this task batch = 4.2247e-01, Meta loss averaged over last 500 steps = 4.1350e-01, PNorm = 71.4209, GNorm = 0.1748
Meta loss on this task batch = 4.1420e-01, Meta loss averaged over last 500 steps = 4.1342e-01, PNorm = 71.4334, GNorm = 0.1675
Meta loss on this task batch = 4.1260e-01, Meta loss averaged over last 500 steps = 4.1330e-01, PNorm = 71.4464, GNorm = 0.1548
Meta loss on this task batch = 3.8826e-01, Meta loss averaged over last 500 steps = 4.1326e-01, PNorm = 71.4600, GNorm = 0.1927
Meta loss on this task batch = 3.8016e-01, Meta loss averaged over last 500 steps = 4.1326e-01, PNorm = 71.4730, GNorm = 0.1850
Meta loss on this task batch = 5.1579e-01, Meta loss averaged over last 500 steps = 4.1352e-01, PNorm = 71.4831, GNorm = 0.2189
Meta loss on this task batch = 4.0338e-01, Meta loss averaged over last 500 steps = 4.1354e-01, PNorm = 71.4939, GNorm = 0.1725
Meta loss on this task batch = 4.0971e-01, Meta loss averaged over last 500 steps = 4.1359e-01, PNorm = 71.5055, GNorm = 0.1631
Meta loss on this task batch = 3.6940e-01, Meta loss averaged over last 500 steps = 4.1351e-01, PNorm = 71.5185, GNorm = 0.1883
Meta loss on this task batch = 4.4398e-01, Meta loss averaged over last 500 steps = 4.1366e-01, PNorm = 71.5316, GNorm = 0.1772
Meta loss on this task batch = 4.2166e-01, Meta loss averaged over last 500 steps = 4.1353e-01, PNorm = 71.5450, GNorm = 0.1615
Meta loss on this task batch = 4.2023e-01, Meta loss averaged over last 500 steps = 4.1351e-01, PNorm = 71.5588, GNorm = 0.1804
Meta loss on this task batch = 4.0781e-01, Meta loss averaged over last 500 steps = 4.1349e-01, PNorm = 71.5727, GNorm = 0.1551
Meta loss on this task batch = 4.8789e-01, Meta loss averaged over last 500 steps = 4.1354e-01, PNorm = 71.5851, GNorm = 0.1882
Meta loss on this task batch = 3.6812e-01, Meta loss averaged over last 500 steps = 4.1344e-01, PNorm = 71.5989, GNorm = 0.1537
Meta loss on this task batch = 3.9122e-01, Meta loss averaged over last 500 steps = 4.1334e-01, PNorm = 71.6132, GNorm = 0.1527
Meta loss on this task batch = 3.7620e-01, Meta loss averaged over last 500 steps = 4.1322e-01, PNorm = 71.6276, GNorm = 0.1880
Took 117.60409164428711 seconds to complete one epoch of meta training
Took 125.6299216747284 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473625
Epoch 94
Meta loss on this task batch = 4.4077e-01, Meta loss averaged over last 500 steps = 4.1321e-01, PNorm = 71.6418, GNorm = 0.1911
Meta loss on this task batch = 4.0894e-01, Meta loss averaged over last 500 steps = 4.1310e-01, PNorm = 71.6540, GNorm = 0.1809
Meta loss on this task batch = 4.1628e-01, Meta loss averaged over last 500 steps = 4.1309e-01, PNorm = 71.6653, GNorm = 0.2275
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 4.1286e-01, PNorm = 71.6755, GNorm = 0.1870
Meta loss on this task batch = 3.6369e-01, Meta loss averaged over last 500 steps = 4.1273e-01, PNorm = 71.6859, GNorm = 0.1451
Meta loss on this task batch = 4.7078e-01, Meta loss averaged over last 500 steps = 4.1275e-01, PNorm = 71.6956, GNorm = 0.2050
Meta loss on this task batch = 4.2371e-01, Meta loss averaged over last 500 steps = 4.1272e-01, PNorm = 71.7061, GNorm = 0.1645
Meta loss on this task batch = 3.5718e-01, Meta loss averaged over last 500 steps = 4.1248e-01, PNorm = 71.7184, GNorm = 0.1907
Meta loss on this task batch = 4.4378e-01, Meta loss averaged over last 500 steps = 4.1250e-01, PNorm = 71.7310, GNorm = 0.1356
Meta loss on this task batch = 4.3682e-01, Meta loss averaged over last 500 steps = 4.1263e-01, PNorm = 71.7440, GNorm = 0.1740
Meta loss on this task batch = 4.2176e-01, Meta loss averaged over last 500 steps = 4.1265e-01, PNorm = 71.7566, GNorm = 0.1462
Meta loss on this task batch = 4.4972e-01, Meta loss averaged over last 500 steps = 4.1272e-01, PNorm = 71.7694, GNorm = 0.1809
Meta loss on this task batch = 3.9820e-01, Meta loss averaged over last 500 steps = 4.1267e-01, PNorm = 71.7827, GNorm = 0.1594
Meta loss on this task batch = 3.7219e-01, Meta loss averaged over last 500 steps = 4.1253e-01, PNorm = 71.7971, GNorm = 0.1358
Meta loss on this task batch = 4.2789e-01, Meta loss averaged over last 500 steps = 4.1261e-01, PNorm = 71.8118, GNorm = 0.1679
Meta loss on this task batch = 4.0296e-01, Meta loss averaged over last 500 steps = 4.1270e-01, PNorm = 71.8269, GNorm = 0.1798
Meta loss on this task batch = 3.9898e-01, Meta loss averaged over last 500 steps = 4.1273e-01, PNorm = 71.8413, GNorm = 0.1505
Meta loss on this task batch = 3.7278e-01, Meta loss averaged over last 500 steps = 4.1264e-01, PNorm = 71.8556, GNorm = 0.1517
Meta loss on this task batch = 3.6924e-01, Meta loss averaged over last 500 steps = 4.1246e-01, PNorm = 71.8695, GNorm = 0.1719
Took 115.8166253566742 seconds to complete one epoch of meta training
Took 123.8558087348938 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499133
Found better MAML checkpoint after meta validation, saving now
Epoch 95
Meta loss on this task batch = 4.0647e-01, Meta loss averaged over last 500 steps = 4.1243e-01, PNorm = 71.8837, GNorm = 0.1799
Meta loss on this task batch = 3.5544e-01, Meta loss averaged over last 500 steps = 4.1231e-01, PNorm = 71.8966, GNorm = 0.1454
Meta loss on this task batch = 3.2135e-01, Meta loss averaged over last 500 steps = 4.1209e-01, PNorm = 71.9094, GNorm = 0.1507
Meta loss on this task batch = 4.6000e-01, Meta loss averaged over last 500 steps = 4.1211e-01, PNorm = 71.9211, GNorm = 0.2761
Meta loss on this task batch = 4.6073e-01, Meta loss averaged over last 500 steps = 4.1222e-01, PNorm = 71.9285, GNorm = 0.2909
Meta loss on this task batch = 3.8639e-01, Meta loss averaged over last 500 steps = 4.1204e-01, PNorm = 71.9370, GNorm = 0.1705
Meta loss on this task batch = 4.3126e-01, Meta loss averaged over last 500 steps = 4.1200e-01, PNorm = 71.9460, GNorm = 0.1684
Meta loss on this task batch = 4.1156e-01, Meta loss averaged over last 500 steps = 4.1208e-01, PNorm = 71.9567, GNorm = 0.1844
Meta loss on this task batch = 4.2678e-01, Meta loss averaged over last 500 steps = 4.1196e-01, PNorm = 71.9684, GNorm = 0.1676
Meta loss on this task batch = 4.3836e-01, Meta loss averaged over last 500 steps = 4.1203e-01, PNorm = 71.9800, GNorm = 0.1655
Meta loss on this task batch = 4.3250e-01, Meta loss averaged over last 500 steps = 4.1205e-01, PNorm = 71.9918, GNorm = 0.1739
Meta loss on this task batch = 3.8562e-01, Meta loss averaged over last 500 steps = 4.1196e-01, PNorm = 72.0050, GNorm = 0.1556
Meta loss on this task batch = 3.9142e-01, Meta loss averaged over last 500 steps = 4.1186e-01, PNorm = 72.0191, GNorm = 0.1542
Meta loss on this task batch = 4.3830e-01, Meta loss averaged over last 500 steps = 4.1181e-01, PNorm = 72.0340, GNorm = 0.1979
Meta loss on this task batch = 3.7665e-01, Meta loss averaged over last 500 steps = 4.1163e-01, PNorm = 72.0494, GNorm = 0.1791
Meta loss on this task batch = 3.5879e-01, Meta loss averaged over last 500 steps = 4.1150e-01, PNorm = 72.0649, GNorm = 0.2111
Meta loss on this task batch = 3.6990e-01, Meta loss averaged over last 500 steps = 4.1134e-01, PNorm = 72.0790, GNorm = 0.1503
Meta loss on this task batch = 4.1609e-01, Meta loss averaged over last 500 steps = 4.1138e-01, PNorm = 72.0912, GNorm = 0.2154
Meta loss on this task batch = 3.9360e-01, Meta loss averaged over last 500 steps = 4.1138e-01, PNorm = 72.1020, GNorm = 0.2156
Took 116.71490359306335 seconds to complete one epoch of meta training
Took 124.3900499343872 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464043
Epoch 96
Meta loss on this task batch = 4.4317e-01, Meta loss averaged over last 500 steps = 4.1150e-01, PNorm = 72.1119, GNorm = 0.1918
Meta loss on this task batch = 3.8092e-01, Meta loss averaged over last 500 steps = 4.1146e-01, PNorm = 72.1209, GNorm = 0.1878
Meta loss on this task batch = 3.8035e-01, Meta loss averaged over last 500 steps = 4.1142e-01, PNorm = 72.1307, GNorm = 0.1464
Meta loss on this task batch = 3.7530e-01, Meta loss averaged over last 500 steps = 4.1136e-01, PNorm = 72.1430, GNorm = 0.2280
Meta loss on this task batch = 4.1123e-01, Meta loss averaged over last 500 steps = 4.1128e-01, PNorm = 72.1555, GNorm = 0.1787
Meta loss on this task batch = 4.6057e-01, Meta loss averaged over last 500 steps = 4.1130e-01, PNorm = 72.1672, GNorm = 0.2086
Meta loss on this task batch = 4.1582e-01, Meta loss averaged over last 500 steps = 4.1133e-01, PNorm = 72.1794, GNorm = 0.1768
Meta loss on this task batch = 4.1071e-01, Meta loss averaged over last 500 steps = 4.1131e-01, PNorm = 72.1922, GNorm = 0.1754
Meta loss on this task batch = 3.4852e-01, Meta loss averaged over last 500 steps = 4.1122e-01, PNorm = 72.2061, GNorm = 0.1695
Meta loss on this task batch = 4.0422e-01, Meta loss averaged over last 500 steps = 4.1123e-01, PNorm = 72.2185, GNorm = 0.2018
Meta loss on this task batch = 3.5929e-01, Meta loss averaged over last 500 steps = 4.1108e-01, PNorm = 72.2308, GNorm = 0.1487
Meta loss on this task batch = 4.0581e-01, Meta loss averaged over last 500 steps = 4.1100e-01, PNorm = 72.2413, GNorm = 0.2739
Meta loss on this task batch = 3.9940e-01, Meta loss averaged over last 500 steps = 4.1089e-01, PNorm = 72.2510, GNorm = 0.1769
Meta loss on this task batch = 3.2553e-01, Meta loss averaged over last 500 steps = 4.1063e-01, PNorm = 72.2611, GNorm = 0.1922
Meta loss on this task batch = 4.6939e-01, Meta loss averaged over last 500 steps = 4.1066e-01, PNorm = 72.2730, GNorm = 0.2144
Meta loss on this task batch = 3.8911e-01, Meta loss averaged over last 500 steps = 4.1062e-01, PNorm = 72.2852, GNorm = 0.1526
Meta loss on this task batch = 4.0113e-01, Meta loss averaged over last 500 steps = 4.1049e-01, PNorm = 72.2983, GNorm = 0.1476
Meta loss on this task batch = 4.5071e-01, Meta loss averaged over last 500 steps = 4.1059e-01, PNorm = 72.3109, GNorm = 0.1657
Meta loss on this task batch = 4.5802e-01, Meta loss averaged over last 500 steps = 4.1072e-01, PNorm = 72.3235, GNorm = 0.2090
Took 114.19569969177246 seconds to complete one epoch of meta training
Took 121.27333545684814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477275
Epoch 97
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 4.1055e-01, PNorm = 72.3361, GNorm = 0.1905
Meta loss on this task batch = 3.7357e-01, Meta loss averaged over last 500 steps = 4.1040e-01, PNorm = 72.3485, GNorm = 0.1649
Meta loss on this task batch = 3.8470e-01, Meta loss averaged over last 500 steps = 4.1045e-01, PNorm = 72.3602, GNorm = 0.1838
Meta loss on this task batch = 4.6275e-01, Meta loss averaged over last 500 steps = 4.1056e-01, PNorm = 72.3713, GNorm = 0.1738
Meta loss on this task batch = 3.9925e-01, Meta loss averaged over last 500 steps = 4.1051e-01, PNorm = 72.3835, GNorm = 0.1722
Meta loss on this task batch = 3.8098e-01, Meta loss averaged over last 500 steps = 4.1033e-01, PNorm = 72.3963, GNorm = 0.1525
Meta loss on this task batch = 4.7049e-01, Meta loss averaged over last 500 steps = 4.1055e-01, PNorm = 72.4084, GNorm = 0.2052
Meta loss on this task batch = 4.3789e-01, Meta loss averaged over last 500 steps = 4.1059e-01, PNorm = 72.4201, GNorm = 0.1549
Meta loss on this task batch = 3.8102e-01, Meta loss averaged over last 500 steps = 4.1044e-01, PNorm = 72.4323, GNorm = 0.1623
Meta loss on this task batch = 4.0199e-01, Meta loss averaged over last 500 steps = 4.1043e-01, PNorm = 72.4445, GNorm = 0.1836
Meta loss on this task batch = 3.9366e-01, Meta loss averaged over last 500 steps = 4.1043e-01, PNorm = 72.4561, GNorm = 0.1809
Meta loss on this task batch = 4.3215e-01, Meta loss averaged over last 500 steps = 4.1038e-01, PNorm = 72.4685, GNorm = 0.1676
Meta loss on this task batch = 4.0494e-01, Meta loss averaged over last 500 steps = 4.1037e-01, PNorm = 72.4819, GNorm = 0.1730
Meta loss on this task batch = 4.4869e-01, Meta loss averaged over last 500 steps = 4.1038e-01, PNorm = 72.4958, GNorm = 0.1770
Meta loss on this task batch = 3.4262e-01, Meta loss averaged over last 500 steps = 4.1026e-01, PNorm = 72.5098, GNorm = 0.1516
Meta loss on this task batch = 4.6037e-01, Meta loss averaged over last 500 steps = 4.1050e-01, PNorm = 72.5238, GNorm = 0.1746
Meta loss on this task batch = 3.9647e-01, Meta loss averaged over last 500 steps = 4.1035e-01, PNorm = 72.5377, GNorm = 0.2413
Meta loss on this task batch = 3.6857e-01, Meta loss averaged over last 500 steps = 4.1028e-01, PNorm = 72.5524, GNorm = 0.1723
Meta loss on this task batch = 4.0752e-01, Meta loss averaged over last 500 steps = 4.1038e-01, PNorm = 72.5660, GNorm = 0.2286
Took 116.88170599937439 seconds to complete one epoch of meta training
Took 124.68850517272949 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475103
Epoch 98
Meta loss on this task batch = 3.5908e-01, Meta loss averaged over last 500 steps = 4.1027e-01, PNorm = 72.5802, GNorm = 0.1804
Meta loss on this task batch = 3.3684e-01, Meta loss averaged over last 500 steps = 4.1010e-01, PNorm = 72.5947, GNorm = 0.1498
Meta loss on this task batch = 2.9618e-01, Meta loss averaged over last 500 steps = 4.0977e-01, PNorm = 72.6085, GNorm = 0.1696
Meta loss on this task batch = 3.9361e-01, Meta loss averaged over last 500 steps = 4.0966e-01, PNorm = 72.6206, GNorm = 0.2180
Meta loss on this task batch = 4.4392e-01, Meta loss averaged over last 500 steps = 4.0985e-01, PNorm = 72.6323, GNorm = 0.2077
Meta loss on this task batch = 4.0059e-01, Meta loss averaged over last 500 steps = 4.0974e-01, PNorm = 72.6433, GNorm = 0.2305
Meta loss on this task batch = 4.3492e-01, Meta loss averaged over last 500 steps = 4.0974e-01, PNorm = 72.6543, GNorm = 0.1903
Meta loss on this task batch = 4.4604e-01, Meta loss averaged over last 500 steps = 4.0978e-01, PNorm = 72.6652, GNorm = 0.1850
Meta loss on this task batch = 3.9007e-01, Meta loss averaged over last 500 steps = 4.0959e-01, PNorm = 72.6775, GNorm = 0.2513
Meta loss on this task batch = 4.0017e-01, Meta loss averaged over last 500 steps = 4.0967e-01, PNorm = 72.6902, GNorm = 0.2131
Meta loss on this task batch = 4.4308e-01, Meta loss averaged over last 500 steps = 4.0971e-01, PNorm = 72.7027, GNorm = 0.1955
Meta loss on this task batch = 3.7835e-01, Meta loss averaged over last 500 steps = 4.0954e-01, PNorm = 72.7157, GNorm = 0.1573
Meta loss on this task batch = 4.5566e-01, Meta loss averaged over last 500 steps = 4.0952e-01, PNorm = 72.7274, GNorm = 0.2143
Meta loss on this task batch = 4.0000e-01, Meta loss averaged over last 500 steps = 4.0953e-01, PNorm = 72.7390, GNorm = 0.1933
Meta loss on this task batch = 3.9389e-01, Meta loss averaged over last 500 steps = 4.0947e-01, PNorm = 72.7504, GNorm = 0.1533
Meta loss on this task batch = 3.6480e-01, Meta loss averaged over last 500 steps = 4.0939e-01, PNorm = 72.7628, GNorm = 0.1686
Meta loss on this task batch = 4.4890e-01, Meta loss averaged over last 500 steps = 4.0946e-01, PNorm = 72.7747, GNorm = 0.1849
Meta loss on this task batch = 4.1815e-01, Meta loss averaged over last 500 steps = 4.0941e-01, PNorm = 72.7875, GNorm = 0.1657
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 4.0934e-01, PNorm = 72.8021, GNorm = 0.2028
Took 114.49714875221252 seconds to complete one epoch of meta training
Took 122.6952064037323 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480962
Epoch 99
Meta loss on this task batch = 4.3778e-01, Meta loss averaged over last 500 steps = 4.0940e-01, PNorm = 72.8159, GNorm = 0.1958
Meta loss on this task batch = 4.4410e-01, Meta loss averaged over last 500 steps = 4.0931e-01, PNorm = 72.8297, GNorm = 0.1688
Meta loss on this task batch = 4.3496e-01, Meta loss averaged over last 500 steps = 4.0928e-01, PNorm = 72.8432, GNorm = 0.1743
Meta loss on this task batch = 3.2816e-01, Meta loss averaged over last 500 steps = 4.0912e-01, PNorm = 72.8562, GNorm = 0.1726
Meta loss on this task batch = 4.7771e-01, Meta loss averaged over last 500 steps = 4.0935e-01, PNorm = 72.8674, GNorm = 0.2208
Meta loss on this task batch = 4.2993e-01, Meta loss averaged over last 500 steps = 4.0950e-01, PNorm = 72.8780, GNorm = 0.1927
Meta loss on this task batch = 4.0041e-01, Meta loss averaged over last 500 steps = 4.0948e-01, PNorm = 72.8886, GNorm = 0.1581
Meta loss on this task batch = 3.8290e-01, Meta loss averaged over last 500 steps = 4.0932e-01, PNorm = 72.9005, GNorm = 0.1883
Meta loss on this task batch = 3.6815e-01, Meta loss averaged over last 500 steps = 4.0909e-01, PNorm = 72.9131, GNorm = 0.1808
Meta loss on this task batch = 4.0941e-01, Meta loss averaged over last 500 steps = 4.0904e-01, PNorm = 72.9260, GNorm = 0.2003
Meta loss on this task batch = 4.1676e-01, Meta loss averaged over last 500 steps = 4.0910e-01, PNorm = 72.9404, GNorm = 0.1757
Meta loss on this task batch = 3.2844e-01, Meta loss averaged over last 500 steps = 4.0899e-01, PNorm = 72.9549, GNorm = 0.1772
Meta loss on this task batch = 4.0764e-01, Meta loss averaged over last 500 steps = 4.0904e-01, PNorm = 72.9690, GNorm = 0.1947
Meta loss on this task batch = 4.0082e-01, Meta loss averaged over last 500 steps = 4.0898e-01, PNorm = 72.9818, GNorm = 0.2190
Meta loss on this task batch = 4.1804e-01, Meta loss averaged over last 500 steps = 4.0902e-01, PNorm = 72.9951, GNorm = 0.2189
Meta loss on this task batch = 3.9242e-01, Meta loss averaged over last 500 steps = 4.0895e-01, PNorm = 73.0071, GNorm = 0.1961
Meta loss on this task batch = 3.3400e-01, Meta loss averaged over last 500 steps = 4.0873e-01, PNorm = 73.0186, GNorm = 0.1760
Meta loss on this task batch = 3.8129e-01, Meta loss averaged over last 500 steps = 4.0854e-01, PNorm = 73.0291, GNorm = 0.2356
Meta loss on this task batch = 4.5187e-01, Meta loss averaged over last 500 steps = 4.0866e-01, PNorm = 73.0393, GNorm = 0.2332
Took 113.05486416816711 seconds to complete one epoch of meta training
Took 120.90217399597168 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481903
Epoch 100
Meta loss on this task batch = 4.2662e-01, Meta loss averaged over last 500 steps = 4.0882e-01, PNorm = 73.0484, GNorm = 0.1883
Meta loss on this task batch = 4.3162e-01, Meta loss averaged over last 500 steps = 4.0882e-01, PNorm = 73.0575, GNorm = 0.1661
Meta loss on this task batch = 3.6552e-01, Meta loss averaged over last 500 steps = 4.0863e-01, PNorm = 73.0684, GNorm = 0.1719
Meta loss on this task batch = 3.7985e-01, Meta loss averaged over last 500 steps = 4.0868e-01, PNorm = 73.0812, GNorm = 0.1692
Meta loss on this task batch = 3.6989e-01, Meta loss averaged over last 500 steps = 4.0857e-01, PNorm = 73.0967, GNorm = 0.1869
Meta loss on this task batch = 4.1499e-01, Meta loss averaged over last 500 steps = 4.0853e-01, PNorm = 73.1125, GNorm = 0.1505
Meta loss on this task batch = 3.8949e-01, Meta loss averaged over last 500 steps = 4.0851e-01, PNorm = 73.1292, GNorm = 0.1911
Meta loss on this task batch = 4.4007e-01, Meta loss averaged over last 500 steps = 4.0852e-01, PNorm = 73.1447, GNorm = 0.1556
Meta loss on this task batch = 4.0723e-01, Meta loss averaged over last 500 steps = 4.0851e-01, PNorm = 73.1594, GNorm = 0.1921
Meta loss on this task batch = 3.9364e-01, Meta loss averaged over last 500 steps = 4.0843e-01, PNorm = 73.1739, GNorm = 0.1862
Meta loss on this task batch = 3.6740e-01, Meta loss averaged over last 500 steps = 4.0838e-01, PNorm = 73.1873, GNorm = 0.1630
Meta loss on this task batch = 4.3724e-01, Meta loss averaged over last 500 steps = 4.0834e-01, PNorm = 73.1994, GNorm = 0.2045
Meta loss on this task batch = 4.5371e-01, Meta loss averaged over last 500 steps = 4.0847e-01, PNorm = 73.2102, GNorm = 0.2675
Meta loss on this task batch = 4.3829e-01, Meta loss averaged over last 500 steps = 4.0850e-01, PNorm = 73.2218, GNorm = 0.1902
Meta loss on this task batch = 3.7794e-01, Meta loss averaged over last 500 steps = 4.0839e-01, PNorm = 73.2333, GNorm = 0.1675
Meta loss on this task batch = 4.2426e-01, Meta loss averaged over last 500 steps = 4.0839e-01, PNorm = 73.2451, GNorm = 0.1928
Meta loss on this task batch = 3.0385e-01, Meta loss averaged over last 500 steps = 4.0829e-01, PNorm = 73.2572, GNorm = 0.1820
Meta loss on this task batch = 4.3353e-01, Meta loss averaged over last 500 steps = 4.0842e-01, PNorm = 73.2678, GNorm = 0.1897
Meta loss on this task batch = 3.9507e-01, Meta loss averaged over last 500 steps = 4.0850e-01, PNorm = 73.2792, GNorm = 0.1891
Took 116.37677764892578 seconds to complete one epoch of meta training
Took 124.00825190544128 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496201
Epoch 101
Meta loss on this task batch = 3.8611e-01, Meta loss averaged over last 500 steps = 4.0833e-01, PNorm = 73.2901, GNorm = 0.1520
Meta loss on this task batch = 4.1593e-01, Meta loss averaged over last 500 steps = 4.0814e-01, PNorm = 73.3017, GNorm = 0.1700
Meta loss on this task batch = 3.7684e-01, Meta loss averaged over last 500 steps = 4.0812e-01, PNorm = 73.3136, GNorm = 0.1535
Meta loss on this task batch = 4.2486e-01, Meta loss averaged over last 500 steps = 4.0816e-01, PNorm = 73.3254, GNorm = 0.1667
Meta loss on this task batch = 3.7392e-01, Meta loss averaged over last 500 steps = 4.0804e-01, PNorm = 73.3368, GNorm = 0.1469
Meta loss on this task batch = 3.8741e-01, Meta loss averaged over last 500 steps = 4.0805e-01, PNorm = 73.3485, GNorm = 0.1626
Meta loss on this task batch = 3.7810e-01, Meta loss averaged over last 500 steps = 4.0798e-01, PNorm = 73.3602, GNorm = 0.1761
Meta loss on this task batch = 3.6477e-01, Meta loss averaged over last 500 steps = 4.0789e-01, PNorm = 73.3724, GNorm = 0.1571
Meta loss on this task batch = 4.2651e-01, Meta loss averaged over last 500 steps = 4.0789e-01, PNorm = 73.3842, GNorm = 0.1816
Meta loss on this task batch = 3.9178e-01, Meta loss averaged over last 500 steps = 4.0796e-01, PNorm = 73.3956, GNorm = 0.1892
Meta loss on this task batch = 3.3844e-01, Meta loss averaged over last 500 steps = 4.0777e-01, PNorm = 73.4089, GNorm = 0.1816
Meta loss on this task batch = 4.0755e-01, Meta loss averaged over last 500 steps = 4.0773e-01, PNorm = 73.4215, GNorm = 0.1895
Meta loss on this task batch = 4.5904e-01, Meta loss averaged over last 500 steps = 4.0778e-01, PNorm = 73.4322, GNorm = 0.2419
Meta loss on this task batch = 4.2182e-01, Meta loss averaged over last 500 steps = 4.0783e-01, PNorm = 73.4420, GNorm = 0.1873
Meta loss on this task batch = 3.6769e-01, Meta loss averaged over last 500 steps = 4.0756e-01, PNorm = 73.4537, GNorm = 0.1979
Meta loss on this task batch = 4.1435e-01, Meta loss averaged over last 500 steps = 4.0755e-01, PNorm = 73.4657, GNorm = 0.1864
Meta loss on this task batch = 4.5358e-01, Meta loss averaged over last 500 steps = 4.0758e-01, PNorm = 73.4769, GNorm = 0.1957
Meta loss on this task batch = 4.3670e-01, Meta loss averaged over last 500 steps = 4.0761e-01, PNorm = 73.4878, GNorm = 0.1708
Meta loss on this task batch = 4.0433e-01, Meta loss averaged over last 500 steps = 4.0758e-01, PNorm = 73.4986, GNorm = 0.1734
Took 118.34599947929382 seconds to complete one epoch of meta training
Took 126.30447626113892 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473211
Epoch 102
Meta loss on this task batch = 3.8675e-01, Meta loss averaged over last 500 steps = 4.0756e-01, PNorm = 73.5111, GNorm = 0.1450
Meta loss on this task batch = 4.4111e-01, Meta loss averaged over last 500 steps = 4.0765e-01, PNorm = 73.5245, GNorm = 0.1647
Meta loss on this task batch = 4.2918e-01, Meta loss averaged over last 500 steps = 4.0768e-01, PNorm = 73.5375, GNorm = 0.1612
Meta loss on this task batch = 3.8437e-01, Meta loss averaged over last 500 steps = 4.0772e-01, PNorm = 73.5517, GNorm = 0.2017
Meta loss on this task batch = 3.8687e-01, Meta loss averaged over last 500 steps = 4.0780e-01, PNorm = 73.5658, GNorm = 0.1837
Meta loss on this task batch = 3.8455e-01, Meta loss averaged over last 500 steps = 4.0775e-01, PNorm = 73.5792, GNorm = 0.1634
Meta loss on this task batch = 3.9333e-01, Meta loss averaged over last 500 steps = 4.0769e-01, PNorm = 73.5922, GNorm = 0.1803
Meta loss on this task batch = 4.1179e-01, Meta loss averaged over last 500 steps = 4.0771e-01, PNorm = 73.6045, GNorm = 0.1615
Meta loss on this task batch = 3.7779e-01, Meta loss averaged over last 500 steps = 4.0756e-01, PNorm = 73.6172, GNorm = 0.1928
Meta loss on this task batch = 4.0295e-01, Meta loss averaged over last 500 steps = 4.0760e-01, PNorm = 73.6295, GNorm = 0.1797
Meta loss on this task batch = 3.8431e-01, Meta loss averaged over last 500 steps = 4.0754e-01, PNorm = 73.6422, GNorm = 0.1760
Meta loss on this task batch = 4.0574e-01, Meta loss averaged over last 500 steps = 4.0770e-01, PNorm = 73.6548, GNorm = 0.1541
Meta loss on this task batch = 4.5509e-01, Meta loss averaged over last 500 steps = 4.0780e-01, PNorm = 73.6674, GNorm = 0.1719
Meta loss on this task batch = 4.0736e-01, Meta loss averaged over last 500 steps = 4.0772e-01, PNorm = 73.6786, GNorm = 0.1702
Meta loss on this task batch = 3.7262e-01, Meta loss averaged over last 500 steps = 4.0762e-01, PNorm = 73.6905, GNorm = 0.1928
Meta loss on this task batch = 3.5574e-01, Meta loss averaged over last 500 steps = 4.0747e-01, PNorm = 73.7029, GNorm = 0.1731
Meta loss on this task batch = 3.7060e-01, Meta loss averaged over last 500 steps = 4.0725e-01, PNorm = 73.7158, GNorm = 0.1608
Meta loss on this task batch = 4.3736e-01, Meta loss averaged over last 500 steps = 4.0736e-01, PNorm = 73.7275, GNorm = 0.1889
Meta loss on this task batch = 3.2348e-01, Meta loss averaged over last 500 steps = 4.0719e-01, PNorm = 73.7394, GNorm = 0.1878
Took 114.9455029964447 seconds to complete one epoch of meta training
Took 121.60874962806702 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484283
Epoch 103
Meta loss on this task batch = 3.7147e-01, Meta loss averaged over last 500 steps = 4.0721e-01, PNorm = 73.7512, GNorm = 0.1786
Meta loss on this task batch = 4.1431e-01, Meta loss averaged over last 500 steps = 4.0722e-01, PNorm = 73.7636, GNorm = 0.1965
Meta loss on this task batch = 3.9734e-01, Meta loss averaged over last 500 steps = 4.0707e-01, PNorm = 73.7764, GNorm = 0.1761
Meta loss on this task batch = 3.9995e-01, Meta loss averaged over last 500 steps = 4.0709e-01, PNorm = 73.7898, GNorm = 0.1814
Meta loss on this task batch = 3.8529e-01, Meta loss averaged over last 500 steps = 4.0701e-01, PNorm = 73.8025, GNorm = 0.1756
Meta loss on this task batch = 3.8663e-01, Meta loss averaged over last 500 steps = 4.0691e-01, PNorm = 73.8158, GNorm = 0.1711
Meta loss on this task batch = 4.3035e-01, Meta loss averaged over last 500 steps = 4.0698e-01, PNorm = 73.8294, GNorm = 0.1543
Meta loss on this task batch = 3.9339e-01, Meta loss averaged over last 500 steps = 4.0705e-01, PNorm = 73.8423, GNorm = 0.1729
Meta loss on this task batch = 3.9227e-01, Meta loss averaged over last 500 steps = 4.0705e-01, PNorm = 73.8552, GNorm = 0.1701
Meta loss on this task batch = 4.2194e-01, Meta loss averaged over last 500 steps = 4.0707e-01, PNorm = 73.8669, GNorm = 0.1845
Meta loss on this task batch = 4.1631e-01, Meta loss averaged over last 500 steps = 4.0705e-01, PNorm = 73.8795, GNorm = 0.2191
Meta loss on this task batch = 3.4940e-01, Meta loss averaged over last 500 steps = 4.0687e-01, PNorm = 73.8928, GNorm = 0.1456
Meta loss on this task batch = 3.9329e-01, Meta loss averaged over last 500 steps = 4.0684e-01, PNorm = 73.9067, GNorm = 0.1536
Meta loss on this task batch = 3.8559e-01, Meta loss averaged over last 500 steps = 4.0676e-01, PNorm = 73.9209, GNorm = 0.1544
Meta loss on this task batch = 4.2760e-01, Meta loss averaged over last 500 steps = 4.0671e-01, PNorm = 73.9355, GNorm = 0.1824
Meta loss on this task batch = 4.2807e-01, Meta loss averaged over last 500 steps = 4.0678e-01, PNorm = 73.9497, GNorm = 0.1753
Meta loss on this task batch = 3.4537e-01, Meta loss averaged over last 500 steps = 4.0665e-01, PNorm = 73.9636, GNorm = 0.1571
Meta loss on this task batch = 4.2752e-01, Meta loss averaged over last 500 steps = 4.0651e-01, PNorm = 73.9767, GNorm = 0.1921
Meta loss on this task batch = 3.0733e-01, Meta loss averaged over last 500 steps = 4.0627e-01, PNorm = 73.9897, GNorm = 0.1811
Took 116.30943155288696 seconds to complete one epoch of meta training
Took 123.76110816001892 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496350
Epoch 104
Meta loss on this task batch = 4.0676e-01, Meta loss averaged over last 500 steps = 4.0624e-01, PNorm = 74.0020, GNorm = 0.1893
Meta loss on this task batch = 3.9846e-01, Meta loss averaged over last 500 steps = 4.0617e-01, PNorm = 74.0136, GNorm = 0.1898
Meta loss on this task batch = 3.4697e-01, Meta loss averaged over last 500 steps = 4.0594e-01, PNorm = 74.0250, GNorm = 0.1704
Meta loss on this task batch = 3.8481e-01, Meta loss averaged over last 500 steps = 4.0594e-01, PNorm = 74.0371, GNorm = 0.1614
Meta loss on this task batch = 4.1984e-01, Meta loss averaged over last 500 steps = 4.0600e-01, PNorm = 74.0506, GNorm = 0.1973
Meta loss on this task batch = 4.3948e-01, Meta loss averaged over last 500 steps = 4.0618e-01, PNorm = 74.0622, GNorm = 0.1725
Meta loss on this task batch = 4.0567e-01, Meta loss averaged over last 500 steps = 4.0619e-01, PNorm = 74.0741, GNorm = 0.1692
Meta loss on this task batch = 3.7520e-01, Meta loss averaged over last 500 steps = 4.0607e-01, PNorm = 74.0861, GNorm = 0.1799
Meta loss on this task batch = 3.8076e-01, Meta loss averaged over last 500 steps = 4.0611e-01, PNorm = 74.0988, GNorm = 0.1879
Meta loss on this task batch = 4.2687e-01, Meta loss averaged over last 500 steps = 4.0599e-01, PNorm = 74.1111, GNorm = 0.1617
Meta loss on this task batch = 3.9390e-01, Meta loss averaged over last 500 steps = 4.0606e-01, PNorm = 74.1237, GNorm = 0.1697
Meta loss on this task batch = 4.4303e-01, Meta loss averaged over last 500 steps = 4.0607e-01, PNorm = 74.1364, GNorm = 0.1772
Meta loss on this task batch = 3.8622e-01, Meta loss averaged over last 500 steps = 4.0594e-01, PNorm = 74.1492, GNorm = 0.1790
Meta loss on this task batch = 4.4934e-01, Meta loss averaged over last 500 steps = 4.0620e-01, PNorm = 74.1608, GNorm = 0.1892
Meta loss on this task batch = 3.6858e-01, Meta loss averaged over last 500 steps = 4.0604e-01, PNorm = 74.1698, GNorm = 0.2548
Meta loss on this task batch = 4.1654e-01, Meta loss averaged over last 500 steps = 4.0593e-01, PNorm = 74.1797, GNorm = 0.1705
Meta loss on this task batch = 3.6532e-01, Meta loss averaged over last 500 steps = 4.0589e-01, PNorm = 74.1905, GNorm = 0.2003
Meta loss on this task batch = 4.2120e-01, Meta loss averaged over last 500 steps = 4.0586e-01, PNorm = 74.2018, GNorm = 0.1996
Meta loss on this task batch = 3.6207e-01, Meta loss averaged over last 500 steps = 4.0565e-01, PNorm = 74.2147, GNorm = 0.1928
Took 117.80641841888428 seconds to complete one epoch of meta training
Took 126.38370108604431 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494689
Epoch 105
Meta loss on this task batch = 3.9213e-01, Meta loss averaged over last 500 steps = 4.0562e-01, PNorm = 74.2281, GNorm = 0.1929
Meta loss on this task batch = 4.4327e-01, Meta loss averaged over last 500 steps = 4.0574e-01, PNorm = 74.2426, GNorm = 0.1892
Meta loss on this task batch = 3.3955e-01, Meta loss averaged over last 500 steps = 4.0563e-01, PNorm = 74.2572, GNorm = 0.1530
Meta loss on this task batch = 3.9963e-01, Meta loss averaged over last 500 steps = 4.0567e-01, PNorm = 74.2720, GNorm = 0.1850
Meta loss on this task batch = 3.9280e-01, Meta loss averaged over last 500 steps = 4.0556e-01, PNorm = 74.2857, GNorm = 0.1736
Meta loss on this task batch = 4.2135e-01, Meta loss averaged over last 500 steps = 4.0560e-01, PNorm = 74.2983, GNorm = 0.2191
Meta loss on this task batch = 3.6460e-01, Meta loss averaged over last 500 steps = 4.0552e-01, PNorm = 74.3114, GNorm = 0.2183
Meta loss on this task batch = 4.2118e-01, Meta loss averaged over last 500 steps = 4.0548e-01, PNorm = 74.3230, GNorm = 0.2371
Meta loss on this task batch = 4.0277e-01, Meta loss averaged over last 500 steps = 4.0544e-01, PNorm = 74.3338, GNorm = 0.2095
Meta loss on this task batch = 4.0822e-01, Meta loss averaged over last 500 steps = 4.0548e-01, PNorm = 74.3439, GNorm = 0.1939
Meta loss on this task batch = 4.0412e-01, Meta loss averaged over last 500 steps = 4.0553e-01, PNorm = 74.3542, GNorm = 0.2088
Meta loss on this task batch = 4.3184e-01, Meta loss averaged over last 500 steps = 4.0568e-01, PNorm = 74.3648, GNorm = 0.1729
Meta loss on this task batch = 3.4704e-01, Meta loss averaged over last 500 steps = 4.0546e-01, PNorm = 74.3757, GNorm = 0.1666
Meta loss on this task batch = 4.3077e-01, Meta loss averaged over last 500 steps = 4.0539e-01, PNorm = 74.3864, GNorm = 0.1675
Meta loss on this task batch = 3.9556e-01, Meta loss averaged over last 500 steps = 4.0536e-01, PNorm = 74.3980, GNorm = 0.1954
Meta loss on this task batch = 3.4409e-01, Meta loss averaged over last 500 steps = 4.0517e-01, PNorm = 74.4105, GNorm = 0.1889
Meta loss on this task batch = 3.6926e-01, Meta loss averaged over last 500 steps = 4.0506e-01, PNorm = 74.4236, GNorm = 0.1728
Meta loss on this task batch = 3.6343e-01, Meta loss averaged over last 500 steps = 4.0503e-01, PNorm = 74.4375, GNorm = 0.1835
Meta loss on this task batch = 4.4802e-01, Meta loss averaged over last 500 steps = 4.0511e-01, PNorm = 74.4516, GNorm = 0.2372
Took 117.68957543373108 seconds to complete one epoch of meta training
Took 125.95848822593689 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489935
Epoch 106
Meta loss on this task batch = 3.7645e-01, Meta loss averaged over last 500 steps = 4.0496e-01, PNorm = 74.4666, GNorm = 0.1604
Meta loss on this task batch = 3.8030e-01, Meta loss averaged over last 500 steps = 4.0490e-01, PNorm = 74.4803, GNorm = 0.1634
Meta loss on this task batch = 4.0980e-01, Meta loss averaged over last 500 steps = 4.0479e-01, PNorm = 74.4932, GNorm = 0.2423
Meta loss on this task batch = 3.8928e-01, Meta loss averaged over last 500 steps = 4.0496e-01, PNorm = 74.5067, GNorm = 0.2130
Meta loss on this task batch = 3.7630e-01, Meta loss averaged over last 500 steps = 4.0476e-01, PNorm = 74.5200, GNorm = 0.1778
Meta loss on this task batch = 3.3168e-01, Meta loss averaged over last 500 steps = 4.0469e-01, PNorm = 74.5348, GNorm = 0.1677
Meta loss on this task batch = 4.1259e-01, Meta loss averaged over last 500 steps = 4.0481e-01, PNorm = 74.5481, GNorm = 0.1937
Meta loss on this task batch = 3.8231e-01, Meta loss averaged over last 500 steps = 4.0473e-01, PNorm = 74.5606, GNorm = 0.1917
Meta loss on this task batch = 4.3489e-01, Meta loss averaged over last 500 steps = 4.0480e-01, PNorm = 74.5716, GNorm = 0.2265
Meta loss on this task batch = 3.7818e-01, Meta loss averaged over last 500 steps = 4.0479e-01, PNorm = 74.5821, GNorm = 0.2035
Meta loss on this task batch = 4.2817e-01, Meta loss averaged over last 500 steps = 4.0477e-01, PNorm = 74.5930, GNorm = 0.2133
Meta loss on this task batch = 3.9790e-01, Meta loss averaged over last 500 steps = 4.0466e-01, PNorm = 74.6044, GNorm = 0.1958
Meta loss on this task batch = 3.9320e-01, Meta loss averaged over last 500 steps = 4.0454e-01, PNorm = 74.6166, GNorm = 0.2109
Meta loss on this task batch = 4.2748e-01, Meta loss averaged over last 500 steps = 4.0448e-01, PNorm = 74.6284, GNorm = 0.2432
Meta loss on this task batch = 3.9997e-01, Meta loss averaged over last 500 steps = 4.0451e-01, PNorm = 74.6394, GNorm = 0.1937
Meta loss on this task batch = 4.1736e-01, Meta loss averaged over last 500 steps = 4.0453e-01, PNorm = 74.6508, GNorm = 0.1929
Meta loss on this task batch = 4.2578e-01, Meta loss averaged over last 500 steps = 4.0455e-01, PNorm = 74.6622, GNorm = 0.2472
Meta loss on this task batch = 3.4906e-01, Meta loss averaged over last 500 steps = 4.0445e-01, PNorm = 74.6749, GNorm = 0.1594
Meta loss on this task batch = 4.1824e-01, Meta loss averaged over last 500 steps = 4.0452e-01, PNorm = 74.6878, GNorm = 0.2136
Took 112.89435148239136 seconds to complete one epoch of meta training
Took 121.09771347045898 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492713
Epoch 107
Meta loss on this task batch = 4.5918e-01, Meta loss averaged over last 500 steps = 4.0465e-01, PNorm = 74.7012, GNorm = 0.2225
Meta loss on this task batch = 3.8292e-01, Meta loss averaged over last 500 steps = 4.0447e-01, PNorm = 74.7143, GNorm = 0.1797
Meta loss on this task batch = 3.8353e-01, Meta loss averaged over last 500 steps = 4.0434e-01, PNorm = 74.7263, GNorm = 0.1991
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 4.0421e-01, PNorm = 74.7394, GNorm = 0.1934
Meta loss on this task batch = 3.6689e-01, Meta loss averaged over last 500 steps = 4.0417e-01, PNorm = 74.7539, GNorm = 0.1796
Meta loss on this task batch = 4.3739e-01, Meta loss averaged over last 500 steps = 4.0406e-01, PNorm = 74.7688, GNorm = 0.1815
Meta loss on this task batch = 3.3418e-01, Meta loss averaged over last 500 steps = 4.0382e-01, PNorm = 74.7828, GNorm = 0.1729
Meta loss on this task batch = 4.1909e-01, Meta loss averaged over last 500 steps = 4.0387e-01, PNorm = 74.7957, GNorm = 0.1941
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 4.0370e-01, PNorm = 74.8078, GNorm = 0.1682
Meta loss on this task batch = 4.7904e-01, Meta loss averaged over last 500 steps = 4.0390e-01, PNorm = 74.8163, GNorm = 0.2484
Meta loss on this task batch = 3.7911e-01, Meta loss averaged over last 500 steps = 4.0376e-01, PNorm = 74.8248, GNorm = 0.1689
Meta loss on this task batch = 4.0585e-01, Meta loss averaged over last 500 steps = 4.0379e-01, PNorm = 74.8345, GNorm = 0.1927
Meta loss on this task batch = 3.9005e-01, Meta loss averaged over last 500 steps = 4.0394e-01, PNorm = 74.8453, GNorm = 0.2154
Meta loss on this task batch = 4.2414e-01, Meta loss averaged over last 500 steps = 4.0400e-01, PNorm = 74.8568, GNorm = 0.1847
Meta loss on this task batch = 4.0934e-01, Meta loss averaged over last 500 steps = 4.0384e-01, PNorm = 74.8687, GNorm = 0.1602
Meta loss on this task batch = 4.6138e-01, Meta loss averaged over last 500 steps = 4.0391e-01, PNorm = 74.8810, GNorm = 0.1686
Meta loss on this task batch = 4.3622e-01, Meta loss averaged over last 500 steps = 4.0396e-01, PNorm = 74.8934, GNorm = 0.1967
Meta loss on this task batch = 3.4467e-01, Meta loss averaged over last 500 steps = 4.0378e-01, PNorm = 74.9067, GNorm = 0.1658
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 4.0369e-01, PNorm = 74.9214, GNorm = 0.2278
Took 112.90325808525085 seconds to complete one epoch of meta training
Took 119.68821859359741 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500843
Found better MAML checkpoint after meta validation, saving now
Epoch 108
Meta loss on this task batch = 3.8788e-01, Meta loss averaged over last 500 steps = 4.0364e-01, PNorm = 74.9360, GNorm = 0.1598
Meta loss on this task batch = 4.3276e-01, Meta loss averaged over last 500 steps = 4.0373e-01, PNorm = 74.9498, GNorm = 0.2046
Meta loss on this task batch = 4.2183e-01, Meta loss averaged over last 500 steps = 4.0369e-01, PNorm = 74.9621, GNorm = 0.2247
Meta loss on this task batch = 4.0133e-01, Meta loss averaged over last 500 steps = 4.0374e-01, PNorm = 74.9744, GNorm = 0.1716
Meta loss on this task batch = 3.8408e-01, Meta loss averaged over last 500 steps = 4.0357e-01, PNorm = 74.9866, GNorm = 0.1943
Meta loss on this task batch = 3.6271e-01, Meta loss averaged over last 500 steps = 4.0343e-01, PNorm = 74.9988, GNorm = 0.1573
Meta loss on this task batch = 4.3905e-01, Meta loss averaged over last 500 steps = 4.0349e-01, PNorm = 75.0091, GNorm = 0.2325
Meta loss on this task batch = 3.3148e-01, Meta loss averaged over last 500 steps = 4.0337e-01, PNorm = 75.0197, GNorm = 0.1819
Meta loss on this task batch = 3.5567e-01, Meta loss averaged over last 500 steps = 4.0323e-01, PNorm = 75.0293, GNorm = 0.1712
Meta loss on this task batch = 3.7395e-01, Meta loss averaged over last 500 steps = 4.0324e-01, PNorm = 75.0393, GNorm = 0.2182
Meta loss on this task batch = 4.2420e-01, Meta loss averaged over last 500 steps = 4.0334e-01, PNorm = 75.0499, GNorm = 0.1659
Meta loss on this task batch = 4.1164e-01, Meta loss averaged over last 500 steps = 4.0340e-01, PNorm = 75.0608, GNorm = 0.1817
Meta loss on this task batch = 4.4066e-01, Meta loss averaged over last 500 steps = 4.0343e-01, PNorm = 75.0733, GNorm = 0.2040
Meta loss on this task batch = 4.0239e-01, Meta loss averaged over last 500 steps = 4.0334e-01, PNorm = 75.0864, GNorm = 0.1853
Meta loss on this task batch = 3.4501e-01, Meta loss averaged over last 500 steps = 4.0314e-01, PNorm = 75.1000, GNorm = 0.1670
Meta loss on this task batch = 3.8079e-01, Meta loss averaged over last 500 steps = 4.0301e-01, PNorm = 75.1141, GNorm = 0.1460
Meta loss on this task batch = 3.7167e-01, Meta loss averaged over last 500 steps = 4.0295e-01, PNorm = 75.1280, GNorm = 0.1613
Meta loss on this task batch = 3.7339e-01, Meta loss averaged over last 500 steps = 4.0295e-01, PNorm = 75.1418, GNorm = 0.1759
Meta loss on this task batch = 4.0600e-01, Meta loss averaged over last 500 steps = 4.0301e-01, PNorm = 75.1553, GNorm = 0.2533
Took 122.22418141365051 seconds to complete one epoch of meta training
Took 130.38229870796204 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479900
Epoch 109
Meta loss on this task batch = 4.0787e-01, Meta loss averaged over last 500 steps = 4.0291e-01, PNorm = 75.1683, GNorm = 0.1770
Meta loss on this task batch = 3.3791e-01, Meta loss averaged over last 500 steps = 4.0286e-01, PNorm = 75.1815, GNorm = 0.1716
Meta loss on this task batch = 3.9471e-01, Meta loss averaged over last 500 steps = 4.0286e-01, PNorm = 75.1945, GNorm = 0.1993
Meta loss on this task batch = 3.6166e-01, Meta loss averaged over last 500 steps = 4.0284e-01, PNorm = 75.2074, GNorm = 0.2059
Meta loss on this task batch = 4.0707e-01, Meta loss averaged over last 500 steps = 4.0276e-01, PNorm = 75.2200, GNorm = 0.1869
Meta loss on this task batch = 3.9252e-01, Meta loss averaged over last 500 steps = 4.0284e-01, PNorm = 75.2329, GNorm = 0.1894
Meta loss on this task batch = 4.5367e-01, Meta loss averaged over last 500 steps = 4.0288e-01, PNorm = 75.2452, GNorm = 0.1998
Meta loss on this task batch = 4.2794e-01, Meta loss averaged over last 500 steps = 4.0293e-01, PNorm = 75.2574, GNorm = 0.1757
Meta loss on this task batch = 4.0181e-01, Meta loss averaged over last 500 steps = 4.0312e-01, PNorm = 75.2696, GNorm = 0.1730
Meta loss on this task batch = 4.6150e-01, Meta loss averaged over last 500 steps = 4.0330e-01, PNorm = 75.2821, GNorm = 0.2114
Meta loss on this task batch = 4.4868e-01, Meta loss averaged over last 500 steps = 4.0332e-01, PNorm = 75.2933, GNorm = 0.2113
Meta loss on this task batch = 3.4713e-01, Meta loss averaged over last 500 steps = 4.0315e-01, PNorm = 75.3052, GNorm = 0.1643
Meta loss on this task batch = 3.2351e-01, Meta loss averaged over last 500 steps = 4.0299e-01, PNorm = 75.3165, GNorm = 0.1757
Meta loss on this task batch = 4.0664e-01, Meta loss averaged over last 500 steps = 4.0312e-01, PNorm = 75.3264, GNorm = 0.2105
Meta loss on this task batch = 3.5022e-01, Meta loss averaged over last 500 steps = 4.0293e-01, PNorm = 75.3375, GNorm = 0.1698
Meta loss on this task batch = 4.3000e-01, Meta loss averaged over last 500 steps = 4.0299e-01, PNorm = 75.3489, GNorm = 0.1757
Meta loss on this task batch = 3.5965e-01, Meta loss averaged over last 500 steps = 4.0276e-01, PNorm = 75.3605, GNorm = 0.1523
Meta loss on this task batch = 4.1715e-01, Meta loss averaged over last 500 steps = 4.0279e-01, PNorm = 75.3721, GNorm = 0.1662
Meta loss on this task batch = 3.6267e-01, Meta loss averaged over last 500 steps = 4.0280e-01, PNorm = 75.3840, GNorm = 0.1989
Took 116.54593586921692 seconds to complete one epoch of meta training
Took 124.44767832756042 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493930
Epoch 110
Meta loss on this task batch = 3.6896e-01, Meta loss averaged over last 500 steps = 4.0277e-01, PNorm = 75.3954, GNorm = 0.1713
Meta loss on this task batch = 3.8342e-01, Meta loss averaged over last 500 steps = 4.0283e-01, PNorm = 75.4070, GNorm = 0.1760
Meta loss on this task batch = 3.8006e-01, Meta loss averaged over last 500 steps = 4.0270e-01, PNorm = 75.4182, GNorm = 0.1900
Meta loss on this task batch = 4.5982e-01, Meta loss averaged over last 500 steps = 4.0264e-01, PNorm = 75.4280, GNorm = 0.1903
Meta loss on this task batch = 4.3086e-01, Meta loss averaged over last 500 steps = 4.0273e-01, PNorm = 75.4373, GNorm = 0.1797
Meta loss on this task batch = 3.8637e-01, Meta loss averaged over last 500 steps = 4.0272e-01, PNorm = 75.4460, GNorm = 0.1664
Meta loss on this task batch = 3.4971e-01, Meta loss averaged over last 500 steps = 4.0262e-01, PNorm = 75.4558, GNorm = 0.1745
Meta loss on this task batch = 3.9196e-01, Meta loss averaged over last 500 steps = 4.0251e-01, PNorm = 75.4667, GNorm = 0.1532
Meta loss on this task batch = 3.4037e-01, Meta loss averaged over last 500 steps = 4.0246e-01, PNorm = 75.4793, GNorm = 0.1891
Meta loss on this task batch = 3.8426e-01, Meta loss averaged over last 500 steps = 4.0242e-01, PNorm = 75.4930, GNorm = 0.1708
Meta loss on this task batch = 4.5912e-01, Meta loss averaged over last 500 steps = 4.0265e-01, PNorm = 75.5067, GNorm = 0.1760
Meta loss on this task batch = 4.0024e-01, Meta loss averaged over last 500 steps = 4.0265e-01, PNorm = 75.5200, GNorm = 0.1617
Meta loss on this task batch = 3.3624e-01, Meta loss averaged over last 500 steps = 4.0249e-01, PNorm = 75.5339, GNorm = 0.2145
Meta loss on this task batch = 4.2340e-01, Meta loss averaged over last 500 steps = 4.0270e-01, PNorm = 75.5471, GNorm = 0.2047
Meta loss on this task batch = 3.6997e-01, Meta loss averaged over last 500 steps = 4.0258e-01, PNorm = 75.5575, GNorm = 0.1990
Meta loss on this task batch = 4.6630e-01, Meta loss averaged over last 500 steps = 4.0263e-01, PNorm = 75.5667, GNorm = 0.2067
Meta loss on this task batch = 3.9341e-01, Meta loss averaged over last 500 steps = 4.0253e-01, PNorm = 75.5750, GNorm = 0.1675
Meta loss on this task batch = 4.3350e-01, Meta loss averaged over last 500 steps = 4.0257e-01, PNorm = 75.5833, GNorm = 0.1828
Meta loss on this task batch = 3.7027e-01, Meta loss averaged over last 500 steps = 4.0239e-01, PNorm = 75.5912, GNorm = 0.1783
Took 114.39182996749878 seconds to complete one epoch of meta training
Took 122.6622908115387 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474808
Epoch 111
Meta loss on this task batch = 4.4354e-01, Meta loss averaged over last 500 steps = 4.0251e-01, PNorm = 75.5987, GNorm = 0.2138
Meta loss on this task batch = 3.3579e-01, Meta loss averaged over last 500 steps = 4.0233e-01, PNorm = 75.6085, GNorm = 0.1834
Meta loss on this task batch = 3.2389e-01, Meta loss averaged over last 500 steps = 4.0217e-01, PNorm = 75.6200, GNorm = 0.1742
Meta loss on this task batch = 3.6643e-01, Meta loss averaged over last 500 steps = 4.0204e-01, PNorm = 75.6322, GNorm = 0.1747
Meta loss on this task batch = 3.9823e-01, Meta loss averaged over last 500 steps = 4.0195e-01, PNorm = 75.6448, GNorm = 0.1762
Meta loss on this task batch = 3.7600e-01, Meta loss averaged over last 500 steps = 4.0182e-01, PNorm = 75.6571, GNorm = 0.1679
Meta loss on this task batch = 3.6981e-01, Meta loss averaged over last 500 steps = 4.0169e-01, PNorm = 75.6703, GNorm = 0.2183
Meta loss on this task batch = 3.5020e-01, Meta loss averaged over last 500 steps = 4.0157e-01, PNorm = 75.6836, GNorm = 0.2005
Meta loss on this task batch = 4.2388e-01, Meta loss averaged over last 500 steps = 4.0158e-01, PNorm = 75.6966, GNorm = 0.1938
Meta loss on this task batch = 4.0184e-01, Meta loss averaged over last 500 steps = 4.0165e-01, PNorm = 75.7100, GNorm = 0.1901
Meta loss on this task batch = 4.0008e-01, Meta loss averaged over last 500 steps = 4.0167e-01, PNorm = 75.7238, GNorm = 0.1870
Meta loss on this task batch = 3.8592e-01, Meta loss averaged over last 500 steps = 4.0166e-01, PNorm = 75.7377, GNorm = 0.1670
Meta loss on this task batch = 3.4343e-01, Meta loss averaged over last 500 steps = 4.0152e-01, PNorm = 75.7519, GNorm = 0.1604
Meta loss on this task batch = 4.0925e-01, Meta loss averaged over last 500 steps = 4.0156e-01, PNorm = 75.7652, GNorm = 0.1897
Meta loss on this task batch = 4.2814e-01, Meta loss averaged over last 500 steps = 4.0164e-01, PNorm = 75.7789, GNorm = 0.1547
Meta loss on this task batch = 4.0936e-01, Meta loss averaged over last 500 steps = 4.0161e-01, PNorm = 75.7920, GNorm = 0.1820
Meta loss on this task batch = 4.0914e-01, Meta loss averaged over last 500 steps = 4.0156e-01, PNorm = 75.8038, GNorm = 0.2068
Meta loss on this task batch = 4.5895e-01, Meta loss averaged over last 500 steps = 4.0161e-01, PNorm = 75.8136, GNorm = 0.2409
Meta loss on this task batch = 3.9095e-01, Meta loss averaged over last 500 steps = 4.0152e-01, PNorm = 75.8244, GNorm = 0.2709
Took 114.58616018295288 seconds to complete one epoch of meta training
Took 122.06506896018982 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484400
Epoch 112
Meta loss on this task batch = 3.8737e-01, Meta loss averaged over last 500 steps = 4.0144e-01, PNorm = 75.8369, GNorm = 0.1784
Meta loss on this task batch = 3.6670e-01, Meta loss averaged over last 500 steps = 4.0143e-01, PNorm = 75.8502, GNorm = 0.2092
Meta loss on this task batch = 4.1264e-01, Meta loss averaged over last 500 steps = 4.0148e-01, PNorm = 75.8644, GNorm = 0.1511
Meta loss on this task batch = 4.0808e-01, Meta loss averaged over last 500 steps = 4.0150e-01, PNorm = 75.8791, GNorm = 0.1670
Meta loss on this task batch = 3.6318e-01, Meta loss averaged over last 500 steps = 4.0139e-01, PNorm = 75.8948, GNorm = 0.1988
Meta loss on this task batch = 3.7907e-01, Meta loss averaged over last 500 steps = 4.0116e-01, PNorm = 75.9099, GNorm = 0.1602
Meta loss on this task batch = 4.1535e-01, Meta loss averaged over last 500 steps = 4.0125e-01, PNorm = 75.9242, GNorm = 0.1719
Meta loss on this task batch = 3.7557e-01, Meta loss averaged over last 500 steps = 4.0127e-01, PNorm = 75.9385, GNorm = 0.1863
Meta loss on this task batch = 4.1146e-01, Meta loss averaged over last 500 steps = 4.0133e-01, PNorm = 75.9527, GNorm = 0.2161
Meta loss on this task batch = 4.4700e-01, Meta loss averaged over last 500 steps = 4.0153e-01, PNorm = 75.9656, GNorm = 0.1922
Meta loss on this task batch = 3.8691e-01, Meta loss averaged over last 500 steps = 4.0149e-01, PNorm = 75.9780, GNorm = 0.2091
Meta loss on this task batch = 3.7519e-01, Meta loss averaged over last 500 steps = 4.0139e-01, PNorm = 75.9911, GNorm = 0.1877
Meta loss on this task batch = 3.8671e-01, Meta loss averaged over last 500 steps = 4.0128e-01, PNorm = 76.0048, GNorm = 0.2406
Meta loss on this task batch = 3.4564e-01, Meta loss averaged over last 500 steps = 4.0109e-01, PNorm = 76.0189, GNorm = 0.1873
Meta loss on this task batch = 4.0840e-01, Meta loss averaged over last 500 steps = 4.0100e-01, PNorm = 76.0319, GNorm = 0.1756
Meta loss on this task batch = 4.4614e-01, Meta loss averaged over last 500 steps = 4.0115e-01, PNorm = 76.0429, GNorm = 0.2176
Meta loss on this task batch = 4.1074e-01, Meta loss averaged over last 500 steps = 4.0127e-01, PNorm = 76.0510, GNorm = 0.2526
Meta loss on this task batch = 3.4464e-01, Meta loss averaged over last 500 steps = 4.0109e-01, PNorm = 76.0599, GNorm = 0.1723
Meta loss on this task batch = 3.8401e-01, Meta loss averaged over last 500 steps = 4.0101e-01, PNorm = 76.0695, GNorm = 0.1946
Took 115.85056185722351 seconds to complete one epoch of meta training
Took 123.8933572769165 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493278
Epoch 113
Meta loss on this task batch = 3.5454e-01, Meta loss averaged over last 500 steps = 4.0096e-01, PNorm = 76.0797, GNorm = 0.1599
Meta loss on this task batch = 4.5181e-01, Meta loss averaged over last 500 steps = 4.0091e-01, PNorm = 76.0907, GNorm = 0.1745
Meta loss on this task batch = 3.4611e-01, Meta loss averaged over last 500 steps = 4.0069e-01, PNorm = 76.1028, GNorm = 0.1493
Meta loss on this task batch = 3.8956e-01, Meta loss averaged over last 500 steps = 4.0071e-01, PNorm = 76.1155, GNorm = 0.1868
Meta loss on this task batch = 4.1488e-01, Meta loss averaged over last 500 steps = 4.0080e-01, PNorm = 76.1282, GNorm = 0.1696
Meta loss on this task batch = 4.4445e-01, Meta loss averaged over last 500 steps = 4.0082e-01, PNorm = 76.1395, GNorm = 0.1966
Meta loss on this task batch = 3.8000e-01, Meta loss averaged over last 500 steps = 4.0080e-01, PNorm = 76.1507, GNorm = 0.2210
Meta loss on this task batch = 4.2121e-01, Meta loss averaged over last 500 steps = 4.0089e-01, PNorm = 76.1617, GNorm = 0.2087
Meta loss on this task batch = 4.1012e-01, Meta loss averaged over last 500 steps = 4.0082e-01, PNorm = 76.1733, GNorm = 0.1826
Meta loss on this task batch = 4.6094e-01, Meta loss averaged over last 500 steps = 4.0088e-01, PNorm = 76.1826, GNorm = 0.2220
Meta loss on this task batch = 4.6197e-01, Meta loss averaged over last 500 steps = 4.0103e-01, PNorm = 76.1924, GNorm = 0.1659
Meta loss on this task batch = 3.2670e-01, Meta loss averaged over last 500 steps = 4.0076e-01, PNorm = 76.2042, GNorm = 0.1755
Meta loss on this task batch = 4.1593e-01, Meta loss averaged over last 500 steps = 4.0079e-01, PNorm = 76.2167, GNorm = 0.2168
Meta loss on this task batch = 3.6665e-01, Meta loss averaged over last 500 steps = 4.0063e-01, PNorm = 76.2286, GNorm = 0.1587
Meta loss on this task batch = 3.8633e-01, Meta loss averaged over last 500 steps = 4.0060e-01, PNorm = 76.2412, GNorm = 0.1722
Meta loss on this task batch = 3.1973e-01, Meta loss averaged over last 500 steps = 4.0033e-01, PNorm = 76.2549, GNorm = 0.1873
Meta loss on this task batch = 4.0020e-01, Meta loss averaged over last 500 steps = 4.0038e-01, PNorm = 76.2686, GNorm = 0.1918
Meta loss on this task batch = 4.1280e-01, Meta loss averaged over last 500 steps = 4.0045e-01, PNorm = 76.2816, GNorm = 0.1565
Meta loss on this task batch = 3.1407e-01, Meta loss averaged over last 500 steps = 4.0021e-01, PNorm = 76.2937, GNorm = 0.2004
Took 115.97227382659912 seconds to complete one epoch of meta training
Took 123.71730089187622 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476616
Epoch 114
Meta loss on this task batch = 3.4155e-01, Meta loss averaged over last 500 steps = 4.0008e-01, PNorm = 76.3062, GNorm = 0.1710
Meta loss on this task batch = 4.2098e-01, Meta loss averaged over last 500 steps = 4.0006e-01, PNorm = 76.3184, GNorm = 0.2439
Meta loss on this task batch = 4.0522e-01, Meta loss averaged over last 500 steps = 4.0007e-01, PNorm = 76.3298, GNorm = 0.1986
Meta loss on this task batch = 3.4678e-01, Meta loss averaged over last 500 steps = 3.9999e-01, PNorm = 76.3420, GNorm = 0.1693
Meta loss on this task batch = 3.8250e-01, Meta loss averaged over last 500 steps = 3.9996e-01, PNorm = 76.3539, GNorm = 0.1960
Meta loss on this task batch = 4.3172e-01, Meta loss averaged over last 500 steps = 3.9997e-01, PNorm = 76.3651, GNorm = 0.2019
Meta loss on this task batch = 3.5106e-01, Meta loss averaged over last 500 steps = 3.9987e-01, PNorm = 76.3761, GNorm = 0.2323
Meta loss on this task batch = 3.2730e-01, Meta loss averaged over last 500 steps = 3.9976e-01, PNorm = 76.3877, GNorm = 0.1523
Meta loss on this task batch = 4.2392e-01, Meta loss averaged over last 500 steps = 3.9991e-01, PNorm = 76.3994, GNorm = 0.2035
Meta loss on this task batch = 3.7509e-01, Meta loss averaged over last 500 steps = 3.9980e-01, PNorm = 76.4115, GNorm = 0.1835
Meta loss on this task batch = 4.2946e-01, Meta loss averaged over last 500 steps = 3.9995e-01, PNorm = 76.4239, GNorm = 0.1860
Meta loss on this task batch = 3.5847e-01, Meta loss averaged over last 500 steps = 3.9985e-01, PNorm = 76.4370, GNorm = 0.1668
Meta loss on this task batch = 4.2863e-01, Meta loss averaged over last 500 steps = 3.9982e-01, PNorm = 76.4495, GNorm = 0.1992
Meta loss on this task batch = 3.9647e-01, Meta loss averaged over last 500 steps = 3.9982e-01, PNorm = 76.4623, GNorm = 0.1688
Meta loss on this task batch = 4.0605e-01, Meta loss averaged over last 500 steps = 3.9975e-01, PNorm = 76.4770, GNorm = 0.1905
Meta loss on this task batch = 4.1905e-01, Meta loss averaged over last 500 steps = 3.9988e-01, PNorm = 76.4913, GNorm = 0.2022
Meta loss on this task batch = 4.1470e-01, Meta loss averaged over last 500 steps = 3.9989e-01, PNorm = 76.5043, GNorm = 0.2341
Meta loss on this task batch = 3.3927e-01, Meta loss averaged over last 500 steps = 3.9966e-01, PNorm = 76.5179, GNorm = 0.1778
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 3.9950e-01, PNorm = 76.5302, GNorm = 0.1719
Took 114.91427254676819 seconds to complete one epoch of meta training
Took 122.94909930229187 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493287
Epoch 115
Meta loss on this task batch = 3.7315e-01, Meta loss averaged over last 500 steps = 3.9939e-01, PNorm = 76.5412, GNorm = 0.1948
Meta loss on this task batch = 3.9996e-01, Meta loss averaged over last 500 steps = 3.9948e-01, PNorm = 76.5516, GNorm = 0.1860
Meta loss on this task batch = 4.0396e-01, Meta loss averaged over last 500 steps = 3.9931e-01, PNorm = 76.5614, GNorm = 0.1784
Meta loss on this task batch = 3.8802e-01, Meta loss averaged over last 500 steps = 3.9914e-01, PNorm = 76.5715, GNorm = 0.1816
Meta loss on this task batch = 3.9834e-01, Meta loss averaged over last 500 steps = 3.9908e-01, PNorm = 76.5826, GNorm = 0.1539
Meta loss on this task batch = 4.0128e-01, Meta loss averaged over last 500 steps = 3.9908e-01, PNorm = 76.5947, GNorm = 0.2109
Meta loss on this task batch = 4.1078e-01, Meta loss averaged over last 500 steps = 3.9891e-01, PNorm = 76.6071, GNorm = 0.1780
Meta loss on this task batch = 3.7011e-01, Meta loss averaged over last 500 steps = 3.9891e-01, PNorm = 76.6204, GNorm = 0.1932
Meta loss on this task batch = 4.0601e-01, Meta loss averaged over last 500 steps = 3.9897e-01, PNorm = 76.6335, GNorm = 0.1820
Meta loss on this task batch = 4.6089e-01, Meta loss averaged over last 500 steps = 3.9910e-01, PNorm = 76.6462, GNorm = 0.2196
Meta loss on this task batch = 3.7335e-01, Meta loss averaged over last 500 steps = 3.9905e-01, PNorm = 76.6590, GNorm = 0.2138
Meta loss on this task batch = 3.0982e-01, Meta loss averaged over last 500 steps = 3.9895e-01, PNorm = 76.6717, GNorm = 0.1460
Meta loss on this task batch = 3.4039e-01, Meta loss averaged over last 500 steps = 3.9874e-01, PNorm = 76.6840, GNorm = 0.2052
Meta loss on this task batch = 3.6169e-01, Meta loss averaged over last 500 steps = 3.9866e-01, PNorm = 76.6965, GNorm = 0.2052
Meta loss on this task batch = 3.9748e-01, Meta loss averaged over last 500 steps = 3.9880e-01, PNorm = 76.7076, GNorm = 0.2111
Meta loss on this task batch = 4.1138e-01, Meta loss averaged over last 500 steps = 3.9887e-01, PNorm = 76.7175, GNorm = 0.2587
Meta loss on this task batch = 4.2434e-01, Meta loss averaged over last 500 steps = 3.9888e-01, PNorm = 76.7271, GNorm = 0.1895
Meta loss on this task batch = 3.5958e-01, Meta loss averaged over last 500 steps = 3.9870e-01, PNorm = 76.7380, GNorm = 0.1785
Meta loss on this task batch = 4.3178e-01, Meta loss averaged over last 500 steps = 3.9882e-01, PNorm = 76.7484, GNorm = 0.2091
Took 114.69808793067932 seconds to complete one epoch of meta training
Took 122.4728147983551 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484270
Epoch 116
Meta loss on this task batch = 4.1633e-01, Meta loss averaged over last 500 steps = 3.9881e-01, PNorm = 76.7608, GNorm = 0.1836
Meta loss on this task batch = 3.4879e-01, Meta loss averaged over last 500 steps = 3.9868e-01, PNorm = 76.7734, GNorm = 0.1857
Meta loss on this task batch = 4.2662e-01, Meta loss averaged over last 500 steps = 3.9865e-01, PNorm = 76.7839, GNorm = 0.1950
Meta loss on this task batch = 3.8601e-01, Meta loss averaged over last 500 steps = 3.9850e-01, PNorm = 76.7949, GNorm = 0.2124
Meta loss on this task batch = 4.5323e-01, Meta loss averaged over last 500 steps = 3.9852e-01, PNorm = 76.8050, GNorm = 0.1985
Meta loss on this task batch = 3.7943e-01, Meta loss averaged over last 500 steps = 3.9853e-01, PNorm = 76.8160, GNorm = 0.1789
Meta loss on this task batch = 3.7646e-01, Meta loss averaged over last 500 steps = 3.9842e-01, PNorm = 76.8272, GNorm = 0.2027
Meta loss on this task batch = 3.6383e-01, Meta loss averaged over last 500 steps = 3.9833e-01, PNorm = 76.8393, GNorm = 0.1626
Meta loss on this task batch = 4.3172e-01, Meta loss averaged over last 500 steps = 3.9836e-01, PNorm = 76.8514, GNorm = 0.1727
Meta loss on this task batch = 3.5045e-01, Meta loss averaged over last 500 steps = 3.9835e-01, PNorm = 76.8636, GNorm = 0.1704
Meta loss on this task batch = 3.8575e-01, Meta loss averaged over last 500 steps = 3.9827e-01, PNorm = 76.8760, GNorm = 0.1945
Meta loss on this task batch = 3.7607e-01, Meta loss averaged over last 500 steps = 3.9818e-01, PNorm = 76.8889, GNorm = 0.1810
Meta loss on this task batch = 3.8032e-01, Meta loss averaged over last 500 steps = 3.9812e-01, PNorm = 76.9019, GNorm = 0.1551
Meta loss on this task batch = 3.7296e-01, Meta loss averaged over last 500 steps = 3.9798e-01, PNorm = 76.9153, GNorm = 0.1788
Meta loss on this task batch = 4.0122e-01, Meta loss averaged over last 500 steps = 3.9810e-01, PNorm = 76.9270, GNorm = 0.1911
Meta loss on this task batch = 3.9489e-01, Meta loss averaged over last 500 steps = 3.9811e-01, PNorm = 76.9384, GNorm = 0.2073
Meta loss on this task batch = 3.4501e-01, Meta loss averaged over last 500 steps = 3.9796e-01, PNorm = 76.9494, GNorm = 0.1765
Meta loss on this task batch = 3.4509e-01, Meta loss averaged over last 500 steps = 3.9774e-01, PNorm = 76.9603, GNorm = 0.1712
Meta loss on this task batch = 4.3170e-01, Meta loss averaged over last 500 steps = 3.9783e-01, PNorm = 76.9719, GNorm = 0.2184
Took 192.44254207611084 seconds to complete one epoch of meta training
Took 199.76900386810303 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478196
Epoch 117
Meta loss on this task batch = 4.6838e-01, Meta loss averaged over last 500 steps = 3.9798e-01, PNorm = 76.9821, GNorm = 0.2424
Meta loss on this task batch = 3.9042e-01, Meta loss averaged over last 500 steps = 3.9799e-01, PNorm = 76.9937, GNorm = 0.1866
Meta loss on this task batch = 4.3650e-01, Meta loss averaged over last 500 steps = 3.9788e-01, PNorm = 77.0052, GNorm = 0.1780
Meta loss on this task batch = 4.0393e-01, Meta loss averaged over last 500 steps = 3.9786e-01, PNorm = 77.0164, GNorm = 0.1790
Meta loss on this task batch = 3.9206e-01, Meta loss averaged over last 500 steps = 3.9792e-01, PNorm = 77.0276, GNorm = 0.1913
Meta loss on this task batch = 3.8379e-01, Meta loss averaged over last 500 steps = 3.9793e-01, PNorm = 77.0392, GNorm = 0.1726
Meta loss on this task batch = 3.6861e-01, Meta loss averaged over last 500 steps = 3.9782e-01, PNorm = 77.0513, GNorm = 0.2234
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.9780e-01, PNorm = 77.0638, GNorm = 0.1626
Meta loss on this task batch = 3.8253e-01, Meta loss averaged over last 500 steps = 3.9783e-01, PNorm = 77.0768, GNorm = 0.2180
Meta loss on this task batch = 4.0658e-01, Meta loss averaged over last 500 steps = 3.9772e-01, PNorm = 77.0906, GNorm = 0.1988
Meta loss on this task batch = 3.5328e-01, Meta loss averaged over last 500 steps = 3.9772e-01, PNorm = 77.1043, GNorm = 0.1925
Meta loss on this task batch = 3.3683e-01, Meta loss averaged over last 500 steps = 3.9754e-01, PNorm = 77.1174, GNorm = 0.1730
Meta loss on this task batch = 3.4274e-01, Meta loss averaged over last 500 steps = 3.9740e-01, PNorm = 77.1304, GNorm = 0.1670
Meta loss on this task batch = 3.9411e-01, Meta loss averaged over last 500 steps = 3.9738e-01, PNorm = 77.1428, GNorm = 0.2023
Meta loss on this task batch = 3.8916e-01, Meta loss averaged over last 500 steps = 3.9733e-01, PNorm = 77.1539, GNorm = 0.2085
Meta loss on this task batch = 5.1198e-01, Meta loss averaged over last 500 steps = 3.9749e-01, PNorm = 77.1615, GNorm = 0.2753
Meta loss on this task batch = 3.9261e-01, Meta loss averaged over last 500 steps = 3.9741e-01, PNorm = 77.1702, GNorm = 0.1964
Meta loss on this task batch = 3.8665e-01, Meta loss averaged over last 500 steps = 3.9733e-01, PNorm = 77.1786, GNorm = 0.1922
Meta loss on this task batch = 3.6653e-01, Meta loss averaged over last 500 steps = 3.9730e-01, PNorm = 77.1883, GNorm = 0.1801
Took 271.48403000831604 seconds to complete one epoch of meta training
Took 279.05143785476685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487688
Epoch 118
Meta loss on this task batch = 3.6944e-01, Meta loss averaged over last 500 steps = 3.9698e-01, PNorm = 77.1986, GNorm = 0.1756
Meta loss on this task batch = 3.3297e-01, Meta loss averaged over last 500 steps = 3.9688e-01, PNorm = 77.2106, GNorm = 0.1776
Meta loss on this task batch = 3.4385e-01, Meta loss averaged over last 500 steps = 3.9685e-01, PNorm = 77.2237, GNorm = 0.1598
Meta loss on this task batch = 4.2486e-01, Meta loss averaged over last 500 steps = 3.9692e-01, PNorm = 77.2367, GNorm = 0.1646
Meta loss on this task batch = 3.9329e-01, Meta loss averaged over last 500 steps = 3.9694e-01, PNorm = 77.2503, GNorm = 0.2070
Meta loss on this task batch = 4.1507e-01, Meta loss averaged over last 500 steps = 3.9695e-01, PNorm = 77.2636, GNorm = 0.1981
Meta loss on this task batch = 3.2648e-01, Meta loss averaged over last 500 steps = 3.9685e-01, PNorm = 77.2769, GNorm = 0.1632
Meta loss on this task batch = 3.2874e-01, Meta loss averaged over last 500 steps = 3.9681e-01, PNorm = 77.2899, GNorm = 0.1804
Meta loss on this task batch = 4.9930e-01, Meta loss averaged over last 500 steps = 3.9696e-01, PNorm = 77.2996, GNorm = 0.2632
Meta loss on this task batch = 3.3618e-01, Meta loss averaged over last 500 steps = 3.9675e-01, PNorm = 77.3092, GNorm = 0.1704
Meta loss on this task batch = 4.1434e-01, Meta loss averaged over last 500 steps = 3.9668e-01, PNorm = 77.3169, GNorm = 0.2286
Meta loss on this task batch = 4.1485e-01, Meta loss averaged over last 500 steps = 3.9670e-01, PNorm = 77.3251, GNorm = 0.2153
Meta loss on this task batch = 4.2170e-01, Meta loss averaged over last 500 steps = 3.9671e-01, PNorm = 77.3338, GNorm = 0.2022
Meta loss on this task batch = 4.3152e-01, Meta loss averaged over last 500 steps = 3.9679e-01, PNorm = 77.3448, GNorm = 0.2260
Meta loss on this task batch = 3.8360e-01, Meta loss averaged over last 500 steps = 3.9675e-01, PNorm = 77.3567, GNorm = 0.1874
Meta loss on this task batch = 3.5693e-01, Meta loss averaged over last 500 steps = 3.9658e-01, PNorm = 77.3694, GNorm = 0.1807
Meta loss on this task batch = 3.7626e-01, Meta loss averaged over last 500 steps = 3.9659e-01, PNorm = 77.3834, GNorm = 0.1963
Meta loss on this task batch = 3.9938e-01, Meta loss averaged over last 500 steps = 3.9662e-01, PNorm = 77.3983, GNorm = 0.2101
Meta loss on this task batch = 4.2745e-01, Meta loss averaged over last 500 steps = 3.9648e-01, PNorm = 77.4117, GNorm = 0.1997
Took 113.05775332450867 seconds to complete one epoch of meta training
Took 121.1616461277008 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490437
Epoch 119
Meta loss on this task batch = 3.9411e-01, Meta loss averaged over last 500 steps = 3.9644e-01, PNorm = 77.4242, GNorm = 0.2066
Meta loss on this task batch = 3.6907e-01, Meta loss averaged over last 500 steps = 3.9628e-01, PNorm = 77.4356, GNorm = 0.2059
Meta loss on this task batch = 3.5448e-01, Meta loss averaged over last 500 steps = 3.9626e-01, PNorm = 77.4480, GNorm = 0.1984
Meta loss on this task batch = 3.9711e-01, Meta loss averaged over last 500 steps = 3.9620e-01, PNorm = 77.4596, GNorm = 0.1910
Meta loss on this task batch = 3.8233e-01, Meta loss averaged over last 500 steps = 3.9618e-01, PNorm = 77.4713, GNorm = 0.1864
Meta loss on this task batch = 4.5565e-01, Meta loss averaged over last 500 steps = 3.9637e-01, PNorm = 77.4823, GNorm = 0.2202
Meta loss on this task batch = 3.9715e-01, Meta loss averaged over last 500 steps = 3.9656e-01, PNorm = 77.4931, GNorm = 0.2077
Meta loss on this task batch = 3.8133e-01, Meta loss averaged over last 500 steps = 3.9650e-01, PNorm = 77.5044, GNorm = 0.1661
Meta loss on this task batch = 3.9253e-01, Meta loss averaged over last 500 steps = 3.9644e-01, PNorm = 77.5162, GNorm = 0.1729
Meta loss on this task batch = 3.7838e-01, Meta loss averaged over last 500 steps = 3.9637e-01, PNorm = 77.5286, GNorm = 0.1573
Meta loss on this task batch = 3.5989e-01, Meta loss averaged over last 500 steps = 3.9627e-01, PNorm = 77.5421, GNorm = 0.1618
Meta loss on this task batch = 4.2157e-01, Meta loss averaged over last 500 steps = 3.9633e-01, PNorm = 77.5559, GNorm = 0.1699
Meta loss on this task batch = 4.4316e-01, Meta loss averaged over last 500 steps = 3.9646e-01, PNorm = 77.5696, GNorm = 0.1777
Meta loss on this task batch = 3.3092e-01, Meta loss averaged over last 500 steps = 3.9609e-01, PNorm = 77.5834, GNorm = 0.1502
Meta loss on this task batch = 4.4218e-01, Meta loss averaged over last 500 steps = 3.9617e-01, PNorm = 77.5963, GNorm = 0.2070
Meta loss on this task batch = 3.3208e-01, Meta loss averaged over last 500 steps = 3.9601e-01, PNorm = 77.6087, GNorm = 0.1590
Meta loss on this task batch = 3.6089e-01, Meta loss averaged over last 500 steps = 3.9599e-01, PNorm = 77.6213, GNorm = 0.1928
Meta loss on this task batch = 3.3100e-01, Meta loss averaged over last 500 steps = 3.9577e-01, PNorm = 77.6333, GNorm = 0.1837
Meta loss on this task batch = 3.8824e-01, Meta loss averaged over last 500 steps = 3.9570e-01, PNorm = 77.6459, GNorm = 0.2206
Took 115.04469561576843 seconds to complete one epoch of meta training
Took 122.74683260917664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489874
Epoch 120
Meta loss on this task batch = 4.0177e-01, Meta loss averaged over last 500 steps = 3.9567e-01, PNorm = 77.6585, GNorm = 0.2013
Meta loss on this task batch = 3.9899e-01, Meta loss averaged over last 500 steps = 3.9565e-01, PNorm = 77.6709, GNorm = 0.2019
Meta loss on this task batch = 3.5677e-01, Meta loss averaged over last 500 steps = 3.9539e-01, PNorm = 77.6828, GNorm = 0.1706
Meta loss on this task batch = 3.3425e-01, Meta loss averaged over last 500 steps = 3.9532e-01, PNorm = 77.6946, GNorm = 0.1601
Meta loss on this task batch = 3.6969e-01, Meta loss averaged over last 500 steps = 3.9527e-01, PNorm = 77.7069, GNorm = 0.1895
Meta loss on this task batch = 3.6387e-01, Meta loss averaged over last 500 steps = 3.9525e-01, PNorm = 77.7187, GNorm = 0.1944
Meta loss on this task batch = 3.8921e-01, Meta loss averaged over last 500 steps = 3.9515e-01, PNorm = 77.7298, GNorm = 0.1698
Meta loss on this task batch = 4.3517e-01, Meta loss averaged over last 500 steps = 3.9520e-01, PNorm = 77.7405, GNorm = 0.2074
Meta loss on this task batch = 4.3768e-01, Meta loss averaged over last 500 steps = 3.9524e-01, PNorm = 77.7503, GNorm = 0.1732
Meta loss on this task batch = 4.2428e-01, Meta loss averaged over last 500 steps = 3.9538e-01, PNorm = 77.7608, GNorm = 0.1601
Meta loss on this task batch = 4.0779e-01, Meta loss averaged over last 500 steps = 3.9547e-01, PNorm = 77.7712, GNorm = 0.1918
Meta loss on this task batch = 3.2805e-01, Meta loss averaged over last 500 steps = 3.9518e-01, PNorm = 77.7819, GNorm = 0.1896
Meta loss on this task batch = 3.4812e-01, Meta loss averaged over last 500 steps = 3.9503e-01, PNorm = 77.7928, GNorm = 0.1616
Meta loss on this task batch = 4.0019e-01, Meta loss averaged over last 500 steps = 3.9512e-01, PNorm = 77.8034, GNorm = 0.1629
Meta loss on this task batch = 3.6877e-01, Meta loss averaged over last 500 steps = 3.9497e-01, PNorm = 77.8152, GNorm = 0.1629
Meta loss on this task batch = 4.2614e-01, Meta loss averaged over last 500 steps = 3.9495e-01, PNorm = 77.8270, GNorm = 0.2060
Meta loss on this task batch = 3.7516e-01, Meta loss averaged over last 500 steps = 3.9485e-01, PNorm = 77.8393, GNorm = 0.2211
Meta loss on this task batch = 4.4136e-01, Meta loss averaged over last 500 steps = 3.9484e-01, PNorm = 77.8521, GNorm = 0.1849
Meta loss on this task batch = 3.6998e-01, Meta loss averaged over last 500 steps = 3.9478e-01, PNorm = 77.8652, GNorm = 0.2062
Took 114.00667905807495 seconds to complete one epoch of meta training
Took 120.71579766273499 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495170
Epoch 121
Meta loss on this task batch = 3.5892e-01, Meta loss averaged over last 500 steps = 3.9475e-01, PNorm = 77.8782, GNorm = 0.1531
Meta loss on this task batch = 4.1845e-01, Meta loss averaged over last 500 steps = 3.9473e-01, PNorm = 77.8909, GNorm = 0.2335
Meta loss on this task batch = 4.1535e-01, Meta loss averaged over last 500 steps = 3.9476e-01, PNorm = 77.9033, GNorm = 0.1989
Meta loss on this task batch = 3.7672e-01, Meta loss averaged over last 500 steps = 3.9471e-01, PNorm = 77.9158, GNorm = 0.2120
Meta loss on this task batch = 4.1194e-01, Meta loss averaged over last 500 steps = 3.9479e-01, PNorm = 77.9271, GNorm = 0.2147
Meta loss on this task batch = 3.6434e-01, Meta loss averaged over last 500 steps = 3.9478e-01, PNorm = 77.9397, GNorm = 0.1772
Meta loss on this task batch = 2.9806e-01, Meta loss averaged over last 500 steps = 3.9457e-01, PNorm = 77.9525, GNorm = 0.1744
Meta loss on this task batch = 3.3533e-01, Meta loss averaged over last 500 steps = 3.9453e-01, PNorm = 77.9659, GNorm = 0.1858
Meta loss on this task batch = 3.6727e-01, Meta loss averaged over last 500 steps = 3.9462e-01, PNorm = 77.9785, GNorm = 0.2015
Meta loss on this task batch = 4.0123e-01, Meta loss averaged over last 500 steps = 3.9450e-01, PNorm = 77.9897, GNorm = 0.2332
Meta loss on this task batch = 3.8133e-01, Meta loss averaged over last 500 steps = 3.9434e-01, PNorm = 77.9988, GNorm = 0.2514
Meta loss on this task batch = 4.4053e-01, Meta loss averaged over last 500 steps = 3.9445e-01, PNorm = 78.0074, GNorm = 0.2303
Meta loss on this task batch = 4.2314e-01, Meta loss averaged over last 500 steps = 3.9443e-01, PNorm = 78.0159, GNorm = 0.2313
Meta loss on this task batch = 4.2144e-01, Meta loss averaged over last 500 steps = 3.9445e-01, PNorm = 78.0241, GNorm = 0.1929
Meta loss on this task batch = 3.6135e-01, Meta loss averaged over last 500 steps = 3.9432e-01, PNorm = 78.0338, GNorm = 0.1837
Meta loss on this task batch = 4.4191e-01, Meta loss averaged over last 500 steps = 3.9433e-01, PNorm = 78.0433, GNorm = 0.2043
Meta loss on this task batch = 4.1887e-01, Meta loss averaged over last 500 steps = 3.9430e-01, PNorm = 78.0526, GNorm = 0.1979
Meta loss on this task batch = 2.9958e-01, Meta loss averaged over last 500 steps = 3.9413e-01, PNorm = 78.0630, GNorm = 0.1386
Meta loss on this task batch = 3.8206e-01, Meta loss averaged over last 500 steps = 3.9411e-01, PNorm = 78.0740, GNorm = 0.1889
Took 113.22056579589844 seconds to complete one epoch of meta training
Took 120.96989750862122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477437
Epoch 122
Meta loss on this task batch = 3.9731e-01, Meta loss averaged over last 500 steps = 3.9403e-01, PNorm = 78.0857, GNorm = 0.1990
Meta loss on this task batch = 4.3053e-01, Meta loss averaged over last 500 steps = 3.9414e-01, PNorm = 78.0976, GNorm = 0.2187
Meta loss on this task batch = 3.6123e-01, Meta loss averaged over last 500 steps = 3.9414e-01, PNorm = 78.1103, GNorm = 0.1647
Meta loss on this task batch = 3.6617e-01, Meta loss averaged over last 500 steps = 3.9413e-01, PNorm = 78.1242, GNorm = 0.1638
Meta loss on this task batch = 3.5287e-01, Meta loss averaged over last 500 steps = 3.9401e-01, PNorm = 78.1389, GNorm = 0.1671
Meta loss on this task batch = 4.0723e-01, Meta loss averaged over last 500 steps = 3.9404e-01, PNorm = 78.1524, GNorm = 0.1778
Meta loss on this task batch = 4.1291e-01, Meta loss averaged over last 500 steps = 3.9397e-01, PNorm = 78.1660, GNorm = 0.2112
Meta loss on this task batch = 4.0227e-01, Meta loss averaged over last 500 steps = 3.9402e-01, PNorm = 78.1770, GNorm = 0.2125
Meta loss on this task batch = 3.8364e-01, Meta loss averaged over last 500 steps = 3.9402e-01, PNorm = 78.1874, GNorm = 0.1846
Meta loss on this task batch = 3.9631e-01, Meta loss averaged over last 500 steps = 3.9407e-01, PNorm = 78.1963, GNorm = 0.2077
Meta loss on this task batch = 3.7176e-01, Meta loss averaged over last 500 steps = 3.9399e-01, PNorm = 78.2056, GNorm = 0.1798
Meta loss on this task batch = 3.4739e-01, Meta loss averaged over last 500 steps = 3.9376e-01, PNorm = 78.2162, GNorm = 0.2058
Meta loss on this task batch = 4.1295e-01, Meta loss averaged over last 500 steps = 3.9375e-01, PNorm = 78.2266, GNorm = 0.2317
Meta loss on this task batch = 3.2369e-01, Meta loss averaged over last 500 steps = 3.9358e-01, PNorm = 78.2386, GNorm = 0.1659
Meta loss on this task batch = 4.4188e-01, Meta loss averaged over last 500 steps = 3.9377e-01, PNorm = 78.2494, GNorm = 0.2000
Meta loss on this task batch = 3.5569e-01, Meta loss averaged over last 500 steps = 3.9367e-01, PNorm = 78.2617, GNorm = 0.1843
Meta loss on this task batch = 4.1439e-01, Meta loss averaged over last 500 steps = 3.9378e-01, PNorm = 78.2740, GNorm = 0.1860
Meta loss on this task batch = 3.5594e-01, Meta loss averaged over last 500 steps = 3.9368e-01, PNorm = 78.2863, GNorm = 0.1426
Meta loss on this task batch = 4.3604e-01, Meta loss averaged over last 500 steps = 3.9375e-01, PNorm = 78.2978, GNorm = 0.1991
Took 116.45713019371033 seconds to complete one epoch of meta training
Took 124.10077595710754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470107
Epoch 123
Meta loss on this task batch = 4.1703e-01, Meta loss averaged over last 500 steps = 3.9394e-01, PNorm = 78.3090, GNorm = 0.2201
Meta loss on this task batch = 3.2404e-01, Meta loss averaged over last 500 steps = 3.9365e-01, PNorm = 78.3203, GNorm = 0.1661
Meta loss on this task batch = 4.1536e-01, Meta loss averaged over last 500 steps = 3.9370e-01, PNorm = 78.3314, GNorm = 0.1921
Meta loss on this task batch = 4.1744e-01, Meta loss averaged over last 500 steps = 3.9373e-01, PNorm = 78.3424, GNorm = 0.1829
Meta loss on this task batch = 3.5141e-01, Meta loss averaged over last 500 steps = 3.9353e-01, PNorm = 78.3533, GNorm = 0.1943
Meta loss on this task batch = 4.0287e-01, Meta loss averaged over last 500 steps = 3.9342e-01, PNorm = 78.3644, GNorm = 0.2015
Meta loss on this task batch = 3.9656e-01, Meta loss averaged over last 500 steps = 3.9360e-01, PNorm = 78.3772, GNorm = 0.1939
Meta loss on this task batch = 3.6868e-01, Meta loss averaged over last 500 steps = 3.9359e-01, PNorm = 78.3899, GNorm = 0.1570
Meta loss on this task batch = 4.1455e-01, Meta loss averaged over last 500 steps = 3.9365e-01, PNorm = 78.4024, GNorm = 0.2060
Meta loss on this task batch = 3.8481e-01, Meta loss averaged over last 500 steps = 3.9349e-01, PNorm = 78.4145, GNorm = 0.1912
Meta loss on this task batch = 2.9569e-01, Meta loss averaged over last 500 steps = 3.9329e-01, PNorm = 78.4282, GNorm = 0.1677
Meta loss on this task batch = 3.7838e-01, Meta loss averaged over last 500 steps = 3.9328e-01, PNorm = 78.4413, GNorm = 0.1918
Meta loss on this task batch = 3.2610e-01, Meta loss averaged over last 500 steps = 3.9299e-01, PNorm = 78.4549, GNorm = 0.1664
Meta loss on this task batch = 4.4672e-01, Meta loss averaged over last 500 steps = 3.9301e-01, PNorm = 78.4661, GNorm = 0.2373
Meta loss on this task batch = 4.3250e-01, Meta loss averaged over last 500 steps = 3.9311e-01, PNorm = 78.4764, GNorm = 0.2041
Meta loss on this task batch = 3.8868e-01, Meta loss averaged over last 500 steps = 3.9309e-01, PNorm = 78.4877, GNorm = 0.1925
Meta loss on this task batch = 3.6299e-01, Meta loss averaged over last 500 steps = 3.9303e-01, PNorm = 78.4999, GNorm = 0.1757
Meta loss on this task batch = 4.2552e-01, Meta loss averaged over last 500 steps = 3.9301e-01, PNorm = 78.5118, GNorm = 0.1715
Meta loss on this task batch = 4.1699e-01, Meta loss averaged over last 500 steps = 3.9304e-01, PNorm = 78.5212, GNorm = 0.2430
Took 118.21031618118286 seconds to complete one epoch of meta training
Took 126.0485999584198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473405
Epoch 124
Meta loss on this task batch = 3.8087e-01, Meta loss averaged over last 500 steps = 3.9290e-01, PNorm = 78.5308, GNorm = 0.1757
Meta loss on this task batch = 3.5992e-01, Meta loss averaged over last 500 steps = 3.9293e-01, PNorm = 78.5410, GNorm = 0.1804
Meta loss on this task batch = 3.6696e-01, Meta loss averaged over last 500 steps = 3.9275e-01, PNorm = 78.5516, GNorm = 0.1715
Meta loss on this task batch = 4.2316e-01, Meta loss averaged over last 500 steps = 3.9280e-01, PNorm = 78.5612, GNorm = 0.1957
Meta loss on this task batch = 3.9256e-01, Meta loss averaged over last 500 steps = 3.9285e-01, PNorm = 78.5709, GNorm = 0.2162
Meta loss on this task batch = 4.1091e-01, Meta loss averaged over last 500 steps = 3.9286e-01, PNorm = 78.5810, GNorm = 0.1918
Meta loss on this task batch = 3.5821e-01, Meta loss averaged over last 500 steps = 3.9285e-01, PNorm = 78.5921, GNorm = 0.1657
Meta loss on this task batch = 3.7810e-01, Meta loss averaged over last 500 steps = 3.9294e-01, PNorm = 78.6036, GNorm = 0.1768
Meta loss on this task batch = 3.2097e-01, Meta loss averaged over last 500 steps = 3.9299e-01, PNorm = 78.6156, GNorm = 0.1997
Meta loss on this task batch = 4.2572e-01, Meta loss averaged over last 500 steps = 3.9305e-01, PNorm = 78.6280, GNorm = 0.1970
Meta loss on this task batch = 3.4460e-01, Meta loss averaged over last 500 steps = 3.9285e-01, PNorm = 78.6400, GNorm = 0.1891
Meta loss on this task batch = 3.3856e-01, Meta loss averaged over last 500 steps = 3.9273e-01, PNorm = 78.6524, GNorm = 0.1777
Meta loss on this task batch = 4.5375e-01, Meta loss averaged over last 500 steps = 3.9277e-01, PNorm = 78.6632, GNorm = 0.2481
Meta loss on this task batch = 4.3394e-01, Meta loss averaged over last 500 steps = 3.9274e-01, PNorm = 78.6713, GNorm = 0.2490
Meta loss on this task batch = 3.7506e-01, Meta loss averaged over last 500 steps = 3.9271e-01, PNorm = 78.6790, GNorm = 0.2295
Meta loss on this task batch = 3.9850e-01, Meta loss averaged over last 500 steps = 3.9271e-01, PNorm = 78.6857, GNorm = 0.2023
Meta loss on this task batch = 3.8494e-01, Meta loss averaged over last 500 steps = 3.9259e-01, PNorm = 78.6939, GNorm = 0.1816
Meta loss on this task batch = 4.1336e-01, Meta loss averaged over last 500 steps = 3.9266e-01, PNorm = 78.7040, GNorm = 0.1779
Meta loss on this task batch = 4.3397e-01, Meta loss averaged over last 500 steps = 3.9262e-01, PNorm = 78.7154, GNorm = 0.2086
Took 114.5281195640564 seconds to complete one epoch of meta training
Took 122.57842826843262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479975
Epoch 125
Meta loss on this task batch = 4.3158e-01, Meta loss averaged over last 500 steps = 3.9268e-01, PNorm = 78.7278, GNorm = 0.1972
Meta loss on this task batch = 4.3268e-01, Meta loss averaged over last 500 steps = 3.9276e-01, PNorm = 78.7412, GNorm = 0.1858
Meta loss on this task batch = 4.0406e-01, Meta loss averaged over last 500 steps = 3.9284e-01, PNorm = 78.7544, GNorm = 0.1772
Meta loss on this task batch = 3.9141e-01, Meta loss averaged over last 500 steps = 3.9272e-01, PNorm = 78.7672, GNorm = 0.1863
Meta loss on this task batch = 4.9852e-01, Meta loss averaged over last 500 steps = 3.9288e-01, PNorm = 78.7789, GNorm = 0.2535
Meta loss on this task batch = 3.2236e-01, Meta loss averaged over last 500 steps = 3.9283e-01, PNorm = 78.7918, GNorm = 0.1728
Meta loss on this task batch = 3.8527e-01, Meta loss averaged over last 500 steps = 3.9273e-01, PNorm = 78.8046, GNorm = 0.1780
Meta loss on this task batch = 3.8192e-01, Meta loss averaged over last 500 steps = 3.9260e-01, PNorm = 78.8187, GNorm = 0.1875
Meta loss on this task batch = 3.8702e-01, Meta loss averaged over last 500 steps = 3.9251e-01, PNorm = 78.8338, GNorm = 0.1851
Meta loss on this task batch = 4.2258e-01, Meta loss averaged over last 500 steps = 3.9270e-01, PNorm = 78.8472, GNorm = 0.2055
Meta loss on this task batch = 3.5394e-01, Meta loss averaged over last 500 steps = 3.9245e-01, PNorm = 78.8611, GNorm = 0.1645
Meta loss on this task batch = 3.5696e-01, Meta loss averaged over last 500 steps = 3.9230e-01, PNorm = 78.8731, GNorm = 0.2217
Meta loss on this task batch = 3.3981e-01, Meta loss averaged over last 500 steps = 3.9218e-01, PNorm = 78.8852, GNorm = 0.1707
Meta loss on this task batch = 3.8964e-01, Meta loss averaged over last 500 steps = 3.9220e-01, PNorm = 78.8952, GNorm = 0.2180
Meta loss on this task batch = 3.1795e-01, Meta loss averaged over last 500 steps = 3.9210e-01, PNorm = 78.9064, GNorm = 0.1975
Meta loss on this task batch = 3.4120e-01, Meta loss averaged over last 500 steps = 3.9196e-01, PNorm = 78.9181, GNorm = 0.1625
Meta loss on this task batch = 3.4607e-01, Meta loss averaged over last 500 steps = 3.9182e-01, PNorm = 78.9299, GNorm = 0.1970
Meta loss on this task batch = 4.0301e-01, Meta loss averaged over last 500 steps = 3.9197e-01, PNorm = 78.9414, GNorm = 0.1904
Meta loss on this task batch = 4.1765e-01, Meta loss averaged over last 500 steps = 3.9199e-01, PNorm = 78.9522, GNorm = 0.2889
Took 114.86439990997314 seconds to complete one epoch of meta training
Took 122.54721689224243 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477066
Epoch 126
Meta loss on this task batch = 3.9550e-01, Meta loss averaged over last 500 steps = 3.9198e-01, PNorm = 78.9626, GNorm = 0.1810
Meta loss on this task batch = 3.4400e-01, Meta loss averaged over last 500 steps = 3.9183e-01, PNorm = 78.9732, GNorm = 0.1625
Meta loss on this task batch = 3.8368e-01, Meta loss averaged over last 500 steps = 3.9181e-01, PNorm = 78.9836, GNorm = 0.1889
Meta loss on this task batch = 3.6181e-01, Meta loss averaged over last 500 steps = 3.9187e-01, PNorm = 78.9948, GNorm = 0.1840
Meta loss on this task batch = 4.0578e-01, Meta loss averaged over last 500 steps = 3.9191e-01, PNorm = 79.0070, GNorm = 0.1806
Meta loss on this task batch = 4.1893e-01, Meta loss averaged over last 500 steps = 3.9185e-01, PNorm = 79.0202, GNorm = 0.1934
Meta loss on this task batch = 4.0684e-01, Meta loss averaged over last 500 steps = 3.9181e-01, PNorm = 79.0331, GNorm = 0.1909
Meta loss on this task batch = 3.9880e-01, Meta loss averaged over last 500 steps = 3.9174e-01, PNorm = 79.0444, GNorm = 0.2032
Meta loss on this task batch = 3.5138e-01, Meta loss averaged over last 500 steps = 3.9172e-01, PNorm = 79.0556, GNorm = 0.1942
Meta loss on this task batch = 3.3288e-01, Meta loss averaged over last 500 steps = 3.9162e-01, PNorm = 79.0681, GNorm = 0.1887
Meta loss on this task batch = 3.5148e-01, Meta loss averaged over last 500 steps = 3.9158e-01, PNorm = 79.0821, GNorm = 0.2129
Meta loss on this task batch = 3.8702e-01, Meta loss averaged over last 500 steps = 3.9153e-01, PNorm = 79.0961, GNorm = 0.1643
Meta loss on this task batch = 3.2946e-01, Meta loss averaged over last 500 steps = 3.9141e-01, PNorm = 79.1095, GNorm = 0.1757
Meta loss on this task batch = 4.2995e-01, Meta loss averaged over last 500 steps = 3.9139e-01, PNorm = 79.1206, GNorm = 0.2486
Meta loss on this task batch = 3.7831e-01, Meta loss averaged over last 500 steps = 3.9133e-01, PNorm = 79.1288, GNorm = 0.2324
Meta loss on this task batch = 4.0499e-01, Meta loss averaged over last 500 steps = 3.9135e-01, PNorm = 79.1363, GNorm = 0.1848
Meta loss on this task batch = 3.8364e-01, Meta loss averaged over last 500 steps = 3.9139e-01, PNorm = 79.1449, GNorm = 0.1800
Meta loss on this task batch = 4.3156e-01, Meta loss averaged over last 500 steps = 3.9137e-01, PNorm = 79.1544, GNorm = 0.1862
Meta loss on this task batch = 3.7240e-01, Meta loss averaged over last 500 steps = 3.9121e-01, PNorm = 79.1640, GNorm = 0.2016
Took 115.58036088943481 seconds to complete one epoch of meta training
Took 122.52561974525452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466257
Epoch 127
Meta loss on this task batch = 3.6078e-01, Meta loss averaged over last 500 steps = 3.9106e-01, PNorm = 79.1741, GNorm = 0.1405
Meta loss on this task batch = 4.3389e-01, Meta loss averaged over last 500 steps = 3.9117e-01, PNorm = 79.1849, GNorm = 0.1855
Meta loss on this task batch = 4.2132e-01, Meta loss averaged over last 500 steps = 3.9116e-01, PNorm = 79.1968, GNorm = 0.1782
Meta loss on this task batch = 3.9688e-01, Meta loss averaged over last 500 steps = 3.9135e-01, PNorm = 79.2109, GNorm = 0.2217
Meta loss on this task batch = 3.8672e-01, Meta loss averaged over last 500 steps = 3.9126e-01, PNorm = 79.2258, GNorm = 0.1922
Meta loss on this task batch = 4.3066e-01, Meta loss averaged over last 500 steps = 3.9133e-01, PNorm = 79.2409, GNorm = 0.1764
Meta loss on this task batch = 3.6656e-01, Meta loss averaged over last 500 steps = 3.9129e-01, PNorm = 79.2551, GNorm = 0.1648
Meta loss on this task batch = 4.5603e-01, Meta loss averaged over last 500 steps = 3.9137e-01, PNorm = 79.2669, GNorm = 0.2313
Meta loss on this task batch = 3.4455e-01, Meta loss averaged over last 500 steps = 3.9130e-01, PNorm = 79.2782, GNorm = 0.1712
Meta loss on this task batch = 3.8840e-01, Meta loss averaged over last 500 steps = 3.9123e-01, PNorm = 79.2900, GNorm = 0.1646
Meta loss on this task batch = 3.2281e-01, Meta loss averaged over last 500 steps = 3.9113e-01, PNorm = 79.3023, GNorm = 0.1681
Meta loss on this task batch = 3.7221e-01, Meta loss averaged over last 500 steps = 3.9110e-01, PNorm = 79.3137, GNorm = 0.2043
Meta loss on this task batch = 3.7007e-01, Meta loss averaged over last 500 steps = 3.9108e-01, PNorm = 79.3250, GNorm = 0.1794
Meta loss on this task batch = 3.4883e-01, Meta loss averaged over last 500 steps = 3.9105e-01, PNorm = 79.3372, GNorm = 0.1782
Meta loss on this task batch = 4.1530e-01, Meta loss averaged over last 500 steps = 3.9103e-01, PNorm = 79.3471, GNorm = 0.2303
Meta loss on this task batch = 3.9409e-01, Meta loss averaged over last 500 steps = 3.9103e-01, PNorm = 79.3562, GNorm = 0.2273
Meta loss on this task batch = 2.7482e-01, Meta loss averaged over last 500 steps = 3.9090e-01, PNorm = 79.3666, GNorm = 0.1665
Meta loss on this task batch = 3.5896e-01, Meta loss averaged over last 500 steps = 3.9081e-01, PNorm = 79.3755, GNorm = 0.1948
Meta loss on this task batch = 3.6389e-01, Meta loss averaged over last 500 steps = 3.9062e-01, PNorm = 79.3850, GNorm = 0.2224
Took 115.34412360191345 seconds to complete one epoch of meta training
Took 122.14262914657593 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482585
Epoch 128
Meta loss on this task batch = 4.2117e-01, Meta loss averaged over last 500 steps = 3.9062e-01, PNorm = 79.3956, GNorm = 0.2103
Meta loss on this task batch = 3.4279e-01, Meta loss averaged over last 500 steps = 3.9057e-01, PNorm = 79.4071, GNorm = 0.2010
Meta loss on this task batch = 3.5712e-01, Meta loss averaged over last 500 steps = 3.9045e-01, PNorm = 79.4185, GNorm = 0.2021
Meta loss on this task batch = 4.1451e-01, Meta loss averaged over last 500 steps = 3.9037e-01, PNorm = 79.4303, GNorm = 0.2062
Meta loss on this task batch = 3.7389e-01, Meta loss averaged over last 500 steps = 3.9025e-01, PNorm = 79.4427, GNorm = 0.2252
Meta loss on this task batch = 3.3780e-01, Meta loss averaged over last 500 steps = 3.9011e-01, PNorm = 79.4561, GNorm = 0.1923
Meta loss on this task batch = 3.7153e-01, Meta loss averaged over last 500 steps = 3.9008e-01, PNorm = 79.4708, GNorm = 0.2145
Meta loss on this task batch = 4.2835e-01, Meta loss averaged over last 500 steps = 3.9006e-01, PNorm = 79.4849, GNorm = 0.2048
Meta loss on this task batch = 4.0663e-01, Meta loss averaged over last 500 steps = 3.9001e-01, PNorm = 79.4974, GNorm = 0.1973
Meta loss on this task batch = 3.7292e-01, Meta loss averaged over last 500 steps = 3.8999e-01, PNorm = 79.5069, GNorm = 0.2303
Meta loss on this task batch = 3.8887e-01, Meta loss averaged over last 500 steps = 3.8999e-01, PNorm = 79.5151, GNorm = 0.2791
Meta loss on this task batch = 3.7542e-01, Meta loss averaged over last 500 steps = 3.8998e-01, PNorm = 79.5244, GNorm = 0.1764
Meta loss on this task batch = 3.9045e-01, Meta loss averaged over last 500 steps = 3.8997e-01, PNorm = 79.5342, GNorm = 0.1840
Meta loss on this task batch = 3.9321e-01, Meta loss averaged over last 500 steps = 3.8993e-01, PNorm = 79.5447, GNorm = 0.1995
Meta loss on this task batch = 3.8286e-01, Meta loss averaged over last 500 steps = 3.8994e-01, PNorm = 79.5559, GNorm = 0.1703
Meta loss on this task batch = 3.5207e-01, Meta loss averaged over last 500 steps = 3.8984e-01, PNorm = 79.5671, GNorm = 0.1781
Meta loss on this task batch = 3.9655e-01, Meta loss averaged over last 500 steps = 3.8987e-01, PNorm = 79.5792, GNorm = 0.1672
Meta loss on this task batch = 4.0625e-01, Meta loss averaged over last 500 steps = 3.8987e-01, PNorm = 79.5915, GNorm = 0.1727
Meta loss on this task batch = 3.8565e-01, Meta loss averaged over last 500 steps = 3.8973e-01, PNorm = 79.6045, GNorm = 0.1774
Took 117.9411678314209 seconds to complete one epoch of meta training
Took 125.70216178894043 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466600
Epoch 129
Meta loss on this task batch = 4.2164e-01, Meta loss averaged over last 500 steps = 3.8976e-01, PNorm = 79.6170, GNorm = 0.1834
Meta loss on this task batch = 3.5749e-01, Meta loss averaged over last 500 steps = 3.8973e-01, PNorm = 79.6306, GNorm = 0.1862
Meta loss on this task batch = 3.7457e-01, Meta loss averaged over last 500 steps = 3.8976e-01, PNorm = 79.6448, GNorm = 0.1608
Meta loss on this task batch = 3.5273e-01, Meta loss averaged over last 500 steps = 3.8973e-01, PNorm = 79.6589, GNorm = 0.1806
Meta loss on this task batch = 3.4413e-01, Meta loss averaged over last 500 steps = 3.8954e-01, PNorm = 79.6730, GNorm = 0.1943
Meta loss on this task batch = 3.9986e-01, Meta loss averaged over last 500 steps = 3.8970e-01, PNorm = 79.6860, GNorm = 0.1839
Meta loss on this task batch = 4.0530e-01, Meta loss averaged over last 500 steps = 3.8976e-01, PNorm = 79.6995, GNorm = 0.2280
Meta loss on this task batch = 3.9081e-01, Meta loss averaged over last 500 steps = 3.8972e-01, PNorm = 79.7133, GNorm = 0.2126
Meta loss on this task batch = 4.0882e-01, Meta loss averaged over last 500 steps = 3.8974e-01, PNorm = 79.7236, GNorm = 0.2595
Meta loss on this task batch = 4.0919e-01, Meta loss averaged over last 500 steps = 3.8976e-01, PNorm = 79.7342, GNorm = 0.2135
Meta loss on this task batch = 4.0715e-01, Meta loss averaged over last 500 steps = 3.8980e-01, PNorm = 79.7433, GNorm = 0.2265
Meta loss on this task batch = 2.9928e-01, Meta loss averaged over last 500 steps = 3.8963e-01, PNorm = 79.7536, GNorm = 0.1800
Meta loss on this task batch = 3.3815e-01, Meta loss averaged over last 500 steps = 3.8944e-01, PNorm = 79.7649, GNorm = 0.1649
Meta loss on this task batch = 3.6616e-01, Meta loss averaged over last 500 steps = 3.8939e-01, PNorm = 79.7772, GNorm = 0.1677
Meta loss on this task batch = 3.7143e-01, Meta loss averaged over last 500 steps = 3.8935e-01, PNorm = 79.7895, GNorm = 0.1611
Meta loss on this task batch = 3.7393e-01, Meta loss averaged over last 500 steps = 3.8925e-01, PNorm = 79.8023, GNorm = 0.1702
Meta loss on this task batch = 4.3229e-01, Meta loss averaged over last 500 steps = 3.8928e-01, PNorm = 79.8142, GNorm = 0.2078
Meta loss on this task batch = 3.9952e-01, Meta loss averaged over last 500 steps = 3.8938e-01, PNorm = 79.8256, GNorm = 0.1758
Meta loss on this task batch = 2.9861e-01, Meta loss averaged over last 500 steps = 3.8919e-01, PNorm = 79.8376, GNorm = 0.2060
Took 115.00094485282898 seconds to complete one epoch of meta training
Took 122.49023795127869 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491435
Epoch 130
Meta loss on this task batch = 4.1147e-01, Meta loss averaged over last 500 steps = 3.8924e-01, PNorm = 79.8497, GNorm = 0.2301
Meta loss on this task batch = 4.4946e-01, Meta loss averaged over last 500 steps = 3.8929e-01, PNorm = 79.8602, GNorm = 0.2836
Meta loss on this task batch = 3.4676e-01, Meta loss averaged over last 500 steps = 3.8913e-01, PNorm = 79.8716, GNorm = 0.1864
Meta loss on this task batch = 3.3274e-01, Meta loss averaged over last 500 steps = 3.8910e-01, PNorm = 79.8845, GNorm = 0.1837
Meta loss on this task batch = 3.7274e-01, Meta loss averaged over last 500 steps = 3.8899e-01, PNorm = 79.8979, GNorm = 0.1678
Meta loss on this task batch = 3.7090e-01, Meta loss averaged over last 500 steps = 3.8912e-01, PNorm = 79.9116, GNorm = 0.1995
Meta loss on this task batch = 3.5447e-01, Meta loss averaged over last 500 steps = 3.8901e-01, PNorm = 79.9259, GNorm = 0.2201
Meta loss on this task batch = 3.4814e-01, Meta loss averaged over last 500 steps = 3.8891e-01, PNorm = 79.9397, GNorm = 0.1582
Meta loss on this task batch = 4.0001e-01, Meta loss averaged over last 500 steps = 3.8902e-01, PNorm = 79.9518, GNorm = 0.2116
Meta loss on this task batch = 3.8840e-01, Meta loss averaged over last 500 steps = 3.8903e-01, PNorm = 79.9607, GNorm = 0.2151
Meta loss on this task batch = 3.6638e-01, Meta loss averaged over last 500 steps = 3.8892e-01, PNorm = 79.9695, GNorm = 0.1789
Meta loss on this task batch = 3.6219e-01, Meta loss averaged over last 500 steps = 3.8876e-01, PNorm = 79.9778, GNorm = 0.1788
Meta loss on this task batch = 4.2837e-01, Meta loss averaged over last 500 steps = 3.8881e-01, PNorm = 79.9853, GNorm = 0.2211
Meta loss on this task batch = 4.1916e-01, Meta loss averaged over last 500 steps = 3.8890e-01, PNorm = 79.9923, GNorm = 0.1875
Meta loss on this task batch = 3.6072e-01, Meta loss averaged over last 500 steps = 3.8886e-01, PNorm = 80.0007, GNorm = 0.2009
Meta loss on this task batch = 3.8942e-01, Meta loss averaged over last 500 steps = 3.8878e-01, PNorm = 80.0101, GNorm = 0.1906
Meta loss on this task batch = 3.5506e-01, Meta loss averaged over last 500 steps = 3.8870e-01, PNorm = 80.0207, GNorm = 0.1518
Meta loss on this task batch = 4.3540e-01, Meta loss averaged over last 500 steps = 3.8869e-01, PNorm = 80.0308, GNorm = 0.1851
Meta loss on this task batch = 3.8771e-01, Meta loss averaged over last 500 steps = 3.8869e-01, PNorm = 80.0412, GNorm = 0.2107
Took 115.1047351360321 seconds to complete one epoch of meta training
Took 122.6224536895752 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498115
Epoch 131
Meta loss on this task batch = 3.8226e-01, Meta loss averaged over last 500 steps = 3.8856e-01, PNorm = 80.0530, GNorm = 0.1723
Meta loss on this task batch = 3.5008e-01, Meta loss averaged over last 500 steps = 3.8852e-01, PNorm = 80.0660, GNorm = 0.1909
Meta loss on this task batch = 3.6624e-01, Meta loss averaged over last 500 steps = 3.8842e-01, PNorm = 80.0790, GNorm = 0.1919
Meta loss on this task batch = 3.7253e-01, Meta loss averaged over last 500 steps = 3.8844e-01, PNorm = 80.0935, GNorm = 0.1929
Meta loss on this task batch = 4.0934e-01, Meta loss averaged over last 500 steps = 3.8841e-01, PNorm = 80.1069, GNorm = 0.2059
Meta loss on this task batch = 3.7046e-01, Meta loss averaged over last 500 steps = 3.8843e-01, PNorm = 80.1204, GNorm = 0.1819
Meta loss on this task batch = 3.4992e-01, Meta loss averaged over last 500 steps = 3.8834e-01, PNorm = 80.1331, GNorm = 0.1841
Meta loss on this task batch = 3.2001e-01, Meta loss averaged over last 500 steps = 3.8810e-01, PNorm = 80.1456, GNorm = 0.1780
Meta loss on this task batch = 3.7201e-01, Meta loss averaged over last 500 steps = 3.8816e-01, PNorm = 80.1578, GNorm = 0.1980
Meta loss on this task batch = 3.8060e-01, Meta loss averaged over last 500 steps = 3.8812e-01, PNorm = 80.1688, GNorm = 0.1884
Meta loss on this task batch = 4.1395e-01, Meta loss averaged over last 500 steps = 3.8817e-01, PNorm = 80.1768, GNorm = 0.2749
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 3.8802e-01, PNorm = 80.1852, GNorm = 0.1962
Meta loss on this task batch = 3.7240e-01, Meta loss averaged over last 500 steps = 3.8804e-01, PNorm = 80.1942, GNorm = 0.1791
Meta loss on this task batch = 3.8755e-01, Meta loss averaged over last 500 steps = 3.8797e-01, PNorm = 80.2036, GNorm = 0.2089
Meta loss on this task batch = 4.3243e-01, Meta loss averaged over last 500 steps = 3.8803e-01, PNorm = 80.2130, GNorm = 0.1901
Meta loss on this task batch = 3.7582e-01, Meta loss averaged over last 500 steps = 3.8797e-01, PNorm = 80.2239, GNorm = 0.2165
Meta loss on this task batch = 3.8958e-01, Meta loss averaged over last 500 steps = 3.8794e-01, PNorm = 80.2355, GNorm = 0.2045
Meta loss on this task batch = 4.1612e-01, Meta loss averaged over last 500 steps = 3.8791e-01, PNorm = 80.2480, GNorm = 0.1958
Meta loss on this task batch = 3.9449e-01, Meta loss averaged over last 500 steps = 3.8800e-01, PNorm = 80.2609, GNorm = 0.2078
Took 113.097975730896 seconds to complete one epoch of meta training
Took 120.36123967170715 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458935
Epoch 132
Meta loss on this task batch = 3.4600e-01, Meta loss averaged over last 500 steps = 3.8783e-01, PNorm = 80.2734, GNorm = 0.1799
Meta loss on this task batch = 3.8456e-01, Meta loss averaged over last 500 steps = 3.8781e-01, PNorm = 80.2854, GNorm = 0.2008
Meta loss on this task batch = 3.6506e-01, Meta loss averaged over last 500 steps = 3.8785e-01, PNorm = 80.2969, GNorm = 0.2002
Meta loss on this task batch = 3.4755e-01, Meta loss averaged over last 500 steps = 3.8781e-01, PNorm = 80.3095, GNorm = 0.2078
Meta loss on this task batch = 3.8594e-01, Meta loss averaged over last 500 steps = 3.8785e-01, PNorm = 80.3214, GNorm = 0.1807
Meta loss on this task batch = 4.3767e-01, Meta loss averaged over last 500 steps = 3.8783e-01, PNorm = 80.3323, GNorm = 0.2184
Meta loss on this task batch = 3.5992e-01, Meta loss averaged over last 500 steps = 3.8780e-01, PNorm = 80.3424, GNorm = 0.2120
Meta loss on this task batch = 3.8591e-01, Meta loss averaged over last 500 steps = 3.8781e-01, PNorm = 80.3530, GNorm = 0.1802
Meta loss on this task batch = 4.1654e-01, Meta loss averaged over last 500 steps = 3.8782e-01, PNorm = 80.3619, GNorm = 0.2373
Meta loss on this task batch = 3.3443e-01, Meta loss averaged over last 500 steps = 3.8771e-01, PNorm = 80.3706, GNorm = 0.1848
Meta loss on this task batch = 3.3374e-01, Meta loss averaged over last 500 steps = 3.8763e-01, PNorm = 80.3801, GNorm = 0.1658
Meta loss on this task batch = 3.9353e-01, Meta loss averaged over last 500 steps = 3.8775e-01, PNorm = 80.3901, GNorm = 0.1934
Meta loss on this task batch = 4.0044e-01, Meta loss averaged over last 500 steps = 3.8773e-01, PNorm = 80.4014, GNorm = 0.1838
Meta loss on this task batch = 3.7451e-01, Meta loss averaged over last 500 steps = 3.8771e-01, PNorm = 80.4139, GNorm = 0.1827
Meta loss on this task batch = 3.8027e-01, Meta loss averaged over last 500 steps = 3.8760e-01, PNorm = 80.4276, GNorm = 0.2155
Meta loss on this task batch = 3.4970e-01, Meta loss averaged over last 500 steps = 3.8755e-01, PNorm = 80.4416, GNorm = 0.1711
Meta loss on this task batch = 4.1733e-01, Meta loss averaged over last 500 steps = 3.8752e-01, PNorm = 80.4552, GNorm = 0.2015
Meta loss on this task batch = 3.8101e-01, Meta loss averaged over last 500 steps = 3.8749e-01, PNorm = 80.4685, GNorm = 0.1734
Meta loss on this task batch = 3.7962e-01, Meta loss averaged over last 500 steps = 3.8746e-01, PNorm = 80.4820, GNorm = 0.2268
Took 115.39682483673096 seconds to complete one epoch of meta training
Took 123.10750985145569 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484351
Epoch 133
Meta loss on this task batch = 4.0377e-01, Meta loss averaged over last 500 steps = 3.8742e-01, PNorm = 80.4957, GNorm = 0.2210
Meta loss on this task batch = 4.7891e-01, Meta loss averaged over last 500 steps = 3.8757e-01, PNorm = 80.5069, GNorm = 0.2661
Meta loss on this task batch = 3.6678e-01, Meta loss averaged over last 500 steps = 3.8747e-01, PNorm = 80.5181, GNorm = 0.1883
Meta loss on this task batch = 4.0850e-01, Meta loss averaged over last 500 steps = 3.8744e-01, PNorm = 80.5282, GNorm = 0.2176
Meta loss on this task batch = 3.7244e-01, Meta loss averaged over last 500 steps = 3.8749e-01, PNorm = 80.5382, GNorm = 0.1841
Meta loss on this task batch = 4.2764e-01, Meta loss averaged over last 500 steps = 3.8750e-01, PNorm = 80.5484, GNorm = 0.2033
Meta loss on this task batch = 3.9168e-01, Meta loss averaged over last 500 steps = 3.8737e-01, PNorm = 80.5583, GNorm = 0.2063
Meta loss on this task batch = 3.8525e-01, Meta loss averaged over last 500 steps = 3.8737e-01, PNorm = 80.5676, GNorm = 0.2317
Meta loss on this task batch = 3.3062e-01, Meta loss averaged over last 500 steps = 3.8727e-01, PNorm = 80.5780, GNorm = 0.1660
Meta loss on this task batch = 3.4781e-01, Meta loss averaged over last 500 steps = 3.8731e-01, PNorm = 80.5880, GNorm = 0.1916
Meta loss on this task batch = 3.4058e-01, Meta loss averaged over last 500 steps = 3.8726e-01, PNorm = 80.5989, GNorm = 0.2018
Meta loss on this task batch = 3.7008e-01, Meta loss averaged over last 500 steps = 3.8713e-01, PNorm = 80.6102, GNorm = 0.1823
Meta loss on this task batch = 3.9443e-01, Meta loss averaged over last 500 steps = 3.8725e-01, PNorm = 80.6215, GNorm = 0.1969
Meta loss on this task batch = 3.0742e-01, Meta loss averaged over last 500 steps = 3.8702e-01, PNorm = 80.6326, GNorm = 0.1690
Meta loss on this task batch = 3.5785e-01, Meta loss averaged over last 500 steps = 3.8706e-01, PNorm = 80.6440, GNorm = 0.1733
Meta loss on this task batch = 3.6029e-01, Meta loss averaged over last 500 steps = 3.8683e-01, PNorm = 80.6555, GNorm = 0.1865
Meta loss on this task batch = 3.7327e-01, Meta loss averaged over last 500 steps = 3.8681e-01, PNorm = 80.6659, GNorm = 0.1939
Meta loss on this task batch = 3.6419e-01, Meta loss averaged over last 500 steps = 3.8673e-01, PNorm = 80.6766, GNorm = 0.1896
Meta loss on this task batch = 4.0326e-01, Meta loss averaged over last 500 steps = 3.8676e-01, PNorm = 80.6865, GNorm = 0.2358
Took 113.50166034698486 seconds to complete one epoch of meta training
Took 121.26151132583618 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477924
Epoch 134
Meta loss on this task batch = 4.1439e-01, Meta loss averaged over last 500 steps = 3.8674e-01, PNorm = 80.6953, GNorm = 0.2086
Meta loss on this task batch = 2.9406e-01, Meta loss averaged over last 500 steps = 3.8651e-01, PNorm = 80.7060, GNorm = 0.1586
Meta loss on this task batch = 3.9003e-01, Meta loss averaged over last 500 steps = 3.8636e-01, PNorm = 80.7167, GNorm = 0.1839
Meta loss on this task batch = 4.3484e-01, Meta loss averaged over last 500 steps = 3.8636e-01, PNorm = 80.7272, GNorm = 0.2154
Meta loss on this task batch = 3.3386e-01, Meta loss averaged over last 500 steps = 3.8634e-01, PNorm = 80.7389, GNorm = 0.1887
Meta loss on this task batch = 3.4762e-01, Meta loss averaged over last 500 steps = 3.8641e-01, PNorm = 80.7507, GNorm = 0.1601
Meta loss on this task batch = 3.3699e-01, Meta loss averaged over last 500 steps = 3.8630e-01, PNorm = 80.7631, GNorm = 0.1729
Meta loss on this task batch = 4.0578e-01, Meta loss averaged over last 500 steps = 3.8625e-01, PNorm = 80.7752, GNorm = 0.2010
Meta loss on this task batch = 3.7140e-01, Meta loss averaged over last 500 steps = 3.8615e-01, PNorm = 80.7868, GNorm = 0.1957
Meta loss on this task batch = 3.5335e-01, Meta loss averaged over last 500 steps = 3.8605e-01, PNorm = 80.7975, GNorm = 0.2034
Meta loss on this task batch = 3.7597e-01, Meta loss averaged over last 500 steps = 3.8604e-01, PNorm = 80.8079, GNorm = 0.2036
Meta loss on this task batch = 3.7031e-01, Meta loss averaged over last 500 steps = 3.8605e-01, PNorm = 80.8177, GNorm = 0.1794
Meta loss on this task batch = 3.7180e-01, Meta loss averaged over last 500 steps = 3.8592e-01, PNorm = 80.8291, GNorm = 0.1969
Meta loss on this task batch = 4.2890e-01, Meta loss averaged over last 500 steps = 3.8611e-01, PNorm = 80.8407, GNorm = 0.1876
Meta loss on this task batch = 3.6164e-01, Meta loss averaged over last 500 steps = 3.8612e-01, PNorm = 80.8533, GNorm = 0.1780
Meta loss on this task batch = 3.9084e-01, Meta loss averaged over last 500 steps = 3.8616e-01, PNorm = 80.8658, GNorm = 0.1811
Meta loss on this task batch = 4.5142e-01, Meta loss averaged over last 500 steps = 3.8621e-01, PNorm = 80.8772, GNorm = 0.2152
Meta loss on this task batch = 3.9831e-01, Meta loss averaged over last 500 steps = 3.8619e-01, PNorm = 80.8881, GNorm = 0.1851
Meta loss on this task batch = 3.2679e-01, Meta loss averaged over last 500 steps = 3.8596e-01, PNorm = 80.8993, GNorm = 0.2034
Took 114.24322438240051 seconds to complete one epoch of meta training
Took 122.28829264640808 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476075
Epoch 135
Meta loss on this task batch = 3.8142e-01, Meta loss averaged over last 500 steps = 3.8592e-01, PNorm = 80.9106, GNorm = 0.1786
Meta loss on this task batch = 3.6923e-01, Meta loss averaged over last 500 steps = 3.8597e-01, PNorm = 80.9218, GNorm = 0.1704
Meta loss on this task batch = 4.4146e-01, Meta loss averaged over last 500 steps = 3.8609e-01, PNorm = 80.9337, GNorm = 0.2235
Meta loss on this task batch = 3.9006e-01, Meta loss averaged over last 500 steps = 3.8612e-01, PNorm = 80.9458, GNorm = 0.2104
Meta loss on this task batch = 3.4771e-01, Meta loss averaged over last 500 steps = 3.8607e-01, PNorm = 80.9572, GNorm = 0.1664
Meta loss on this task batch = 4.6144e-01, Meta loss averaged over last 500 steps = 3.8618e-01, PNorm = 80.9677, GNorm = 0.2320
Meta loss on this task batch = 4.2426e-01, Meta loss averaged over last 500 steps = 3.8622e-01, PNorm = 80.9778, GNorm = 0.2058
Meta loss on this task batch = 3.5798e-01, Meta loss averaged over last 500 steps = 3.8626e-01, PNorm = 80.9878, GNorm = 0.1969
Meta loss on this task batch = 3.1738e-01, Meta loss averaged over last 500 steps = 3.8610e-01, PNorm = 80.9990, GNorm = 0.1933
Meta loss on this task batch = 3.4927e-01, Meta loss averaged over last 500 steps = 3.8608e-01, PNorm = 81.0113, GNorm = 0.1856
Meta loss on this task batch = 3.7050e-01, Meta loss averaged over last 500 steps = 3.8600e-01, PNorm = 81.0226, GNorm = 0.1862
Meta loss on this task batch = 3.1601e-01, Meta loss averaged over last 500 steps = 3.8585e-01, PNorm = 81.0342, GNorm = 0.1731
Meta loss on this task batch = 3.5487e-01, Meta loss averaged over last 500 steps = 3.8565e-01, PNorm = 81.0459, GNorm = 0.1839
Meta loss on this task batch = 4.0344e-01, Meta loss averaged over last 500 steps = 3.8560e-01, PNorm = 81.0561, GNorm = 0.2216
Meta loss on this task batch = 3.6609e-01, Meta loss averaged over last 500 steps = 3.8553e-01, PNorm = 81.0665, GNorm = 0.1991
Meta loss on this task batch = 3.5585e-01, Meta loss averaged over last 500 steps = 3.8532e-01, PNorm = 81.0769, GNorm = 0.1942
Meta loss on this task batch = 3.7826e-01, Meta loss averaged over last 500 steps = 3.8518e-01, PNorm = 81.0862, GNorm = 0.2022
Meta loss on this task batch = 3.9366e-01, Meta loss averaged over last 500 steps = 3.8527e-01, PNorm = 81.0950, GNorm = 0.1830
Meta loss on this task batch = 3.4714e-01, Meta loss averaged over last 500 steps = 3.8532e-01, PNorm = 81.1021, GNorm = 0.2210
Took 114.51526308059692 seconds to complete one epoch of meta training
Took 121.84895992279053 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462466
Epoch 136
Meta loss on this task batch = 3.9231e-01, Meta loss averaged over last 500 steps = 3.8529e-01, PNorm = 81.1091, GNorm = 0.1722
Meta loss on this task batch = 3.2201e-01, Meta loss averaged over last 500 steps = 3.8524e-01, PNorm = 81.1179, GNorm = 0.1878
Meta loss on this task batch = 4.4969e-01, Meta loss averaged over last 500 steps = 3.8527e-01, PNorm = 81.1272, GNorm = 0.2181
Meta loss on this task batch = 3.6761e-01, Meta loss averaged over last 500 steps = 3.8529e-01, PNorm = 81.1371, GNorm = 0.1897
Meta loss on this task batch = 3.6623e-01, Meta loss averaged over last 500 steps = 3.8519e-01, PNorm = 81.1470, GNorm = 0.1977
Meta loss on this task batch = 3.5321e-01, Meta loss averaged over last 500 steps = 3.8517e-01, PNorm = 81.1579, GNorm = 0.1798
Meta loss on this task batch = 3.4676e-01, Meta loss averaged over last 500 steps = 3.8513e-01, PNorm = 81.1695, GNorm = 0.1751
Meta loss on this task batch = 3.7190e-01, Meta loss averaged over last 500 steps = 3.8510e-01, PNorm = 81.1812, GNorm = 0.1807
Meta loss on this task batch = 4.3738e-01, Meta loss averaged over last 500 steps = 3.8522e-01, PNorm = 81.1934, GNorm = 0.2393
Meta loss on this task batch = 3.9105e-01, Meta loss averaged over last 500 steps = 3.8508e-01, PNorm = 81.2052, GNorm = 0.2469
Meta loss on this task batch = 3.9898e-01, Meta loss averaged over last 500 steps = 3.8502e-01, PNorm = 81.2164, GNorm = 0.2553
Meta loss on this task batch = 3.7544e-01, Meta loss averaged over last 500 steps = 3.8499e-01, PNorm = 81.2273, GNorm = 0.1735
Meta loss on this task batch = 3.8145e-01, Meta loss averaged over last 500 steps = 3.8506e-01, PNorm = 81.2376, GNorm = 0.1883
Meta loss on this task batch = 3.2353e-01, Meta loss averaged over last 500 steps = 3.8492e-01, PNorm = 81.2492, GNorm = 0.1737
Meta loss on this task batch = 3.7790e-01, Meta loss averaged over last 500 steps = 3.8500e-01, PNorm = 81.2610, GNorm = 0.2436
Meta loss on this task batch = 3.8284e-01, Meta loss averaged over last 500 steps = 3.8499e-01, PNorm = 81.2729, GNorm = 0.2164
Meta loss on this task batch = 3.6730e-01, Meta loss averaged over last 500 steps = 3.8481e-01, PNorm = 81.2844, GNorm = 0.2226
Meta loss on this task batch = 3.8263e-01, Meta loss averaged over last 500 steps = 3.8477e-01, PNorm = 81.2965, GNorm = 0.2341
Meta loss on this task batch = 3.9676e-01, Meta loss averaged over last 500 steps = 3.8489e-01, PNorm = 81.3087, GNorm = 0.2654
Took 115.50091052055359 seconds to complete one epoch of meta training
Took 123.2595808506012 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481461
Epoch 137
Meta loss on this task batch = 3.7347e-01, Meta loss averaged over last 500 steps = 3.8479e-01, PNorm = 81.3195, GNorm = 0.2114
Meta loss on this task batch = 4.0083e-01, Meta loss averaged over last 500 steps = 3.8486e-01, PNorm = 81.3305, GNorm = 0.1637
Meta loss on this task batch = 3.5539e-01, Meta loss averaged over last 500 steps = 3.8463e-01, PNorm = 81.3412, GNorm = 0.2064
Meta loss on this task batch = 4.2383e-01, Meta loss averaged over last 500 steps = 3.8470e-01, PNorm = 81.3512, GNorm = 0.2264
Meta loss on this task batch = 3.4423e-01, Meta loss averaged over last 500 steps = 3.8452e-01, PNorm = 81.3620, GNorm = 0.1724
Meta loss on this task batch = 3.7758e-01, Meta loss averaged over last 500 steps = 3.8453e-01, PNorm = 81.3735, GNorm = 0.1930
Meta loss on this task batch = 3.6175e-01, Meta loss averaged over last 500 steps = 3.8437e-01, PNorm = 81.3853, GNorm = 0.1844
Meta loss on this task batch = 3.7230e-01, Meta loss averaged over last 500 steps = 3.8444e-01, PNorm = 81.3981, GNorm = 0.1854
Meta loss on this task batch = 3.9042e-01, Meta loss averaged over last 500 steps = 3.8457e-01, PNorm = 81.4106, GNorm = 0.1871
Meta loss on this task batch = 3.5661e-01, Meta loss averaged over last 500 steps = 3.8455e-01, PNorm = 81.4229, GNorm = 0.1857
Meta loss on this task batch = 3.4223e-01, Meta loss averaged over last 500 steps = 3.8444e-01, PNorm = 81.4357, GNorm = 0.1805
Meta loss on this task batch = 3.2123e-01, Meta loss averaged over last 500 steps = 3.8433e-01, PNorm = 81.4488, GNorm = 0.1648
Meta loss on this task batch = 3.7978e-01, Meta loss averaged over last 500 steps = 3.8435e-01, PNorm = 81.4619, GNorm = 0.2323
Meta loss on this task batch = 4.5513e-01, Meta loss averaged over last 500 steps = 3.8456e-01, PNorm = 81.4746, GNorm = 0.2488
Meta loss on this task batch = 3.7903e-01, Meta loss averaged over last 500 steps = 3.8447e-01, PNorm = 81.4876, GNorm = 0.2040
Meta loss on this task batch = 3.7790e-01, Meta loss averaged over last 500 steps = 3.8443e-01, PNorm = 81.4996, GNorm = 0.2095
Meta loss on this task batch = 4.0838e-01, Meta loss averaged over last 500 steps = 3.8444e-01, PNorm = 81.5102, GNorm = 0.1961
Meta loss on this task batch = 3.7734e-01, Meta loss averaged over last 500 steps = 3.8442e-01, PNorm = 81.5213, GNorm = 0.1950
Meta loss on this task batch = 4.1570e-01, Meta loss averaged over last 500 steps = 3.8457e-01, PNorm = 81.5319, GNorm = 0.2956
Took 115.70182943344116 seconds to complete one epoch of meta training
Took 123.14627814292908 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499521
Epoch 138
Meta loss on this task batch = 4.0530e-01, Meta loss averaged over last 500 steps = 3.8456e-01, PNorm = 81.5421, GNorm = 0.1929
Meta loss on this task batch = 3.7321e-01, Meta loss averaged over last 500 steps = 3.8445e-01, PNorm = 81.5537, GNorm = 0.1774
Meta loss on this task batch = 4.1711e-01, Meta loss averaged over last 500 steps = 3.8447e-01, PNorm = 81.5662, GNorm = 0.2034
Meta loss on this task batch = 3.9007e-01, Meta loss averaged over last 500 steps = 3.8443e-01, PNorm = 81.5779, GNorm = 0.1660
Meta loss on this task batch = 4.1108e-01, Meta loss averaged over last 500 steps = 3.8433e-01, PNorm = 81.5897, GNorm = 0.2047
Meta loss on this task batch = 3.5030e-01, Meta loss averaged over last 500 steps = 3.8425e-01, PNorm = 81.6024, GNorm = 0.1988
Meta loss on this task batch = 3.8304e-01, Meta loss averaged over last 500 steps = 3.8424e-01, PNorm = 81.6153, GNorm = 0.1817
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 3.8422e-01, PNorm = 81.6276, GNorm = 0.1641
Meta loss on this task batch = 3.6678e-01, Meta loss averaged over last 500 steps = 3.8413e-01, PNorm = 81.6395, GNorm = 0.1608
Meta loss on this task batch = 3.8479e-01, Meta loss averaged over last 500 steps = 3.8408e-01, PNorm = 81.6507, GNorm = 0.2264
Meta loss on this task batch = 3.6946e-01, Meta loss averaged over last 500 steps = 3.8409e-01, PNorm = 81.6612, GNorm = 0.2114
Meta loss on this task batch = 3.4876e-01, Meta loss averaged over last 500 steps = 3.8403e-01, PNorm = 81.6715, GNorm = 0.2352
Meta loss on this task batch = 3.8603e-01, Meta loss averaged over last 500 steps = 3.8397e-01, PNorm = 81.6821, GNorm = 0.2025
Meta loss on this task batch = 3.4690e-01, Meta loss averaged over last 500 steps = 3.8392e-01, PNorm = 81.6937, GNorm = 0.1873
Meta loss on this task batch = 3.1932e-01, Meta loss averaged over last 500 steps = 3.8373e-01, PNorm = 81.7056, GNorm = 0.1715
Meta loss on this task batch = 4.1930e-01, Meta loss averaged over last 500 steps = 3.8368e-01, PNorm = 81.7156, GNorm = 0.2289
Meta loss on this task batch = 3.9166e-01, Meta loss averaged over last 500 steps = 3.8369e-01, PNorm = 81.7256, GNorm = 0.2384
Meta loss on this task batch = 3.5091e-01, Meta loss averaged over last 500 steps = 3.8364e-01, PNorm = 81.7348, GNorm = 0.1657
Meta loss on this task batch = 3.4093e-01, Meta loss averaged over last 500 steps = 3.8355e-01, PNorm = 81.7443, GNorm = 0.2248
Took 115.47950291633606 seconds to complete one epoch of meta training
Took 123.6982753276825 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475204
Epoch 139
Meta loss on this task batch = 3.4701e-01, Meta loss averaged over last 500 steps = 3.8355e-01, PNorm = 81.7550, GNorm = 0.1580
Meta loss on this task batch = 3.5073e-01, Meta loss averaged over last 500 steps = 3.8343e-01, PNorm = 81.7668, GNorm = 0.2021
Meta loss on this task batch = 4.1979e-01, Meta loss averaged over last 500 steps = 3.8338e-01, PNorm = 81.7781, GNorm = 0.2475
Meta loss on this task batch = 3.8884e-01, Meta loss averaged over last 500 steps = 3.8334e-01, PNorm = 81.7887, GNorm = 0.1988
Meta loss on this task batch = 3.8348e-01, Meta loss averaged over last 500 steps = 3.8341e-01, PNorm = 81.8004, GNorm = 0.1747
Meta loss on this task batch = 3.8191e-01, Meta loss averaged over last 500 steps = 3.8341e-01, PNorm = 81.8123, GNorm = 0.2467
Meta loss on this task batch = 3.1069e-01, Meta loss averaged over last 500 steps = 3.8332e-01, PNorm = 81.8260, GNorm = 0.1561
Meta loss on this task batch = 4.0280e-01, Meta loss averaged over last 500 steps = 3.8322e-01, PNorm = 81.8388, GNorm = 0.1793
Meta loss on this task batch = 3.9612e-01, Meta loss averaged over last 500 steps = 3.8332e-01, PNorm = 81.8514, GNorm = 0.1939
Meta loss on this task batch = 3.8576e-01, Meta loss averaged over last 500 steps = 3.8332e-01, PNorm = 81.8635, GNorm = 0.2085
Meta loss on this task batch = 3.8873e-01, Meta loss averaged over last 500 steps = 3.8326e-01, PNorm = 81.8756, GNorm = 0.2193
Meta loss on this task batch = 3.6287e-01, Meta loss averaged over last 500 steps = 3.8310e-01, PNorm = 81.8876, GNorm = 0.2081
Meta loss on this task batch = 3.5000e-01, Meta loss averaged over last 500 steps = 3.8304e-01, PNorm = 81.8991, GNorm = 0.1959
Meta loss on this task batch = 3.4893e-01, Meta loss averaged over last 500 steps = 3.8290e-01, PNorm = 81.9119, GNorm = 0.1844
Meta loss on this task batch = 3.7099e-01, Meta loss averaged over last 500 steps = 3.8282e-01, PNorm = 81.9250, GNorm = 0.1983
Meta loss on this task batch = 4.6985e-01, Meta loss averaged over last 500 steps = 3.8284e-01, PNorm = 81.9365, GNorm = 0.2302
Meta loss on this task batch = 3.6676e-01, Meta loss averaged over last 500 steps = 3.8265e-01, PNorm = 81.9478, GNorm = 0.1948
Meta loss on this task batch = 3.4011e-01, Meta loss averaged over last 500 steps = 3.8267e-01, PNorm = 81.9593, GNorm = 0.1995
Meta loss on this task batch = 3.3334e-01, Meta loss averaged over last 500 steps = 3.8251e-01, PNorm = 81.9719, GNorm = 0.2355
Took 117.12491631507874 seconds to complete one epoch of meta training
Took 125.74545669555664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470185
Epoch 140
Meta loss on this task batch = 3.2615e-01, Meta loss averaged over last 500 steps = 3.8243e-01, PNorm = 81.9846, GNorm = 0.1622
Meta loss on this task batch = 3.8503e-01, Meta loss averaged over last 500 steps = 3.8242e-01, PNorm = 81.9982, GNorm = 0.1930
Meta loss on this task batch = 3.4271e-01, Meta loss averaged over last 500 steps = 3.8247e-01, PNorm = 82.0126, GNorm = 0.1856
Meta loss on this task batch = 3.4708e-01, Meta loss averaged over last 500 steps = 3.8236e-01, PNorm = 82.0271, GNorm = 0.1921
Meta loss on this task batch = 2.9266e-01, Meta loss averaged over last 500 steps = 3.8212e-01, PNorm = 82.0405, GNorm = 0.1812
Meta loss on this task batch = 3.8952e-01, Meta loss averaged over last 500 steps = 3.8227e-01, PNorm = 82.0537, GNorm = 0.2121
Meta loss on this task batch = 3.8364e-01, Meta loss averaged over last 500 steps = 3.8236e-01, PNorm = 82.0641, GNorm = 0.2531
Meta loss on this task batch = 4.2908e-01, Meta loss averaged over last 500 steps = 3.8237e-01, PNorm = 82.0742, GNorm = 0.2552
Meta loss on this task batch = 4.2983e-01, Meta loss averaged over last 500 steps = 3.8242e-01, PNorm = 82.0854, GNorm = 0.2131
Meta loss on this task batch = 4.0070e-01, Meta loss averaged over last 500 steps = 3.8253e-01, PNorm = 82.0974, GNorm = 0.1799
Meta loss on this task batch = 3.7259e-01, Meta loss averaged over last 500 steps = 3.8251e-01, PNorm = 82.1101, GNorm = 0.1729
Meta loss on this task batch = 3.7695e-01, Meta loss averaged over last 500 steps = 3.8240e-01, PNorm = 82.1218, GNorm = 0.1713
Meta loss on this task batch = 3.4983e-01, Meta loss averaged over last 500 steps = 3.8240e-01, PNorm = 82.1345, GNorm = 0.1870
Meta loss on this task batch = 3.8224e-01, Meta loss averaged over last 500 steps = 3.8251e-01, PNorm = 82.1477, GNorm = 0.1891
Meta loss on this task batch = 3.6445e-01, Meta loss averaged over last 500 steps = 3.8239e-01, PNorm = 82.1612, GNorm = 0.2234
Meta loss on this task batch = 4.1112e-01, Meta loss averaged over last 500 steps = 3.8246e-01, PNorm = 82.1745, GNorm = 0.1757
Meta loss on this task batch = 4.2021e-01, Meta loss averaged over last 500 steps = 3.8244e-01, PNorm = 82.1874, GNorm = 0.2191
Meta loss on this task batch = 3.5192e-01, Meta loss averaged over last 500 steps = 3.8243e-01, PNorm = 82.2015, GNorm = 0.1759
Meta loss on this task batch = 4.7261e-01, Meta loss averaged over last 500 steps = 3.8252e-01, PNorm = 82.2133, GNorm = 0.2742
Took 115.60549235343933 seconds to complete one epoch of meta training
Took 123.52801966667175 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477258
Epoch 141
Meta loss on this task batch = 4.1573e-01, Meta loss averaged over last 500 steps = 3.8256e-01, PNorm = 82.2250, GNorm = 0.2209
Meta loss on this task batch = 4.1460e-01, Meta loss averaged over last 500 steps = 3.8257e-01, PNorm = 82.2352, GNorm = 0.2002
Meta loss on this task batch = 3.7090e-01, Meta loss averaged over last 500 steps = 3.8248e-01, PNorm = 82.2441, GNorm = 0.2441
Meta loss on this task batch = 3.9313e-01, Meta loss averaged over last 500 steps = 3.8244e-01, PNorm = 82.2533, GNorm = 0.1962
Meta loss on this task batch = 3.1362e-01, Meta loss averaged over last 500 steps = 3.8238e-01, PNorm = 82.2638, GNorm = 0.1713
Meta loss on this task batch = 3.7785e-01, Meta loss averaged over last 500 steps = 3.8254e-01, PNorm = 82.2752, GNorm = 0.1794
Meta loss on this task batch = 3.6444e-01, Meta loss averaged over last 500 steps = 3.8253e-01, PNorm = 82.2865, GNorm = 0.1915
Meta loss on this task batch = 3.9161e-01, Meta loss averaged over last 500 steps = 3.8251e-01, PNorm = 82.2974, GNorm = 0.1970
Meta loss on this task batch = 3.4261e-01, Meta loss averaged over last 500 steps = 3.8239e-01, PNorm = 82.3089, GNorm = 0.1884
Meta loss on this task batch = 4.0163e-01, Meta loss averaged over last 500 steps = 3.8241e-01, PNorm = 82.3199, GNorm = 0.1856
Meta loss on this task batch = 4.0827e-01, Meta loss averaged over last 500 steps = 3.8243e-01, PNorm = 82.3303, GNorm = 0.1886
Meta loss on this task batch = 3.6465e-01, Meta loss averaged over last 500 steps = 3.8236e-01, PNorm = 82.3406, GNorm = 0.2116
Meta loss on this task batch = 3.5488e-01, Meta loss averaged over last 500 steps = 3.8225e-01, PNorm = 82.3516, GNorm = 0.1880
Meta loss on this task batch = 3.9719e-01, Meta loss averaged over last 500 steps = 3.8230e-01, PNorm = 82.3627, GNorm = 0.1727
Meta loss on this task batch = 3.4005e-01, Meta loss averaged over last 500 steps = 3.8217e-01, PNorm = 82.3737, GNorm = 0.1990
Meta loss on this task batch = 3.6545e-01, Meta loss averaged over last 500 steps = 3.8198e-01, PNorm = 82.3846, GNorm = 0.1888
Meta loss on this task batch = 3.9843e-01, Meta loss averaged over last 500 steps = 3.8203e-01, PNorm = 82.3950, GNorm = 0.2010
Meta loss on this task batch = 3.4004e-01, Meta loss averaged over last 500 steps = 3.8209e-01, PNorm = 82.4063, GNorm = 0.1760
Meta loss on this task batch = 4.1189e-01, Meta loss averaged over last 500 steps = 3.8223e-01, PNorm = 82.4170, GNorm = 0.2247
Took 112.79720187187195 seconds to complete one epoch of meta training
Took 120.55842804908752 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478849
Epoch 142
Meta loss on this task batch = 3.2662e-01, Meta loss averaged over last 500 steps = 3.8216e-01, PNorm = 82.4291, GNorm = 0.1629
Meta loss on this task batch = 4.0121e-01, Meta loss averaged over last 500 steps = 3.8217e-01, PNorm = 82.4414, GNorm = 0.2545
Meta loss on this task batch = 4.2850e-01, Meta loss averaged over last 500 steps = 3.8220e-01, PNorm = 82.4530, GNorm = 0.2787
Meta loss on this task batch = 3.8470e-01, Meta loss averaged over last 500 steps = 3.8213e-01, PNorm = 82.4654, GNorm = 0.1873
Meta loss on this task batch = 2.9653e-01, Meta loss averaged over last 500 steps = 3.8200e-01, PNorm = 82.4784, GNorm = 0.1646
Meta loss on this task batch = 3.1369e-01, Meta loss averaged over last 500 steps = 3.8176e-01, PNorm = 82.4920, GNorm = 0.1752
Meta loss on this task batch = 4.3248e-01, Meta loss averaged over last 500 steps = 3.8180e-01, PNorm = 82.5043, GNorm = 0.2074
Meta loss on this task batch = 4.1910e-01, Meta loss averaged over last 500 steps = 3.8194e-01, PNorm = 82.5160, GNorm = 0.2354
Meta loss on this task batch = 3.8106e-01, Meta loss averaged over last 500 steps = 3.8184e-01, PNorm = 82.5263, GNorm = 0.1966
Meta loss on this task batch = 4.1315e-01, Meta loss averaged over last 500 steps = 3.8190e-01, PNorm = 82.5358, GNorm = 0.2054
Meta loss on this task batch = 3.9337e-01, Meta loss averaged over last 500 steps = 3.8178e-01, PNorm = 82.5446, GNorm = 0.2204
Meta loss on this task batch = 3.4125e-01, Meta loss averaged over last 500 steps = 3.8170e-01, PNorm = 82.5539, GNorm = 0.1818
Meta loss on this task batch = 3.7807e-01, Meta loss averaged over last 500 steps = 3.8171e-01, PNorm = 82.5640, GNorm = 0.1937
Meta loss on this task batch = 4.3220e-01, Meta loss averaged over last 500 steps = 3.8184e-01, PNorm = 82.5745, GNorm = 0.1998
Meta loss on this task batch = 3.6627e-01, Meta loss averaged over last 500 steps = 3.8171e-01, PNorm = 82.5867, GNorm = 0.2079
Meta loss on this task batch = 3.8318e-01, Meta loss averaged over last 500 steps = 3.8178e-01, PNorm = 82.5986, GNorm = 0.1735
Meta loss on this task batch = 3.8910e-01, Meta loss averaged over last 500 steps = 3.8178e-01, PNorm = 82.6110, GNorm = 0.1637
Meta loss on this task batch = 3.4665e-01, Meta loss averaged over last 500 steps = 3.8173e-01, PNorm = 82.6231, GNorm = 0.1684
Meta loss on this task batch = 3.8058e-01, Meta loss averaged over last 500 steps = 3.8173e-01, PNorm = 82.6358, GNorm = 0.2202
Took 115.66980290412903 seconds to complete one epoch of meta training
Took 123.51707601547241 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468107
Epoch 143
Meta loss on this task batch = 3.9034e-01, Meta loss averaged over last 500 steps = 3.8176e-01, PNorm = 82.6475, GNorm = 0.1857
Meta loss on this task batch = 3.6449e-01, Meta loss averaged over last 500 steps = 3.8169e-01, PNorm = 82.6579, GNorm = 0.1753
Meta loss on this task batch = 3.3051e-01, Meta loss averaged over last 500 steps = 3.8156e-01, PNorm = 82.6679, GNorm = 0.1799
Meta loss on this task batch = 4.2088e-01, Meta loss averaged over last 500 steps = 3.8171e-01, PNorm = 82.6775, GNorm = 0.1794
Meta loss on this task batch = 3.8509e-01, Meta loss averaged over last 500 steps = 3.8179e-01, PNorm = 82.6879, GNorm = 0.1956
Meta loss on this task batch = 4.1252e-01, Meta loss averaged over last 500 steps = 3.8175e-01, PNorm = 82.6985, GNorm = 0.2392
Meta loss on this task batch = 3.9659e-01, Meta loss averaged over last 500 steps = 3.8161e-01, PNorm = 82.7091, GNorm = 0.2064
Meta loss on this task batch = 3.8851e-01, Meta loss averaged over last 500 steps = 3.8160e-01, PNorm = 82.7193, GNorm = 0.1916
Meta loss on this task batch = 3.5268e-01, Meta loss averaged over last 500 steps = 3.8144e-01, PNorm = 82.7312, GNorm = 0.1733
Meta loss on this task batch = 3.8065e-01, Meta loss averaged over last 500 steps = 3.8139e-01, PNorm = 82.7433, GNorm = 0.2252
Meta loss on this task batch = 3.5381e-01, Meta loss averaged over last 500 steps = 3.8131e-01, PNorm = 82.7555, GNorm = 0.2087
Meta loss on this task batch = 3.4534e-01, Meta loss averaged over last 500 steps = 3.8124e-01, PNorm = 82.7677, GNorm = 0.2122
Meta loss on this task batch = 3.0854e-01, Meta loss averaged over last 500 steps = 3.8112e-01, PNorm = 82.7797, GNorm = 0.1803
Meta loss on this task batch = 3.1513e-01, Meta loss averaged over last 500 steps = 3.8105e-01, PNorm = 82.7927, GNorm = 0.1842
Meta loss on this task batch = 3.8730e-01, Meta loss averaged over last 500 steps = 3.8106e-01, PNorm = 82.8064, GNorm = 0.1961
Meta loss on this task batch = 3.6569e-01, Meta loss averaged over last 500 steps = 3.8098e-01, PNorm = 82.8198, GNorm = 0.2073
Meta loss on this task batch = 4.5888e-01, Meta loss averaged over last 500 steps = 3.8119e-01, PNorm = 82.8315, GNorm = 0.1867
Meta loss on this task batch = 4.3365e-01, Meta loss averaged over last 500 steps = 3.8139e-01, PNorm = 82.8422, GNorm = 0.1931
Meta loss on this task batch = 3.8601e-01, Meta loss averaged over last 500 steps = 3.8147e-01, PNorm = 82.8526, GNorm = 0.2147
Took 112.81823897361755 seconds to complete one epoch of meta training
Took 120.30015563964844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468724
Epoch 144
Meta loss on this task batch = 3.8717e-01, Meta loss averaged over last 500 steps = 3.8146e-01, PNorm = 82.8640, GNorm = 0.2157
Meta loss on this task batch = 3.8518e-01, Meta loss averaged over last 500 steps = 3.8145e-01, PNorm = 82.8757, GNorm = 0.1649
Meta loss on this task batch = 3.3169e-01, Meta loss averaged over last 500 steps = 3.8109e-01, PNorm = 82.8887, GNorm = 0.2242
Meta loss on this task batch = 3.9965e-01, Meta loss averaged over last 500 steps = 3.8110e-01, PNorm = 82.9012, GNorm = 0.1729
Meta loss on this task batch = 3.6811e-01, Meta loss averaged over last 500 steps = 3.8107e-01, PNorm = 82.9142, GNorm = 0.1772
Meta loss on this task batch = 3.1961e-01, Meta loss averaged over last 500 steps = 3.8097e-01, PNorm = 82.9280, GNorm = 0.1921
Meta loss on this task batch = 3.8080e-01, Meta loss averaged over last 500 steps = 3.8100e-01, PNorm = 82.9407, GNorm = 0.2196
Meta loss on this task batch = 3.5684e-01, Meta loss averaged over last 500 steps = 3.8104e-01, PNorm = 82.9527, GNorm = 0.1985
Meta loss on this task batch = 3.9896e-01, Meta loss averaged over last 500 steps = 3.8115e-01, PNorm = 82.9634, GNorm = 0.2617
Meta loss on this task batch = 3.3707e-01, Meta loss averaged over last 500 steps = 3.8098e-01, PNorm = 82.9748, GNorm = 0.1671
Meta loss on this task batch = 4.2663e-01, Meta loss averaged over last 500 steps = 3.8105e-01, PNorm = 82.9850, GNorm = 0.2155
Meta loss on this task batch = 3.4559e-01, Meta loss averaged over last 500 steps = 3.8091e-01, PNorm = 82.9947, GNorm = 0.1974
Meta loss on this task batch = 3.9049e-01, Meta loss averaged over last 500 steps = 3.8103e-01, PNorm = 83.0041, GNorm = 0.2118
Meta loss on this task batch = 3.6738e-01, Meta loss averaged over last 500 steps = 3.8111e-01, PNorm = 83.0145, GNorm = 0.1861
Meta loss on this task batch = 3.9047e-01, Meta loss averaged over last 500 steps = 3.8089e-01, PNorm = 83.0257, GNorm = 0.1966
Meta loss on this task batch = 3.7681e-01, Meta loss averaged over last 500 steps = 3.8098e-01, PNorm = 83.0386, GNorm = 0.2233
Meta loss on this task batch = 3.6859e-01, Meta loss averaged over last 500 steps = 3.8088e-01, PNorm = 83.0511, GNorm = 0.1758
Meta loss on this task batch = 4.0824e-01, Meta loss averaged over last 500 steps = 3.8087e-01, PNorm = 83.0633, GNorm = 0.2323
Meta loss on this task batch = 3.8082e-01, Meta loss averaged over last 500 steps = 3.8079e-01, PNorm = 83.0757, GNorm = 0.2081
Took 116.24499487876892 seconds to complete one epoch of meta training
Took 123.97607111930847 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481130
Epoch 145
Meta loss on this task batch = 3.9619e-01, Meta loss averaged over last 500 steps = 3.8072e-01, PNorm = 83.0875, GNorm = 0.2115
Meta loss on this task batch = 4.5595e-01, Meta loss averaged over last 500 steps = 3.8086e-01, PNorm = 83.0971, GNorm = 0.2455
Meta loss on this task batch = 3.2900e-01, Meta loss averaged over last 500 steps = 3.8081e-01, PNorm = 83.1078, GNorm = 0.1888
Meta loss on this task batch = 3.6131e-01, Meta loss averaged over last 500 steps = 3.8078e-01, PNorm = 83.1196, GNorm = 0.2125
Meta loss on this task batch = 3.6203e-01, Meta loss averaged over last 500 steps = 3.8070e-01, PNorm = 83.1312, GNorm = 0.2019
Meta loss on this task batch = 3.8938e-01, Meta loss averaged over last 500 steps = 3.8063e-01, PNorm = 83.1424, GNorm = 0.2108
Meta loss on this task batch = 3.4524e-01, Meta loss averaged over last 500 steps = 3.8053e-01, PNorm = 83.1543, GNorm = 0.1776
Meta loss on this task batch = 3.1431e-01, Meta loss averaged over last 500 steps = 3.8042e-01, PNorm = 83.1669, GNorm = 0.1962
Meta loss on this task batch = 3.9927e-01, Meta loss averaged over last 500 steps = 3.8051e-01, PNorm = 83.1794, GNorm = 0.2445
Meta loss on this task batch = 3.8255e-01, Meta loss averaged over last 500 steps = 3.8048e-01, PNorm = 83.1917, GNorm = 0.1976
Meta loss on this task batch = 3.4306e-01, Meta loss averaged over last 500 steps = 3.8040e-01, PNorm = 83.2040, GNorm = 0.1799
Meta loss on this task batch = 3.4780e-01, Meta loss averaged over last 500 steps = 3.8019e-01, PNorm = 83.2157, GNorm = 0.1999
Meta loss on this task batch = 4.0026e-01, Meta loss averaged over last 500 steps = 3.8019e-01, PNorm = 83.2261, GNorm = 0.2077
Meta loss on this task batch = 4.0382e-01, Meta loss averaged over last 500 steps = 3.8024e-01, PNorm = 83.2364, GNorm = 0.2437
Meta loss on this task batch = 3.7046e-01, Meta loss averaged over last 500 steps = 3.8019e-01, PNorm = 83.2473, GNorm = 0.2237
Meta loss on this task batch = 3.7074e-01, Meta loss averaged over last 500 steps = 3.8018e-01, PNorm = 83.2586, GNorm = 0.1898
Meta loss on this task batch = 3.6071e-01, Meta loss averaged over last 500 steps = 3.8018e-01, PNorm = 83.2709, GNorm = 0.1900
Meta loss on this task batch = 3.0455e-01, Meta loss averaged over last 500 steps = 3.7994e-01, PNorm = 83.2835, GNorm = 0.1706
Meta loss on this task batch = 3.1115e-01, Meta loss averaged over last 500 steps = 3.7968e-01, PNorm = 83.2964, GNorm = 0.1789
Took 115.52499604225159 seconds to complete one epoch of meta training
Took 122.91634607315063 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472864
Epoch 146
Meta loss on this task batch = 3.8122e-01, Meta loss averaged over last 500 steps = 3.7978e-01, PNorm = 83.3089, GNorm = 0.1776
Meta loss on this task batch = 4.4600e-01, Meta loss averaged over last 500 steps = 3.7979e-01, PNorm = 83.3206, GNorm = 0.2093
Meta loss on this task batch = 3.2447e-01, Meta loss averaged over last 500 steps = 3.7977e-01, PNorm = 83.3317, GNorm = 0.1852
Meta loss on this task batch = 4.2093e-01, Meta loss averaged over last 500 steps = 3.7989e-01, PNorm = 83.3434, GNorm = 0.2300
Meta loss on this task batch = 3.1396e-01, Meta loss averaged over last 500 steps = 3.7986e-01, PNorm = 83.3555, GNorm = 0.1558
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 3.7978e-01, PNorm = 83.3677, GNorm = 0.1768
Meta loss on this task batch = 4.1420e-01, Meta loss averaged over last 500 steps = 3.7980e-01, PNorm = 83.3801, GNorm = 0.2187
Meta loss on this task batch = 3.4852e-01, Meta loss averaged over last 500 steps = 3.7970e-01, PNorm = 83.3933, GNorm = 0.1833
Meta loss on this task batch = 3.8716e-01, Meta loss averaged over last 500 steps = 3.7976e-01, PNorm = 83.4059, GNorm = 0.1949
Meta loss on this task batch = 3.0422e-01, Meta loss averaged over last 500 steps = 3.7970e-01, PNorm = 83.4186, GNorm = 0.2076
Meta loss on this task batch = 3.6466e-01, Meta loss averaged over last 500 steps = 3.7969e-01, PNorm = 83.4316, GNorm = 0.1858
Meta loss on this task batch = 3.8663e-01, Meta loss averaged over last 500 steps = 3.7974e-01, PNorm = 83.4421, GNorm = 0.2363
Meta loss on this task batch = 3.6610e-01, Meta loss averaged over last 500 steps = 3.7969e-01, PNorm = 83.4525, GNorm = 0.2264
Meta loss on this task batch = 3.6846e-01, Meta loss averaged over last 500 steps = 3.7956e-01, PNorm = 83.4630, GNorm = 0.1704
Meta loss on this task batch = 3.7537e-01, Meta loss averaged over last 500 steps = 3.7943e-01, PNorm = 83.4740, GNorm = 0.1987
Meta loss on this task batch = 4.1410e-01, Meta loss averaged over last 500 steps = 3.7941e-01, PNorm = 83.4849, GNorm = 0.1974
Meta loss on this task batch = 3.4633e-01, Meta loss averaged over last 500 steps = 3.7929e-01, PNorm = 83.4964, GNorm = 0.1834
Meta loss on this task batch = 3.7604e-01, Meta loss averaged over last 500 steps = 3.7938e-01, PNorm = 83.5085, GNorm = 0.1885
Meta loss on this task batch = 4.2051e-01, Meta loss averaged over last 500 steps = 3.7953e-01, PNorm = 83.5220, GNorm = 0.2777
Took 114.55321025848389 seconds to complete one epoch of meta training
Took 122.3136887550354 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475654
Epoch 147
Meta loss on this task batch = 3.8090e-01, Meta loss averaged over last 500 steps = 3.7949e-01, PNorm = 83.5355, GNorm = 0.2195
Meta loss on this task batch = 3.4534e-01, Meta loss averaged over last 500 steps = 3.7944e-01, PNorm = 83.5496, GNorm = 0.1825
Meta loss on this task batch = 3.8281e-01, Meta loss averaged over last 500 steps = 3.7936e-01, PNorm = 83.5633, GNorm = 0.2223
Meta loss on this task batch = 3.3755e-01, Meta loss averaged over last 500 steps = 3.7928e-01, PNorm = 83.5769, GNorm = 0.1936
Meta loss on this task batch = 3.9062e-01, Meta loss averaged over last 500 steps = 3.7918e-01, PNorm = 83.5886, GNorm = 0.2355
Meta loss on this task batch = 3.5631e-01, Meta loss averaged over last 500 steps = 3.7915e-01, PNorm = 83.5995, GNorm = 0.2058
Meta loss on this task batch = 4.5503e-01, Meta loss averaged over last 500 steps = 3.7935e-01, PNorm = 83.6086, GNorm = 0.2346
Meta loss on this task batch = 3.8532e-01, Meta loss averaged over last 500 steps = 3.7928e-01, PNorm = 83.6173, GNorm = 0.2029
Meta loss on this task batch = 3.7896e-01, Meta loss averaged over last 500 steps = 3.7921e-01, PNorm = 83.6262, GNorm = 0.2155
Meta loss on this task batch = 3.6753e-01, Meta loss averaged over last 500 steps = 3.7919e-01, PNorm = 83.6352, GNorm = 0.2040
Meta loss on this task batch = 3.4529e-01, Meta loss averaged over last 500 steps = 3.7905e-01, PNorm = 83.6448, GNorm = 0.1940
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 3.7899e-01, PNorm = 83.6553, GNorm = 0.1791
Meta loss on this task batch = 3.2864e-01, Meta loss averaged over last 500 steps = 3.7905e-01, PNorm = 83.6672, GNorm = 0.1658
Meta loss on this task batch = 3.8279e-01, Meta loss averaged over last 500 steps = 3.7914e-01, PNorm = 83.6796, GNorm = 0.1898
Meta loss on this task batch = 3.4182e-01, Meta loss averaged over last 500 steps = 3.7909e-01, PNorm = 83.6914, GNorm = 0.1465
Meta loss on this task batch = 3.9043e-01, Meta loss averaged over last 500 steps = 3.7907e-01, PNorm = 83.7014, GNorm = 0.2412
Meta loss on this task batch = 4.2501e-01, Meta loss averaged over last 500 steps = 3.7916e-01, PNorm = 83.7107, GNorm = 0.2165
Meta loss on this task batch = 3.8220e-01, Meta loss averaged over last 500 steps = 3.7904e-01, PNorm = 83.7199, GNorm = 0.2089
Meta loss on this task batch = 3.4807e-01, Meta loss averaged over last 500 steps = 3.7889e-01, PNorm = 83.7294, GNorm = 0.2216
Took 114.46555852890015 seconds to complete one epoch of meta training
Took 122.06263518333435 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485031
Epoch 148
Meta loss on this task batch = 4.2264e-01, Meta loss averaged over last 500 steps = 3.7889e-01, PNorm = 83.7389, GNorm = 0.2214
Meta loss on this task batch = 3.2739e-01, Meta loss averaged over last 500 steps = 3.7882e-01, PNorm = 83.7497, GNorm = 0.2105
Meta loss on this task batch = 3.8669e-01, Meta loss averaged over last 500 steps = 3.7871e-01, PNorm = 83.7616, GNorm = 0.1836
Meta loss on this task batch = 3.5398e-01, Meta loss averaged over last 500 steps = 3.7858e-01, PNorm = 83.7748, GNorm = 0.2009
Meta loss on this task batch = 3.2416e-01, Meta loss averaged over last 500 steps = 3.7863e-01, PNorm = 83.7887, GNorm = 0.1870
Meta loss on this task batch = 3.5302e-01, Meta loss averaged over last 500 steps = 3.7857e-01, PNorm = 83.8019, GNorm = 0.2256
Meta loss on this task batch = 3.9189e-01, Meta loss averaged over last 500 steps = 3.7856e-01, PNorm = 83.8144, GNorm = 0.2272
Meta loss on this task batch = 4.2081e-01, Meta loss averaged over last 500 steps = 3.7854e-01, PNorm = 83.8253, GNorm = 0.2364
Meta loss on this task batch = 3.7557e-01, Meta loss averaged over last 500 steps = 3.7857e-01, PNorm = 83.8367, GNorm = 0.1970
Meta loss on this task batch = 3.6507e-01, Meta loss averaged over last 500 steps = 3.7857e-01, PNorm = 83.8465, GNorm = 0.2007
Meta loss on this task batch = 3.9445e-01, Meta loss averaged over last 500 steps = 3.7865e-01, PNorm = 83.8555, GNorm = 0.2107
Meta loss on this task batch = 3.2180e-01, Meta loss averaged over last 500 steps = 3.7848e-01, PNorm = 83.8658, GNorm = 0.2068
Meta loss on this task batch = 3.3736e-01, Meta loss averaged over last 500 steps = 3.7833e-01, PNorm = 83.8762, GNorm = 0.1894
Meta loss on this task batch = 4.1854e-01, Meta loss averaged over last 500 steps = 3.7836e-01, PNorm = 83.8856, GNorm = 0.2736
Meta loss on this task batch = 3.7151e-01, Meta loss averaged over last 500 steps = 3.7834e-01, PNorm = 83.8959, GNorm = 0.2077
Meta loss on this task batch = 3.9223e-01, Meta loss averaged over last 500 steps = 3.7833e-01, PNorm = 83.9062, GNorm = 0.1989
Meta loss on this task batch = 3.3337e-01, Meta loss averaged over last 500 steps = 3.7826e-01, PNorm = 83.9169, GNorm = 0.1847
Meta loss on this task batch = 3.6711e-01, Meta loss averaged over last 500 steps = 3.7829e-01, PNorm = 83.9273, GNorm = 0.1719
Meta loss on this task batch = 4.0978e-01, Meta loss averaged over last 500 steps = 3.7829e-01, PNorm = 83.9369, GNorm = 0.2320
Took 113.91219449043274 seconds to complete one epoch of meta training
Took 121.48881840705872 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467050
Epoch 149
Meta loss on this task batch = 3.8121e-01, Meta loss averaged over last 500 steps = 3.7840e-01, PNorm = 83.9472, GNorm = 0.1939
Meta loss on this task batch = 3.6621e-01, Meta loss averaged over last 500 steps = 3.7825e-01, PNorm = 83.9580, GNorm = 0.2375
Meta loss on this task batch = 3.8057e-01, Meta loss averaged over last 500 steps = 3.7830e-01, PNorm = 83.9691, GNorm = 0.1839
Meta loss on this task batch = 3.7398e-01, Meta loss averaged over last 500 steps = 3.7822e-01, PNorm = 83.9806, GNorm = 0.1579
Meta loss on this task batch = 4.1182e-01, Meta loss averaged over last 500 steps = 3.7833e-01, PNorm = 83.9919, GNorm = 0.1977
Meta loss on this task batch = 3.6720e-01, Meta loss averaged over last 500 steps = 3.7820e-01, PNorm = 84.0045, GNorm = 0.1902
Meta loss on this task batch = 3.2037e-01, Meta loss averaged over last 500 steps = 3.7800e-01, PNorm = 84.0170, GNorm = 0.1758
Meta loss on this task batch = 3.7263e-01, Meta loss averaged over last 500 steps = 3.7810e-01, PNorm = 84.0300, GNorm = 0.2168
Meta loss on this task batch = 3.6238e-01, Meta loss averaged over last 500 steps = 3.7799e-01, PNorm = 84.0438, GNorm = 0.1937
Meta loss on this task batch = 3.9910e-01, Meta loss averaged over last 500 steps = 3.7796e-01, PNorm = 84.0565, GNorm = 0.2078
Meta loss on this task batch = 3.7397e-01, Meta loss averaged over last 500 steps = 3.7800e-01, PNorm = 84.0674, GNorm = 0.2196
Meta loss on this task batch = 3.8409e-01, Meta loss averaged over last 500 steps = 3.7796e-01, PNorm = 84.0777, GNorm = 0.2085
Meta loss on this task batch = 3.4400e-01, Meta loss averaged over last 500 steps = 3.7786e-01, PNorm = 84.0882, GNorm = 0.2120
Meta loss on this task batch = 3.7516e-01, Meta loss averaged over last 500 steps = 3.7787e-01, PNorm = 84.0982, GNorm = 0.2125
Meta loss on this task batch = 4.1543e-01, Meta loss averaged over last 500 steps = 3.7787e-01, PNorm = 84.1074, GNorm = 0.2132
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 3.7770e-01, PNorm = 84.1178, GNorm = 0.2107
Meta loss on this task batch = 3.2500e-01, Meta loss averaged over last 500 steps = 3.7776e-01, PNorm = 84.1278, GNorm = 0.1747
Meta loss on this task batch = 3.9707e-01, Meta loss averaged over last 500 steps = 3.7780e-01, PNorm = 84.1365, GNorm = 0.2725
Meta loss on this task batch = 3.7218e-01, Meta loss averaged over last 500 steps = 3.7789e-01, PNorm = 84.1454, GNorm = 0.2178
Took 117.78312563896179 seconds to complete one epoch of meta training
Took 126.04724192619324 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.457581
Epoch 150
Meta loss on this task batch = 3.5265e-01, Meta loss averaged over last 500 steps = 3.7770e-01, PNorm = 84.1552, GNorm = 0.1636
Meta loss on this task batch = 3.7746e-01, Meta loss averaged over last 500 steps = 3.7759e-01, PNorm = 84.1643, GNorm = 0.1679
Meta loss on this task batch = 4.0545e-01, Meta loss averaged over last 500 steps = 3.7763e-01, PNorm = 84.1742, GNorm = 0.2094
Meta loss on this task batch = 4.0930e-01, Meta loss averaged over last 500 steps = 3.7772e-01, PNorm = 84.1850, GNorm = 0.1931
Meta loss on this task batch = 2.9317e-01, Meta loss averaged over last 500 steps = 3.7745e-01, PNorm = 84.1974, GNorm = 0.1927
Meta loss on this task batch = 4.0837e-01, Meta loss averaged over last 500 steps = 3.7744e-01, PNorm = 84.2098, GNorm = 0.1878
Meta loss on this task batch = 3.2106e-01, Meta loss averaged over last 500 steps = 3.7732e-01, PNorm = 84.2232, GNorm = 0.1845
Meta loss on this task batch = 4.0734e-01, Meta loss averaged over last 500 steps = 3.7741e-01, PNorm = 84.2359, GNorm = 0.2246
Meta loss on this task batch = 4.0148e-01, Meta loss averaged over last 500 steps = 3.7748e-01, PNorm = 84.2478, GNorm = 0.2039
Meta loss on this task batch = 4.2254e-01, Meta loss averaged over last 500 steps = 3.7748e-01, PNorm = 84.2584, GNorm = 0.2446
Meta loss on this task batch = 3.8277e-01, Meta loss averaged over last 500 steps = 3.7746e-01, PNorm = 84.2688, GNorm = 0.1974
Meta loss on this task batch = 3.1209e-01, Meta loss averaged over last 500 steps = 3.7726e-01, PNorm = 84.2796, GNorm = 0.2138
Meta loss on this task batch = 3.7692e-01, Meta loss averaged over last 500 steps = 3.7730e-01, PNorm = 84.2900, GNorm = 0.2154
Meta loss on this task batch = 4.2588e-01, Meta loss averaged over last 500 steps = 3.7740e-01, PNorm = 84.2988, GNorm = 0.2420
Meta loss on this task batch = 2.9398e-01, Meta loss averaged over last 500 steps = 3.7734e-01, PNorm = 84.3088, GNorm = 0.1656
Meta loss on this task batch = 3.5830e-01, Meta loss averaged over last 500 steps = 3.7721e-01, PNorm = 84.3194, GNorm = 0.2019
Meta loss on this task batch = 3.6722e-01, Meta loss averaged over last 500 steps = 3.7725e-01, PNorm = 84.3291, GNorm = 0.2346
Meta loss on this task batch = 3.7736e-01, Meta loss averaged over last 500 steps = 3.7733e-01, PNorm = 84.3375, GNorm = 0.2585
Meta loss on this task batch = 3.7617e-01, Meta loss averaged over last 500 steps = 3.7718e-01, PNorm = 84.3462, GNorm = 0.2121
Took 115.50514483451843 seconds to complete one epoch of meta training
Took 123.4709620475769 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491419
Epoch 151
Meta loss on this task batch = 3.7522e-01, Meta loss averaged over last 500 steps = 3.7706e-01, PNorm = 84.3554, GNorm = 0.1830
Meta loss on this task batch = 4.0576e-01, Meta loss averaged over last 500 steps = 3.7712e-01, PNorm = 84.3649, GNorm = 0.2098
Meta loss on this task batch = 4.1137e-01, Meta loss averaged over last 500 steps = 3.7714e-01, PNorm = 84.3734, GNorm = 0.2427
Meta loss on this task batch = 3.7265e-01, Meta loss averaged over last 500 steps = 3.7712e-01, PNorm = 84.3839, GNorm = 0.1669
Meta loss on this task batch = 3.4935e-01, Meta loss averaged over last 500 steps = 3.7699e-01, PNorm = 84.3957, GNorm = 0.1880
Meta loss on this task batch = 3.2107e-01, Meta loss averaged over last 500 steps = 3.7677e-01, PNorm = 84.4082, GNorm = 0.2056
Meta loss on this task batch = 3.7318e-01, Meta loss averaged over last 500 steps = 3.7665e-01, PNorm = 84.4212, GNorm = 0.1715
Meta loss on this task batch = 3.5269e-01, Meta loss averaged over last 500 steps = 3.7649e-01, PNorm = 84.4327, GNorm = 0.1901
Meta loss on this task batch = 3.5541e-01, Meta loss averaged over last 500 steps = 3.7639e-01, PNorm = 84.4452, GNorm = 0.2046
Meta loss on this task batch = 3.9006e-01, Meta loss averaged over last 500 steps = 3.7639e-01, PNorm = 84.4564, GNorm = 0.2040
Meta loss on this task batch = 3.5743e-01, Meta loss averaged over last 500 steps = 3.7611e-01, PNorm = 84.4687, GNorm = 0.1811
Meta loss on this task batch = 3.5760e-01, Meta loss averaged over last 500 steps = 3.7618e-01, PNorm = 84.4809, GNorm = 0.1856
Meta loss on this task batch = 3.4238e-01, Meta loss averaged over last 500 steps = 3.7609e-01, PNorm = 84.4945, GNorm = 0.2112
Meta loss on this task batch = 3.7676e-01, Meta loss averaged over last 500 steps = 3.7608e-01, PNorm = 84.5084, GNorm = 0.2052
Meta loss on this task batch = 3.4271e-01, Meta loss averaged over last 500 steps = 3.7599e-01, PNorm = 84.5215, GNorm = 0.1855
Meta loss on this task batch = 4.4269e-01, Meta loss averaged over last 500 steps = 3.7603e-01, PNorm = 84.5335, GNorm = 0.2650
Meta loss on this task batch = 3.7710e-01, Meta loss averaged over last 500 steps = 3.7608e-01, PNorm = 84.5454, GNorm = 0.2288
Meta loss on this task batch = 3.3797e-01, Meta loss averaged over last 500 steps = 3.7604e-01, PNorm = 84.5571, GNorm = 0.1989
Meta loss on this task batch = 3.6605e-01, Meta loss averaged over last 500 steps = 3.7609e-01, PNorm = 84.5677, GNorm = 0.2496
Took 120.3598313331604 seconds to complete one epoch of meta training
Took 128.34520077705383 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494883
Epoch 152
Meta loss on this task batch = 3.2558e-01, Meta loss averaged over last 500 steps = 3.7597e-01, PNorm = 84.5793, GNorm = 0.1612
Meta loss on this task batch = 3.8756e-01, Meta loss averaged over last 500 steps = 3.7611e-01, PNorm = 84.5905, GNorm = 0.2039
Meta loss on this task batch = 3.4958e-01, Meta loss averaged over last 500 steps = 3.7612e-01, PNorm = 84.6019, GNorm = 0.2050
Meta loss on this task batch = 4.6039e-01, Meta loss averaged over last 500 steps = 3.7635e-01, PNorm = 84.6123, GNorm = 0.3262
Meta loss on this task batch = 3.4582e-01, Meta loss averaged over last 500 steps = 3.7624e-01, PNorm = 84.6236, GNorm = 0.2416
Meta loss on this task batch = 4.1065e-01, Meta loss averaged over last 500 steps = 3.7622e-01, PNorm = 84.6340, GNorm = 0.2174
Meta loss on this task batch = 3.9466e-01, Meta loss averaged over last 500 steps = 3.7622e-01, PNorm = 84.6448, GNorm = 0.2018
Meta loss on this task batch = 3.3101e-01, Meta loss averaged over last 500 steps = 3.7619e-01, PNorm = 84.6558, GNorm = 0.1770
Meta loss on this task batch = 3.6393e-01, Meta loss averaged over last 500 steps = 3.7616e-01, PNorm = 84.6674, GNorm = 0.1865
Meta loss on this task batch = 4.1397e-01, Meta loss averaged over last 500 steps = 3.7626e-01, PNorm = 84.6791, GNorm = 0.2900
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.7609e-01, PNorm = 84.6911, GNorm = 0.2110
Meta loss on this task batch = 4.1557e-01, Meta loss averaged over last 500 steps = 3.7609e-01, PNorm = 84.7013, GNorm = 0.2183
Meta loss on this task batch = 3.8472e-01, Meta loss averaged over last 500 steps = 3.7604e-01, PNorm = 84.7123, GNorm = 0.2295
Meta loss on this task batch = 3.8007e-01, Meta loss averaged over last 500 steps = 3.7600e-01, PNorm = 84.7229, GNorm = 0.1884
Meta loss on this task batch = 3.9599e-01, Meta loss averaged over last 500 steps = 3.7609e-01, PNorm = 84.7323, GNorm = 0.2138
Meta loss on this task batch = 3.2294e-01, Meta loss averaged over last 500 steps = 3.7607e-01, PNorm = 84.7424, GNorm = 0.1859
Meta loss on this task batch = 4.0174e-01, Meta loss averaged over last 500 steps = 3.7617e-01, PNorm = 84.7522, GNorm = 0.1899
Meta loss on this task batch = 2.7802e-01, Meta loss averaged over last 500 steps = 3.7596e-01, PNorm = 84.7636, GNorm = 0.1480
Meta loss on this task batch = 4.0979e-01, Meta loss averaged over last 500 steps = 3.7612e-01, PNorm = 84.7739, GNorm = 0.2347
Took 118.14467191696167 seconds to complete one epoch of meta training
Took 125.63831806182861 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480906
Epoch 153
Meta loss on this task batch = 3.4134e-01, Meta loss averaged over last 500 steps = 3.7594e-01, PNorm = 84.7847, GNorm = 0.1865
Meta loss on this task batch = 3.4057e-01, Meta loss averaged over last 500 steps = 3.7586e-01, PNorm = 84.7964, GNorm = 0.1794
Meta loss on this task batch = 3.6983e-01, Meta loss averaged over last 500 steps = 3.7579e-01, PNorm = 84.8078, GNorm = 0.2074
Meta loss on this task batch = 4.0840e-01, Meta loss averaged over last 500 steps = 3.7584e-01, PNorm = 84.8178, GNorm = 0.2041
Meta loss on this task batch = 3.5446e-01, Meta loss averaged over last 500 steps = 3.7569e-01, PNorm = 84.8283, GNorm = 0.1887
Meta loss on this task batch = 3.4147e-01, Meta loss averaged over last 500 steps = 3.7563e-01, PNorm = 84.8397, GNorm = 0.1892
Meta loss on this task batch = 3.6857e-01, Meta loss averaged over last 500 steps = 3.7564e-01, PNorm = 84.8520, GNorm = 0.1825
Meta loss on this task batch = 3.9760e-01, Meta loss averaged over last 500 steps = 3.7557e-01, PNorm = 84.8635, GNorm = 0.2425
Meta loss on this task batch = 3.6899e-01, Meta loss averaged over last 500 steps = 3.7547e-01, PNorm = 84.8735, GNorm = 0.2479
Meta loss on this task batch = 3.5056e-01, Meta loss averaged over last 500 steps = 3.7537e-01, PNorm = 84.8835, GNorm = 0.1829
Meta loss on this task batch = 4.2233e-01, Meta loss averaged over last 500 steps = 3.7544e-01, PNorm = 84.8940, GNorm = 0.2259
Meta loss on this task batch = 3.2863e-01, Meta loss averaged over last 500 steps = 3.7524e-01, PNorm = 84.9049, GNorm = 0.1915
Meta loss on this task batch = 3.7434e-01, Meta loss averaged over last 500 steps = 3.7526e-01, PNorm = 84.9168, GNorm = 0.2198
Meta loss on this task batch = 4.0444e-01, Meta loss averaged over last 500 steps = 3.7515e-01, PNorm = 84.9288, GNorm = 0.2056
Meta loss on this task batch = 3.6383e-01, Meta loss averaged over last 500 steps = 3.7519e-01, PNorm = 84.9412, GNorm = 0.1817
Meta loss on this task batch = 3.9882e-01, Meta loss averaged over last 500 steps = 3.7521e-01, PNorm = 84.9538, GNorm = 0.2005
Meta loss on this task batch = 3.2895e-01, Meta loss averaged over last 500 steps = 3.7522e-01, PNorm = 84.9670, GNorm = 0.1828
Meta loss on this task batch = 3.6873e-01, Meta loss averaged over last 500 steps = 3.7522e-01, PNorm = 84.9801, GNorm = 0.1867
Meta loss on this task batch = 3.8830e-01, Meta loss averaged over last 500 steps = 3.7525e-01, PNorm = 84.9927, GNorm = 0.2594
Took 116.21750855445862 seconds to complete one epoch of meta training
Took 124.1331160068512 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495675
Epoch 154
Meta loss on this task batch = 3.1892e-01, Meta loss averaged over last 500 steps = 3.7519e-01, PNorm = 85.0056, GNorm = 0.1709
Meta loss on this task batch = 3.7467e-01, Meta loss averaged over last 500 steps = 3.7511e-01, PNorm = 85.0175, GNorm = 0.1739
Meta loss on this task batch = 3.9968e-01, Meta loss averaged over last 500 steps = 3.7512e-01, PNorm = 85.0293, GNorm = 0.1947
Meta loss on this task batch = 3.7033e-01, Meta loss averaged over last 500 steps = 3.7531e-01, PNorm = 85.0419, GNorm = 0.1842
Meta loss on this task batch = 3.6079e-01, Meta loss averaged over last 500 steps = 3.7532e-01, PNorm = 85.0547, GNorm = 0.1923
Meta loss on this task batch = 3.7891e-01, Meta loss averaged over last 500 steps = 3.7535e-01, PNorm = 85.0672, GNorm = 0.1708
Meta loss on this task batch = 3.3539e-01, Meta loss averaged over last 500 steps = 3.7518e-01, PNorm = 85.0794, GNorm = 0.1914
Meta loss on this task batch = 3.9516e-01, Meta loss averaged over last 500 steps = 3.7528e-01, PNorm = 85.0912, GNorm = 0.2074
Meta loss on this task batch = 4.3015e-01, Meta loss averaged over last 500 steps = 3.7543e-01, PNorm = 85.1026, GNorm = 0.1993
Meta loss on this task batch = 3.4737e-01, Meta loss averaged over last 500 steps = 3.7529e-01, PNorm = 85.1135, GNorm = 0.1735
Meta loss on this task batch = 3.4616e-01, Meta loss averaged over last 500 steps = 3.7524e-01, PNorm = 85.1234, GNorm = 0.1908
Meta loss on this task batch = 4.1085e-01, Meta loss averaged over last 500 steps = 3.7538e-01, PNorm = 85.1334, GNorm = 0.1991
Meta loss on this task batch = 3.2633e-01, Meta loss averaged over last 500 steps = 3.7529e-01, PNorm = 85.1443, GNorm = 0.1990
Meta loss on this task batch = 3.4364e-01, Meta loss averaged over last 500 steps = 3.7512e-01, PNorm = 85.1561, GNorm = 0.1766
Meta loss on this task batch = 2.7773e-01, Meta loss averaged over last 500 steps = 3.7487e-01, PNorm = 85.1691, GNorm = 0.1834
Meta loss on this task batch = 4.2045e-01, Meta loss averaged over last 500 steps = 3.7496e-01, PNorm = 85.1803, GNorm = 0.2199
Meta loss on this task batch = 4.0524e-01, Meta loss averaged over last 500 steps = 3.7499e-01, PNorm = 85.1911, GNorm = 0.2104
Meta loss on this task batch = 3.9723e-01, Meta loss averaged over last 500 steps = 3.7504e-01, PNorm = 85.2013, GNorm = 0.2133
Meta loss on this task batch = 3.8249e-01, Meta loss averaged over last 500 steps = 3.7502e-01, PNorm = 85.2119, GNorm = 0.2391
Took 117.9148063659668 seconds to complete one epoch of meta training
Took 126.35857367515564 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488090
Epoch 155
Meta loss on this task batch = 3.7042e-01, Meta loss averaged over last 500 steps = 3.7498e-01, PNorm = 85.2221, GNorm = 0.1996
Meta loss on this task batch = 3.5517e-01, Meta loss averaged over last 500 steps = 3.7492e-01, PNorm = 85.2333, GNorm = 0.1708
Meta loss on this task batch = 3.7400e-01, Meta loss averaged over last 500 steps = 3.7496e-01, PNorm = 85.2448, GNorm = 0.2038
Meta loss on this task batch = 3.9746e-01, Meta loss averaged over last 500 steps = 3.7497e-01, PNorm = 85.2560, GNorm = 0.2487
Meta loss on this task batch = 3.3409e-01, Meta loss averaged over last 500 steps = 3.7482e-01, PNorm = 85.2680, GNorm = 0.1750
Meta loss on this task batch = 3.6526e-01, Meta loss averaged over last 500 steps = 3.7478e-01, PNorm = 85.2787, GNorm = 0.2201
Meta loss on this task batch = 3.7772e-01, Meta loss averaged over last 500 steps = 3.7469e-01, PNorm = 85.2877, GNorm = 0.2154
Meta loss on this task batch = 3.7665e-01, Meta loss averaged over last 500 steps = 3.7473e-01, PNorm = 85.2973, GNorm = 0.1786
Meta loss on this task batch = 3.3364e-01, Meta loss averaged over last 500 steps = 3.7465e-01, PNorm = 85.3076, GNorm = 0.1756
Meta loss on this task batch = 2.9636e-01, Meta loss averaged over last 500 steps = 3.7454e-01, PNorm = 85.3185, GNorm = 0.1912
Meta loss on this task batch = 3.4333e-01, Meta loss averaged over last 500 steps = 3.7454e-01, PNorm = 85.3285, GNorm = 0.1781
Meta loss on this task batch = 3.9984e-01, Meta loss averaged over last 500 steps = 3.7454e-01, PNorm = 85.3388, GNorm = 0.2245
Meta loss on this task batch = 3.3029e-01, Meta loss averaged over last 500 steps = 3.7439e-01, PNorm = 85.3496, GNorm = 0.1862
Meta loss on this task batch = 4.3282e-01, Meta loss averaged over last 500 steps = 3.7447e-01, PNorm = 85.3595, GNorm = 0.2291
Meta loss on this task batch = 3.8545e-01, Meta loss averaged over last 500 steps = 3.7442e-01, PNorm = 85.3693, GNorm = 0.1823
Meta loss on this task batch = 4.0647e-01, Meta loss averaged over last 500 steps = 3.7442e-01, PNorm = 85.3785, GNorm = 0.2259
Meta loss on this task batch = 3.6934e-01, Meta loss averaged over last 500 steps = 3.7434e-01, PNorm = 85.3876, GNorm = 0.2063
Meta loss on this task batch = 3.5435e-01, Meta loss averaged over last 500 steps = 3.7445e-01, PNorm = 85.3969, GNorm = 0.2323
Meta loss on this task batch = 3.2765e-01, Meta loss averaged over last 500 steps = 3.7443e-01, PNorm = 85.4063, GNorm = 0.2066
Took 118.48954391479492 seconds to complete one epoch of meta training
Took 126.43758296966553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471788
Epoch 156
Meta loss on this task batch = 4.2341e-01, Meta loss averaged over last 500 steps = 3.7455e-01, PNorm = 85.4146, GNorm = 0.2832
Meta loss on this task batch = 3.3868e-01, Meta loss averaged over last 500 steps = 3.7448e-01, PNorm = 85.4242, GNorm = 0.1942
Meta loss on this task batch = 2.9282e-01, Meta loss averaged over last 500 steps = 3.7432e-01, PNorm = 85.4346, GNorm = 0.1639
Meta loss on this task batch = 3.3589e-01, Meta loss averaged over last 500 steps = 3.7413e-01, PNorm = 85.4449, GNorm = 0.1587
Meta loss on this task batch = 3.6666e-01, Meta loss averaged over last 500 steps = 3.7406e-01, PNorm = 85.4555, GNorm = 0.2181
Meta loss on this task batch = 4.0236e-01, Meta loss averaged over last 500 steps = 3.7427e-01, PNorm = 85.4658, GNorm = 0.2222
Meta loss on this task batch = 3.5233e-01, Meta loss averaged over last 500 steps = 3.7415e-01, PNorm = 85.4769, GNorm = 0.2049
Meta loss on this task batch = 3.8916e-01, Meta loss averaged over last 500 steps = 3.7403e-01, PNorm = 85.4872, GNorm = 0.2069
Meta loss on this task batch = 3.5777e-01, Meta loss averaged over last 500 steps = 3.7405e-01, PNorm = 85.4972, GNorm = 0.2339
Meta loss on this task batch = 3.7737e-01, Meta loss averaged over last 500 steps = 3.7414e-01, PNorm = 85.5071, GNorm = 0.2130
Meta loss on this task batch = 3.9879e-01, Meta loss averaged over last 500 steps = 3.7419e-01, PNorm = 85.5170, GNorm = 0.2308
Meta loss on this task batch = 4.2095e-01, Meta loss averaged over last 500 steps = 3.7429e-01, PNorm = 85.5263, GNorm = 0.2017
Meta loss on this task batch = 3.2410e-01, Meta loss averaged over last 500 steps = 3.7423e-01, PNorm = 85.5359, GNorm = 0.1791
Meta loss on this task batch = 4.2032e-01, Meta loss averaged over last 500 steps = 3.7438e-01, PNorm = 85.5451, GNorm = 0.2272
Meta loss on this task batch = 2.9303e-01, Meta loss averaged over last 500 steps = 3.7416e-01, PNorm = 85.5553, GNorm = 0.1609
Meta loss on this task batch = 3.5194e-01, Meta loss averaged over last 500 steps = 3.7409e-01, PNorm = 85.5670, GNorm = 0.2123
Meta loss on this task batch = 4.1649e-01, Meta loss averaged over last 500 steps = 3.7419e-01, PNorm = 85.5776, GNorm = 0.2161
Meta loss on this task batch = 4.0290e-01, Meta loss averaged over last 500 steps = 3.7427e-01, PNorm = 85.5880, GNorm = 0.1826
Meta loss on this task batch = 3.5879e-01, Meta loss averaged over last 500 steps = 3.7413e-01, PNorm = 85.5985, GNorm = 0.2341
Took 114.2181601524353 seconds to complete one epoch of meta training
Took 122.17590951919556 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490318
Epoch 157
Meta loss on this task batch = 3.3446e-01, Meta loss averaged over last 500 steps = 3.7396e-01, PNorm = 85.6086, GNorm = 0.2562
Meta loss on this task batch = 3.1810e-01, Meta loss averaged over last 500 steps = 3.7388e-01, PNorm = 85.6189, GNorm = 0.1954
Meta loss on this task batch = 4.3063e-01, Meta loss averaged over last 500 steps = 3.7396e-01, PNorm = 85.6287, GNorm = 0.1793
Meta loss on this task batch = 3.6497e-01, Meta loss averaged over last 500 steps = 3.7398e-01, PNorm = 85.6384, GNorm = 0.2080
Meta loss on this task batch = 4.1665e-01, Meta loss averaged over last 500 steps = 3.7394e-01, PNorm = 85.6459, GNorm = 0.2282
Meta loss on this task batch = 3.7165e-01, Meta loss averaged over last 500 steps = 3.7391e-01, PNorm = 85.6541, GNorm = 0.2200
Meta loss on this task batch = 4.1957e-01, Meta loss averaged over last 500 steps = 3.7398e-01, PNorm = 85.6622, GNorm = 0.2780
Meta loss on this task batch = 3.5807e-01, Meta loss averaged over last 500 steps = 3.7400e-01, PNorm = 85.6710, GNorm = 0.1801
Meta loss on this task batch = 3.2022e-01, Meta loss averaged over last 500 steps = 3.7391e-01, PNorm = 85.6823, GNorm = 0.2375
Meta loss on this task batch = 3.4026e-01, Meta loss averaged over last 500 steps = 3.7384e-01, PNorm = 85.6946, GNorm = 0.1909
Meta loss on this task batch = 3.9646e-01, Meta loss averaged over last 500 steps = 3.7382e-01, PNorm = 85.7073, GNorm = 0.2235
Meta loss on this task batch = 3.2674e-01, Meta loss averaged over last 500 steps = 3.7373e-01, PNorm = 85.7205, GNorm = 0.1857
Meta loss on this task batch = 3.3282e-01, Meta loss averaged over last 500 steps = 3.7370e-01, PNorm = 85.7320, GNorm = 0.2164
Meta loss on this task batch = 3.5352e-01, Meta loss averaged over last 500 steps = 3.7376e-01, PNorm = 85.7421, GNorm = 0.2425
Meta loss on this task batch = 4.0512e-01, Meta loss averaged over last 500 steps = 3.7383e-01, PNorm = 85.7499, GNorm = 0.3058
Meta loss on this task batch = 3.9813e-01, Meta loss averaged over last 500 steps = 3.7386e-01, PNorm = 85.7578, GNorm = 0.2219
Meta loss on this task batch = 3.4253e-01, Meta loss averaged over last 500 steps = 3.7372e-01, PNorm = 85.7668, GNorm = 0.1883
Meta loss on this task batch = 3.4537e-01, Meta loss averaged over last 500 steps = 3.7371e-01, PNorm = 85.7766, GNorm = 0.1969
Meta loss on this task batch = 3.6915e-01, Meta loss averaged over last 500 steps = 3.7371e-01, PNorm = 85.7869, GNorm = 0.2095
Took 116.37388801574707 seconds to complete one epoch of meta training
Took 123.09259748458862 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486102
Epoch 158
Meta loss on this task batch = 4.0328e-01, Meta loss averaged over last 500 steps = 3.7374e-01, PNorm = 85.7968, GNorm = 0.1894
Meta loss on this task batch = 3.9309e-01, Meta loss averaged over last 500 steps = 3.7366e-01, PNorm = 85.8067, GNorm = 0.1800
Meta loss on this task batch = 3.4855e-01, Meta loss averaged over last 500 steps = 3.7360e-01, PNorm = 85.8171, GNorm = 0.1681
Meta loss on this task batch = 3.3083e-01, Meta loss averaged over last 500 steps = 3.7349e-01, PNorm = 85.8280, GNorm = 0.2064
Meta loss on this task batch = 3.6755e-01, Meta loss averaged over last 500 steps = 3.7339e-01, PNorm = 85.8393, GNorm = 0.1857
Meta loss on this task batch = 3.2623e-01, Meta loss averaged over last 500 steps = 3.7325e-01, PNorm = 85.8507, GNorm = 0.1657
Meta loss on this task batch = 3.7481e-01, Meta loss averaged over last 500 steps = 3.7331e-01, PNorm = 85.8608, GNorm = 0.1927
Meta loss on this task batch = 3.4565e-01, Meta loss averaged over last 500 steps = 3.7323e-01, PNorm = 85.8706, GNorm = 0.2197
Meta loss on this task batch = 4.2512e-01, Meta loss averaged over last 500 steps = 3.7335e-01, PNorm = 85.8798, GNorm = 0.2074
Meta loss on this task batch = 4.1335e-01, Meta loss averaged over last 500 steps = 3.7348e-01, PNorm = 85.8870, GNorm = 0.2951
Meta loss on this task batch = 3.4484e-01, Meta loss averaged over last 500 steps = 3.7340e-01, PNorm = 85.8957, GNorm = 0.2229
Meta loss on this task batch = 3.3949e-01, Meta loss averaged over last 500 steps = 3.7321e-01, PNorm = 85.9053, GNorm = 0.1960
Meta loss on this task batch = 3.7208e-01, Meta loss averaged over last 500 steps = 3.7323e-01, PNorm = 85.9155, GNorm = 0.2127
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 3.7312e-01, PNorm = 85.9261, GNorm = 0.2101
Meta loss on this task batch = 3.9391e-01, Meta loss averaged over last 500 steps = 3.7308e-01, PNorm = 85.9363, GNorm = 0.2149
Meta loss on this task batch = 3.6831e-01, Meta loss averaged over last 500 steps = 3.7315e-01, PNorm = 85.9463, GNorm = 0.1975
Meta loss on this task batch = 3.8863e-01, Meta loss averaged over last 500 steps = 3.7326e-01, PNorm = 85.9557, GNorm = 0.1986
Meta loss on this task batch = 3.7184e-01, Meta loss averaged over last 500 steps = 3.7321e-01, PNorm = 85.9653, GNorm = 0.2294
Meta loss on this task batch = 3.4433e-01, Meta loss averaged over last 500 steps = 3.7310e-01, PNorm = 85.9744, GNorm = 0.2296
Took 120.40699243545532 seconds to complete one epoch of meta training
Took 128.57572054862976 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468251
Epoch 159
Meta loss on this task batch = 3.2247e-01, Meta loss averaged over last 500 steps = 3.7300e-01, PNorm = 85.9838, GNorm = 0.1964
Meta loss on this task batch = 3.6009e-01, Meta loss averaged over last 500 steps = 3.7296e-01, PNorm = 85.9932, GNorm = 0.2034
Meta loss on this task batch = 3.6480e-01, Meta loss averaged over last 500 steps = 3.7299e-01, PNorm = 86.0027, GNorm = 0.1647
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 3.7276e-01, PNorm = 86.0133, GNorm = 0.1585
Meta loss on this task batch = 3.7650e-01, Meta loss averaged over last 500 steps = 3.7275e-01, PNorm = 86.0244, GNorm = 0.2342
Meta loss on this task batch = 3.9843e-01, Meta loss averaged over last 500 steps = 3.7279e-01, PNorm = 86.0351, GNorm = 0.1995
Meta loss on this task batch = 3.3252e-01, Meta loss averaged over last 500 steps = 3.7265e-01, PNorm = 86.0469, GNorm = 0.1721
Meta loss on this task batch = 3.7576e-01, Meta loss averaged over last 500 steps = 3.7244e-01, PNorm = 86.0590, GNorm = 0.1939
Meta loss on this task batch = 3.4916e-01, Meta loss averaged over last 500 steps = 3.7240e-01, PNorm = 86.0708, GNorm = 0.1878
Meta loss on this task batch = 3.7065e-01, Meta loss averaged over last 500 steps = 3.7233e-01, PNorm = 86.0835, GNorm = 0.1967
Meta loss on this task batch = 3.8423e-01, Meta loss averaged over last 500 steps = 3.7235e-01, PNorm = 86.0954, GNorm = 0.2661
Meta loss on this task batch = 3.6627e-01, Meta loss averaged over last 500 steps = 3.7223e-01, PNorm = 86.1073, GNorm = 0.1830
Meta loss on this task batch = 3.8963e-01, Meta loss averaged over last 500 steps = 3.7223e-01, PNorm = 86.1194, GNorm = 0.1933
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.7211e-01, PNorm = 86.1313, GNorm = 0.1761
Meta loss on this task batch = 3.7733e-01, Meta loss averaged over last 500 steps = 3.7221e-01, PNorm = 86.1437, GNorm = 0.2098
Meta loss on this task batch = 3.3118e-01, Meta loss averaged over last 500 steps = 3.7217e-01, PNorm = 86.1565, GNorm = 0.1624
Meta loss on this task batch = 3.5121e-01, Meta loss averaged over last 500 steps = 3.7219e-01, PNorm = 86.1691, GNorm = 0.1952
Meta loss on this task batch = 3.6816e-01, Meta loss averaged over last 500 steps = 3.7219e-01, PNorm = 86.1810, GNorm = 0.1854
Meta loss on this task batch = 3.2990e-01, Meta loss averaged over last 500 steps = 3.7206e-01, PNorm = 86.1924, GNorm = 0.2553
Took 117.85811758041382 seconds to complete one epoch of meta training
Took 125.62683367729187 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467590
Epoch 160
Meta loss on this task batch = 3.8840e-01, Meta loss averaged over last 500 steps = 3.7222e-01, PNorm = 86.2030, GNorm = 0.2340
Meta loss on this task batch = 3.6866e-01, Meta loss averaged over last 500 steps = 3.7225e-01, PNorm = 86.2137, GNorm = 0.1973
Meta loss on this task batch = 3.5435e-01, Meta loss averaged over last 500 steps = 3.7223e-01, PNorm = 86.2243, GNorm = 0.1866
Meta loss on this task batch = 3.7157e-01, Meta loss averaged over last 500 steps = 3.7223e-01, PNorm = 86.2360, GNorm = 0.2046
Meta loss on this task batch = 3.5978e-01, Meta loss averaged over last 500 steps = 3.7222e-01, PNorm = 86.2490, GNorm = 0.1903
Meta loss on this task batch = 3.5771e-01, Meta loss averaged over last 500 steps = 3.7213e-01, PNorm = 86.2616, GNorm = 0.2355
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 3.7188e-01, PNorm = 86.2745, GNorm = 0.1711
Meta loss on this task batch = 3.4332e-01, Meta loss averaged over last 500 steps = 3.7198e-01, PNorm = 86.2885, GNorm = 0.2224
Meta loss on this task batch = 3.0115e-01, Meta loss averaged over last 500 steps = 3.7180e-01, PNorm = 86.3017, GNorm = 0.2297
Meta loss on this task batch = 4.1539e-01, Meta loss averaged over last 500 steps = 3.7177e-01, PNorm = 86.3121, GNorm = 0.2447
Meta loss on this task batch = 4.4536e-01, Meta loss averaged over last 500 steps = 3.7199e-01, PNorm = 86.3213, GNorm = 0.2716
Meta loss on this task batch = 3.6198e-01, Meta loss averaged over last 500 steps = 3.7202e-01, PNorm = 86.3312, GNorm = 0.2073
Meta loss on this task batch = 3.6542e-01, Meta loss averaged over last 500 steps = 3.7207e-01, PNorm = 86.3419, GNorm = 0.2175
Meta loss on this task batch = 4.0071e-01, Meta loss averaged over last 500 steps = 3.7206e-01, PNorm = 86.3517, GNorm = 0.2167
Meta loss on this task batch = 3.5590e-01, Meta loss averaged over last 500 steps = 3.7203e-01, PNorm = 86.3617, GNorm = 0.1981
Meta loss on this task batch = 3.7066e-01, Meta loss averaged over last 500 steps = 3.7207e-01, PNorm = 86.3719, GNorm = 0.1908
Meta loss on this task batch = 4.0814e-01, Meta loss averaged over last 500 steps = 3.7213e-01, PNorm = 86.3820, GNorm = 0.2017
Meta loss on this task batch = 3.4992e-01, Meta loss averaged over last 500 steps = 3.7209e-01, PNorm = 86.3941, GNorm = 0.2080
Meta loss on this task batch = 3.8907e-01, Meta loss averaged over last 500 steps = 3.7213e-01, PNorm = 86.4066, GNorm = 0.2242
Took 116.15441727638245 seconds to complete one epoch of meta training
Took 124.37956285476685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478133
Epoch 161
Meta loss on this task batch = 3.4815e-01, Meta loss averaged over last 500 steps = 3.7196e-01, PNorm = 86.4189, GNorm = 0.2062
Meta loss on this task batch = 3.3811e-01, Meta loss averaged over last 500 steps = 3.7192e-01, PNorm = 86.4325, GNorm = 0.1715
Meta loss on this task batch = 3.5987e-01, Meta loss averaged over last 500 steps = 3.7185e-01, PNorm = 86.4460, GNorm = 0.1952
Meta loss on this task batch = 3.5072e-01, Meta loss averaged over last 500 steps = 3.7165e-01, PNorm = 86.4579, GNorm = 0.2030
Meta loss on this task batch = 2.9697e-01, Meta loss averaged over last 500 steps = 3.7145e-01, PNorm = 86.4710, GNorm = 0.1905
Meta loss on this task batch = 3.8023e-01, Meta loss averaged over last 500 steps = 3.7156e-01, PNorm = 86.4829, GNorm = 0.2362
Meta loss on this task batch = 3.6685e-01, Meta loss averaged over last 500 steps = 3.7153e-01, PNorm = 86.4939, GNorm = 0.2126
Meta loss on this task batch = 3.3993e-01, Meta loss averaged over last 500 steps = 3.7147e-01, PNorm = 86.5057, GNorm = 0.1886
Meta loss on this task batch = 3.9806e-01, Meta loss averaged over last 500 steps = 3.7138e-01, PNorm = 86.5164, GNorm = 0.2183
Meta loss on this task batch = 3.6554e-01, Meta loss averaged over last 500 steps = 3.7133e-01, PNorm = 86.5266, GNorm = 0.2216
Meta loss on this task batch = 3.7780e-01, Meta loss averaged over last 500 steps = 3.7139e-01, PNorm = 86.5356, GNorm = 0.1974
Meta loss on this task batch = 3.7846e-01, Meta loss averaged over last 500 steps = 3.7123e-01, PNorm = 86.5437, GNorm = 0.2167
Meta loss on this task batch = 3.5263e-01, Meta loss averaged over last 500 steps = 3.7108e-01, PNorm = 86.5523, GNorm = 0.2399
Meta loss on this task batch = 3.8264e-01, Meta loss averaged over last 500 steps = 3.7113e-01, PNorm = 86.5610, GNorm = 0.1839
Meta loss on this task batch = 3.4666e-01, Meta loss averaged over last 500 steps = 3.7119e-01, PNorm = 86.5695, GNorm = 0.1847
Meta loss on this task batch = 3.9681e-01, Meta loss averaged over last 500 steps = 3.7129e-01, PNorm = 86.5780, GNorm = 0.1976
Meta loss on this task batch = 3.4994e-01, Meta loss averaged over last 500 steps = 3.7125e-01, PNorm = 86.5872, GNorm = 0.1766
Meta loss on this task batch = 4.3683e-01, Meta loss averaged over last 500 steps = 3.7149e-01, PNorm = 86.5970, GNorm = 0.2227
Meta loss on this task batch = 3.6952e-01, Meta loss averaged over last 500 steps = 3.7152e-01, PNorm = 86.6063, GNorm = 0.2243
Took 116.51568126678467 seconds to complete one epoch of meta training
Took 124.34617757797241 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474184
Epoch 162
Meta loss on this task batch = 4.1185e-01, Meta loss averaged over last 500 steps = 3.7153e-01, PNorm = 86.6165, GNorm = 0.2220
Meta loss on this task batch = 3.9071e-01, Meta loss averaged over last 500 steps = 3.7158e-01, PNorm = 86.6271, GNorm = 0.2429
Meta loss on this task batch = 3.4518e-01, Meta loss averaged over last 500 steps = 3.7156e-01, PNorm = 86.6380, GNorm = 0.1603
Meta loss on this task batch = 3.9762e-01, Meta loss averaged over last 500 steps = 3.7160e-01, PNorm = 86.6492, GNorm = 0.1743
Meta loss on this task batch = 3.1339e-01, Meta loss averaged over last 500 steps = 3.7144e-01, PNorm = 86.6617, GNorm = 0.1885
Meta loss on this task batch = 3.8745e-01, Meta loss averaged over last 500 steps = 3.7152e-01, PNorm = 86.6739, GNorm = 0.1821
Meta loss on this task batch = 3.9339e-01, Meta loss averaged over last 500 steps = 3.7152e-01, PNorm = 86.6860, GNorm = 0.2201
Meta loss on this task batch = 3.7063e-01, Meta loss averaged over last 500 steps = 3.7162e-01, PNorm = 86.6986, GNorm = 0.1850
Meta loss on this task batch = 3.3221e-01, Meta loss averaged over last 500 steps = 3.7139e-01, PNorm = 86.7121, GNorm = 0.2024
Meta loss on this task batch = 3.7090e-01, Meta loss averaged over last 500 steps = 3.7139e-01, PNorm = 86.7251, GNorm = 0.2133
Meta loss on this task batch = 3.7314e-01, Meta loss averaged over last 500 steps = 3.7141e-01, PNorm = 86.7375, GNorm = 0.2364
Meta loss on this task batch = 3.1025e-01, Meta loss averaged over last 500 steps = 3.7132e-01, PNorm = 86.7503, GNorm = 0.1806
Meta loss on this task batch = 3.8734e-01, Meta loss averaged over last 500 steps = 3.7140e-01, PNorm = 86.7624, GNorm = 0.2377
Meta loss on this task batch = 3.3457e-01, Meta loss averaged over last 500 steps = 3.7133e-01, PNorm = 86.7731, GNorm = 0.2179
Meta loss on this task batch = 3.4272e-01, Meta loss averaged over last 500 steps = 3.7114e-01, PNorm = 86.7833, GNorm = 0.2464
Meta loss on this task batch = 3.8028e-01, Meta loss averaged over last 500 steps = 3.7112e-01, PNorm = 86.7938, GNorm = 0.2064
Meta loss on this task batch = 3.5228e-01, Meta loss averaged over last 500 steps = 3.7102e-01, PNorm = 86.8046, GNorm = 0.2175
Meta loss on this task batch = 3.7855e-01, Meta loss averaged over last 500 steps = 3.7103e-01, PNorm = 86.8151, GNorm = 0.2264
Meta loss on this task batch = 3.7769e-01, Meta loss averaged over last 500 steps = 3.7102e-01, PNorm = 86.8258, GNorm = 0.2133
Took 113.62251138687134 seconds to complete one epoch of meta training
Took 121.87198829650879 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482299
Epoch 163
Meta loss on this task batch = 3.0167e-01, Meta loss averaged over last 500 steps = 3.7098e-01, PNorm = 86.8372, GNorm = 0.1581
Meta loss on this task batch = 3.3228e-01, Meta loss averaged over last 500 steps = 3.7089e-01, PNorm = 86.8507, GNorm = 0.1913
Meta loss on this task batch = 3.8646e-01, Meta loss averaged over last 500 steps = 3.7089e-01, PNorm = 86.8646, GNorm = 0.1749
Meta loss on this task batch = 4.2133e-01, Meta loss averaged over last 500 steps = 3.7100e-01, PNorm = 86.8776, GNorm = 0.2207
Meta loss on this task batch = 3.4980e-01, Meta loss averaged over last 500 steps = 3.7094e-01, PNorm = 86.8909, GNorm = 0.2304
Meta loss on this task batch = 3.4851e-01, Meta loss averaged over last 500 steps = 3.7084e-01, PNorm = 86.9039, GNorm = 0.1820
Meta loss on this task batch = 3.9857e-01, Meta loss averaged over last 500 steps = 3.7089e-01, PNorm = 86.9158, GNorm = 0.2187
Meta loss on this task batch = 2.9892e-01, Meta loss averaged over last 500 steps = 3.7069e-01, PNorm = 86.9279, GNorm = 0.1656
Meta loss on this task batch = 3.9886e-01, Meta loss averaged over last 500 steps = 3.7077e-01, PNorm = 86.9396, GNorm = 0.2372
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 3.7054e-01, PNorm = 86.9524, GNorm = 0.2119
Meta loss on this task batch = 3.5940e-01, Meta loss averaged over last 500 steps = 3.7057e-01, PNorm = 86.9648, GNorm = 0.2373
Meta loss on this task batch = 3.7189e-01, Meta loss averaged over last 500 steps = 3.7056e-01, PNorm = 86.9774, GNorm = 0.2431
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 3.7045e-01, PNorm = 86.9899, GNorm = 0.1964
Meta loss on this task batch = 3.6635e-01, Meta loss averaged over last 500 steps = 3.7044e-01, PNorm = 87.0021, GNorm = 0.2205
Meta loss on this task batch = 3.5769e-01, Meta loss averaged over last 500 steps = 3.7037e-01, PNorm = 87.0149, GNorm = 0.1750
Meta loss on this task batch = 3.7255e-01, Meta loss averaged over last 500 steps = 3.7040e-01, PNorm = 87.0270, GNorm = 0.2242
Meta loss on this task batch = 4.2838e-01, Meta loss averaged over last 500 steps = 3.7057e-01, PNorm = 87.0372, GNorm = 0.2390
Meta loss on this task batch = 3.7467e-01, Meta loss averaged over last 500 steps = 3.7068e-01, PNorm = 87.0482, GNorm = 0.1913
Meta loss on this task batch = 4.1155e-01, Meta loss averaged over last 500 steps = 3.7075e-01, PNorm = 87.0568, GNorm = 0.2820
Took 118.85388493537903 seconds to complete one epoch of meta training
Took 126.74921679496765 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469807
Epoch 164
Meta loss on this task batch = 3.9636e-01, Meta loss averaged over last 500 steps = 3.7063e-01, PNorm = 87.0654, GNorm = 0.2391
Meta loss on this task batch = 3.7459e-01, Meta loss averaged over last 500 steps = 3.7062e-01, PNorm = 87.0739, GNorm = 0.2246
Meta loss on this task batch = 3.1565e-01, Meta loss averaged over last 500 steps = 3.7049e-01, PNorm = 87.0844, GNorm = 0.1933
Meta loss on this task batch = 3.5877e-01, Meta loss averaged over last 500 steps = 3.7039e-01, PNorm = 87.0958, GNorm = 0.1763
Meta loss on this task batch = 3.6186e-01, Meta loss averaged over last 500 steps = 3.7036e-01, PNorm = 87.1080, GNorm = 0.2018
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.7024e-01, PNorm = 87.1200, GNorm = 0.1764
Meta loss on this task batch = 3.0038e-01, Meta loss averaged over last 500 steps = 3.7003e-01, PNorm = 87.1328, GNorm = 0.1704
Meta loss on this task batch = 3.6641e-01, Meta loss averaged over last 500 steps = 3.7002e-01, PNorm = 87.1461, GNorm = 0.1893
Meta loss on this task batch = 4.0469e-01, Meta loss averaged over last 500 steps = 3.6999e-01, PNorm = 87.1584, GNorm = 0.2333
Meta loss on this task batch = 3.9401e-01, Meta loss averaged over last 500 steps = 3.7000e-01, PNorm = 87.1707, GNorm = 0.1997
Meta loss on this task batch = 3.7347e-01, Meta loss averaged over last 500 steps = 3.6993e-01, PNorm = 87.1820, GNorm = 0.2266
Meta loss on this task batch = 3.8054e-01, Meta loss averaged over last 500 steps = 3.6999e-01, PNorm = 87.1926, GNorm = 0.2433
Meta loss on this task batch = 3.2185e-01, Meta loss averaged over last 500 steps = 3.6987e-01, PNorm = 87.2036, GNorm = 0.1783
Meta loss on this task batch = 3.1336e-01, Meta loss averaged over last 500 steps = 3.6978e-01, PNorm = 87.2158, GNorm = 0.1741
Meta loss on this task batch = 3.7518e-01, Meta loss averaged over last 500 steps = 3.6980e-01, PNorm = 87.2280, GNorm = 0.2178
Meta loss on this task batch = 3.3388e-01, Meta loss averaged over last 500 steps = 3.6970e-01, PNorm = 87.2395, GNorm = 0.2042
Meta loss on this task batch = 4.0547e-01, Meta loss averaged over last 500 steps = 3.6977e-01, PNorm = 87.2494, GNorm = 0.2565
Meta loss on this task batch = 3.4282e-01, Meta loss averaged over last 500 steps = 3.6976e-01, PNorm = 87.2593, GNorm = 0.1880
Meta loss on this task batch = 4.2340e-01, Meta loss averaged over last 500 steps = 3.6983e-01, PNorm = 87.2681, GNorm = 0.3037
Took 117.55163073539734 seconds to complete one epoch of meta training
Took 125.56846117973328 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465429
Epoch 165
Meta loss on this task batch = 3.2755e-01, Meta loss averaged over last 500 steps = 3.6979e-01, PNorm = 87.2771, GNorm = 0.1963
Meta loss on this task batch = 3.7952e-01, Meta loss averaged over last 500 steps = 3.6992e-01, PNorm = 87.2868, GNorm = 0.2021
Meta loss on this task batch = 3.6310e-01, Meta loss averaged over last 500 steps = 3.6980e-01, PNorm = 87.2975, GNorm = 0.1922
Meta loss on this task batch = 3.9948e-01, Meta loss averaged over last 500 steps = 3.6982e-01, PNorm = 87.3075, GNorm = 0.2237
Meta loss on this task batch = 3.3213e-01, Meta loss averaged over last 500 steps = 3.6978e-01, PNorm = 87.3181, GNorm = 0.1828
Meta loss on this task batch = 3.9192e-01, Meta loss averaged over last 500 steps = 3.6988e-01, PNorm = 87.3283, GNorm = 0.2246
Meta loss on this task batch = 3.4102e-01, Meta loss averaged over last 500 steps = 3.6987e-01, PNorm = 87.3385, GNorm = 0.2063
Meta loss on this task batch = 3.3529e-01, Meta loss averaged over last 500 steps = 3.6984e-01, PNorm = 87.3503, GNorm = 0.1903
Meta loss on this task batch = 3.6780e-01, Meta loss averaged over last 500 steps = 3.6974e-01, PNorm = 87.3614, GNorm = 0.1822
Meta loss on this task batch = 3.6465e-01, Meta loss averaged over last 500 steps = 3.6969e-01, PNorm = 87.3738, GNorm = 0.1873
Meta loss on this task batch = 3.4525e-01, Meta loss averaged over last 500 steps = 3.6961e-01, PNorm = 87.3860, GNorm = 0.2051
Meta loss on this task batch = 3.6806e-01, Meta loss averaged over last 500 steps = 3.6958e-01, PNorm = 87.3989, GNorm = 0.2047
Meta loss on this task batch = 3.6121e-01, Meta loss averaged over last 500 steps = 3.6968e-01, PNorm = 87.4113, GNorm = 0.2104
Meta loss on this task batch = 3.8210e-01, Meta loss averaged over last 500 steps = 3.6964e-01, PNorm = 87.4239, GNorm = 0.1971
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.6956e-01, PNorm = 87.4358, GNorm = 0.2591
Meta loss on this task batch = 3.5014e-01, Meta loss averaged over last 500 steps = 3.6949e-01, PNorm = 87.4484, GNorm = 0.1934
Meta loss on this task batch = 3.8325e-01, Meta loss averaged over last 500 steps = 3.6948e-01, PNorm = 87.4608, GNorm = 0.1986
Meta loss on this task batch = 3.2959e-01, Meta loss averaged over last 500 steps = 3.6941e-01, PNorm = 87.4729, GNorm = 0.2121
Meta loss on this task batch = 4.0886e-01, Meta loss averaged over last 500 steps = 3.6953e-01, PNorm = 87.4840, GNorm = 0.2579
Took 117.00643992424011 seconds to complete one epoch of meta training
Took 124.92013764381409 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494794
Epoch 166
Meta loss on this task batch = 3.6219e-01, Meta loss averaged over last 500 steps = 3.6956e-01, PNorm = 87.4942, GNorm = 0.2311
Meta loss on this task batch = 3.7297e-01, Meta loss averaged over last 500 steps = 3.6956e-01, PNorm = 87.5035, GNorm = 0.2173
Meta loss on this task batch = 3.9106e-01, Meta loss averaged over last 500 steps = 3.6940e-01, PNorm = 87.5123, GNorm = 0.1852
Meta loss on this task batch = 4.0605e-01, Meta loss averaged over last 500 steps = 3.6948e-01, PNorm = 87.5182, GNorm = 0.2426
Meta loss on this task batch = 3.5587e-01, Meta loss averaged over last 500 steps = 3.6951e-01, PNorm = 87.5258, GNorm = 0.1538
Meta loss on this task batch = 3.9113e-01, Meta loss averaged over last 500 steps = 3.6963e-01, PNorm = 87.5345, GNorm = 0.1799
Meta loss on this task batch = 3.2752e-01, Meta loss averaged over last 500 steps = 3.6963e-01, PNorm = 87.5436, GNorm = 0.1581
Meta loss on this task batch = 3.3757e-01, Meta loss averaged over last 500 steps = 3.6954e-01, PNorm = 87.5541, GNorm = 0.1685
Meta loss on this task batch = 3.4892e-01, Meta loss averaged over last 500 steps = 3.6955e-01, PNorm = 87.5653, GNorm = 0.1749
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 3.6952e-01, PNorm = 87.5775, GNorm = 0.1808
Meta loss on this task batch = 3.8404e-01, Meta loss averaged over last 500 steps = 3.6970e-01, PNorm = 87.5902, GNorm = 0.1778
Meta loss on this task batch = 3.8372e-01, Meta loss averaged over last 500 steps = 3.6969e-01, PNorm = 87.6034, GNorm = 0.1939
Meta loss on this task batch = 3.5774e-01, Meta loss averaged over last 500 steps = 3.6964e-01, PNorm = 87.6162, GNorm = 0.1910
Meta loss on this task batch = 3.2776e-01, Meta loss averaged over last 500 steps = 3.6943e-01, PNorm = 87.6304, GNorm = 0.1849
Meta loss on this task batch = 3.2178e-01, Meta loss averaged over last 500 steps = 3.6922e-01, PNorm = 87.6454, GNorm = 0.1727
Meta loss on this task batch = 3.6939e-01, Meta loss averaged over last 500 steps = 3.6916e-01, PNorm = 87.6595, GNorm = 0.2126
Meta loss on this task batch = 3.6623e-01, Meta loss averaged over last 500 steps = 3.6914e-01, PNorm = 87.6740, GNorm = 0.2372
Meta loss on this task batch = 4.7864e-01, Meta loss averaged over last 500 steps = 3.6935e-01, PNorm = 87.6851, GNorm = 0.2838
Meta loss on this task batch = 3.0500e-01, Meta loss averaged over last 500 steps = 3.6926e-01, PNorm = 87.6942, GNorm = 0.2408
Took 117.88761043548584 seconds to complete one epoch of meta training
Took 125.71609258651733 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497380
Epoch 167
Meta loss on this task batch = 3.7413e-01, Meta loss averaged over last 500 steps = 3.6924e-01, PNorm = 87.7013, GNorm = 0.2356
Meta loss on this task batch = 3.9502e-01, Meta loss averaged over last 500 steps = 3.6930e-01, PNorm = 87.7075, GNorm = 0.2495
Meta loss on this task batch = 4.1066e-01, Meta loss averaged over last 500 steps = 3.6930e-01, PNorm = 87.7144, GNorm = 0.2317
Meta loss on this task batch = 3.5987e-01, Meta loss averaged over last 500 steps = 3.6918e-01, PNorm = 87.7229, GNorm = 0.1989
Meta loss on this task batch = 3.5440e-01, Meta loss averaged over last 500 steps = 3.6918e-01, PNorm = 87.7322, GNorm = 0.1743
Meta loss on this task batch = 4.0688e-01, Meta loss averaged over last 500 steps = 3.6905e-01, PNorm = 87.7417, GNorm = 0.2086
Meta loss on this task batch = 2.7450e-01, Meta loss averaged over last 500 steps = 3.6877e-01, PNorm = 87.7537, GNorm = 0.2051
Meta loss on this task batch = 3.3082e-01, Meta loss averaged over last 500 steps = 3.6860e-01, PNorm = 87.7672, GNorm = 0.1931
Meta loss on this task batch = 3.6609e-01, Meta loss averaged over last 500 steps = 3.6859e-01, PNorm = 87.7811, GNorm = 0.1928
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 3.6856e-01, PNorm = 87.7956, GNorm = 0.2455
Meta loss on this task batch = 3.3941e-01, Meta loss averaged over last 500 steps = 3.6862e-01, PNorm = 87.8102, GNorm = 0.1893
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 3.6850e-01, PNorm = 87.8249, GNorm = 0.2060
Meta loss on this task batch = 3.6467e-01, Meta loss averaged over last 500 steps = 3.6850e-01, PNorm = 87.8392, GNorm = 0.3026
Meta loss on this task batch = 3.9656e-01, Meta loss averaged over last 500 steps = 3.6851e-01, PNorm = 87.8503, GNorm = 0.2749
Meta loss on this task batch = 3.4152e-01, Meta loss averaged over last 500 steps = 3.6851e-01, PNorm = 87.8603, GNorm = 0.2503
Meta loss on this task batch = 3.9855e-01, Meta loss averaged over last 500 steps = 3.6850e-01, PNorm = 87.8699, GNorm = 0.2496
Meta loss on this task batch = 3.6231e-01, Meta loss averaged over last 500 steps = 3.6841e-01, PNorm = 87.8794, GNorm = 0.2280
Meta loss on this task batch = 3.7829e-01, Meta loss averaged over last 500 steps = 3.6844e-01, PNorm = 87.8888, GNorm = 0.2227
Meta loss on this task batch = 3.5338e-01, Meta loss averaged over last 500 steps = 3.6843e-01, PNorm = 87.8991, GNorm = 0.2642
Took 117.9535219669342 seconds to complete one epoch of meta training
Took 126.08745193481445 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468000
Epoch 168
Meta loss on this task batch = 4.1254e-01, Meta loss averaged over last 500 steps = 3.6846e-01, PNorm = 87.9090, GNorm = 0.2512
Meta loss on this task batch = 3.3149e-01, Meta loss averaged over last 500 steps = 3.6845e-01, PNorm = 87.9204, GNorm = 0.1906
Meta loss on this task batch = 3.8558e-01, Meta loss averaged over last 500 steps = 3.6849e-01, PNorm = 87.9318, GNorm = 0.2051
Meta loss on this task batch = 3.4012e-01, Meta loss averaged over last 500 steps = 3.6837e-01, PNorm = 87.9438, GNorm = 0.2021
Meta loss on this task batch = 3.2746e-01, Meta loss averaged over last 500 steps = 3.6835e-01, PNorm = 87.9560, GNorm = 0.1741
Meta loss on this task batch = 3.9882e-01, Meta loss averaged over last 500 steps = 3.6832e-01, PNorm = 87.9685, GNorm = 0.1807
Meta loss on this task batch = 3.7029e-01, Meta loss averaged over last 500 steps = 3.6841e-01, PNorm = 87.9805, GNorm = 0.2005
Meta loss on this task batch = 4.1622e-01, Meta loss averaged over last 500 steps = 3.6844e-01, PNorm = 87.9913, GNorm = 0.2284
Meta loss on this task batch = 3.3579e-01, Meta loss averaged over last 500 steps = 3.6825e-01, PNorm = 88.0019, GNorm = 0.2243
Meta loss on this task batch = 3.6092e-01, Meta loss averaged over last 500 steps = 3.6820e-01, PNorm = 88.0115, GNorm = 0.2244
Meta loss on this task batch = 3.9049e-01, Meta loss averaged over last 500 steps = 3.6839e-01, PNorm = 88.0216, GNorm = 0.1926
Meta loss on this task batch = 3.8063e-01, Meta loss averaged over last 500 steps = 3.6853e-01, PNorm = 88.0308, GNorm = 0.2036
Meta loss on this task batch = 3.4922e-01, Meta loss averaged over last 500 steps = 3.6836e-01, PNorm = 88.0410, GNorm = 0.1966
Meta loss on this task batch = 3.3062e-01, Meta loss averaged over last 500 steps = 3.6818e-01, PNorm = 88.0529, GNorm = 0.2008
Meta loss on this task batch = 3.6268e-01, Meta loss averaged over last 500 steps = 3.6815e-01, PNorm = 88.0641, GNorm = 0.2119
Meta loss on this task batch = 3.7989e-01, Meta loss averaged over last 500 steps = 3.6808e-01, PNorm = 88.0751, GNorm = 0.1961
Meta loss on this task batch = 3.4730e-01, Meta loss averaged over last 500 steps = 3.6799e-01, PNorm = 88.0858, GNorm = 0.2060
Meta loss on this task batch = 3.2624e-01, Meta loss averaged over last 500 steps = 3.6796e-01, PNorm = 88.0970, GNorm = 0.1885
Meta loss on this task batch = 4.0490e-01, Meta loss averaged over last 500 steps = 3.6801e-01, PNorm = 88.1078, GNorm = 0.2460
Took 114.92338871955872 seconds to complete one epoch of meta training
Took 122.94968962669373 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464460
Epoch 169
Meta loss on this task batch = 3.6721e-01, Meta loss averaged over last 500 steps = 3.6788e-01, PNorm = 88.1189, GNorm = 0.2051
Meta loss on this task batch = 3.1317e-01, Meta loss averaged over last 500 steps = 3.6777e-01, PNorm = 88.1302, GNorm = 0.1581
Meta loss on this task batch = 3.6531e-01, Meta loss averaged over last 500 steps = 3.6774e-01, PNorm = 88.1402, GNorm = 0.1818
Meta loss on this task batch = 3.6013e-01, Meta loss averaged over last 500 steps = 3.6768e-01, PNorm = 88.1507, GNorm = 0.1924
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 3.6760e-01, PNorm = 88.1609, GNorm = 0.2147
Meta loss on this task batch = 3.6989e-01, Meta loss averaged over last 500 steps = 3.6758e-01, PNorm = 88.1701, GNorm = 0.2145
Meta loss on this task batch = 3.6205e-01, Meta loss averaged over last 500 steps = 3.6753e-01, PNorm = 88.1781, GNorm = 0.2151
Meta loss on this task batch = 3.7116e-01, Meta loss averaged over last 500 steps = 3.6754e-01, PNorm = 88.1859, GNorm = 0.2138
Meta loss on this task batch = 3.6640e-01, Meta loss averaged over last 500 steps = 3.6761e-01, PNorm = 88.1943, GNorm = 0.1969
Meta loss on this task batch = 3.8741e-01, Meta loss averaged over last 500 steps = 3.6754e-01, PNorm = 88.2034, GNorm = 0.2193
Meta loss on this task batch = 3.2272e-01, Meta loss averaged over last 500 steps = 3.6742e-01, PNorm = 88.2128, GNorm = 0.1923
Meta loss on this task batch = 3.7865e-01, Meta loss averaged over last 500 steps = 3.6735e-01, PNorm = 88.2231, GNorm = 0.2450
Meta loss on this task batch = 3.9560e-01, Meta loss averaged over last 500 steps = 3.6735e-01, PNorm = 88.2341, GNorm = 0.2533
Meta loss on this task batch = 3.3068e-01, Meta loss averaged over last 500 steps = 3.6723e-01, PNorm = 88.2451, GNorm = 0.2271
Meta loss on this task batch = 3.1110e-01, Meta loss averaged over last 500 steps = 3.6715e-01, PNorm = 88.2572, GNorm = 0.1963
Meta loss on this task batch = 3.5943e-01, Meta loss averaged over last 500 steps = 3.6711e-01, PNorm = 88.2685, GNorm = 0.2291
Meta loss on this task batch = 3.6601e-01, Meta loss averaged over last 500 steps = 3.6713e-01, PNorm = 88.2790, GNorm = 0.2045
Meta loss on this task batch = 3.6063e-01, Meta loss averaged over last 500 steps = 3.6716e-01, PNorm = 88.2896, GNorm = 0.2025
Meta loss on this task batch = 4.4829e-01, Meta loss averaged over last 500 steps = 3.6744e-01, PNorm = 88.2985, GNorm = 0.2655
Took 116.47712635993958 seconds to complete one epoch of meta training
Took 124.241286277771 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464090
Epoch 170
Meta loss on this task batch = 3.5594e-01, Meta loss averaged over last 500 steps = 3.6752e-01, PNorm = 88.3071, GNorm = 0.1772
Meta loss on this task batch = 3.0041e-01, Meta loss averaged over last 500 steps = 3.6735e-01, PNorm = 88.3159, GNorm = 0.1669
Meta loss on this task batch = 3.6130e-01, Meta loss averaged over last 500 steps = 3.6734e-01, PNorm = 88.3245, GNorm = 0.1654
Meta loss on this task batch = 3.4583e-01, Meta loss averaged over last 500 steps = 3.6712e-01, PNorm = 88.3342, GNorm = 0.1852
Meta loss on this task batch = 3.9164e-01, Meta loss averaged over last 500 steps = 3.6703e-01, PNorm = 88.3444, GNorm = 0.2221
Meta loss on this task batch = 3.3407e-01, Meta loss averaged over last 500 steps = 3.6693e-01, PNorm = 88.3548, GNorm = 0.1936
Meta loss on this task batch = 3.3673e-01, Meta loss averaged over last 500 steps = 3.6683e-01, PNorm = 88.3647, GNorm = 0.2032
Meta loss on this task batch = 3.5789e-01, Meta loss averaged over last 500 steps = 3.6677e-01, PNorm = 88.3752, GNorm = 0.2200
Meta loss on this task batch = 3.5461e-01, Meta loss averaged over last 500 steps = 3.6682e-01, PNorm = 88.3860, GNorm = 0.2060
Meta loss on this task batch = 3.6070e-01, Meta loss averaged over last 500 steps = 3.6674e-01, PNorm = 88.3971, GNorm = 0.1957
Meta loss on this task batch = 3.6400e-01, Meta loss averaged over last 500 steps = 3.6673e-01, PNorm = 88.4091, GNorm = 0.3110
Meta loss on this task batch = 3.5688e-01, Meta loss averaged over last 500 steps = 3.6681e-01, PNorm = 88.4203, GNorm = 0.2031
Meta loss on this task batch = 3.7279e-01, Meta loss averaged over last 500 steps = 3.6679e-01, PNorm = 88.4317, GNorm = 0.2127
Meta loss on this task batch = 3.6187e-01, Meta loss averaged over last 500 steps = 3.6680e-01, PNorm = 88.4430, GNorm = 0.1932
Meta loss on this task batch = 3.6716e-01, Meta loss averaged over last 500 steps = 3.6674e-01, PNorm = 88.4541, GNorm = 0.1909
Meta loss on this task batch = 3.9437e-01, Meta loss averaged over last 500 steps = 3.6685e-01, PNorm = 88.4643, GNorm = 0.2134
Meta loss on this task batch = 3.7360e-01, Meta loss averaged over last 500 steps = 3.6675e-01, PNorm = 88.4745, GNorm = 0.2023
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 3.6672e-01, PNorm = 88.4856, GNorm = 0.2101
Meta loss on this task batch = 4.3642e-01, Meta loss averaged over last 500 steps = 3.6681e-01, PNorm = 88.4959, GNorm = 0.2485
Took 114.3735830783844 seconds to complete one epoch of meta training
Took 122.24043607711792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492862
Epoch 171
Meta loss on this task batch = 3.2544e-01, Meta loss averaged over last 500 steps = 3.6673e-01, PNorm = 88.5071, GNorm = 0.1984
Meta loss on this task batch = 4.2676e-01, Meta loss averaged over last 500 steps = 3.6680e-01, PNorm = 88.5171, GNorm = 0.2447
Meta loss on this task batch = 4.1796e-01, Meta loss averaged over last 500 steps = 3.6688e-01, PNorm = 88.5261, GNorm = 0.2247
Meta loss on this task batch = 3.6406e-01, Meta loss averaged over last 500 steps = 3.6687e-01, PNorm = 88.5352, GNorm = 0.1836
Meta loss on this task batch = 3.8040e-01, Meta loss averaged over last 500 steps = 3.6682e-01, PNorm = 88.5435, GNorm = 0.2055
Meta loss on this task batch = 3.2471e-01, Meta loss averaged over last 500 steps = 3.6671e-01, PNorm = 88.5532, GNorm = 0.1880
Meta loss on this task batch = 3.8351e-01, Meta loss averaged over last 500 steps = 3.6668e-01, PNorm = 88.5623, GNorm = 0.2292
Meta loss on this task batch = 3.2944e-01, Meta loss averaged over last 500 steps = 3.6643e-01, PNorm = 88.5715, GNorm = 0.2030
Meta loss on this task batch = 3.7837e-01, Meta loss averaged over last 500 steps = 3.6653e-01, PNorm = 88.5816, GNorm = 0.2234
Meta loss on this task batch = 3.4514e-01, Meta loss averaged over last 500 steps = 3.6649e-01, PNorm = 88.5922, GNorm = 0.1828
Meta loss on this task batch = 3.5422e-01, Meta loss averaged over last 500 steps = 3.6648e-01, PNorm = 88.6026, GNorm = 0.1974
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 3.6629e-01, PNorm = 88.6126, GNorm = 0.2109
Meta loss on this task batch = 3.2673e-01, Meta loss averaged over last 500 steps = 3.6625e-01, PNorm = 88.6225, GNorm = 0.1943
Meta loss on this task batch = 3.8793e-01, Meta loss averaged over last 500 steps = 3.6640e-01, PNorm = 88.6315, GNorm = 0.2299
Meta loss on this task batch = 3.9581e-01, Meta loss averaged over last 500 steps = 3.6639e-01, PNorm = 88.6407, GNorm = 0.2336
Meta loss on this task batch = 3.3832e-01, Meta loss averaged over last 500 steps = 3.6630e-01, PNorm = 88.6507, GNorm = 0.2063
Meta loss on this task batch = 4.4537e-01, Meta loss averaged over last 500 steps = 3.6651e-01, PNorm = 88.6602, GNorm = 0.2081
Meta loss on this task batch = 3.2048e-01, Meta loss averaged over last 500 steps = 3.6645e-01, PNorm = 88.6703, GNorm = 0.1648
Meta loss on this task batch = 3.7550e-01, Meta loss averaged over last 500 steps = 3.6640e-01, PNorm = 88.6810, GNorm = 0.2478
Took 114.58666467666626 seconds to complete one epoch of meta training
Took 122.79799032211304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506754
Found better MAML checkpoint after meta validation, saving now
Epoch 172
Meta loss on this task batch = 3.4295e-01, Meta loss averaged over last 500 steps = 3.6628e-01, PNorm = 88.6921, GNorm = 0.1866
Meta loss on this task batch = 3.5715e-01, Meta loss averaged over last 500 steps = 3.6625e-01, PNorm = 88.7036, GNorm = 0.1888
Meta loss on this task batch = 3.5050e-01, Meta loss averaged over last 500 steps = 3.6621e-01, PNorm = 88.7153, GNorm = 0.2105
Meta loss on this task batch = 3.9064e-01, Meta loss averaged over last 500 steps = 3.6627e-01, PNorm = 88.7258, GNorm = 0.2186
Meta loss on this task batch = 3.6588e-01, Meta loss averaged over last 500 steps = 3.6639e-01, PNorm = 88.7367, GNorm = 0.1877
Meta loss on this task batch = 3.4554e-01, Meta loss averaged over last 500 steps = 3.6646e-01, PNorm = 88.7472, GNorm = 0.2090
Meta loss on this task batch = 3.4437e-01, Meta loss averaged over last 500 steps = 3.6639e-01, PNorm = 88.7585, GNorm = 0.2161
Meta loss on this task batch = 3.3355e-01, Meta loss averaged over last 500 steps = 3.6616e-01, PNorm = 88.7693, GNorm = 0.2087
Meta loss on this task batch = 3.5689e-01, Meta loss averaged over last 500 steps = 3.6623e-01, PNorm = 88.7799, GNorm = 0.2202
Meta loss on this task batch = 3.8022e-01, Meta loss averaged over last 500 steps = 3.6615e-01, PNorm = 88.7903, GNorm = 0.2006
Meta loss on this task batch = 3.8029e-01, Meta loss averaged over last 500 steps = 3.6628e-01, PNorm = 88.8004, GNorm = 0.2384
Meta loss on this task batch = 3.4686e-01, Meta loss averaged over last 500 steps = 3.6628e-01, PNorm = 88.8110, GNorm = 0.1883
Meta loss on this task batch = 3.9231e-01, Meta loss averaged over last 500 steps = 3.6624e-01, PNorm = 88.8212, GNorm = 0.1876
Meta loss on this task batch = 4.4250e-01, Meta loss averaged over last 500 steps = 3.6643e-01, PNorm = 88.8312, GNorm = 0.1944
Meta loss on this task batch = 3.2312e-01, Meta loss averaged over last 500 steps = 3.6630e-01, PNorm = 88.8416, GNorm = 0.2050
Meta loss on this task batch = 3.5299e-01, Meta loss averaged over last 500 steps = 3.6640e-01, PNorm = 88.8525, GNorm = 0.2112
Meta loss on this task batch = 4.0588e-01, Meta loss averaged over last 500 steps = 3.6648e-01, PNorm = 88.8632, GNorm = 0.2030
Meta loss on this task batch = 3.2682e-01, Meta loss averaged over last 500 steps = 3.6636e-01, PNorm = 88.8746, GNorm = 0.1682
Meta loss on this task batch = 3.5530e-01, Meta loss averaged over last 500 steps = 3.6634e-01, PNorm = 88.8851, GNorm = 0.2191
Took 115.27655029296875 seconds to complete one epoch of meta training
Took 123.44593906402588 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465052
Epoch 173
Meta loss on this task batch = 4.1770e-01, Meta loss averaged over last 500 steps = 3.6644e-01, PNorm = 88.8948, GNorm = 0.2342
Meta loss on this task batch = 2.9793e-01, Meta loss averaged over last 500 steps = 3.6628e-01, PNorm = 88.9054, GNorm = 0.1763
Meta loss on this task batch = 3.6548e-01, Meta loss averaged over last 500 steps = 3.6618e-01, PNorm = 88.9151, GNorm = 0.1977
Meta loss on this task batch = 3.7816e-01, Meta loss averaged over last 500 steps = 3.6625e-01, PNorm = 88.9244, GNorm = 0.2416
Meta loss on this task batch = 3.8707e-01, Meta loss averaged over last 500 steps = 3.6627e-01, PNorm = 88.9327, GNorm = 0.2649
Meta loss on this task batch = 3.5732e-01, Meta loss averaged over last 500 steps = 3.6614e-01, PNorm = 88.9413, GNorm = 0.1881
Meta loss on this task batch = 3.3501e-01, Meta loss averaged over last 500 steps = 3.6605e-01, PNorm = 88.9503, GNorm = 0.2336
Meta loss on this task batch = 3.2194e-01, Meta loss averaged over last 500 steps = 3.6600e-01, PNorm = 88.9585, GNorm = 0.1733
Meta loss on this task batch = 3.8284e-01, Meta loss averaged over last 500 steps = 3.6600e-01, PNorm = 88.9667, GNorm = 0.2082
Meta loss on this task batch = 3.4106e-01, Meta loss averaged over last 500 steps = 3.6601e-01, PNorm = 88.9756, GNorm = 0.1973
Meta loss on this task batch = 3.4298e-01, Meta loss averaged over last 500 steps = 3.6592e-01, PNorm = 88.9857, GNorm = 0.2227
Meta loss on this task batch = 3.8024e-01, Meta loss averaged over last 500 steps = 3.6596e-01, PNorm = 88.9953, GNorm = 0.2272
Meta loss on this task batch = 3.6998e-01, Meta loss averaged over last 500 steps = 3.6579e-01, PNorm = 89.0046, GNorm = 0.2083
Meta loss on this task batch = 3.4278e-01, Meta loss averaged over last 500 steps = 3.6571e-01, PNorm = 89.0134, GNorm = 0.1817
Meta loss on this task batch = 4.5074e-01, Meta loss averaged over last 500 steps = 3.6585e-01, PNorm = 89.0215, GNorm = 0.2395
Meta loss on this task batch = 3.2352e-01, Meta loss averaged over last 500 steps = 3.6576e-01, PNorm = 89.0306, GNorm = 0.1812
Meta loss on this task batch = 3.5050e-01, Meta loss averaged over last 500 steps = 3.6577e-01, PNorm = 89.0398, GNorm = 0.2039
Meta loss on this task batch = 3.6254e-01, Meta loss averaged over last 500 steps = 3.6584e-01, PNorm = 89.0493, GNorm = 0.1931
Meta loss on this task batch = 3.7884e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 89.0596, GNorm = 0.2613
Took 113.1527910232544 seconds to complete one epoch of meta training
Took 120.92442774772644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499658
Epoch 174
Meta loss on this task batch = 3.4143e-01, Meta loss averaged over last 500 steps = 3.6586e-01, PNorm = 89.0696, GNorm = 0.1933
Meta loss on this task batch = 3.8153e-01, Meta loss averaged over last 500 steps = 3.6594e-01, PNorm = 89.0796, GNorm = 0.2054
Meta loss on this task batch = 3.6434e-01, Meta loss averaged over last 500 steps = 3.6589e-01, PNorm = 89.0897, GNorm = 0.2053
Meta loss on this task batch = 3.4315e-01, Meta loss averaged over last 500 steps = 3.6572e-01, PNorm = 89.0998, GNorm = 0.2093
Meta loss on this task batch = 3.5309e-01, Meta loss averaged over last 500 steps = 3.6566e-01, PNorm = 89.1096, GNorm = 0.1837
Meta loss on this task batch = 3.8898e-01, Meta loss averaged over last 500 steps = 3.6574e-01, PNorm = 89.1183, GNorm = 0.2685
Meta loss on this task batch = 3.8993e-01, Meta loss averaged over last 500 steps = 3.6568e-01, PNorm = 89.1271, GNorm = 0.2174
Meta loss on this task batch = 3.5369e-01, Meta loss averaged over last 500 steps = 3.6573e-01, PNorm = 89.1367, GNorm = 0.2206
Meta loss on this task batch = 3.3556e-01, Meta loss averaged over last 500 steps = 3.6563e-01, PNorm = 89.1475, GNorm = 0.1970
Meta loss on this task batch = 3.5483e-01, Meta loss averaged over last 500 steps = 3.6563e-01, PNorm = 89.1592, GNorm = 0.1931
Meta loss on this task batch = 3.5505e-01, Meta loss averaged over last 500 steps = 3.6569e-01, PNorm = 89.1712, GNorm = 0.1940
Meta loss on this task batch = 4.0818e-01, Meta loss averaged over last 500 steps = 3.6580e-01, PNorm = 89.1829, GNorm = 0.2089
Meta loss on this task batch = 3.6982e-01, Meta loss averaged over last 500 steps = 3.6576e-01, PNorm = 89.1943, GNorm = 0.2276
Meta loss on this task batch = 3.8164e-01, Meta loss averaged over last 500 steps = 3.6568e-01, PNorm = 89.2057, GNorm = 0.2157
Meta loss on this task batch = 3.6157e-01, Meta loss averaged over last 500 steps = 3.6565e-01, PNorm = 89.2165, GNorm = 0.2044
Meta loss on this task batch = 3.5023e-01, Meta loss averaged over last 500 steps = 3.6562e-01, PNorm = 89.2275, GNorm = 0.1946
Meta loss on this task batch = 3.2602e-01, Meta loss averaged over last 500 steps = 3.6549e-01, PNorm = 89.2387, GNorm = 0.1741
Meta loss on this task batch = 3.2991e-01, Meta loss averaged over last 500 steps = 3.6550e-01, PNorm = 89.2490, GNorm = 0.1842
Meta loss on this task batch = 3.5576e-01, Meta loss averaged over last 500 steps = 3.6554e-01, PNorm = 89.2582, GNorm = 0.2504
Took 114.25233554840088 seconds to complete one epoch of meta training
Took 120.9719865322113 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494686
Epoch 175
Meta loss on this task batch = 3.7600e-01, Meta loss averaged over last 500 steps = 3.6545e-01, PNorm = 89.2676, GNorm = 0.2106
Meta loss on this task batch = 4.2561e-01, Meta loss averaged over last 500 steps = 3.6556e-01, PNorm = 89.2760, GNorm = 0.2147
Meta loss on this task batch = 3.4101e-01, Meta loss averaged over last 500 steps = 3.6546e-01, PNorm = 89.2848, GNorm = 0.1968
Meta loss on this task batch = 3.7390e-01, Meta loss averaged over last 500 steps = 3.6554e-01, PNorm = 89.2934, GNorm = 0.2074
Meta loss on this task batch = 4.2162e-01, Meta loss averaged over last 500 steps = 3.6565e-01, PNorm = 89.3021, GNorm = 0.2281
Meta loss on this task batch = 3.7078e-01, Meta loss averaged over last 500 steps = 3.6557e-01, PNorm = 89.3115, GNorm = 0.1974
Meta loss on this task batch = 3.3654e-01, Meta loss averaged over last 500 steps = 3.6548e-01, PNorm = 89.3221, GNorm = 0.1922
Meta loss on this task batch = 3.0979e-01, Meta loss averaged over last 500 steps = 3.6537e-01, PNorm = 89.3345, GNorm = 0.2055
Meta loss on this task batch = 2.9568e-01, Meta loss averaged over last 500 steps = 3.6520e-01, PNorm = 89.3480, GNorm = 0.1966
Meta loss on this task batch = 3.4244e-01, Meta loss averaged over last 500 steps = 3.6514e-01, PNorm = 89.3609, GNorm = 0.2044
Meta loss on this task batch = 3.7550e-01, Meta loss averaged over last 500 steps = 3.6506e-01, PNorm = 89.3735, GNorm = 0.2316
Meta loss on this task batch = 3.6037e-01, Meta loss averaged over last 500 steps = 3.6505e-01, PNorm = 89.3835, GNorm = 0.2715
Meta loss on this task batch = 3.4128e-01, Meta loss averaged over last 500 steps = 3.6509e-01, PNorm = 89.3929, GNorm = 0.1980
Meta loss on this task batch = 3.5679e-01, Meta loss averaged over last 500 steps = 3.6506e-01, PNorm = 89.4031, GNorm = 0.1946
Meta loss on this task batch = 3.6646e-01, Meta loss averaged over last 500 steps = 3.6507e-01, PNorm = 89.4134, GNorm = 0.1986
Meta loss on this task batch = 3.8289e-01, Meta loss averaged over last 500 steps = 3.6504e-01, PNorm = 89.4235, GNorm = 0.2131
Meta loss on this task batch = 3.5806e-01, Meta loss averaged over last 500 steps = 3.6501e-01, PNorm = 89.4331, GNorm = 0.2165
Meta loss on this task batch = 3.1775e-01, Meta loss averaged over last 500 steps = 3.6487e-01, PNorm = 89.4435, GNorm = 0.1761
Meta loss on this task batch = 4.2911e-01, Meta loss averaged over last 500 steps = 3.6504e-01, PNorm = 89.4531, GNorm = 0.2494
Took 115.50301218032837 seconds to complete one epoch of meta training
Took 123.22984218597412 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499083
Epoch 176
Meta loss on this task batch = 3.1363e-01, Meta loss averaged over last 500 steps = 3.6492e-01, PNorm = 89.4645, GNorm = 0.2039
Meta loss on this task batch = 3.6348e-01, Meta loss averaged over last 500 steps = 3.6482e-01, PNorm = 89.4758, GNorm = 0.1709
Meta loss on this task batch = 3.8110e-01, Meta loss averaged over last 500 steps = 3.6498e-01, PNorm = 89.4880, GNorm = 0.2039
Meta loss on this task batch = 3.6999e-01, Meta loss averaged over last 500 steps = 3.6507e-01, PNorm = 89.5003, GNorm = 0.1996
Meta loss on this task batch = 3.6434e-01, Meta loss averaged over last 500 steps = 3.6500e-01, PNorm = 89.5123, GNorm = 0.2290
Meta loss on this task batch = 3.5290e-01, Meta loss averaged over last 500 steps = 3.6496e-01, PNorm = 89.5227, GNorm = 0.2188
Meta loss on this task batch = 3.7580e-01, Meta loss averaged over last 500 steps = 3.6501e-01, PNorm = 89.5332, GNorm = 0.1854
Meta loss on this task batch = 4.0540e-01, Meta loss averaged over last 500 steps = 3.6507e-01, PNorm = 89.5400, GNorm = 0.2970
Meta loss on this task batch = 3.1947e-01, Meta loss averaged over last 500 steps = 3.6489e-01, PNorm = 89.5475, GNorm = 0.2331
Meta loss on this task batch = 3.6204e-01, Meta loss averaged over last 500 steps = 3.6480e-01, PNorm = 89.5549, GNorm = 0.2095
Meta loss on this task batch = 3.5151e-01, Meta loss averaged over last 500 steps = 3.6492e-01, PNorm = 89.5633, GNorm = 0.1966
Meta loss on this task batch = 3.1694e-01, Meta loss averaged over last 500 steps = 3.6473e-01, PNorm = 89.5717, GNorm = 0.1806
Meta loss on this task batch = 3.6403e-01, Meta loss averaged over last 500 steps = 3.6482e-01, PNorm = 89.5814, GNorm = 0.1994
Meta loss on this task batch = 3.4234e-01, Meta loss averaged over last 500 steps = 3.6469e-01, PNorm = 89.5920, GNorm = 0.2296
Meta loss on this task batch = 3.8032e-01, Meta loss averaged over last 500 steps = 3.6465e-01, PNorm = 89.6033, GNorm = 0.2292
Meta loss on this task batch = 3.0943e-01, Meta loss averaged over last 500 steps = 3.6442e-01, PNorm = 89.6161, GNorm = 0.1960
Meta loss on this task batch = 3.5608e-01, Meta loss averaged over last 500 steps = 3.6437e-01, PNorm = 89.6284, GNorm = 0.2170
Meta loss on this task batch = 3.8559e-01, Meta loss averaged over last 500 steps = 3.6451e-01, PNorm = 89.6393, GNorm = 0.2167
Meta loss on this task batch = 4.2898e-01, Meta loss averaged over last 500 steps = 3.6462e-01, PNorm = 89.6483, GNorm = 0.2945
Took 112.7538754940033 seconds to complete one epoch of meta training
Took 120.50599813461304 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499491
Epoch 177
Meta loss on this task batch = 3.4387e-01, Meta loss averaged over last 500 steps = 3.6446e-01, PNorm = 89.6566, GNorm = 0.2038
Meta loss on this task batch = 3.5459e-01, Meta loss averaged over last 500 steps = 3.6458e-01, PNorm = 89.6662, GNorm = 0.2003
Meta loss on this task batch = 3.4870e-01, Meta loss averaged over last 500 steps = 3.6456e-01, PNorm = 89.6764, GNorm = 0.1940
Meta loss on this task batch = 3.6001e-01, Meta loss averaged over last 500 steps = 3.6454e-01, PNorm = 89.6865, GNorm = 0.1796
Meta loss on this task batch = 4.0338e-01, Meta loss averaged over last 500 steps = 3.6459e-01, PNorm = 89.6959, GNorm = 0.2510
Meta loss on this task batch = 3.6125e-01, Meta loss averaged over last 500 steps = 3.6456e-01, PNorm = 89.7064, GNorm = 0.1882
Meta loss on this task batch = 3.0939e-01, Meta loss averaged over last 500 steps = 3.6443e-01, PNorm = 89.7175, GNorm = 0.1730
Meta loss on this task batch = 4.5927e-01, Meta loss averaged over last 500 steps = 3.6454e-01, PNorm = 89.7276, GNorm = 0.2036
Meta loss on this task batch = 3.7539e-01, Meta loss averaged over last 500 steps = 3.6447e-01, PNorm = 89.7364, GNorm = 0.2144
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.6436e-01, PNorm = 89.7462, GNorm = 0.1925
Meta loss on this task batch = 4.0311e-01, Meta loss averaged over last 500 steps = 3.6446e-01, PNorm = 89.7561, GNorm = 0.2037
Meta loss on this task batch = 3.5468e-01, Meta loss averaged over last 500 steps = 3.6453e-01, PNorm = 89.7683, GNorm = 0.2222
Meta loss on this task batch = 3.7044e-01, Meta loss averaged over last 500 steps = 3.6453e-01, PNorm = 89.7809, GNorm = 0.2390
Meta loss on this task batch = 3.4773e-01, Meta loss averaged over last 500 steps = 3.6452e-01, PNorm = 89.7941, GNorm = 0.2206
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 3.6448e-01, PNorm = 89.8068, GNorm = 0.1951
Meta loss on this task batch = 3.2411e-01, Meta loss averaged over last 500 steps = 3.6435e-01, PNorm = 89.8197, GNorm = 0.2317
Meta loss on this task batch = 3.5445e-01, Meta loss averaged over last 500 steps = 3.6434e-01, PNorm = 89.8327, GNorm = 0.2019
Meta loss on this task batch = 3.3831e-01, Meta loss averaged over last 500 steps = 3.6430e-01, PNorm = 89.8455, GNorm = 0.2066
Meta loss on this task batch = 3.6638e-01, Meta loss averaged over last 500 steps = 3.6435e-01, PNorm = 89.8572, GNorm = 0.2504
Took 111.79871129989624 seconds to complete one epoch of meta training
Took 119.78352499008179 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503622
Epoch 178
Meta loss on this task batch = 3.6203e-01, Meta loss averaged over last 500 steps = 3.6432e-01, PNorm = 89.8660, GNorm = 0.2587
Meta loss on this task batch = 3.6602e-01, Meta loss averaged over last 500 steps = 3.6437e-01, PNorm = 89.8744, GNorm = 0.1993
Meta loss on this task batch = 3.7327e-01, Meta loss averaged over last 500 steps = 3.6423e-01, PNorm = 89.8835, GNorm = 0.1849
Meta loss on this task batch = 4.4393e-01, Meta loss averaged over last 500 steps = 3.6436e-01, PNorm = 89.8930, GNorm = 0.2232
Meta loss on this task batch = 2.9582e-01, Meta loss averaged over last 500 steps = 3.6428e-01, PNorm = 89.9039, GNorm = 0.1812
Meta loss on this task batch = 3.6626e-01, Meta loss averaged over last 500 steps = 3.6428e-01, PNorm = 89.9151, GNorm = 0.2564
Meta loss on this task batch = 3.5706e-01, Meta loss averaged over last 500 steps = 3.6434e-01, PNorm = 89.9267, GNorm = 0.1860
Meta loss on this task batch = 2.7533e-01, Meta loss averaged over last 500 steps = 3.6412e-01, PNorm = 89.9396, GNorm = 0.1780
Meta loss on this task batch = 3.7951e-01, Meta loss averaged over last 500 steps = 3.6418e-01, PNorm = 89.9529, GNorm = 0.1912
Meta loss on this task batch = 4.0440e-01, Meta loss averaged over last 500 steps = 3.6407e-01, PNorm = 89.9645, GNorm = 0.2108
Meta loss on this task batch = 3.2129e-01, Meta loss averaged over last 500 steps = 3.6402e-01, PNorm = 89.9767, GNorm = 0.1796
Meta loss on this task batch = 3.5615e-01, Meta loss averaged over last 500 steps = 3.6391e-01, PNorm = 89.9878, GNorm = 0.2094
Meta loss on this task batch = 3.2828e-01, Meta loss averaged over last 500 steps = 3.6378e-01, PNorm = 89.9991, GNorm = 0.1819
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 3.6375e-01, PNorm = 90.0100, GNorm = 0.1797
Meta loss on this task batch = 3.3467e-01, Meta loss averaged over last 500 steps = 3.6370e-01, PNorm = 90.0215, GNorm = 0.1874
Meta loss on this task batch = 3.7649e-01, Meta loss averaged over last 500 steps = 3.6362e-01, PNorm = 90.0327, GNorm = 0.2252
Meta loss on this task batch = 4.0090e-01, Meta loss averaged over last 500 steps = 3.6378e-01, PNorm = 90.0431, GNorm = 0.2288
Meta loss on this task batch = 3.3584e-01, Meta loss averaged over last 500 steps = 3.6362e-01, PNorm = 90.0537, GNorm = 0.1854
Meta loss on this task batch = 3.7118e-01, Meta loss averaged over last 500 steps = 3.6359e-01, PNorm = 90.0659, GNorm = 0.2935
Took 114.67412567138672 seconds to complete one epoch of meta training
Took 122.75115585327148 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477540
Epoch 179
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 3.6348e-01, PNorm = 90.0792, GNorm = 0.1971
Meta loss on this task batch = 4.5029e-01, Meta loss averaged over last 500 steps = 3.6359e-01, PNorm = 90.0916, GNorm = 0.2452
Meta loss on this task batch = 3.9771e-01, Meta loss averaged over last 500 steps = 3.6374e-01, PNorm = 90.1023, GNorm = 0.2772
Meta loss on this task batch = 3.7560e-01, Meta loss averaged over last 500 steps = 3.6369e-01, PNorm = 90.1119, GNorm = 0.2056
Meta loss on this task batch = 3.5464e-01, Meta loss averaged over last 500 steps = 3.6384e-01, PNorm = 90.1216, GNorm = 0.1808
Meta loss on this task batch = 3.7827e-01, Meta loss averaged over last 500 steps = 3.6378e-01, PNorm = 90.1314, GNorm = 0.2057
Meta loss on this task batch = 3.3215e-01, Meta loss averaged over last 500 steps = 3.6376e-01, PNorm = 90.1417, GNorm = 0.1739
Meta loss on this task batch = 4.1227e-01, Meta loss averaged over last 500 steps = 3.6390e-01, PNorm = 90.1492, GNorm = 0.2910
Meta loss on this task batch = 3.2230e-01, Meta loss averaged over last 500 steps = 3.6381e-01, PNorm = 90.1579, GNorm = 0.1896
Meta loss on this task batch = 3.3175e-01, Meta loss averaged over last 500 steps = 3.6365e-01, PNorm = 90.1665, GNorm = 0.1631
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 3.6357e-01, PNorm = 90.1762, GNorm = 0.1984
Meta loss on this task batch = 3.5613e-01, Meta loss averaged over last 500 steps = 3.6360e-01, PNorm = 90.1869, GNorm = 0.1678
Meta loss on this task batch = 3.6069e-01, Meta loss averaged over last 500 steps = 3.6358e-01, PNorm = 90.1980, GNorm = 0.2037
Meta loss on this task batch = 3.5240e-01, Meta loss averaged over last 500 steps = 3.6349e-01, PNorm = 90.2092, GNorm = 0.1895
Meta loss on this task batch = 3.3524e-01, Meta loss averaged over last 500 steps = 3.6343e-01, PNorm = 90.2209, GNorm = 0.1900
Meta loss on this task batch = 3.4012e-01, Meta loss averaged over last 500 steps = 3.6340e-01, PNorm = 90.2330, GNorm = 0.2050
Meta loss on this task batch = 3.7569e-01, Meta loss averaged over last 500 steps = 3.6331e-01, PNorm = 90.2448, GNorm = 0.2155
Meta loss on this task batch = 3.3564e-01, Meta loss averaged over last 500 steps = 3.6332e-01, PNorm = 90.2576, GNorm = 0.1950
Meta loss on this task batch = 4.3732e-01, Meta loss averaged over last 500 steps = 3.6345e-01, PNorm = 90.2681, GNorm = 0.3021
Took 129.30701518058777 seconds to complete one epoch of meta training
Took 137.2035768032074 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499527
Epoch 180
Meta loss on this task batch = 3.6003e-01, Meta loss averaged over last 500 steps = 3.6336e-01, PNorm = 90.2791, GNorm = 0.2169
Meta loss on this task batch = 3.5337e-01, Meta loss averaged over last 500 steps = 3.6334e-01, PNorm = 90.2900, GNorm = 0.2142
Meta loss on this task batch = 3.5130e-01, Meta loss averaged over last 500 steps = 3.6325e-01, PNorm = 90.2999, GNorm = 0.2091
Meta loss on this task batch = 3.6224e-01, Meta loss averaged over last 500 steps = 3.6331e-01, PNorm = 90.3112, GNorm = 0.2124
Meta loss on this task batch = 3.2654e-01, Meta loss averaged over last 500 steps = 3.6323e-01, PNorm = 90.3223, GNorm = 0.2222
Meta loss on this task batch = 3.3751e-01, Meta loss averaged over last 500 steps = 3.6313e-01, PNorm = 90.3339, GNorm = 0.2002
Meta loss on this task batch = 3.0109e-01, Meta loss averaged over last 500 steps = 3.6309e-01, PNorm = 90.3452, GNorm = 0.1840
Meta loss on this task batch = 4.2199e-01, Meta loss averaged over last 500 steps = 3.6319e-01, PNorm = 90.3559, GNorm = 0.2718
Meta loss on this task batch = 2.8984e-01, Meta loss averaged over last 500 steps = 3.6297e-01, PNorm = 90.3672, GNorm = 0.1683
Meta loss on this task batch = 4.0356e-01, Meta loss averaged over last 500 steps = 3.6303e-01, PNorm = 90.3781, GNorm = 0.2191
Meta loss on this task batch = 4.0705e-01, Meta loss averaged over last 500 steps = 3.6313e-01, PNorm = 90.3881, GNorm = 0.2173
Meta loss on this task batch = 3.2076e-01, Meta loss averaged over last 500 steps = 3.6301e-01, PNorm = 90.3986, GNorm = 0.1994
Meta loss on this task batch = 3.6634e-01, Meta loss averaged over last 500 steps = 3.6307e-01, PNorm = 90.4096, GNorm = 0.2013
Meta loss on this task batch = 3.3253e-01, Meta loss averaged over last 500 steps = 3.6295e-01, PNorm = 90.4200, GNorm = 0.1935
Meta loss on this task batch = 3.8886e-01, Meta loss averaged over last 500 steps = 3.6286e-01, PNorm = 90.4314, GNorm = 0.1999
Meta loss on this task batch = 3.3700e-01, Meta loss averaged over last 500 steps = 3.6284e-01, PNorm = 90.4417, GNorm = 0.2006
Meta loss on this task batch = 3.5111e-01, Meta loss averaged over last 500 steps = 3.6285e-01, PNorm = 90.4526, GNorm = 0.1766
Meta loss on this task batch = 3.7896e-01, Meta loss averaged over last 500 steps = 3.6279e-01, PNorm = 90.4635, GNorm = 0.2120
Meta loss on this task batch = 3.6003e-01, Meta loss averaged over last 500 steps = 3.6286e-01, PNorm = 90.4749, GNorm = 0.2175
Took 114.77960681915283 seconds to complete one epoch of meta training
Took 122.42801761627197 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491244
Epoch 181
Meta loss on this task batch = 3.8681e-01, Meta loss averaged over last 500 steps = 3.6294e-01, PNorm = 90.4856, GNorm = 0.2105
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 3.6298e-01, PNorm = 90.4973, GNorm = 0.1866
Meta loss on this task batch = 3.5257e-01, Meta loss averaged over last 500 steps = 3.6285e-01, PNorm = 90.5100, GNorm = 0.2208
Meta loss on this task batch = 3.4626e-01, Meta loss averaged over last 500 steps = 3.6273e-01, PNorm = 90.5211, GNorm = 0.2145
Meta loss on this task batch = 2.9810e-01, Meta loss averaged over last 500 steps = 3.6253e-01, PNorm = 90.5327, GNorm = 0.1867
Meta loss on this task batch = 3.7773e-01, Meta loss averaged over last 500 steps = 3.6252e-01, PNorm = 90.5431, GNorm = 0.2269
Meta loss on this task batch = 4.0605e-01, Meta loss averaged over last 500 steps = 3.6259e-01, PNorm = 90.5535, GNorm = 0.2798
Meta loss on this task batch = 3.1318e-01, Meta loss averaged over last 500 steps = 3.6251e-01, PNorm = 90.5649, GNorm = 0.1798
Meta loss on this task batch = 3.0613e-01, Meta loss averaged over last 500 steps = 3.6237e-01, PNorm = 90.5760, GNorm = 0.2093
Meta loss on this task batch = 3.4449e-01, Meta loss averaged over last 500 steps = 3.6227e-01, PNorm = 90.5852, GNorm = 0.2413
Meta loss on this task batch = 3.5094e-01, Meta loss averaged over last 500 steps = 3.6230e-01, PNorm = 90.5946, GNorm = 0.1845
Meta loss on this task batch = 4.0464e-01, Meta loss averaged over last 500 steps = 3.6238e-01, PNorm = 90.6036, GNorm = 0.2791
Meta loss on this task batch = 3.7865e-01, Meta loss averaged over last 500 steps = 3.6238e-01, PNorm = 90.6135, GNorm = 0.2590
Meta loss on this task batch = 3.5751e-01, Meta loss averaged over last 500 steps = 3.6234e-01, PNorm = 90.6243, GNorm = 0.2040
Meta loss on this task batch = 3.5455e-01, Meta loss averaged over last 500 steps = 3.6239e-01, PNorm = 90.6345, GNorm = 0.1858
Meta loss on this task batch = 3.7137e-01, Meta loss averaged over last 500 steps = 3.6254e-01, PNorm = 90.6447, GNorm = 0.2206
Meta loss on this task batch = 3.1186e-01, Meta loss averaged over last 500 steps = 3.6247e-01, PNorm = 90.6555, GNorm = 0.1821
Meta loss on this task batch = 3.9187e-01, Meta loss averaged over last 500 steps = 3.6246e-01, PNorm = 90.6658, GNorm = 0.2135
Meta loss on this task batch = 3.7245e-01, Meta loss averaged over last 500 steps = 3.6254e-01, PNorm = 90.6746, GNorm = 0.2750
Took 118.23676180839539 seconds to complete one epoch of meta training
Took 126.10593104362488 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464398
Epoch 182
Meta loss on this task batch = 3.3650e-01, Meta loss averaged over last 500 steps = 3.6235e-01, PNorm = 90.6852, GNorm = 0.2100
Meta loss on this task batch = 3.8160e-01, Meta loss averaged over last 500 steps = 3.6234e-01, PNorm = 90.6966, GNorm = 0.2077
Meta loss on this task batch = 3.2869e-01, Meta loss averaged over last 500 steps = 3.6218e-01, PNorm = 90.7087, GNorm = 0.2072
Meta loss on this task batch = 3.1005e-01, Meta loss averaged over last 500 steps = 3.6207e-01, PNorm = 90.7214, GNorm = 0.2035
Meta loss on this task batch = 3.3801e-01, Meta loss averaged over last 500 steps = 3.6203e-01, PNorm = 90.7352, GNorm = 0.2111
Meta loss on this task batch = 3.4086e-01, Meta loss averaged over last 500 steps = 3.6206e-01, PNorm = 90.7495, GNorm = 0.2013
Meta loss on this task batch = 3.7857e-01, Meta loss averaged over last 500 steps = 3.6197e-01, PNorm = 90.7616, GNorm = 0.2381
Meta loss on this task batch = 3.9598e-01, Meta loss averaged over last 500 steps = 3.6208e-01, PNorm = 90.7713, GNorm = 0.2298
Meta loss on this task batch = 3.5500e-01, Meta loss averaged over last 500 steps = 3.6221e-01, PNorm = 90.7801, GNorm = 0.2155
Meta loss on this task batch = 3.4041e-01, Meta loss averaged over last 500 steps = 3.6222e-01, PNorm = 90.7892, GNorm = 0.1829
Meta loss on this task batch = 4.3022e-01, Meta loss averaged over last 500 steps = 3.6235e-01, PNorm = 90.7966, GNorm = 0.2489
Meta loss on this task batch = 3.0805e-01, Meta loss averaged over last 500 steps = 3.6216e-01, PNorm = 90.8056, GNorm = 0.2117
Meta loss on this task batch = 4.0230e-01, Meta loss averaged over last 500 steps = 3.6226e-01, PNorm = 90.8152, GNorm = 0.2070
Meta loss on this task batch = 3.7370e-01, Meta loss averaged over last 500 steps = 3.6223e-01, PNorm = 90.8238, GNorm = 0.3203
Meta loss on this task batch = 3.3151e-01, Meta loss averaged over last 500 steps = 3.6217e-01, PNorm = 90.8327, GNorm = 0.1856
Meta loss on this task batch = 3.5852e-01, Meta loss averaged over last 500 steps = 3.6214e-01, PNorm = 90.8421, GNorm = 0.2226
Meta loss on this task batch = 3.7405e-01, Meta loss averaged over last 500 steps = 3.6209e-01, PNorm = 90.8525, GNorm = 0.2028
Meta loss on this task batch = 3.5000e-01, Meta loss averaged over last 500 steps = 3.6194e-01, PNorm = 90.8627, GNorm = 0.1975
Meta loss on this task batch = 3.5652e-01, Meta loss averaged over last 500 steps = 3.6201e-01, PNorm = 90.8740, GNorm = 0.2407
Took 113.76432394981384 seconds to complete one epoch of meta training
Took 121.51282382011414 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465304
Epoch 183
Meta loss on this task batch = 3.7853e-01, Meta loss averaged over last 500 steps = 3.6193e-01, PNorm = 90.8868, GNorm = 0.2136
Meta loss on this task batch = 3.9706e-01, Meta loss averaged over last 500 steps = 3.6213e-01, PNorm = 90.8985, GNorm = 0.2366
Meta loss on this task batch = 3.2590e-01, Meta loss averaged over last 500 steps = 3.6208e-01, PNorm = 90.9100, GNorm = 0.1859
Meta loss on this task batch = 3.6601e-01, Meta loss averaged over last 500 steps = 3.6198e-01, PNorm = 90.9208, GNorm = 0.2229
Meta loss on this task batch = 3.5234e-01, Meta loss averaged over last 500 steps = 3.6188e-01, PNorm = 90.9306, GNorm = 0.2163
Meta loss on this task batch = 3.5377e-01, Meta loss averaged over last 500 steps = 3.6187e-01, PNorm = 90.9401, GNorm = 0.2152
Meta loss on this task batch = 3.7828e-01, Meta loss averaged over last 500 steps = 3.6196e-01, PNorm = 90.9485, GNorm = 0.2057
Meta loss on this task batch = 3.2932e-01, Meta loss averaged over last 500 steps = 3.6198e-01, PNorm = 90.9575, GNorm = 0.1679
Meta loss on this task batch = 3.7233e-01, Meta loss averaged over last 500 steps = 3.6186e-01, PNorm = 90.9667, GNorm = 0.2269
Meta loss on this task batch = 3.6744e-01, Meta loss averaged over last 500 steps = 3.6187e-01, PNorm = 90.9761, GNorm = 0.1746
Meta loss on this task batch = 3.9850e-01, Meta loss averaged over last 500 steps = 3.6183e-01, PNorm = 90.9852, GNorm = 0.2067
Meta loss on this task batch = 3.7966e-01, Meta loss averaged over last 500 steps = 3.6185e-01, PNorm = 90.9953, GNorm = 0.2056
Meta loss on this task batch = 3.5633e-01, Meta loss averaged over last 500 steps = 3.6172e-01, PNorm = 91.0065, GNorm = 0.2053
Meta loss on this task batch = 2.7997e-01, Meta loss averaged over last 500 steps = 3.6156e-01, PNorm = 91.0186, GNorm = 0.1985
Meta loss on this task batch = 3.9354e-01, Meta loss averaged over last 500 steps = 3.6171e-01, PNorm = 91.0301, GNorm = 0.2151
Meta loss on this task batch = 3.5493e-01, Meta loss averaged over last 500 steps = 3.6174e-01, PNorm = 91.0419, GNorm = 0.2095
Meta loss on this task batch = 3.7512e-01, Meta loss averaged over last 500 steps = 3.6170e-01, PNorm = 91.0545, GNorm = 0.2026
Meta loss on this task batch = 3.0301e-01, Meta loss averaged over last 500 steps = 3.6165e-01, PNorm = 91.0667, GNorm = 0.1805
Meta loss on this task batch = 3.4012e-01, Meta loss averaged over last 500 steps = 3.6167e-01, PNorm = 91.0786, GNorm = 0.2763
Took 114.64759874343872 seconds to complete one epoch of meta training
Took 122.44991636276245 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484160
Epoch 184
Meta loss on this task batch = 3.2020e-01, Meta loss averaged over last 500 steps = 3.6160e-01, PNorm = 91.0910, GNorm = 0.2056
Meta loss on this task batch = 3.6295e-01, Meta loss averaged over last 500 steps = 3.6151e-01, PNorm = 91.1029, GNorm = 0.1962
Meta loss on this task batch = 2.6028e-01, Meta loss averaged over last 500 steps = 3.6124e-01, PNorm = 91.1148, GNorm = 0.2238
Meta loss on this task batch = 4.0457e-01, Meta loss averaged over last 500 steps = 3.6136e-01, PNorm = 91.1256, GNorm = 0.2294
Meta loss on this task batch = 3.2467e-01, Meta loss averaged over last 500 steps = 3.6132e-01, PNorm = 91.1358, GNorm = 0.1888
Meta loss on this task batch = 3.2494e-01, Meta loss averaged over last 500 steps = 3.6123e-01, PNorm = 91.1443, GNorm = 0.1869
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 3.6108e-01, PNorm = 91.1544, GNorm = 0.1944
Meta loss on this task batch = 4.0302e-01, Meta loss averaged over last 500 steps = 3.6110e-01, PNorm = 91.1650, GNorm = 0.2172
Meta loss on this task batch = 3.4847e-01, Meta loss averaged over last 500 steps = 3.6110e-01, PNorm = 91.1748, GNorm = 0.2196
Meta loss on this task batch = 4.1612e-01, Meta loss averaged over last 500 steps = 3.6127e-01, PNorm = 91.1840, GNorm = 0.2265
Meta loss on this task batch = 3.7567e-01, Meta loss averaged over last 500 steps = 3.6128e-01, PNorm = 91.1928, GNorm = 0.1895
Meta loss on this task batch = 3.8133e-01, Meta loss averaged over last 500 steps = 3.6139e-01, PNorm = 91.2011, GNorm = 0.1927
Meta loss on this task batch = 3.6260e-01, Meta loss averaged over last 500 steps = 3.6137e-01, PNorm = 91.2094, GNorm = 0.2009
Meta loss on this task batch = 3.6744e-01, Meta loss averaged over last 500 steps = 3.6141e-01, PNorm = 91.2180, GNorm = 0.2041
Meta loss on this task batch = 3.5586e-01, Meta loss averaged over last 500 steps = 3.6127e-01, PNorm = 91.2260, GNorm = 0.2090
Meta loss on this task batch = 3.7976e-01, Meta loss averaged over last 500 steps = 3.6121e-01, PNorm = 91.2345, GNorm = 0.2029
Meta loss on this task batch = 3.9600e-01, Meta loss averaged over last 500 steps = 3.6131e-01, PNorm = 91.2438, GNorm = 0.2052
Meta loss on this task batch = 3.2087e-01, Meta loss averaged over last 500 steps = 3.6127e-01, PNorm = 91.2539, GNorm = 0.2071
Meta loss on this task batch = 3.5902e-01, Meta loss averaged over last 500 steps = 3.6124e-01, PNorm = 91.2647, GNorm = 0.2146
Took 115.95783829689026 seconds to complete one epoch of meta training
Took 123.64939546585083 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479919
Epoch 185
Meta loss on this task batch = 3.5668e-01, Meta loss averaged over last 500 steps = 3.6129e-01, PNorm = 91.2757, GNorm = 0.2132
Meta loss on this task batch = 3.3404e-01, Meta loss averaged over last 500 steps = 3.6117e-01, PNorm = 91.2881, GNorm = 0.1899
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 3.6108e-01, PNorm = 91.3011, GNorm = 0.2213
Meta loss on this task batch = 3.5451e-01, Meta loss averaged over last 500 steps = 3.6102e-01, PNorm = 91.3136, GNorm = 0.1973
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 3.6087e-01, PNorm = 91.3263, GNorm = 0.1781
Meta loss on this task batch = 3.5710e-01, Meta loss averaged over last 500 steps = 3.6089e-01, PNorm = 91.3373, GNorm = 0.2585
Meta loss on this task batch = 3.9555e-01, Meta loss averaged over last 500 steps = 3.6104e-01, PNorm = 91.3457, GNorm = 0.2648
Meta loss on this task batch = 3.1443e-01, Meta loss averaged over last 500 steps = 3.6095e-01, PNorm = 91.3523, GNorm = 0.2132
Meta loss on this task batch = 3.5517e-01, Meta loss averaged over last 500 steps = 3.6093e-01, PNorm = 91.3595, GNorm = 0.2205
Meta loss on this task batch = 3.7787e-01, Meta loss averaged over last 500 steps = 3.6108e-01, PNorm = 91.3663, GNorm = 0.1917
Meta loss on this task batch = 4.2007e-01, Meta loss averaged over last 500 steps = 3.6117e-01, PNorm = 91.3721, GNorm = 0.2510
Meta loss on this task batch = 4.0139e-01, Meta loss averaged over last 500 steps = 3.6117e-01, PNorm = 91.3784, GNorm = 0.2740
Meta loss on this task batch = 3.5833e-01, Meta loss averaged over last 500 steps = 3.6122e-01, PNorm = 91.3862, GNorm = 0.1949
Meta loss on this task batch = 3.7110e-01, Meta loss averaged over last 500 steps = 3.6121e-01, PNorm = 91.3951, GNorm = 0.2034
Meta loss on this task batch = 3.5408e-01, Meta loss averaged over last 500 steps = 3.6122e-01, PNorm = 91.4056, GNorm = 0.1961
Meta loss on this task batch = 3.2130e-01, Meta loss averaged over last 500 steps = 3.6113e-01, PNorm = 91.4166, GNorm = 0.1907
Meta loss on this task batch = 3.0885e-01, Meta loss averaged over last 500 steps = 3.6097e-01, PNorm = 91.4284, GNorm = 0.1954
Meta loss on this task batch = 3.9766e-01, Meta loss averaged over last 500 steps = 3.6104e-01, PNorm = 91.4400, GNorm = 0.1840
Meta loss on this task batch = 4.0629e-01, Meta loss averaged over last 500 steps = 3.6107e-01, PNorm = 91.4489, GNorm = 0.2725
Took 115.91894054412842 seconds to complete one epoch of meta training
Took 123.46544241905212 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484710
Epoch 186
Meta loss on this task batch = 3.2888e-01, Meta loss averaged over last 500 steps = 3.6107e-01, PNorm = 91.4584, GNorm = 0.1736
Meta loss on this task batch = 3.4719e-01, Meta loss averaged over last 500 steps = 3.6101e-01, PNorm = 91.4671, GNorm = 0.1962
Meta loss on this task batch = 3.2626e-01, Meta loss averaged over last 500 steps = 3.6100e-01, PNorm = 91.4759, GNorm = 0.1868
Meta loss on this task batch = 3.1609e-01, Meta loss averaged over last 500 steps = 3.6093e-01, PNorm = 91.4850, GNorm = 0.1876
Meta loss on this task batch = 3.9775e-01, Meta loss averaged over last 500 steps = 3.6099e-01, PNorm = 91.4943, GNorm = 0.2326
Meta loss on this task batch = 3.9315e-01, Meta loss averaged over last 500 steps = 3.6112e-01, PNorm = 91.5031, GNorm = 0.2224
Meta loss on this task batch = 3.3500e-01, Meta loss averaged over last 500 steps = 3.6101e-01, PNorm = 91.5124, GNorm = 0.2212
Meta loss on this task batch = 2.9221e-01, Meta loss averaged over last 500 steps = 3.6086e-01, PNorm = 91.5228, GNorm = 0.1839
Meta loss on this task batch = 3.0977e-01, Meta loss averaged over last 500 steps = 3.6077e-01, PNorm = 91.5343, GNorm = 0.2144
Meta loss on this task batch = 4.3539e-01, Meta loss averaged over last 500 steps = 3.6089e-01, PNorm = 91.5452, GNorm = 0.2190
Meta loss on this task batch = 3.9455e-01, Meta loss averaged over last 500 steps = 3.6096e-01, PNorm = 91.5552, GNorm = 0.2470
Meta loss on this task batch = 3.4292e-01, Meta loss averaged over last 500 steps = 3.6093e-01, PNorm = 91.5652, GNorm = 0.1969
Meta loss on this task batch = 3.3314e-01, Meta loss averaged over last 500 steps = 3.6102e-01, PNorm = 91.5752, GNorm = 0.2016
Meta loss on this task batch = 3.3344e-01, Meta loss averaged over last 500 steps = 3.6100e-01, PNorm = 91.5841, GNorm = 0.2028
Meta loss on this task batch = 3.4726e-01, Meta loss averaged over last 500 steps = 3.6109e-01, PNorm = 91.5939, GNorm = 0.2271
Meta loss on this task batch = 2.9407e-01, Meta loss averaged over last 500 steps = 3.6085e-01, PNorm = 91.6038, GNorm = 0.1808
Meta loss on this task batch = 4.1567e-01, Meta loss averaged over last 500 steps = 3.6079e-01, PNorm = 91.6139, GNorm = 0.2453
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 3.6070e-01, PNorm = 91.6237, GNorm = 0.1900
Meta loss on this task batch = 3.8699e-01, Meta loss averaged over last 500 steps = 3.6075e-01, PNorm = 91.6339, GNorm = 0.2587
Took 116.66719174385071 seconds to complete one epoch of meta training
Took 124.51806282997131 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488121
Epoch 187
Meta loss on this task batch = 3.8950e-01, Meta loss averaged over last 500 steps = 3.6072e-01, PNorm = 91.6435, GNorm = 0.2383
Meta loss on this task batch = 3.7783e-01, Meta loss averaged over last 500 steps = 3.6077e-01, PNorm = 91.6523, GNorm = 0.2048
Meta loss on this task batch = 3.7327e-01, Meta loss averaged over last 500 steps = 3.6077e-01, PNorm = 91.6616, GNorm = 0.1878
Meta loss on this task batch = 3.0640e-01, Meta loss averaged over last 500 steps = 3.6057e-01, PNorm = 91.6715, GNorm = 0.1839
Meta loss on this task batch = 3.5991e-01, Meta loss averaged over last 500 steps = 3.6059e-01, PNorm = 91.6813, GNorm = 0.1884
Meta loss on this task batch = 3.2832e-01, Meta loss averaged over last 500 steps = 3.6047e-01, PNorm = 91.6919, GNorm = 0.2109
Meta loss on this task batch = 3.4571e-01, Meta loss averaged over last 500 steps = 3.6046e-01, PNorm = 91.7024, GNorm = 0.1856
Meta loss on this task batch = 2.9237e-01, Meta loss averaged over last 500 steps = 3.6037e-01, PNorm = 91.7141, GNorm = 0.1921
Meta loss on this task batch = 3.1730e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 91.7261, GNorm = 0.1799
Meta loss on this task batch = 3.0434e-01, Meta loss averaged over last 500 steps = 3.6019e-01, PNorm = 91.7373, GNorm = 0.2093
Meta loss on this task batch = 3.6689e-01, Meta loss averaged over last 500 steps = 3.6033e-01, PNorm = 91.7485, GNorm = 0.2002
Meta loss on this task batch = 3.4854e-01, Meta loss averaged over last 500 steps = 3.6027e-01, PNorm = 91.7597, GNorm = 0.1844
Meta loss on this task batch = 3.7864e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 91.7700, GNorm = 0.2301
Meta loss on this task batch = 4.3928e-01, Meta loss averaged over last 500 steps = 3.6049e-01, PNorm = 91.7782, GNorm = 0.2757
Meta loss on this task batch = 3.5353e-01, Meta loss averaged over last 500 steps = 3.6040e-01, PNorm = 91.7872, GNorm = 0.2153
Meta loss on this task batch = 3.2912e-01, Meta loss averaged over last 500 steps = 3.6033e-01, PNorm = 91.7961, GNorm = 0.2012
Meta loss on this task batch = 3.6072e-01, Meta loss averaged over last 500 steps = 3.6030e-01, PNorm = 91.8052, GNorm = 0.1931
Meta loss on this task batch = 3.7267e-01, Meta loss averaged over last 500 steps = 3.6029e-01, PNorm = 91.8143, GNorm = 0.2243
Meta loss on this task batch = 3.4345e-01, Meta loss averaged over last 500 steps = 3.6027e-01, PNorm = 91.8242, GNorm = 0.2366
Took 114.90225982666016 seconds to complete one epoch of meta training
Took 122.58353471755981 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475822
Epoch 188
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.6016e-01, PNorm = 91.8352, GNorm = 0.1839
Meta loss on this task batch = 3.4479e-01, Meta loss averaged over last 500 steps = 3.6015e-01, PNorm = 91.8477, GNorm = 0.1791
Meta loss on this task batch = 3.5033e-01, Meta loss averaged over last 500 steps = 3.6006e-01, PNorm = 91.8600, GNorm = 0.1795
Meta loss on this task batch = 3.6718e-01, Meta loss averaged over last 500 steps = 3.6010e-01, PNorm = 91.8722, GNorm = 0.1969
Meta loss on this task batch = 3.4517e-01, Meta loss averaged over last 500 steps = 3.5991e-01, PNorm = 91.8845, GNorm = 0.2237
Meta loss on this task batch = 3.5345e-01, Meta loss averaged over last 500 steps = 3.5988e-01, PNorm = 91.8959, GNorm = 0.2315
Meta loss on this task batch = 3.6667e-01, Meta loss averaged over last 500 steps = 3.5979e-01, PNorm = 91.9057, GNorm = 0.1968
Meta loss on this task batch = 3.3395e-01, Meta loss averaged over last 500 steps = 3.5968e-01, PNorm = 91.9164, GNorm = 0.2082
Meta loss on this task batch = 3.4828e-01, Meta loss averaged over last 500 steps = 3.5968e-01, PNorm = 91.9272, GNorm = 0.2011
Meta loss on this task batch = 3.3071e-01, Meta loss averaged over last 500 steps = 3.5955e-01, PNorm = 91.9380, GNorm = 0.1886
Meta loss on this task batch = 3.3442e-01, Meta loss averaged over last 500 steps = 3.5959e-01, PNorm = 91.9484, GNorm = 0.2029
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 3.5941e-01, PNorm = 91.9593, GNorm = 0.2545
Meta loss on this task batch = 3.7694e-01, Meta loss averaged over last 500 steps = 3.5938e-01, PNorm = 91.9698, GNorm = 0.2385
Meta loss on this task batch = 3.5871e-01, Meta loss averaged over last 500 steps = 3.5936e-01, PNorm = 91.9797, GNorm = 0.2084
Meta loss on this task batch = 3.7067e-01, Meta loss averaged over last 500 steps = 3.5943e-01, PNorm = 91.9899, GNorm = 0.2024
Meta loss on this task batch = 3.4993e-01, Meta loss averaged over last 500 steps = 3.5939e-01, PNorm = 91.9988, GNorm = 0.2425
Meta loss on this task batch = 3.6911e-01, Meta loss averaged over last 500 steps = 3.5938e-01, PNorm = 92.0070, GNorm = 0.2388
Meta loss on this task batch = 3.9667e-01, Meta loss averaged over last 500 steps = 3.5956e-01, PNorm = 92.0141, GNorm = 0.2254
Meta loss on this task batch = 3.1871e-01, Meta loss averaged over last 500 steps = 3.5942e-01, PNorm = 92.0218, GNorm = 0.2256
Took 113.66571497917175 seconds to complete one epoch of meta training
Took 121.38139963150024 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490334
Epoch 189
Meta loss on this task batch = 2.3253e-01, Meta loss averaged over last 500 steps = 3.5921e-01, PNorm = 92.0310, GNorm = 0.1580
Meta loss on this task batch = 3.5184e-01, Meta loss averaged over last 500 steps = 3.5923e-01, PNorm = 92.0407, GNorm = 0.2203
Meta loss on this task batch = 3.5260e-01, Meta loss averaged over last 500 steps = 3.5918e-01, PNorm = 92.0500, GNorm = 0.2301
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 3.5904e-01, PNorm = 92.0606, GNorm = 0.1681
Meta loss on this task batch = 3.2341e-01, Meta loss averaged over last 500 steps = 3.5893e-01, PNorm = 92.0719, GNorm = 0.1874
Meta loss on this task batch = 3.5365e-01, Meta loss averaged over last 500 steps = 3.5889e-01, PNorm = 92.0837, GNorm = 0.2420
Meta loss on this task batch = 4.1301e-01, Meta loss averaged over last 500 steps = 3.5911e-01, PNorm = 92.0955, GNorm = 0.2338
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 3.5905e-01, PNorm = 92.1078, GNorm = 0.1941
Meta loss on this task batch = 3.5210e-01, Meta loss averaged over last 500 steps = 3.5898e-01, PNorm = 92.1201, GNorm = 0.2065
Meta loss on this task batch = 3.6484e-01, Meta loss averaged over last 500 steps = 3.5887e-01, PNorm = 92.1317, GNorm = 0.2057
Meta loss on this task batch = 4.0817e-01, Meta loss averaged over last 500 steps = 3.5899e-01, PNorm = 92.1425, GNorm = 0.2459
Meta loss on this task batch = 3.4129e-01, Meta loss averaged over last 500 steps = 3.5897e-01, PNorm = 92.1532, GNorm = 0.2067
Meta loss on this task batch = 3.5558e-01, Meta loss averaged over last 500 steps = 3.5889e-01, PNorm = 92.1642, GNorm = 0.2122
Meta loss on this task batch = 3.4742e-01, Meta loss averaged over last 500 steps = 3.5898e-01, PNorm = 92.1759, GNorm = 0.2068
Meta loss on this task batch = 4.1053e-01, Meta loss averaged over last 500 steps = 3.5901e-01, PNorm = 92.1861, GNorm = 0.2418
Meta loss on this task batch = 3.7591e-01, Meta loss averaged over last 500 steps = 3.5914e-01, PNorm = 92.1959, GNorm = 0.1765
Meta loss on this task batch = 3.3194e-01, Meta loss averaged over last 500 steps = 3.5909e-01, PNorm = 92.2061, GNorm = 0.1743
Meta loss on this task batch = 3.8148e-01, Meta loss averaged over last 500 steps = 3.5911e-01, PNorm = 92.2158, GNorm = 0.2325
Meta loss on this task batch = 3.5857e-01, Meta loss averaged over last 500 steps = 3.5921e-01, PNorm = 92.2253, GNorm = 0.2326
Took 115.51134657859802 seconds to complete one epoch of meta training
Took 123.13741326332092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465116
Epoch 190
Meta loss on this task batch = 3.5609e-01, Meta loss averaged over last 500 steps = 3.5919e-01, PNorm = 92.2355, GNorm = 0.1894
Meta loss on this task batch = 3.4980e-01, Meta loss averaged over last 500 steps = 3.5918e-01, PNorm = 92.2450, GNorm = 0.2157
Meta loss on this task batch = 3.9581e-01, Meta loss averaged over last 500 steps = 3.5922e-01, PNorm = 92.2536, GNorm = 0.2042
Meta loss on this task batch = 3.9621e-01, Meta loss averaged over last 500 steps = 3.5916e-01, PNorm = 92.2621, GNorm = 0.2422
Meta loss on this task batch = 3.9645e-01, Meta loss averaged over last 500 steps = 3.5920e-01, PNorm = 92.2709, GNorm = 0.2095
Meta loss on this task batch = 3.3274e-01, Meta loss averaged over last 500 steps = 3.5904e-01, PNorm = 92.2808, GNorm = 0.2108
Meta loss on this task batch = 3.5807e-01, Meta loss averaged over last 500 steps = 3.5897e-01, PNorm = 92.2916, GNorm = 0.1861
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 3.5886e-01, PNorm = 92.3041, GNorm = 0.2477
Meta loss on this task batch = 2.9143e-01, Meta loss averaged over last 500 steps = 3.5881e-01, PNorm = 92.3177, GNorm = 0.1662
Meta loss on this task batch = 2.9684e-01, Meta loss averaged over last 500 steps = 3.5869e-01, PNorm = 92.3317, GNorm = 0.1802
Meta loss on this task batch = 3.7792e-01, Meta loss averaged over last 500 steps = 3.5872e-01, PNorm = 92.3452, GNorm = 0.2228
Meta loss on this task batch = 3.9372e-01, Meta loss averaged over last 500 steps = 3.5880e-01, PNorm = 92.3563, GNorm = 0.2524
Meta loss on this task batch = 3.5022e-01, Meta loss averaged over last 500 steps = 3.5889e-01, PNorm = 92.3660, GNorm = 0.2486
Meta loss on this task batch = 3.4544e-01, Meta loss averaged over last 500 steps = 3.5885e-01, PNorm = 92.3733, GNorm = 0.2748
Meta loss on this task batch = 3.8348e-01, Meta loss averaged over last 500 steps = 3.5881e-01, PNorm = 92.3801, GNorm = 0.2413
Meta loss on this task batch = 3.7652e-01, Meta loss averaged over last 500 steps = 3.5878e-01, PNorm = 92.3879, GNorm = 0.2091
Meta loss on this task batch = 2.9889e-01, Meta loss averaged over last 500 steps = 3.5863e-01, PNorm = 92.3962, GNorm = 0.1853
Meta loss on this task batch = 3.8076e-01, Meta loss averaged over last 500 steps = 3.5863e-01, PNorm = 92.4045, GNorm = 0.2253
Meta loss on this task batch = 3.3484e-01, Meta loss averaged over last 500 steps = 3.5865e-01, PNorm = 92.4139, GNorm = 0.2018
Took 115.20886564254761 seconds to complete one epoch of meta training
Took 123.36316680908203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502692
Epoch 191
Meta loss on this task batch = 3.4676e-01, Meta loss averaged over last 500 steps = 3.5872e-01, PNorm = 92.4228, GNorm = 0.2014
Meta loss on this task batch = 3.9238e-01, Meta loss averaged over last 500 steps = 3.5875e-01, PNorm = 92.4328, GNorm = 0.1991
Meta loss on this task batch = 3.7683e-01, Meta loss averaged over last 500 steps = 3.5884e-01, PNorm = 92.4434, GNorm = 0.1942
Meta loss on this task batch = 3.5055e-01, Meta loss averaged over last 500 steps = 3.5873e-01, PNorm = 92.4548, GNorm = 0.1891
Meta loss on this task batch = 3.7545e-01, Meta loss averaged over last 500 steps = 3.5880e-01, PNorm = 92.4668, GNorm = 0.1977
Meta loss on this task batch = 3.3928e-01, Meta loss averaged over last 500 steps = 3.5863e-01, PNorm = 92.4789, GNorm = 0.1847
Meta loss on this task batch = 3.1585e-01, Meta loss averaged over last 500 steps = 3.5860e-01, PNorm = 92.4906, GNorm = 0.1958
Meta loss on this task batch = 3.0908e-01, Meta loss averaged over last 500 steps = 3.5846e-01, PNorm = 92.5027, GNorm = 0.2281
Meta loss on this task batch = 3.2609e-01, Meta loss averaged over last 500 steps = 3.5839e-01, PNorm = 92.5142, GNorm = 0.2198
Meta loss on this task batch = 3.5947e-01, Meta loss averaged over last 500 steps = 3.5831e-01, PNorm = 92.5253, GNorm = 0.2654
Meta loss on this task batch = 3.3632e-01, Meta loss averaged over last 500 steps = 3.5832e-01, PNorm = 92.5350, GNorm = 0.2421
Meta loss on this task batch = 3.7880e-01, Meta loss averaged over last 500 steps = 3.5829e-01, PNorm = 92.5433, GNorm = 0.2586
Meta loss on this task batch = 3.0229e-01, Meta loss averaged over last 500 steps = 3.5821e-01, PNorm = 92.5520, GNorm = 0.1829
Meta loss on this task batch = 3.2224e-01, Meta loss averaged over last 500 steps = 3.5819e-01, PNorm = 92.5604, GNorm = 0.2287
Meta loss on this task batch = 4.6035e-01, Meta loss averaged over last 500 steps = 3.5837e-01, PNorm = 92.5677, GNorm = 0.2895
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 3.5825e-01, PNorm = 92.5760, GNorm = 0.2237
Meta loss on this task batch = 4.0323e-01, Meta loss averaged over last 500 steps = 3.5837e-01, PNorm = 92.5834, GNorm = 0.2579
Meta loss on this task batch = 3.5517e-01, Meta loss averaged over last 500 steps = 3.5834e-01, PNorm = 92.5909, GNorm = 0.2111
Meta loss on this task batch = 3.4740e-01, Meta loss averaged over last 500 steps = 3.5832e-01, PNorm = 92.5989, GNorm = 0.2389
Took 153.1156988143921 seconds to complete one epoch of meta training
Took 160.60327911376953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499860
Epoch 192
Meta loss on this task batch = 3.5382e-01, Meta loss averaged over last 500 steps = 3.5826e-01, PNorm = 92.6075, GNorm = 0.1732
Meta loss on this task batch = 3.7531e-01, Meta loss averaged over last 500 steps = 3.5830e-01, PNorm = 92.6166, GNorm = 0.2113
Meta loss on this task batch = 4.0309e-01, Meta loss averaged over last 500 steps = 3.5841e-01, PNorm = 92.6260, GNorm = 0.2121
Meta loss on this task batch = 3.2968e-01, Meta loss averaged over last 500 steps = 3.5830e-01, PNorm = 92.6356, GNorm = 0.1916
Meta loss on this task batch = 3.3548e-01, Meta loss averaged over last 500 steps = 3.5831e-01, PNorm = 92.6464, GNorm = 0.2143
Meta loss on this task batch = 3.6881e-01, Meta loss averaged over last 500 steps = 3.5823e-01, PNorm = 92.6565, GNorm = 0.2572
Meta loss on this task batch = 4.1044e-01, Meta loss averaged over last 500 steps = 3.5833e-01, PNorm = 92.6658, GNorm = 0.2141
Meta loss on this task batch = 3.4108e-01, Meta loss averaged over last 500 steps = 3.5826e-01, PNorm = 92.6754, GNorm = 0.1954
Meta loss on this task batch = 3.4911e-01, Meta loss averaged over last 500 steps = 3.5818e-01, PNorm = 92.6858, GNorm = 0.1863
Meta loss on this task batch = 3.2128e-01, Meta loss averaged over last 500 steps = 3.5801e-01, PNorm = 92.6973, GNorm = 0.1837
Meta loss on this task batch = 3.2496e-01, Meta loss averaged over last 500 steps = 3.5795e-01, PNorm = 92.7096, GNorm = 0.2163
Meta loss on this task batch = 3.5962e-01, Meta loss averaged over last 500 steps = 3.5788e-01, PNorm = 92.7210, GNorm = 0.2461
Meta loss on this task batch = 3.5285e-01, Meta loss averaged over last 500 steps = 3.5794e-01, PNorm = 92.7309, GNorm = 0.2153
Meta loss on this task batch = 4.0837e-01, Meta loss averaged over last 500 steps = 3.5808e-01, PNorm = 92.7385, GNorm = 0.2392
Meta loss on this task batch = 3.0126e-01, Meta loss averaged over last 500 steps = 3.5798e-01, PNorm = 92.7474, GNorm = 0.1930
Meta loss on this task batch = 3.4047e-01, Meta loss averaged over last 500 steps = 3.5800e-01, PNorm = 92.7564, GNorm = 0.2001
Meta loss on this task batch = 3.4918e-01, Meta loss averaged over last 500 steps = 3.5793e-01, PNorm = 92.7647, GNorm = 0.2179
Meta loss on this task batch = 3.1001e-01, Meta loss averaged over last 500 steps = 3.5778e-01, PNorm = 92.7737, GNorm = 0.2039
Meta loss on this task batch = 2.5496e-01, Meta loss averaged over last 500 steps = 3.5758e-01, PNorm = 92.7832, GNorm = 0.2065
Took 116.42164587974548 seconds to complete one epoch of meta training
Took 123.99188017845154 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474173
Epoch 193
Meta loss on this task batch = 4.0308e-01, Meta loss averaged over last 500 steps = 3.5773e-01, PNorm = 92.7924, GNorm = 0.2469
Meta loss on this task batch = 3.7224e-01, Meta loss averaged over last 500 steps = 3.5783e-01, PNorm = 92.8026, GNorm = 0.2195
Meta loss on this task batch = 3.2436e-01, Meta loss averaged over last 500 steps = 3.5774e-01, PNorm = 92.8137, GNorm = 0.1781
Meta loss on this task batch = 3.3205e-01, Meta loss averaged over last 500 steps = 3.5767e-01, PNorm = 92.8247, GNorm = 0.2133
Meta loss on this task batch = 3.5316e-01, Meta loss averaged over last 500 steps = 3.5742e-01, PNorm = 92.8357, GNorm = 0.2295
Meta loss on this task batch = 4.2821e-01, Meta loss averaged over last 500 steps = 3.5767e-01, PNorm = 92.8465, GNorm = 0.2111
Meta loss on this task batch = 3.9321e-01, Meta loss averaged over last 500 steps = 3.5770e-01, PNorm = 92.8565, GNorm = 0.1984
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.5753e-01, PNorm = 92.8666, GNorm = 0.1814
Meta loss on this task batch = 3.8158e-01, Meta loss averaged over last 500 steps = 3.5747e-01, PNorm = 92.8766, GNorm = 0.2223
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 3.5737e-01, PNorm = 92.8883, GNorm = 0.1934
Meta loss on this task batch = 3.0653e-01, Meta loss averaged over last 500 steps = 3.5727e-01, PNorm = 92.8994, GNorm = 0.1789
Meta loss on this task batch = 2.9885e-01, Meta loss averaged over last 500 steps = 3.5706e-01, PNorm = 92.9101, GNorm = 0.1927
Meta loss on this task batch = 3.5033e-01, Meta loss averaged over last 500 steps = 3.5721e-01, PNorm = 92.9208, GNorm = 0.1895
Meta loss on this task batch = 3.7949e-01, Meta loss averaged over last 500 steps = 3.5731e-01, PNorm = 92.9306, GNorm = 0.2337
Meta loss on this task batch = 4.0102e-01, Meta loss averaged over last 500 steps = 3.5738e-01, PNorm = 92.9405, GNorm = 0.2354
Meta loss on this task batch = 3.5659e-01, Meta loss averaged over last 500 steps = 3.5733e-01, PNorm = 92.9504, GNorm = 0.2252
Meta loss on this task batch = 3.3390e-01, Meta loss averaged over last 500 steps = 3.5732e-01, PNorm = 92.9611, GNorm = 0.1970
Meta loss on this task batch = 3.1409e-01, Meta loss averaged over last 500 steps = 3.5731e-01, PNorm = 92.9727, GNorm = 0.1803
Meta loss on this task batch = 3.8077e-01, Meta loss averaged over last 500 steps = 3.5734e-01, PNorm = 92.9820, GNorm = 0.2892
Took 116.16459798812866 seconds to complete one epoch of meta training
Took 123.79566383361816 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493916
Epoch 194
Meta loss on this task batch = 3.7708e-01, Meta loss averaged over last 500 steps = 3.5731e-01, PNorm = 92.9911, GNorm = 0.2770
Meta loss on this task batch = 3.1204e-01, Meta loss averaged over last 500 steps = 3.5725e-01, PNorm = 93.0000, GNorm = 0.1968
Meta loss on this task batch = 3.7658e-01, Meta loss averaged over last 500 steps = 3.5720e-01, PNorm = 93.0091, GNorm = 0.2096
Meta loss on this task batch = 3.5817e-01, Meta loss averaged over last 500 steps = 3.5719e-01, PNorm = 93.0178, GNorm = 0.2098
Meta loss on this task batch = 3.8551e-01, Meta loss averaged over last 500 steps = 3.5721e-01, PNorm = 93.0265, GNorm = 0.2270
Meta loss on this task batch = 4.1175e-01, Meta loss averaged over last 500 steps = 3.5733e-01, PNorm = 93.0339, GNorm = 0.2332
Meta loss on this task batch = 3.7349e-01, Meta loss averaged over last 500 steps = 3.5725e-01, PNorm = 93.0424, GNorm = 0.2046
Meta loss on this task batch = 3.7998e-01, Meta loss averaged over last 500 steps = 3.5734e-01, PNorm = 93.0505, GNorm = 0.2368
Meta loss on this task batch = 3.3729e-01, Meta loss averaged over last 500 steps = 3.5725e-01, PNorm = 93.0597, GNorm = 0.1863
Meta loss on this task batch = 3.0314e-01, Meta loss averaged over last 500 steps = 3.5717e-01, PNorm = 93.0703, GNorm = 0.1917
Meta loss on this task batch = 3.3109e-01, Meta loss averaged over last 500 steps = 3.5718e-01, PNorm = 93.0810, GNorm = 0.1915
Meta loss on this task batch = 3.3375e-01, Meta loss averaged over last 500 steps = 3.5705e-01, PNorm = 93.0911, GNorm = 0.1954
Meta loss on this task batch = 3.4414e-01, Meta loss averaged over last 500 steps = 3.5700e-01, PNorm = 93.1020, GNorm = 0.2038
Meta loss on this task batch = 3.6395e-01, Meta loss averaged over last 500 steps = 3.5689e-01, PNorm = 93.1135, GNorm = 0.2000
Meta loss on this task batch = 3.5427e-01, Meta loss averaged over last 500 steps = 3.5693e-01, PNorm = 93.1243, GNorm = 0.2063
Meta loss on this task batch = 3.2732e-01, Meta loss averaged over last 500 steps = 3.5686e-01, PNorm = 93.1350, GNorm = 0.2142
Meta loss on this task batch = 3.5075e-01, Meta loss averaged over last 500 steps = 3.5678e-01, PNorm = 93.1452, GNorm = 0.2279
Meta loss on this task batch = 3.1367e-01, Meta loss averaged over last 500 steps = 3.5665e-01, PNorm = 93.1544, GNorm = 0.2305
Meta loss on this task batch = 3.2614e-01, Meta loss averaged over last 500 steps = 3.5660e-01, PNorm = 93.1632, GNorm = 0.2363
Took 114.33132719993591 seconds to complete one epoch of meta training
Took 122.46528100967407 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507033
Found better MAML checkpoint after meta validation, saving now
Epoch 195
Meta loss on this task batch = 2.9384e-01, Meta loss averaged over last 500 steps = 3.5653e-01, PNorm = 93.1716, GNorm = 0.1882
Meta loss on this task batch = 3.6763e-01, Meta loss averaged over last 500 steps = 3.5654e-01, PNorm = 93.1800, GNorm = 0.2209
Meta loss on this task batch = 2.9692e-01, Meta loss averaged over last 500 steps = 3.5637e-01, PNorm = 93.1888, GNorm = 0.1973
Meta loss on this task batch = 4.1908e-01, Meta loss averaged over last 500 steps = 3.5652e-01, PNorm = 93.1954, GNorm = 0.2841
Meta loss on this task batch = 3.3158e-01, Meta loss averaged over last 500 steps = 3.5653e-01, PNorm = 93.2030, GNorm = 0.1938
Meta loss on this task batch = 3.3327e-01, Meta loss averaged over last 500 steps = 3.5639e-01, PNorm = 93.2117, GNorm = 0.1972
Meta loss on this task batch = 3.4416e-01, Meta loss averaged over last 500 steps = 3.5634e-01, PNorm = 93.2196, GNorm = 0.2152
Meta loss on this task batch = 3.2601e-01, Meta loss averaged over last 500 steps = 3.5636e-01, PNorm = 93.2278, GNorm = 0.2037
Meta loss on this task batch = 3.1599e-01, Meta loss averaged over last 500 steps = 3.5627e-01, PNorm = 93.2368, GNorm = 0.2105
Meta loss on this task batch = 3.3714e-01, Meta loss averaged over last 500 steps = 3.5622e-01, PNorm = 93.2460, GNorm = 0.1950
Meta loss on this task batch = 3.1040e-01, Meta loss averaged over last 500 steps = 3.5622e-01, PNorm = 93.2558, GNorm = 0.1918
Meta loss on this task batch = 4.0008e-01, Meta loss averaged over last 500 steps = 3.5628e-01, PNorm = 93.2662, GNorm = 0.2503
Meta loss on this task batch = 3.5593e-01, Meta loss averaged over last 500 steps = 3.5627e-01, PNorm = 93.2747, GNorm = 0.2360
Meta loss on this task batch = 3.9572e-01, Meta loss averaged over last 500 steps = 3.5632e-01, PNorm = 93.2833, GNorm = 0.2700
Meta loss on this task batch = 3.4706e-01, Meta loss averaged over last 500 steps = 3.5628e-01, PNorm = 93.2911, GNorm = 0.2248
Meta loss on this task batch = 3.6569e-01, Meta loss averaged over last 500 steps = 3.5624e-01, PNorm = 93.2972, GNorm = 0.2653
Meta loss on this task batch = 3.9392e-01, Meta loss averaged over last 500 steps = 3.5638e-01, PNorm = 93.3036, GNorm = 0.2190
Meta loss on this task batch = 3.7206e-01, Meta loss averaged over last 500 steps = 3.5637e-01, PNorm = 93.3106, GNorm = 0.2091
Meta loss on this task batch = 4.0304e-01, Meta loss averaged over last 500 steps = 3.5638e-01, PNorm = 93.3173, GNorm = 0.2419
Took 117.60954356193542 seconds to complete one epoch of meta training
Took 125.62626957893372 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476986
Epoch 196
Meta loss on this task batch = 3.4540e-01, Meta loss averaged over last 500 steps = 3.5641e-01, PNorm = 93.3257, GNorm = 0.2035
Meta loss on this task batch = 3.7180e-01, Meta loss averaged over last 500 steps = 3.5653e-01, PNorm = 93.3344, GNorm = 0.2019
Meta loss on this task batch = 2.6608e-01, Meta loss averaged over last 500 steps = 3.5635e-01, PNorm = 93.3444, GNorm = 0.1755
Meta loss on this task batch = 4.2569e-01, Meta loss averaged over last 500 steps = 3.5647e-01, PNorm = 93.3548, GNorm = 0.1959
Meta loss on this task batch = 3.4465e-01, Meta loss averaged over last 500 steps = 3.5644e-01, PNorm = 93.3666, GNorm = 0.1903
Meta loss on this task batch = 3.7805e-01, Meta loss averaged over last 500 steps = 3.5629e-01, PNorm = 93.3783, GNorm = 0.2016
Meta loss on this task batch = 3.7875e-01, Meta loss averaged over last 500 steps = 3.5634e-01, PNorm = 93.3901, GNorm = 0.1819
Meta loss on this task batch = 2.9586e-01, Meta loss averaged over last 500 steps = 3.5633e-01, PNorm = 93.4022, GNorm = 0.1964
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 3.5626e-01, PNorm = 93.4125, GNorm = 0.2195
Meta loss on this task batch = 3.5638e-01, Meta loss averaged over last 500 steps = 3.5628e-01, PNorm = 93.4221, GNorm = 0.2169
Meta loss on this task batch = 3.3594e-01, Meta loss averaged over last 500 steps = 3.5617e-01, PNorm = 93.4306, GNorm = 0.2556
Meta loss on this task batch = 3.2522e-01, Meta loss averaged over last 500 steps = 3.5616e-01, PNorm = 93.4395, GNorm = 0.2155
Meta loss on this task batch = 2.8501e-01, Meta loss averaged over last 500 steps = 3.5605e-01, PNorm = 93.4483, GNorm = 0.1878
Meta loss on this task batch = 3.9923e-01, Meta loss averaged over last 500 steps = 3.5613e-01, PNorm = 93.4562, GNorm = 0.2253
Meta loss on this task batch = 3.8515e-01, Meta loss averaged over last 500 steps = 3.5620e-01, PNorm = 93.4631, GNorm = 0.2612
Meta loss on this task batch = 3.4126e-01, Meta loss averaged over last 500 steps = 3.5616e-01, PNorm = 93.4694, GNorm = 0.2173
Meta loss on this task batch = 3.1193e-01, Meta loss averaged over last 500 steps = 3.5605e-01, PNorm = 93.4771, GNorm = 0.2027
Meta loss on this task batch = 3.6284e-01, Meta loss averaged over last 500 steps = 3.5606e-01, PNorm = 93.4853, GNorm = 0.2082
Meta loss on this task batch = 3.0920e-01, Meta loss averaged over last 500 steps = 3.5594e-01, PNorm = 93.4946, GNorm = 0.2209
Took 121.73278188705444 seconds to complete one epoch of meta training
Took 129.05149364471436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485685
Epoch 197
Meta loss on this task batch = 3.7933e-01, Meta loss averaged over last 500 steps = 3.5597e-01, PNorm = 93.5056, GNorm = 0.2383
Meta loss on this task batch = 3.3570e-01, Meta loss averaged over last 500 steps = 3.5591e-01, PNorm = 93.5182, GNorm = 0.1914
Meta loss on this task batch = 3.4552e-01, Meta loss averaged over last 500 steps = 3.5581e-01, PNorm = 93.5311, GNorm = 0.2010
Meta loss on this task batch = 3.4011e-01, Meta loss averaged over last 500 steps = 3.5574e-01, PNorm = 93.5455, GNorm = 0.1957
Meta loss on this task batch = 3.8255e-01, Meta loss averaged over last 500 steps = 3.5584e-01, PNorm = 93.5602, GNorm = 0.2202
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 3.5554e-01, PNorm = 93.5759, GNorm = 0.1735
Meta loss on this task batch = 3.6968e-01, Meta loss averaged over last 500 steps = 3.5563e-01, PNorm = 93.5896, GNorm = 0.2397
Meta loss on this task batch = 2.9428e-01, Meta loss averaged over last 500 steps = 3.5536e-01, PNorm = 93.6020, GNorm = 0.2088
Meta loss on this task batch = 3.6588e-01, Meta loss averaged over last 500 steps = 3.5526e-01, PNorm = 93.6119, GNorm = 0.2507
Meta loss on this task batch = 3.4082e-01, Meta loss averaged over last 500 steps = 3.5521e-01, PNorm = 93.6208, GNorm = 0.2480
Meta loss on this task batch = 3.5946e-01, Meta loss averaged over last 500 steps = 3.5517e-01, PNorm = 93.6272, GNorm = 0.2669
Meta loss on this task batch = 3.8578e-01, Meta loss averaged over last 500 steps = 3.5529e-01, PNorm = 93.6335, GNorm = 0.2699
Meta loss on this task batch = 3.4409e-01, Meta loss averaged over last 500 steps = 3.5521e-01, PNorm = 93.6406, GNorm = 0.2226
Meta loss on this task batch = 3.9447e-01, Meta loss averaged over last 500 steps = 3.5534e-01, PNorm = 93.6487, GNorm = 0.1881
Meta loss on this task batch = 3.1879e-01, Meta loss averaged over last 500 steps = 3.5522e-01, PNorm = 93.6579, GNorm = 0.1834
Meta loss on this task batch = 3.6173e-01, Meta loss averaged over last 500 steps = 3.5526e-01, PNorm = 93.6678, GNorm = 0.2064
Meta loss on this task batch = 3.5812e-01, Meta loss averaged over last 500 steps = 3.5526e-01, PNorm = 93.6781, GNorm = 0.2127
Meta loss on this task batch = 3.9892e-01, Meta loss averaged over last 500 steps = 3.5548e-01, PNorm = 93.6881, GNorm = 0.2184
Meta loss on this task batch = 3.4786e-01, Meta loss averaged over last 500 steps = 3.5552e-01, PNorm = 93.6998, GNorm = 0.2493
Took 114.55914044380188 seconds to complete one epoch of meta training
Took 122.46733951568604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488098
Epoch 198
Meta loss on this task batch = 3.5780e-01, Meta loss averaged over last 500 steps = 3.5546e-01, PNorm = 93.7112, GNorm = 0.1752
Meta loss on this task batch = 3.6921e-01, Meta loss averaged over last 500 steps = 3.5540e-01, PNorm = 93.7234, GNorm = 0.2165
Meta loss on this task batch = 4.0996e-01, Meta loss averaged over last 500 steps = 3.5555e-01, PNorm = 93.7346, GNorm = 0.2560
Meta loss on this task batch = 3.8205e-01, Meta loss averaged over last 500 steps = 3.5542e-01, PNorm = 93.7459, GNorm = 0.2175
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 3.5539e-01, PNorm = 93.7582, GNorm = 0.1812
Meta loss on this task batch = 3.0499e-01, Meta loss averaged over last 500 steps = 3.5525e-01, PNorm = 93.7701, GNorm = 0.2073
Meta loss on this task batch = 3.7103e-01, Meta loss averaged over last 500 steps = 3.5530e-01, PNorm = 93.7807, GNorm = 0.2559
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 3.5519e-01, PNorm = 93.7906, GNorm = 0.1999
Meta loss on this task batch = 3.2472e-01, Meta loss averaged over last 500 steps = 3.5514e-01, PNorm = 93.7993, GNorm = 0.1956
Meta loss on this task batch = 3.8499e-01, Meta loss averaged over last 500 steps = 3.5513e-01, PNorm = 93.8069, GNorm = 0.2396
Meta loss on this task batch = 3.7687e-01, Meta loss averaged over last 500 steps = 3.5515e-01, PNorm = 93.8139, GNorm = 0.2698
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 3.5506e-01, PNorm = 93.8221, GNorm = 0.2150
Meta loss on this task batch = 3.5895e-01, Meta loss averaged over last 500 steps = 3.5509e-01, PNorm = 93.8302, GNorm = 0.2162
Meta loss on this task batch = 3.4209e-01, Meta loss averaged over last 500 steps = 3.5511e-01, PNorm = 93.8386, GNorm = 0.2083
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 3.5509e-01, PNorm = 93.8468, GNorm = 0.2033
Meta loss on this task batch = 4.1427e-01, Meta loss averaged over last 500 steps = 3.5516e-01, PNorm = 93.8548, GNorm = 0.2445
Meta loss on this task batch = 3.8345e-01, Meta loss averaged over last 500 steps = 3.5516e-01, PNorm = 93.8649, GNorm = 0.2023
Meta loss on this task batch = 3.5344e-01, Meta loss averaged over last 500 steps = 3.5517e-01, PNorm = 93.8753, GNorm = 0.1981
Meta loss on this task batch = 3.3985e-01, Meta loss averaged over last 500 steps = 3.5507e-01, PNorm = 93.8858, GNorm = 0.2157
Took 114.34834718704224 seconds to complete one epoch of meta training
Took 121.38136434555054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497554
Epoch 199
Meta loss on this task batch = 3.2629e-01, Meta loss averaged over last 500 steps = 3.5484e-01, PNorm = 93.8962, GNorm = 0.1732
Meta loss on this task batch = 3.5733e-01, Meta loss averaged over last 500 steps = 3.5491e-01, PNorm = 93.9064, GNorm = 0.1633
Meta loss on this task batch = 3.5140e-01, Meta loss averaged over last 500 steps = 3.5490e-01, PNorm = 93.9167, GNorm = 0.2088
Meta loss on this task batch = 3.9570e-01, Meta loss averaged over last 500 steps = 3.5488e-01, PNorm = 93.9281, GNorm = 0.1981
Meta loss on this task batch = 3.9622e-01, Meta loss averaged over last 500 steps = 3.5502e-01, PNorm = 93.9387, GNorm = 0.2260
Meta loss on this task batch = 3.2222e-01, Meta loss averaged over last 500 steps = 3.5495e-01, PNorm = 93.9494, GNorm = 0.1946
Meta loss on this task batch = 3.8184e-01, Meta loss averaged over last 500 steps = 3.5488e-01, PNorm = 93.9584, GNorm = 0.2094
Meta loss on this task batch = 3.4234e-01, Meta loss averaged over last 500 steps = 3.5497e-01, PNorm = 93.9668, GNorm = 0.2131
Meta loss on this task batch = 3.8109e-01, Meta loss averaged over last 500 steps = 3.5500e-01, PNorm = 93.9740, GNorm = 0.2175
Meta loss on this task batch = 3.5650e-01, Meta loss averaged over last 500 steps = 3.5496e-01, PNorm = 93.9830, GNorm = 0.2085
Meta loss on this task batch = 3.7370e-01, Meta loss averaged over last 500 steps = 3.5493e-01, PNorm = 93.9926, GNorm = 0.2437
Meta loss on this task batch = 3.5673e-01, Meta loss averaged over last 500 steps = 3.5493e-01, PNorm = 94.0008, GNorm = 0.2384
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 3.5480e-01, PNorm = 94.0103, GNorm = 0.1946
Meta loss on this task batch = 3.5887e-01, Meta loss averaged over last 500 steps = 3.5487e-01, PNorm = 94.0189, GNorm = 0.2649
Meta loss on this task batch = 3.3009e-01, Meta loss averaged over last 500 steps = 3.5477e-01, PNorm = 94.0274, GNorm = 0.2138
Meta loss on this task batch = 3.9764e-01, Meta loss averaged over last 500 steps = 3.5488e-01, PNorm = 94.0362, GNorm = 0.2472
Meta loss on this task batch = 3.5635e-01, Meta loss averaged over last 500 steps = 3.5491e-01, PNorm = 94.0455, GNorm = 0.2159
Meta loss on this task batch = 3.7998e-01, Meta loss averaged over last 500 steps = 3.5491e-01, PNorm = 94.0556, GNorm = 0.2368
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 3.5472e-01, PNorm = 94.0673, GNorm = 0.2009
Took 116.48467516899109 seconds to complete one epoch of meta training
Took 124.6386730670929 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478117
Epoch 200
Meta loss on this task batch = 3.4922e-01, Meta loss averaged over last 500 steps = 3.5474e-01, PNorm = 94.0790, GNorm = 0.2074
Meta loss on this task batch = 3.5205e-01, Meta loss averaged over last 500 steps = 3.5454e-01, PNorm = 94.0914, GNorm = 0.2110
Meta loss on this task batch = 3.7284e-01, Meta loss averaged over last 500 steps = 3.5464e-01, PNorm = 94.1039, GNorm = 0.1979
Meta loss on this task batch = 3.6342e-01, Meta loss averaged over last 500 steps = 3.5466e-01, PNorm = 94.1168, GNorm = 0.2124
Meta loss on this task batch = 3.8939e-01, Meta loss averaged over last 500 steps = 3.5472e-01, PNorm = 94.1277, GNorm = 0.2464
Meta loss on this task batch = 3.6434e-01, Meta loss averaged over last 500 steps = 3.5469e-01, PNorm = 94.1385, GNorm = 0.1902
Meta loss on this task batch = 3.2715e-01, Meta loss averaged over last 500 steps = 3.5466e-01, PNorm = 94.1495, GNorm = 0.2042
Meta loss on this task batch = 3.6604e-01, Meta loss averaged over last 500 steps = 3.5463e-01, PNorm = 94.1608, GNorm = 0.2085
Meta loss on this task batch = 3.6067e-01, Meta loss averaged over last 500 steps = 3.5462e-01, PNorm = 94.1725, GNorm = 0.2330
Meta loss on this task batch = 2.6392e-01, Meta loss averaged over last 500 steps = 3.5446e-01, PNorm = 94.1848, GNorm = 0.1862
Meta loss on this task batch = 3.5771e-01, Meta loss averaged over last 500 steps = 3.5447e-01, PNorm = 94.1964, GNorm = 0.2790
Meta loss on this task batch = 3.6192e-01, Meta loss averaged over last 500 steps = 3.5442e-01, PNorm = 94.2076, GNorm = 0.2185
Meta loss on this task batch = 3.0516e-01, Meta loss averaged over last 500 steps = 3.5425e-01, PNorm = 94.2190, GNorm = 0.2088
Meta loss on this task batch = 3.5334e-01, Meta loss averaged over last 500 steps = 3.5425e-01, PNorm = 94.2313, GNorm = 0.2518
Meta loss on this task batch = 3.4556e-01, Meta loss averaged over last 500 steps = 3.5427e-01, PNorm = 94.2431, GNorm = 0.2229
Meta loss on this task batch = 3.4762e-01, Meta loss averaged over last 500 steps = 3.5425e-01, PNorm = 94.2541, GNorm = 0.2238
Meta loss on this task batch = 3.6996e-01, Meta loss averaged over last 500 steps = 3.5428e-01, PNorm = 94.2636, GNorm = 0.2464
Meta loss on this task batch = 3.3857e-01, Meta loss averaged over last 500 steps = 3.5414e-01, PNorm = 94.2730, GNorm = 0.1918
Meta loss on this task batch = 3.4546e-01, Meta loss averaged over last 500 steps = 3.5409e-01, PNorm = 94.2815, GNorm = 0.2458
Took 116.41810369491577 seconds to complete one epoch of meta training
Took 123.83906245231628 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466795
Epoch 201
Meta loss on this task batch = 3.6807e-01, Meta loss averaged over last 500 steps = 3.5407e-01, PNorm = 94.2896, GNorm = 0.1859
Meta loss on this task batch = 3.0898e-01, Meta loss averaged over last 500 steps = 3.5396e-01, PNorm = 94.2976, GNorm = 0.1881
Meta loss on this task batch = 4.0004e-01, Meta loss averaged over last 500 steps = 3.5406e-01, PNorm = 94.3049, GNorm = 0.2435
Meta loss on this task batch = 3.0833e-01, Meta loss averaged over last 500 steps = 3.5403e-01, PNorm = 94.3126, GNorm = 0.1869
Meta loss on this task batch = 3.4469e-01, Meta loss averaged over last 500 steps = 3.5406e-01, PNorm = 94.3206, GNorm = 0.1967
Meta loss on this task batch = 3.5313e-01, Meta loss averaged over last 500 steps = 3.5405e-01, PNorm = 94.3296, GNorm = 0.2071
Meta loss on this task batch = 3.1262e-01, Meta loss averaged over last 500 steps = 3.5392e-01, PNorm = 94.3394, GNorm = 0.1906
Meta loss on this task batch = 3.8202e-01, Meta loss averaged over last 500 steps = 3.5384e-01, PNorm = 94.3485, GNorm = 0.2719
Meta loss on this task batch = 3.1720e-01, Meta loss averaged over last 500 steps = 3.5379e-01, PNorm = 94.3573, GNorm = 0.1815
Meta loss on this task batch = 3.3537e-01, Meta loss averaged over last 500 steps = 3.5371e-01, PNorm = 94.3659, GNorm = 0.1869
Meta loss on this task batch = 3.4283e-01, Meta loss averaged over last 500 steps = 3.5355e-01, PNorm = 94.3743, GNorm = 0.2193
Meta loss on this task batch = 3.7076e-01, Meta loss averaged over last 500 steps = 3.5355e-01, PNorm = 94.3829, GNorm = 0.2005
Meta loss on this task batch = 3.3680e-01, Meta loss averaged over last 500 steps = 3.5355e-01, PNorm = 94.3913, GNorm = 0.2131
Meta loss on this task batch = 3.5670e-01, Meta loss averaged over last 500 steps = 3.5365e-01, PNorm = 94.4000, GNorm = 0.2027
Meta loss on this task batch = 3.5319e-01, Meta loss averaged over last 500 steps = 3.5376e-01, PNorm = 94.4086, GNorm = 0.2033
Meta loss on this task batch = 3.6828e-01, Meta loss averaged over last 500 steps = 3.5381e-01, PNorm = 94.4178, GNorm = 0.2215
Meta loss on this task batch = 3.6715e-01, Meta loss averaged over last 500 steps = 3.5380e-01, PNorm = 94.4268, GNorm = 0.2050
Meta loss on this task batch = 3.8238e-01, Meta loss averaged over last 500 steps = 3.5384e-01, PNorm = 94.4362, GNorm = 0.2009
Meta loss on this task batch = 3.8397e-01, Meta loss averaged over last 500 steps = 3.5393e-01, PNorm = 94.4452, GNorm = 0.2340
Took 113.93722200393677 seconds to complete one epoch of meta training
Took 120.60102915763855 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465039
Epoch 202
Meta loss on this task batch = 3.8240e-01, Meta loss averaged over last 500 steps = 3.5398e-01, PNorm = 94.4539, GNorm = 0.2054
Meta loss on this task batch = 2.9390e-01, Meta loss averaged over last 500 steps = 3.5383e-01, PNorm = 94.4632, GNorm = 0.1762
Meta loss on this task batch = 3.4009e-01, Meta loss averaged over last 500 steps = 3.5375e-01, PNorm = 94.4746, GNorm = 0.2248
Meta loss on this task batch = 3.5150e-01, Meta loss averaged over last 500 steps = 3.5373e-01, PNorm = 94.4851, GNorm = 0.2183
Meta loss on this task batch = 3.7680e-01, Meta loss averaged over last 500 steps = 3.5385e-01, PNorm = 94.4956, GNorm = 0.2242
Meta loss on this task batch = 3.8341e-01, Meta loss averaged over last 500 steps = 3.5376e-01, PNorm = 94.5043, GNorm = 0.2126
Meta loss on this task batch = 3.1792e-01, Meta loss averaged over last 500 steps = 3.5377e-01, PNorm = 94.5119, GNorm = 0.2169
Meta loss on this task batch = 3.5245e-01, Meta loss averaged over last 500 steps = 3.5375e-01, PNorm = 94.5185, GNorm = 0.2055
Meta loss on this task batch = 3.1653e-01, Meta loss averaged over last 500 steps = 3.5362e-01, PNorm = 94.5252, GNorm = 0.2067
Meta loss on this task batch = 3.8294e-01, Meta loss averaged over last 500 steps = 3.5364e-01, PNorm = 94.5321, GNorm = 0.1819
Meta loss on this task batch = 3.2137e-01, Meta loss averaged over last 500 steps = 3.5356e-01, PNorm = 94.5396, GNorm = 0.2130
Meta loss on this task batch = 3.4160e-01, Meta loss averaged over last 500 steps = 3.5354e-01, PNorm = 94.5478, GNorm = 0.2081
Meta loss on this task batch = 3.0821e-01, Meta loss averaged over last 500 steps = 3.5340e-01, PNorm = 94.5567, GNorm = 0.1922
Meta loss on this task batch = 3.1032e-01, Meta loss averaged over last 500 steps = 3.5321e-01, PNorm = 94.5658, GNorm = 0.2483
Meta loss on this task batch = 3.4461e-01, Meta loss averaged over last 500 steps = 3.5326e-01, PNorm = 94.5748, GNorm = 0.1912
Meta loss on this task batch = 3.4825e-01, Meta loss averaged over last 500 steps = 3.5323e-01, PNorm = 94.5846, GNorm = 0.1843
Meta loss on this task batch = 3.4137e-01, Meta loss averaged over last 500 steps = 3.5321e-01, PNorm = 94.5950, GNorm = 0.2204
Meta loss on this task batch = 4.1174e-01, Meta loss averaged over last 500 steps = 3.5340e-01, PNorm = 94.6043, GNorm = 0.2213
Meta loss on this task batch = 3.6947e-01, Meta loss averaged over last 500 steps = 3.5341e-01, PNorm = 94.6136, GNorm = 0.2400
Took 113.66587424278259 seconds to complete one epoch of meta training
Took 121.77048635482788 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468521
Epoch 203
Meta loss on this task batch = 3.6984e-01, Meta loss averaged over last 500 steps = 3.5347e-01, PNorm = 94.6232, GNorm = 0.2114
Meta loss on this task batch = 3.4050e-01, Meta loss averaged over last 500 steps = 3.5339e-01, PNorm = 94.6339, GNorm = 0.1976
Meta loss on this task batch = 2.8573e-01, Meta loss averaged over last 500 steps = 3.5334e-01, PNorm = 94.6452, GNorm = 0.1920
Meta loss on this task batch = 3.8219e-01, Meta loss averaged over last 500 steps = 3.5339e-01, PNorm = 94.6557, GNorm = 0.2197
Meta loss on this task batch = 3.3089e-01, Meta loss averaged over last 500 steps = 3.5328e-01, PNorm = 94.6667, GNorm = 0.2173
Meta loss on this task batch = 3.6256e-01, Meta loss averaged over last 500 steps = 3.5315e-01, PNorm = 94.6761, GNorm = 0.2239
Meta loss on this task batch = 3.2496e-01, Meta loss averaged over last 500 steps = 3.5311e-01, PNorm = 94.6861, GNorm = 0.2089
Meta loss on this task batch = 3.8881e-01, Meta loss averaged over last 500 steps = 3.5318e-01, PNorm = 94.6955, GNorm = 0.2082
Meta loss on this task batch = 3.4304e-01, Meta loss averaged over last 500 steps = 3.5317e-01, PNorm = 94.7048, GNorm = 0.2007
Meta loss on this task batch = 3.6457e-01, Meta loss averaged over last 500 steps = 3.5318e-01, PNorm = 94.7139, GNorm = 0.2065
Meta loss on this task batch = 3.2546e-01, Meta loss averaged over last 500 steps = 3.5302e-01, PNorm = 94.7232, GNorm = 0.1855
Meta loss on this task batch = 3.1883e-01, Meta loss averaged over last 500 steps = 3.5294e-01, PNorm = 94.7328, GNorm = 0.1872
Meta loss on this task batch = 3.1867e-01, Meta loss averaged over last 500 steps = 3.5296e-01, PNorm = 94.7431, GNorm = 0.1842
Meta loss on this task batch = 3.2478e-01, Meta loss averaged over last 500 steps = 3.5269e-01, PNorm = 94.7535, GNorm = 0.1749
Meta loss on this task batch = 3.8318e-01, Meta loss averaged over last 500 steps = 3.5270e-01, PNorm = 94.7643, GNorm = 0.2057
Meta loss on this task batch = 2.9985e-01, Meta loss averaged over last 500 steps = 3.5267e-01, PNorm = 94.7753, GNorm = 0.2030
Meta loss on this task batch = 3.2272e-01, Meta loss averaged over last 500 steps = 3.5251e-01, PNorm = 94.7860, GNorm = 0.1990
Meta loss on this task batch = 4.0401e-01, Meta loss averaged over last 500 steps = 3.5261e-01, PNorm = 94.7960, GNorm = 0.2864
Meta loss on this task batch = 3.5821e-01, Meta loss averaged over last 500 steps = 3.5259e-01, PNorm = 94.8054, GNorm = 0.2403
Took 114.3841700553894 seconds to complete one epoch of meta training
Took 120.95787739753723 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499734
Epoch 204
Meta loss on this task batch = 3.8632e-01, Meta loss averaged over last 500 steps = 3.5266e-01, PNorm = 94.8138, GNorm = 0.2511
Meta loss on this task batch = 3.8345e-01, Meta loss averaged over last 500 steps = 3.5275e-01, PNorm = 94.8208, GNorm = 0.2615
Meta loss on this task batch = 3.5255e-01, Meta loss averaged over last 500 steps = 3.5281e-01, PNorm = 94.8275, GNorm = 0.2152
Meta loss on this task batch = 3.9566e-01, Meta loss averaged over last 500 steps = 3.5289e-01, PNorm = 94.8327, GNorm = 0.2121
Meta loss on this task batch = 3.4549e-01, Meta loss averaged over last 500 steps = 3.5291e-01, PNorm = 94.8388, GNorm = 0.2136
Meta loss on this task batch = 3.5124e-01, Meta loss averaged over last 500 steps = 3.5288e-01, PNorm = 94.8454, GNorm = 0.2283
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 3.5271e-01, PNorm = 94.8535, GNorm = 0.1933
Meta loss on this task batch = 3.2345e-01, Meta loss averaged over last 500 steps = 3.5263e-01, PNorm = 94.8630, GNorm = 0.2401
Meta loss on this task batch = 3.1071e-01, Meta loss averaged over last 500 steps = 3.5250e-01, PNorm = 94.8738, GNorm = 0.2133
Meta loss on this task batch = 3.4444e-01, Meta loss averaged over last 500 steps = 3.5230e-01, PNorm = 94.8839, GNorm = 0.2337
Meta loss on this task batch = 3.2040e-01, Meta loss averaged over last 500 steps = 3.5235e-01, PNorm = 94.8938, GNorm = 0.1972
Meta loss on this task batch = 3.6799e-01, Meta loss averaged over last 500 steps = 3.5235e-01, PNorm = 94.9035, GNorm = 0.2313
Meta loss on this task batch = 3.3100e-01, Meta loss averaged over last 500 steps = 3.5230e-01, PNorm = 94.9132, GNorm = 0.1843
Meta loss on this task batch = 3.3526e-01, Meta loss averaged over last 500 steps = 3.5242e-01, PNorm = 94.9230, GNorm = 0.2124
Meta loss on this task batch = 3.4574e-01, Meta loss averaged over last 500 steps = 3.5235e-01, PNorm = 94.9321, GNorm = 0.2227
Meta loss on this task batch = 3.1958e-01, Meta loss averaged over last 500 steps = 3.5218e-01, PNorm = 94.9414, GNorm = 0.1931
Meta loss on this task batch = 3.1320e-01, Meta loss averaged over last 500 steps = 3.5217e-01, PNorm = 94.9500, GNorm = 0.2060
Meta loss on this task batch = 4.1903e-01, Meta loss averaged over last 500 steps = 3.5229e-01, PNorm = 94.9581, GNorm = 0.2668
Meta loss on this task batch = 3.4002e-01, Meta loss averaged over last 500 steps = 3.5232e-01, PNorm = 94.9677, GNorm = 0.2538
Took 114.6128396987915 seconds to complete one epoch of meta training
Took 122.20359015464783 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506597
Epoch 205
Meta loss on this task batch = 3.3722e-01, Meta loss averaged over last 500 steps = 3.5235e-01, PNorm = 94.9780, GNorm = 0.2321
Meta loss on this task batch = 3.2320e-01, Meta loss averaged over last 500 steps = 3.5233e-01, PNorm = 94.9885, GNorm = 0.1917
Meta loss on this task batch = 3.7533e-01, Meta loss averaged over last 500 steps = 3.5233e-01, PNorm = 94.9990, GNorm = 0.2397
Meta loss on this task batch = 4.0444e-01, Meta loss averaged over last 500 steps = 3.5233e-01, PNorm = 95.0073, GNorm = 0.2366
Meta loss on this task batch = 3.4086e-01, Meta loss averaged over last 500 steps = 3.5234e-01, PNorm = 95.0149, GNorm = 0.2073
Meta loss on this task batch = 3.9713e-01, Meta loss averaged over last 500 steps = 3.5240e-01, PNorm = 95.0221, GNorm = 0.2200
Meta loss on this task batch = 3.6119e-01, Meta loss averaged over last 500 steps = 3.5247e-01, PNorm = 95.0287, GNorm = 0.2219
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 3.5214e-01, PNorm = 95.0376, GNorm = 0.1978
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 3.5199e-01, PNorm = 95.0468, GNorm = 0.1866
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.5190e-01, PNorm = 95.0558, GNorm = 0.2015
Meta loss on this task batch = 3.1568e-01, Meta loss averaged over last 500 steps = 3.5182e-01, PNorm = 95.0645, GNorm = 0.1990
Meta loss on this task batch = 3.3262e-01, Meta loss averaged over last 500 steps = 3.5173e-01, PNorm = 95.0732, GNorm = 0.1990
Meta loss on this task batch = 3.4601e-01, Meta loss averaged over last 500 steps = 3.5176e-01, PNorm = 95.0821, GNorm = 0.1981
Meta loss on this task batch = 2.8955e-01, Meta loss averaged over last 500 steps = 3.5151e-01, PNorm = 95.0918, GNorm = 0.1743
Meta loss on this task batch = 3.9805e-01, Meta loss averaged over last 500 steps = 3.5166e-01, PNorm = 95.0999, GNorm = 0.2410
Meta loss on this task batch = 3.3511e-01, Meta loss averaged over last 500 steps = 3.5167e-01, PNorm = 95.1088, GNorm = 0.2011
Meta loss on this task batch = 3.6921e-01, Meta loss averaged over last 500 steps = 3.5178e-01, PNorm = 95.1176, GNorm = 0.2201
Meta loss on this task batch = 3.8261e-01, Meta loss averaged over last 500 steps = 3.5184e-01, PNorm = 95.1268, GNorm = 0.2308
Meta loss on this task batch = 3.4283e-01, Meta loss averaged over last 500 steps = 3.5180e-01, PNorm = 95.1353, GNorm = 0.2333
Took 113.17837047576904 seconds to complete one epoch of meta training
Took 121.23038339614868 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478298
Epoch 206
Meta loss on this task batch = 3.5640e-01, Meta loss averaged over last 500 steps = 3.5181e-01, PNorm = 95.1441, GNorm = 0.2209
Meta loss on this task batch = 3.2494e-01, Meta loss averaged over last 500 steps = 3.5179e-01, PNorm = 95.1540, GNorm = 0.1993
Meta loss on this task batch = 3.1932e-01, Meta loss averaged over last 500 steps = 3.5175e-01, PNorm = 95.1648, GNorm = 0.1972
Meta loss on this task batch = 3.7622e-01, Meta loss averaged over last 500 steps = 3.5175e-01, PNorm = 95.1761, GNorm = 0.2264
Meta loss on this task batch = 3.1576e-01, Meta loss averaged over last 500 steps = 3.5171e-01, PNorm = 95.1867, GNorm = 0.1959
Meta loss on this task batch = 3.1417e-01, Meta loss averaged over last 500 steps = 3.5146e-01, PNorm = 95.1967, GNorm = 0.2335
Meta loss on this task batch = 2.9211e-01, Meta loss averaged over last 500 steps = 3.5132e-01, PNorm = 95.2074, GNorm = 0.2271
Meta loss on this task batch = 3.9515e-01, Meta loss averaged over last 500 steps = 3.5141e-01, PNorm = 95.2165, GNorm = 0.2311
Meta loss on this task batch = 3.5509e-01, Meta loss averaged over last 500 steps = 3.5142e-01, PNorm = 95.2247, GNorm = 0.2239
Meta loss on this task batch = 3.5930e-01, Meta loss averaged over last 500 steps = 3.5141e-01, PNorm = 95.2336, GNorm = 0.2142
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 3.5128e-01, PNorm = 95.2430, GNorm = 0.1810
Meta loss on this task batch = 3.0745e-01, Meta loss averaged over last 500 steps = 3.5122e-01, PNorm = 95.2525, GNorm = 0.2059
Meta loss on this task batch = 3.5224e-01, Meta loss averaged over last 500 steps = 3.5132e-01, PNorm = 95.2624, GNorm = 0.2109
Meta loss on this task batch = 3.8870e-01, Meta loss averaged over last 500 steps = 3.5125e-01, PNorm = 95.2708, GNorm = 0.2131
Meta loss on this task batch = 3.0619e-01, Meta loss averaged over last 500 steps = 3.5129e-01, PNorm = 95.2800, GNorm = 0.1975
Meta loss on this task batch = 3.6797e-01, Meta loss averaged over last 500 steps = 3.5122e-01, PNorm = 95.2899, GNorm = 0.2472
Meta loss on this task batch = 3.3920e-01, Meta loss averaged over last 500 steps = 3.5108e-01, PNorm = 95.2996, GNorm = 0.2082
Meta loss on this task batch = 4.1557e-01, Meta loss averaged over last 500 steps = 3.5127e-01, PNorm = 95.3095, GNorm = 0.2454
Meta loss on this task batch = 4.3561e-01, Meta loss averaged over last 500 steps = 3.5141e-01, PNorm = 95.3176, GNorm = 0.2758
Took 115.52129817008972 seconds to complete one epoch of meta training
Took 123.38000535964966 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485162
Epoch 207
Meta loss on this task batch = 3.9695e-01, Meta loss averaged over last 500 steps = 3.5154e-01, PNorm = 95.3250, GNorm = 0.2174
Meta loss on this task batch = 3.2958e-01, Meta loss averaged over last 500 steps = 3.5142e-01, PNorm = 95.3338, GNorm = 0.2473
Meta loss on this task batch = 3.4543e-01, Meta loss averaged over last 500 steps = 3.5144e-01, PNorm = 95.3435, GNorm = 0.1972
Meta loss on this task batch = 3.2374e-01, Meta loss averaged over last 500 steps = 3.5138e-01, PNorm = 95.3534, GNorm = 0.2010
Meta loss on this task batch = 3.1176e-01, Meta loss averaged over last 500 steps = 3.5125e-01, PNorm = 95.3628, GNorm = 0.1902
Meta loss on this task batch = 3.1184e-01, Meta loss averaged over last 500 steps = 3.5115e-01, PNorm = 95.3735, GNorm = 0.2038
Meta loss on this task batch = 3.8039e-01, Meta loss averaged over last 500 steps = 3.5114e-01, PNorm = 95.3838, GNorm = 0.2201
Meta loss on this task batch = 3.6165e-01, Meta loss averaged over last 500 steps = 3.5126e-01, PNorm = 95.3925, GNorm = 0.2374
Meta loss on this task batch = 3.4587e-01, Meta loss averaged over last 500 steps = 3.5125e-01, PNorm = 95.4014, GNorm = 0.1959
Meta loss on this task batch = 3.7533e-01, Meta loss averaged over last 500 steps = 3.5131e-01, PNorm = 95.4104, GNorm = 0.2047
Meta loss on this task batch = 3.1779e-01, Meta loss averaged over last 500 steps = 3.5135e-01, PNorm = 95.4199, GNorm = 0.1659
Meta loss on this task batch = 3.3261e-01, Meta loss averaged over last 500 steps = 3.5126e-01, PNorm = 95.4287, GNorm = 0.1876
Meta loss on this task batch = 3.3074e-01, Meta loss averaged over last 500 steps = 3.5111e-01, PNorm = 95.4378, GNorm = 0.2026
Meta loss on this task batch = 3.4856e-01, Meta loss averaged over last 500 steps = 3.5118e-01, PNorm = 95.4468, GNorm = 0.1919
Meta loss on this task batch = 3.7379e-01, Meta loss averaged over last 500 steps = 3.5131e-01, PNorm = 95.4560, GNorm = 0.2247
Meta loss on this task batch = 3.2440e-01, Meta loss averaged over last 500 steps = 3.5127e-01, PNorm = 95.4655, GNorm = 0.1790
Meta loss on this task batch = 3.2544e-01, Meta loss averaged over last 500 steps = 3.5122e-01, PNorm = 95.4743, GNorm = 0.2307
Meta loss on this task batch = 3.2567e-01, Meta loss averaged over last 500 steps = 3.5106e-01, PNorm = 95.4825, GNorm = 0.2144
Meta loss on this task batch = 4.1903e-01, Meta loss averaged over last 500 steps = 3.5114e-01, PNorm = 95.4903, GNorm = 0.2568
Took 114.09413409233093 seconds to complete one epoch of meta training
Took 121.7259521484375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505105
Epoch 208
Meta loss on this task batch = 4.3181e-01, Meta loss averaged over last 500 steps = 3.5129e-01, PNorm = 95.4964, GNorm = 0.2381
Meta loss on this task batch = 3.9898e-01, Meta loss averaged over last 500 steps = 3.5138e-01, PNorm = 95.5028, GNorm = 0.2204
Meta loss on this task batch = 3.3241e-01, Meta loss averaged over last 500 steps = 3.5130e-01, PNorm = 95.5105, GNorm = 0.1909
Meta loss on this task batch = 3.5799e-01, Meta loss averaged over last 500 steps = 3.5140e-01, PNorm = 95.5198, GNorm = 0.2271
Meta loss on this task batch = 3.1941e-01, Meta loss averaged over last 500 steps = 3.5125e-01, PNorm = 95.5305, GNorm = 0.2029
Meta loss on this task batch = 3.6238e-01, Meta loss averaged over last 500 steps = 3.5123e-01, PNorm = 95.5412, GNorm = 0.1810
Meta loss on this task batch = 2.7685e-01, Meta loss averaged over last 500 steps = 3.5111e-01, PNorm = 95.5526, GNorm = 0.1870
Meta loss on this task batch = 3.9625e-01, Meta loss averaged over last 500 steps = 3.5114e-01, PNorm = 95.5641, GNorm = 0.2052
Meta loss on this task batch = 2.8083e-01, Meta loss averaged over last 500 steps = 3.5105e-01, PNorm = 95.5755, GNorm = 0.1794
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 3.5102e-01, PNorm = 95.5869, GNorm = 0.1907
Meta loss on this task batch = 2.9552e-01, Meta loss averaged over last 500 steps = 3.5094e-01, PNorm = 95.5990, GNorm = 0.2130
Meta loss on this task batch = 3.4544e-01, Meta loss averaged over last 500 steps = 3.5095e-01, PNorm = 95.6099, GNorm = 0.2078
Meta loss on this task batch = 2.9466e-01, Meta loss averaged over last 500 steps = 3.5078e-01, PNorm = 95.6209, GNorm = 0.1930
Meta loss on this task batch = 3.3218e-01, Meta loss averaged over last 500 steps = 3.5065e-01, PNorm = 95.6314, GNorm = 0.2099
Meta loss on this task batch = 4.0677e-01, Meta loss averaged over last 500 steps = 3.5076e-01, PNorm = 95.6404, GNorm = 0.2423
Meta loss on this task batch = 3.2903e-01, Meta loss averaged over last 500 steps = 3.5073e-01, PNorm = 95.6499, GNorm = 0.2334
Meta loss on this task batch = 3.7393e-01, Meta loss averaged over last 500 steps = 3.5062e-01, PNorm = 95.6593, GNorm = 0.2144
Meta loss on this task batch = 3.4896e-01, Meta loss averaged over last 500 steps = 3.5070e-01, PNorm = 95.6686, GNorm = 0.2026
Meta loss on this task batch = 3.4131e-01, Meta loss averaged over last 500 steps = 3.5058e-01, PNorm = 95.6787, GNorm = 0.2152
Took 116.97786068916321 seconds to complete one epoch of meta training
Took 124.66723036766052 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512667
Found better MAML checkpoint after meta validation, saving now
Epoch 209
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 3.5049e-01, PNorm = 95.6890, GNorm = 0.1900
Meta loss on this task batch = 3.4584e-01, Meta loss averaged over last 500 steps = 3.5052e-01, PNorm = 95.6984, GNorm = 0.1944
Meta loss on this task batch = 3.3947e-01, Meta loss averaged over last 500 steps = 3.5048e-01, PNorm = 95.7076, GNorm = 0.2413
Meta loss on this task batch = 3.5284e-01, Meta loss averaged over last 500 steps = 3.5044e-01, PNorm = 95.7176, GNorm = 0.2106
Meta loss on this task batch = 3.8278e-01, Meta loss averaged over last 500 steps = 3.5051e-01, PNorm = 95.7273, GNorm = 0.2144
Meta loss on this task batch = 3.4985e-01, Meta loss averaged over last 500 steps = 3.5049e-01, PNorm = 95.7379, GNorm = 0.1887
Meta loss on this task batch = 3.3754e-01, Meta loss averaged over last 500 steps = 3.5041e-01, PNorm = 95.7492, GNorm = 0.2113
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 3.5017e-01, PNorm = 95.7608, GNorm = 0.2101
Meta loss on this task batch = 3.8859e-01, Meta loss averaged over last 500 steps = 3.5029e-01, PNorm = 95.7717, GNorm = 0.2388
Meta loss on this task batch = 2.9559e-01, Meta loss averaged over last 500 steps = 3.5015e-01, PNorm = 95.7832, GNorm = 0.2231
Meta loss on this task batch = 4.0059e-01, Meta loss averaged over last 500 steps = 3.5025e-01, PNorm = 95.7946, GNorm = 0.2085
Meta loss on this task batch = 3.3952e-01, Meta loss averaged over last 500 steps = 3.5022e-01, PNorm = 95.8060, GNorm = 0.1995
Meta loss on this task batch = 3.0953e-01, Meta loss averaged over last 500 steps = 3.5008e-01, PNorm = 95.8168, GNorm = 0.2079
Meta loss on this task batch = 3.1591e-01, Meta loss averaged over last 500 steps = 3.5006e-01, PNorm = 95.8276, GNorm = 0.1986
Meta loss on this task batch = 3.4593e-01, Meta loss averaged over last 500 steps = 3.5000e-01, PNorm = 95.8383, GNorm = 0.2239
Meta loss on this task batch = 2.8015e-01, Meta loss averaged over last 500 steps = 3.4983e-01, PNorm = 95.8496, GNorm = 0.1886
Meta loss on this task batch = 3.6685e-01, Meta loss averaged over last 500 steps = 3.4977e-01, PNorm = 95.8595, GNorm = 0.2228
Meta loss on this task batch = 3.7249e-01, Meta loss averaged over last 500 steps = 3.4975e-01, PNorm = 95.8689, GNorm = 0.2149
Meta loss on this task batch = 3.7237e-01, Meta loss averaged over last 500 steps = 3.4978e-01, PNorm = 95.8777, GNorm = 0.2632
Took 115.14479613304138 seconds to complete one epoch of meta training
Took 122.65996861457825 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505640
Epoch 210
Meta loss on this task batch = 3.1884e-01, Meta loss averaged over last 500 steps = 3.4986e-01, PNorm = 95.8870, GNorm = 0.1973
Meta loss on this task batch = 3.7734e-01, Meta loss averaged over last 500 steps = 3.4983e-01, PNorm = 95.8950, GNorm = 0.2274
Meta loss on this task batch = 3.5186e-01, Meta loss averaged over last 500 steps = 3.4982e-01, PNorm = 95.9011, GNorm = 0.2500
Meta loss on this task batch = 3.7399e-01, Meta loss averaged over last 500 steps = 3.4982e-01, PNorm = 95.9079, GNorm = 0.1935
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 3.4980e-01, PNorm = 95.9156, GNorm = 0.2063
Meta loss on this task batch = 3.7750e-01, Meta loss averaged over last 500 steps = 3.4987e-01, PNorm = 95.9228, GNorm = 0.2266
Meta loss on this task batch = 3.3067e-01, Meta loss averaged over last 500 steps = 3.4989e-01, PNorm = 95.9314, GNorm = 0.2521
Meta loss on this task batch = 3.7233e-01, Meta loss averaged over last 500 steps = 3.4991e-01, PNorm = 95.9409, GNorm = 0.2420
Meta loss on this task batch = 4.0149e-01, Meta loss averaged over last 500 steps = 3.5019e-01, PNorm = 95.9508, GNorm = 0.2222
Meta loss on this task batch = 3.3600e-01, Meta loss averaged over last 500 steps = 3.5006e-01, PNorm = 95.9605, GNorm = 0.1957
Meta loss on this task batch = 3.3072e-01, Meta loss averaged over last 500 steps = 3.5007e-01, PNorm = 95.9707, GNorm = 0.1955
Meta loss on this task batch = 3.4163e-01, Meta loss averaged over last 500 steps = 3.5010e-01, PNorm = 95.9815, GNorm = 0.2138
Meta loss on this task batch = 3.4328e-01, Meta loss averaged over last 500 steps = 3.5014e-01, PNorm = 95.9917, GNorm = 0.2101
Meta loss on this task batch = 3.2327e-01, Meta loss averaged over last 500 steps = 3.4998e-01, PNorm = 96.0014, GNorm = 0.2061
Meta loss on this task batch = 3.5058e-01, Meta loss averaged over last 500 steps = 3.4998e-01, PNorm = 96.0101, GNorm = 0.2149
Meta loss on this task batch = 3.3306e-01, Meta loss averaged over last 500 steps = 3.4982e-01, PNorm = 96.0180, GNorm = 0.2181
Meta loss on this task batch = 3.3055e-01, Meta loss averaged over last 500 steps = 3.4973e-01, PNorm = 96.0260, GNorm = 0.2063
Meta loss on this task batch = 3.5349e-01, Meta loss averaged over last 500 steps = 3.4967e-01, PNorm = 96.0330, GNorm = 0.2036
Meta loss on this task batch = 3.8288e-01, Meta loss averaged over last 500 steps = 3.4971e-01, PNorm = 96.0386, GNorm = 0.2711
Took 112.99286842346191 seconds to complete one epoch of meta training
Took 121.11127066612244 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512199
Epoch 211
Meta loss on this task batch = 3.4845e-01, Meta loss averaged over last 500 steps = 3.4968e-01, PNorm = 96.0446, GNorm = 0.2026
Meta loss on this task batch = 3.5443e-01, Meta loss averaged over last 500 steps = 3.4967e-01, PNorm = 96.0514, GNorm = 0.2063
Meta loss on this task batch = 3.2675e-01, Meta loss averaged over last 500 steps = 3.4957e-01, PNorm = 96.0586, GNorm = 0.1773
Meta loss on this task batch = 3.6372e-01, Meta loss averaged over last 500 steps = 3.4950e-01, PNorm = 96.0660, GNorm = 0.2185
Meta loss on this task batch = 3.4427e-01, Meta loss averaged over last 500 steps = 3.4955e-01, PNorm = 96.0736, GNorm = 0.2296
Meta loss on this task batch = 3.5837e-01, Meta loss averaged over last 500 steps = 3.4955e-01, PNorm = 96.0817, GNorm = 0.2065
Meta loss on this task batch = 3.3671e-01, Meta loss averaged over last 500 steps = 3.4951e-01, PNorm = 96.0907, GNorm = 0.1997
Meta loss on this task batch = 3.9763e-01, Meta loss averaged over last 500 steps = 3.4963e-01, PNorm = 96.1004, GNorm = 0.2253
Meta loss on this task batch = 3.6332e-01, Meta loss averaged over last 500 steps = 3.4971e-01, PNorm = 96.1103, GNorm = 0.2000
Meta loss on this task batch = 3.1141e-01, Meta loss averaged over last 500 steps = 3.4963e-01, PNorm = 96.1216, GNorm = 0.2132
Meta loss on this task batch = 4.0612e-01, Meta loss averaged over last 500 steps = 3.4984e-01, PNorm = 96.1312, GNorm = 0.3140
Meta loss on this task batch = 3.4827e-01, Meta loss averaged over last 500 steps = 3.4982e-01, PNorm = 96.1402, GNorm = 0.2076
Meta loss on this task batch = 3.8852e-01, Meta loss averaged over last 500 steps = 3.4981e-01, PNorm = 96.1500, GNorm = 0.2342
Meta loss on this task batch = 2.8769e-01, Meta loss averaged over last 500 steps = 3.4976e-01, PNorm = 96.1610, GNorm = 0.1748
Meta loss on this task batch = 3.1115e-01, Meta loss averaged over last 500 steps = 3.4967e-01, PNorm = 96.1711, GNorm = 0.2126
Meta loss on this task batch = 3.5030e-01, Meta loss averaged over last 500 steps = 3.4961e-01, PNorm = 96.1806, GNorm = 0.2236
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 3.4933e-01, PNorm = 96.1908, GNorm = 0.1771
Meta loss on this task batch = 3.1821e-01, Meta loss averaged over last 500 steps = 3.4916e-01, PNorm = 96.2008, GNorm = 0.1805
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 3.4906e-01, PNorm = 96.2108, GNorm = 0.2568
Took 115.1843318939209 seconds to complete one epoch of meta training
Took 123.31913375854492 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.525635
Found better MAML checkpoint after meta validation, saving now
Epoch 212
Meta loss on this task batch = 3.5943e-01, Meta loss averaged over last 500 steps = 3.4904e-01, PNorm = 96.2211, GNorm = 0.2129
Meta loss on this task batch = 3.2515e-01, Meta loss averaged over last 500 steps = 3.4898e-01, PNorm = 96.2320, GNorm = 0.1932
Meta loss on this task batch = 3.1040e-01, Meta loss averaged over last 500 steps = 3.4896e-01, PNorm = 96.2434, GNorm = 0.2077
Meta loss on this task batch = 3.8534e-01, Meta loss averaged over last 500 steps = 3.4911e-01, PNorm = 96.2541, GNorm = 0.2484
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 3.4891e-01, PNorm = 96.2656, GNorm = 0.1841
Meta loss on this task batch = 3.8831e-01, Meta loss averaged over last 500 steps = 3.4887e-01, PNorm = 96.2769, GNorm = 0.2784
Meta loss on this task batch = 3.6591e-01, Meta loss averaged over last 500 steps = 3.4894e-01, PNorm = 96.2871, GNorm = 0.2350
Meta loss on this task batch = 3.7979e-01, Meta loss averaged over last 500 steps = 3.4901e-01, PNorm = 96.2969, GNorm = 0.2596
Meta loss on this task batch = 3.3998e-01, Meta loss averaged over last 500 steps = 3.4904e-01, PNorm = 96.3064, GNorm = 0.2012
Meta loss on this task batch = 3.3394e-01, Meta loss averaged over last 500 steps = 3.4907e-01, PNorm = 96.3167, GNorm = 0.1981
Meta loss on this task batch = 3.3053e-01, Meta loss averaged over last 500 steps = 3.4894e-01, PNorm = 96.3269, GNorm = 0.2129
Meta loss on this task batch = 3.2239e-01, Meta loss averaged over last 500 steps = 3.4880e-01, PNorm = 96.3376, GNorm = 0.2164
Meta loss on this task batch = 3.1857e-01, Meta loss averaged over last 500 steps = 3.4876e-01, PNorm = 96.3486, GNorm = 0.2208
Meta loss on this task batch = 3.1805e-01, Meta loss averaged over last 500 steps = 3.4882e-01, PNorm = 96.3599, GNorm = 0.1973
Meta loss on this task batch = 3.4881e-01, Meta loss averaged over last 500 steps = 3.4889e-01, PNorm = 96.3699, GNorm = 0.2312
Meta loss on this task batch = 3.6203e-01, Meta loss averaged over last 500 steps = 3.4875e-01, PNorm = 96.3792, GNorm = 0.2159
Meta loss on this task batch = 3.3818e-01, Meta loss averaged over last 500 steps = 3.4863e-01, PNorm = 96.3885, GNorm = 0.2266
Meta loss on this task batch = 3.2670e-01, Meta loss averaged over last 500 steps = 3.4860e-01, PNorm = 96.3987, GNorm = 0.2103
Meta loss on this task batch = 3.4705e-01, Meta loss averaged over last 500 steps = 3.4863e-01, PNorm = 96.4085, GNorm = 0.2640
Took 116.19763422012329 seconds to complete one epoch of meta training
Took 122.86074614524841 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517155
Epoch 213
Meta loss on this task batch = 3.1938e-01, Meta loss averaged over last 500 steps = 3.4860e-01, PNorm = 96.4188, GNorm = 0.1986
Meta loss on this task batch = 3.2753e-01, Meta loss averaged over last 500 steps = 3.4856e-01, PNorm = 96.4282, GNorm = 0.1953
Meta loss on this task batch = 3.6618e-01, Meta loss averaged over last 500 steps = 3.4871e-01, PNorm = 96.4375, GNorm = 0.2336
Meta loss on this task batch = 2.9851e-01, Meta loss averaged over last 500 steps = 3.4847e-01, PNorm = 96.4469, GNorm = 0.2095
Meta loss on this task batch = 3.7483e-01, Meta loss averaged over last 500 steps = 3.4858e-01, PNorm = 96.4542, GNorm = 0.2442
Meta loss on this task batch = 3.8094e-01, Meta loss averaged over last 500 steps = 3.4857e-01, PNorm = 96.4624, GNorm = 0.2368
Meta loss on this task batch = 3.0805e-01, Meta loss averaged over last 500 steps = 3.4841e-01, PNorm = 96.4716, GNorm = 0.2039
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 3.4827e-01, PNorm = 96.4809, GNorm = 0.1827
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 3.4812e-01, PNorm = 96.4907, GNorm = 0.1857
Meta loss on this task batch = 3.5995e-01, Meta loss averaged over last 500 steps = 3.4822e-01, PNorm = 96.4996, GNorm = 0.1846
Meta loss on this task batch = 3.5773e-01, Meta loss averaged over last 500 steps = 3.4822e-01, PNorm = 96.5087, GNorm = 0.1947
Meta loss on this task batch = 3.6018e-01, Meta loss averaged over last 500 steps = 3.4828e-01, PNorm = 96.5178, GNorm = 0.2032
Meta loss on this task batch = 3.5226e-01, Meta loss averaged over last 500 steps = 3.4829e-01, PNorm = 96.5256, GNorm = 0.2183
Meta loss on this task batch = 3.6486e-01, Meta loss averaged over last 500 steps = 3.4844e-01, PNorm = 96.5339, GNorm = 0.1979
Meta loss on this task batch = 4.1720e-01, Meta loss averaged over last 500 steps = 3.4864e-01, PNorm = 96.5429, GNorm = 0.1944
Meta loss on this task batch = 2.8806e-01, Meta loss averaged over last 500 steps = 3.4861e-01, PNorm = 96.5519, GNorm = 0.1921
Meta loss on this task batch = 3.9191e-01, Meta loss averaged over last 500 steps = 3.4866e-01, PNorm = 96.5615, GNorm = 0.2218
Meta loss on this task batch = 3.1272e-01, Meta loss averaged over last 500 steps = 3.4859e-01, PNorm = 96.5723, GNorm = 0.2043
Meta loss on this task batch = 3.6464e-01, Meta loss averaged over last 500 steps = 3.4856e-01, PNorm = 96.5837, GNorm = 0.2237
Took 116.68306732177734 seconds to complete one epoch of meta training
Took 124.51789093017578 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500929
Epoch 214
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 3.4830e-01, PNorm = 96.5954, GNorm = 0.2156
Meta loss on this task batch = 3.4628e-01, Meta loss averaged over last 500 steps = 3.4828e-01, PNorm = 96.6059, GNorm = 0.2243
Meta loss on this task batch = 3.8544e-01, Meta loss averaged over last 500 steps = 3.4839e-01, PNorm = 96.6150, GNorm = 0.2409
Meta loss on this task batch = 3.0484e-01, Meta loss averaged over last 500 steps = 3.4828e-01, PNorm = 96.6239, GNorm = 0.2053
Meta loss on this task batch = 3.1750e-01, Meta loss averaged over last 500 steps = 3.4817e-01, PNorm = 96.6326, GNorm = 0.2258
Meta loss on this task batch = 3.3888e-01, Meta loss averaged over last 500 steps = 3.4816e-01, PNorm = 96.6405, GNorm = 0.2303
Meta loss on this task batch = 3.1581e-01, Meta loss averaged over last 500 steps = 3.4814e-01, PNorm = 96.6482, GNorm = 0.2062
Meta loss on this task batch = 3.5030e-01, Meta loss averaged over last 500 steps = 3.4815e-01, PNorm = 96.6569, GNorm = 0.2041
Meta loss on this task batch = 3.4283e-01, Meta loss averaged over last 500 steps = 3.4813e-01, PNorm = 96.6656, GNorm = 0.2020
Meta loss on this task batch = 3.4143e-01, Meta loss averaged over last 500 steps = 3.4808e-01, PNorm = 96.6742, GNorm = 0.2228
Meta loss on this task batch = 3.8855e-01, Meta loss averaged over last 500 steps = 3.4817e-01, PNorm = 96.6825, GNorm = 0.2058
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 3.4810e-01, PNorm = 96.6910, GNorm = 0.1982
Meta loss on this task batch = 3.3600e-01, Meta loss averaged over last 500 steps = 3.4804e-01, PNorm = 96.6986, GNorm = 0.1971
Meta loss on this task batch = 3.5848e-01, Meta loss averaged over last 500 steps = 3.4809e-01, PNorm = 96.7057, GNorm = 0.2272
Meta loss on this task batch = 3.7011e-01, Meta loss averaged over last 500 steps = 3.4813e-01, PNorm = 96.7133, GNorm = 0.2123
Meta loss on this task batch = 3.7654e-01, Meta loss averaged over last 500 steps = 3.4822e-01, PNorm = 96.7208, GNorm = 0.2349
Meta loss on this task batch = 3.2801e-01, Meta loss averaged over last 500 steps = 3.4821e-01, PNorm = 96.7296, GNorm = 0.1856
Meta loss on this task batch = 3.6280e-01, Meta loss averaged over last 500 steps = 3.4834e-01, PNorm = 96.7386, GNorm = 0.2188
Meta loss on this task batch = 2.9418e-01, Meta loss averaged over last 500 steps = 3.4817e-01, PNorm = 96.7492, GNorm = 0.2386
Took 116.25396227836609 seconds to complete one epoch of meta training
Took 122.99551343917847 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496901
Epoch 215
Meta loss on this task batch = 3.3793e-01, Meta loss averaged over last 500 steps = 3.4813e-01, PNorm = 96.7612, GNorm = 0.2371
Meta loss on this task batch = 4.1003e-01, Meta loss averaged over last 500 steps = 3.4821e-01, PNorm = 96.7722, GNorm = 0.2638
Meta loss on this task batch = 3.0851e-01, Meta loss averaged over last 500 steps = 3.4813e-01, PNorm = 96.7837, GNorm = 0.2275
Meta loss on this task batch = 3.7617e-01, Meta loss averaged over last 500 steps = 3.4814e-01, PNorm = 96.7956, GNorm = 0.2066
Meta loss on this task batch = 3.4220e-01, Meta loss averaged over last 500 steps = 3.4803e-01, PNorm = 96.8076, GNorm = 0.1877
Meta loss on this task batch = 3.3241e-01, Meta loss averaged over last 500 steps = 3.4806e-01, PNorm = 96.8193, GNorm = 0.2058
Meta loss on this task batch = 3.7051e-01, Meta loss averaged over last 500 steps = 3.4834e-01, PNorm = 96.8299, GNorm = 0.2427
Meta loss on this task batch = 3.8490e-01, Meta loss averaged over last 500 steps = 3.4840e-01, PNorm = 96.8394, GNorm = 0.2390
Meta loss on this task batch = 3.1624e-01, Meta loss averaged over last 500 steps = 3.4833e-01, PNorm = 96.8497, GNorm = 0.1779
Meta loss on this task batch = 3.2760e-01, Meta loss averaged over last 500 steps = 3.4841e-01, PNorm = 96.8592, GNorm = 0.2253
Meta loss on this task batch = 3.1386e-01, Meta loss averaged over last 500 steps = 3.4839e-01, PNorm = 96.8683, GNorm = 0.1939
Meta loss on this task batch = 3.3092e-01, Meta loss averaged over last 500 steps = 3.4835e-01, PNorm = 96.8781, GNorm = 0.1993
Meta loss on this task batch = 3.3796e-01, Meta loss averaged over last 500 steps = 3.4820e-01, PNorm = 96.8889, GNorm = 0.2238
Meta loss on this task batch = 3.0213e-01, Meta loss averaged over last 500 steps = 3.4820e-01, PNorm = 96.9004, GNorm = 0.2009
Meta loss on this task batch = 3.5305e-01, Meta loss averaged over last 500 steps = 3.4820e-01, PNorm = 96.9105, GNorm = 0.2227
Meta loss on this task batch = 3.4428e-01, Meta loss averaged over last 500 steps = 3.4816e-01, PNorm = 96.9204, GNorm = 0.2158
Meta loss on this task batch = 3.4206e-01, Meta loss averaged over last 500 steps = 3.4802e-01, PNorm = 96.9303, GNorm = 0.2038
Meta loss on this task batch = 3.3977e-01, Meta loss averaged over last 500 steps = 3.4802e-01, PNorm = 96.9392, GNorm = 0.2249
Meta loss on this task batch = 3.3264e-01, Meta loss averaged over last 500 steps = 3.4797e-01, PNorm = 96.9476, GNorm = 0.2319
Took 116.15574955940247 seconds to complete one epoch of meta training
Took 124.03877711296082 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511315
Epoch 216
Meta loss on this task batch = 3.7161e-01, Meta loss averaged over last 500 steps = 3.4802e-01, PNorm = 96.9539, GNorm = 0.2445
Meta loss on this task batch = 3.3428e-01, Meta loss averaged over last 500 steps = 3.4787e-01, PNorm = 96.9607, GNorm = 0.2071
Meta loss on this task batch = 3.9083e-01, Meta loss averaged over last 500 steps = 3.4790e-01, PNorm = 96.9669, GNorm = 0.2072
Meta loss on this task batch = 3.4097e-01, Meta loss averaged over last 500 steps = 3.4792e-01, PNorm = 96.9733, GNorm = 0.1788
Meta loss on this task batch = 3.8560e-01, Meta loss averaged over last 500 steps = 3.4793e-01, PNorm = 96.9788, GNorm = 0.2098
Meta loss on this task batch = 3.1472e-01, Meta loss averaged over last 500 steps = 3.4784e-01, PNorm = 96.9856, GNorm = 0.2017
Meta loss on this task batch = 3.3508e-01, Meta loss averaged over last 500 steps = 3.4780e-01, PNorm = 96.9932, GNorm = 0.2060
Meta loss on this task batch = 3.4412e-01, Meta loss averaged over last 500 steps = 3.4779e-01, PNorm = 97.0027, GNorm = 0.2061
Meta loss on this task batch = 3.2360e-01, Meta loss averaged over last 500 steps = 3.4764e-01, PNorm = 97.0130, GNorm = 0.1836
Meta loss on this task batch = 3.1510e-01, Meta loss averaged over last 500 steps = 3.4748e-01, PNorm = 97.0237, GNorm = 0.1992
Meta loss on this task batch = 3.6309e-01, Meta loss averaged over last 500 steps = 3.4741e-01, PNorm = 97.0338, GNorm = 0.2176
Meta loss on this task batch = 3.2558e-01, Meta loss averaged over last 500 steps = 3.4740e-01, PNorm = 97.0447, GNorm = 0.2181
Meta loss on this task batch = 3.2091e-01, Meta loss averaged over last 500 steps = 3.4732e-01, PNorm = 97.0554, GNorm = 0.2246
Meta loss on this task batch = 3.7794e-01, Meta loss averaged over last 500 steps = 3.4744e-01, PNorm = 97.0651, GNorm = 0.2169
Meta loss on this task batch = 3.3077e-01, Meta loss averaged over last 500 steps = 3.4752e-01, PNorm = 97.0741, GNorm = 0.2167
Meta loss on this task batch = 3.3065e-01, Meta loss averaged over last 500 steps = 3.4758e-01, PNorm = 97.0804, GNorm = 0.2592
Meta loss on this task batch = 3.5927e-01, Meta loss averaged over last 500 steps = 3.4755e-01, PNorm = 97.0873, GNorm = 0.2343
Meta loss on this task batch = 3.2322e-01, Meta loss averaged over last 500 steps = 3.4741e-01, PNorm = 97.0947, GNorm = 0.2242
Meta loss on this task batch = 3.2623e-01, Meta loss averaged over last 500 steps = 3.4736e-01, PNorm = 97.1027, GNorm = 0.2394
Took 113.99352049827576 seconds to complete one epoch of meta training
Took 121.7362220287323 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498050
Epoch 217
Meta loss on this task batch = 3.6275e-01, Meta loss averaged over last 500 steps = 3.4739e-01, PNorm = 97.1105, GNorm = 0.1956
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 3.4726e-01, PNorm = 97.1195, GNorm = 0.2121
Meta loss on this task batch = 3.3771e-01, Meta loss averaged over last 500 steps = 3.4718e-01, PNorm = 97.1296, GNorm = 0.2007
Meta loss on this task batch = 3.2540e-01, Meta loss averaged over last 500 steps = 3.4723e-01, PNorm = 97.1403, GNorm = 0.1854
Meta loss on this task batch = 3.7538e-01, Meta loss averaged over last 500 steps = 3.4722e-01, PNorm = 97.1512, GNorm = 0.2365
Meta loss on this task batch = 3.6338e-01, Meta loss averaged over last 500 steps = 3.4728e-01, PNorm = 97.1627, GNorm = 0.2117
Meta loss on this task batch = 3.1320e-01, Meta loss averaged over last 500 steps = 3.4721e-01, PNorm = 97.1742, GNorm = 0.2126
Meta loss on this task batch = 3.2890e-01, Meta loss averaged over last 500 steps = 3.4708e-01, PNorm = 97.1852, GNorm = 0.2206
Meta loss on this task batch = 3.0541e-01, Meta loss averaged over last 500 steps = 3.4694e-01, PNorm = 97.1970, GNorm = 0.2116
Meta loss on this task batch = 3.5811e-01, Meta loss averaged over last 500 steps = 3.4696e-01, PNorm = 97.2088, GNorm = 0.2132
Meta loss on this task batch = 3.7426e-01, Meta loss averaged over last 500 steps = 3.4695e-01, PNorm = 97.2183, GNorm = 0.2734
Meta loss on this task batch = 3.9054e-01, Meta loss averaged over last 500 steps = 3.4706e-01, PNorm = 97.2258, GNorm = 0.2688
Meta loss on this task batch = 3.4480e-01, Meta loss averaged over last 500 steps = 3.4711e-01, PNorm = 97.2326, GNorm = 0.2447
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 3.4707e-01, PNorm = 97.2406, GNorm = 0.2286
Meta loss on this task batch = 2.7571e-01, Meta loss averaged over last 500 steps = 3.4697e-01, PNorm = 97.2497, GNorm = 0.2210
Meta loss on this task batch = 3.6742e-01, Meta loss averaged over last 500 steps = 3.4698e-01, PNorm = 97.2580, GNorm = 0.2212
Meta loss on this task batch = 3.3443e-01, Meta loss averaged over last 500 steps = 3.4698e-01, PNorm = 97.2665, GNorm = 0.2038
Meta loss on this task batch = 3.7502e-01, Meta loss averaged over last 500 steps = 3.4697e-01, PNorm = 97.2747, GNorm = 0.2208
Meta loss on this task batch = 3.4980e-01, Meta loss averaged over last 500 steps = 3.4707e-01, PNorm = 97.2840, GNorm = 0.2224
Took 119.3362488746643 seconds to complete one epoch of meta training
Took 126.63437294960022 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502505
Epoch 218
Meta loss on this task batch = 3.5034e-01, Meta loss averaged over last 500 steps = 3.4712e-01, PNorm = 97.2917, GNorm = 0.2137
Meta loss on this task batch = 3.9417e-01, Meta loss averaged over last 500 steps = 3.4699e-01, PNorm = 97.2992, GNorm = 0.2030
Meta loss on this task batch = 3.2875e-01, Meta loss averaged over last 500 steps = 3.4704e-01, PNorm = 97.3072, GNorm = 0.1987
Meta loss on this task batch = 2.7838e-01, Meta loss averaged over last 500 steps = 3.4679e-01, PNorm = 97.3166, GNorm = 0.1845
Meta loss on this task batch = 3.4111e-01, Meta loss averaged over last 500 steps = 3.4676e-01, PNorm = 97.3264, GNorm = 0.1858
Meta loss on this task batch = 3.7235e-01, Meta loss averaged over last 500 steps = 3.4681e-01, PNorm = 97.3351, GNorm = 0.2587
Meta loss on this task batch = 3.2154e-01, Meta loss averaged over last 500 steps = 3.4675e-01, PNorm = 97.3443, GNorm = 0.1869
Meta loss on this task batch = 3.4694e-01, Meta loss averaged over last 500 steps = 3.4669e-01, PNorm = 97.3538, GNorm = 0.2169
Meta loss on this task batch = 3.3152e-01, Meta loss averaged over last 500 steps = 3.4655e-01, PNorm = 97.3637, GNorm = 0.1828
Meta loss on this task batch = 3.7788e-01, Meta loss averaged over last 500 steps = 3.4664e-01, PNorm = 97.3739, GNorm = 0.1956
Meta loss on this task batch = 4.1228e-01, Meta loss averaged over last 500 steps = 3.4680e-01, PNorm = 97.3828, GNorm = 0.2270
Meta loss on this task batch = 2.8721e-01, Meta loss averaged over last 500 steps = 3.4663e-01, PNorm = 97.3934, GNorm = 0.2043
Meta loss on this task batch = 3.3919e-01, Meta loss averaged over last 500 steps = 3.4649e-01, PNorm = 97.4048, GNorm = 0.1685
Meta loss on this task batch = 3.7335e-01, Meta loss averaged over last 500 steps = 3.4655e-01, PNorm = 97.4154, GNorm = 0.2004
Meta loss on this task batch = 3.2578e-01, Meta loss averaged over last 500 steps = 3.4651e-01, PNorm = 97.4253, GNorm = 0.1775
Meta loss on this task batch = 3.2697e-01, Meta loss averaged over last 500 steps = 3.4652e-01, PNorm = 97.4342, GNorm = 0.1989
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 3.4648e-01, PNorm = 97.4437, GNorm = 0.1844
Meta loss on this task batch = 3.3284e-01, Meta loss averaged over last 500 steps = 3.4643e-01, PNorm = 97.4534, GNorm = 0.2197
Meta loss on this task batch = 3.3100e-01, Meta loss averaged over last 500 steps = 3.4639e-01, PNorm = 97.4628, GNorm = 0.2325
Took 117.05749464035034 seconds to complete one epoch of meta training
Took 124.67172145843506 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519936
Epoch 219
Meta loss on this task batch = 3.3494e-01, Meta loss averaged over last 500 steps = 3.4624e-01, PNorm = 97.4729, GNorm = 0.2192
Meta loss on this task batch = 3.1640e-01, Meta loss averaged over last 500 steps = 3.4627e-01, PNorm = 97.4828, GNorm = 0.2226
Meta loss on this task batch = 3.2939e-01, Meta loss averaged over last 500 steps = 3.4625e-01, PNorm = 97.4924, GNorm = 0.2265
Meta loss on this task batch = 2.9792e-01, Meta loss averaged over last 500 steps = 3.4614e-01, PNorm = 97.5024, GNorm = 0.1812
Meta loss on this task batch = 3.4494e-01, Meta loss averaged over last 500 steps = 3.4621e-01, PNorm = 97.5128, GNorm = 0.2232
Meta loss on this task batch = 3.2797e-01, Meta loss averaged over last 500 steps = 3.4636e-01, PNorm = 97.5222, GNorm = 0.2056
Meta loss on this task batch = 3.2330e-01, Meta loss averaged over last 500 steps = 3.4620e-01, PNorm = 97.5323, GNorm = 0.2241
Meta loss on this task batch = 3.6826e-01, Meta loss averaged over last 500 steps = 3.4619e-01, PNorm = 97.5427, GNorm = 0.2294
Meta loss on this task batch = 2.7100e-01, Meta loss averaged over last 500 steps = 3.4609e-01, PNorm = 97.5533, GNorm = 0.1843
Meta loss on this task batch = 4.2030e-01, Meta loss averaged over last 500 steps = 3.4626e-01, PNorm = 97.5645, GNorm = 0.2043
Meta loss on this task batch = 3.9013e-01, Meta loss averaged over last 500 steps = 3.4634e-01, PNorm = 97.5749, GNorm = 0.2110
Meta loss on this task batch = 3.1992e-01, Meta loss averaged over last 500 steps = 3.4612e-01, PNorm = 97.5837, GNorm = 0.2249
Meta loss on this task batch = 3.4105e-01, Meta loss averaged over last 500 steps = 3.4601e-01, PNorm = 97.5918, GNorm = 0.2260
Meta loss on this task batch = 3.8855e-01, Meta loss averaged over last 500 steps = 3.4618e-01, PNorm = 97.5999, GNorm = 0.2387
Meta loss on this task batch = 3.4403e-01, Meta loss averaged over last 500 steps = 3.4610e-01, PNorm = 97.6082, GNorm = 0.2090
Meta loss on this task batch = 3.2095e-01, Meta loss averaged over last 500 steps = 3.4612e-01, PNorm = 97.6172, GNorm = 0.1791
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 3.4614e-01, PNorm = 97.6266, GNorm = 0.1803
Meta loss on this task batch = 3.6770e-01, Meta loss averaged over last 500 steps = 3.4628e-01, PNorm = 97.6369, GNorm = 0.2108
Meta loss on this task batch = 3.3932e-01, Meta loss averaged over last 500 steps = 3.4626e-01, PNorm = 97.6481, GNorm = 0.2488
Took 116.98941969871521 seconds to complete one epoch of meta training
Took 124.93031144142151 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510934
Epoch 220
Meta loss on this task batch = 3.7542e-01, Meta loss averaged over last 500 steps = 3.4625e-01, PNorm = 97.6590, GNorm = 0.2273
Meta loss on this task batch = 3.4040e-01, Meta loss averaged over last 500 steps = 3.4613e-01, PNorm = 97.6706, GNorm = 0.1961
Meta loss on this task batch = 3.2605e-01, Meta loss averaged over last 500 steps = 3.4607e-01, PNorm = 97.6839, GNorm = 0.2112
Meta loss on this task batch = 3.6209e-01, Meta loss averaged over last 500 steps = 3.4613e-01, PNorm = 97.6964, GNorm = 0.2314
Meta loss on this task batch = 3.8668e-01, Meta loss averaged over last 500 steps = 3.4627e-01, PNorm = 97.7079, GNorm = 0.2342
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 3.4612e-01, PNorm = 97.7193, GNorm = 0.1840
Meta loss on this task batch = 3.4566e-01, Meta loss averaged over last 500 steps = 3.4606e-01, PNorm = 97.7282, GNorm = 0.2334
Meta loss on this task batch = 3.7143e-01, Meta loss averaged over last 500 steps = 3.4618e-01, PNorm = 97.7350, GNorm = 0.2973
Meta loss on this task batch = 3.6146e-01, Meta loss averaged over last 500 steps = 3.4615e-01, PNorm = 97.7412, GNorm = 0.2326
Meta loss on this task batch = 3.5837e-01, Meta loss averaged over last 500 steps = 3.4615e-01, PNorm = 97.7478, GNorm = 0.2141
Meta loss on this task batch = 2.7213e-01, Meta loss averaged over last 500 steps = 3.4592e-01, PNorm = 97.7558, GNorm = 0.1731
Meta loss on this task batch = 3.3600e-01, Meta loss averaged over last 500 steps = 3.4577e-01, PNorm = 97.7640, GNorm = 0.2009
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 3.4563e-01, PNorm = 97.7721, GNorm = 0.1893
Meta loss on this task batch = 3.2322e-01, Meta loss averaged over last 500 steps = 3.4551e-01, PNorm = 97.7814, GNorm = 0.1899
Meta loss on this task batch = 3.6719e-01, Meta loss averaged over last 500 steps = 3.4557e-01, PNorm = 97.7909, GNorm = 0.1951
Meta loss on this task batch = 3.7955e-01, Meta loss averaged over last 500 steps = 3.4573e-01, PNorm = 97.8011, GNorm = 0.2317
Meta loss on this task batch = 3.4804e-01, Meta loss averaged over last 500 steps = 3.4576e-01, PNorm = 97.8112, GNorm = 0.1995
Meta loss on this task batch = 2.8138e-01, Meta loss averaged over last 500 steps = 3.4565e-01, PNorm = 97.8219, GNorm = 0.2034
Meta loss on this task batch = 3.7398e-01, Meta loss averaged over last 500 steps = 3.4571e-01, PNorm = 97.8331, GNorm = 0.2543
Took 118.23913812637329 seconds to complete one epoch of meta training
Took 126.10356044769287 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481603
Epoch 221
Meta loss on this task batch = 3.1900e-01, Meta loss averaged over last 500 steps = 3.4562e-01, PNorm = 97.8437, GNorm = 0.1959
Meta loss on this task batch = 3.3402e-01, Meta loss averaged over last 500 steps = 3.4558e-01, PNorm = 97.8547, GNorm = 0.2045
Meta loss on this task batch = 3.2239e-01, Meta loss averaged over last 500 steps = 3.4557e-01, PNorm = 97.8659, GNorm = 0.2167
Meta loss on this task batch = 3.7349e-01, Meta loss averaged over last 500 steps = 3.4562e-01, PNorm = 97.8770, GNorm = 0.2384
Meta loss on this task batch = 2.7934e-01, Meta loss averaged over last 500 steps = 3.4555e-01, PNorm = 97.8888, GNorm = 0.2006
Meta loss on this task batch = 3.4415e-01, Meta loss averaged over last 500 steps = 3.4559e-01, PNorm = 97.9006, GNorm = 0.2168
Meta loss on this task batch = 3.1717e-01, Meta loss averaged over last 500 steps = 3.4563e-01, PNorm = 97.9110, GNorm = 0.2655
Meta loss on this task batch = 3.1680e-01, Meta loss averaged over last 500 steps = 3.4553e-01, PNorm = 97.9212, GNorm = 0.2030
Meta loss on this task batch = 3.6592e-01, Meta loss averaged over last 500 steps = 3.4567e-01, PNorm = 97.9295, GNorm = 0.2279
Meta loss on this task batch = 3.2365e-01, Meta loss averaged over last 500 steps = 3.4548e-01, PNorm = 97.9369, GNorm = 0.2226
Meta loss on this task batch = 3.7697e-01, Meta loss averaged over last 500 steps = 3.4557e-01, PNorm = 97.9439, GNorm = 0.2081
Meta loss on this task batch = 3.4032e-01, Meta loss averaged over last 500 steps = 3.4558e-01, PNorm = 97.9512, GNorm = 0.2117
Meta loss on this task batch = 3.7146e-01, Meta loss averaged over last 500 steps = 3.4564e-01, PNorm = 97.9577, GNorm = 0.2182
Meta loss on this task batch = 3.3236e-01, Meta loss averaged over last 500 steps = 3.4565e-01, PNorm = 97.9650, GNorm = 0.2265
Meta loss on this task batch = 4.0839e-01, Meta loss averaged over last 500 steps = 3.4584e-01, PNorm = 97.9718, GNorm = 0.2202
Meta loss on this task batch = 3.5805e-01, Meta loss averaged over last 500 steps = 3.4588e-01, PNorm = 97.9788, GNorm = 0.2037
Meta loss on this task batch = 3.4864e-01, Meta loss averaged over last 500 steps = 3.4595e-01, PNorm = 97.9869, GNorm = 0.2329
Meta loss on this task batch = 3.3035e-01, Meta loss averaged over last 500 steps = 3.4582e-01, PNorm = 97.9962, GNorm = 0.1819
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 3.4565e-01, PNorm = 98.0081, GNorm = 0.2155
Took 114.3830337524414 seconds to complete one epoch of meta training
Took 122.60145711898804 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475210
Epoch 222
Meta loss on this task batch = 3.4859e-01, Meta loss averaged over last 500 steps = 3.4556e-01, PNorm = 98.0208, GNorm = 0.1903
Meta loss on this task batch = 2.9032e-01, Meta loss averaged over last 500 steps = 3.4544e-01, PNorm = 98.0323, GNorm = 0.2356
Meta loss on this task batch = 3.5491e-01, Meta loss averaged over last 500 steps = 3.4542e-01, PNorm = 98.0419, GNorm = 0.2712
Meta loss on this task batch = 3.1826e-01, Meta loss averaged over last 500 steps = 3.4527e-01, PNorm = 98.0507, GNorm = 0.2025
Meta loss on this task batch = 4.0813e-01, Meta loss averaged over last 500 steps = 3.4534e-01, PNorm = 98.0564, GNorm = 0.2868
Meta loss on this task batch = 3.6118e-01, Meta loss averaged over last 500 steps = 3.4526e-01, PNorm = 98.0616, GNorm = 0.2158
Meta loss on this task batch = 3.8777e-01, Meta loss averaged over last 500 steps = 3.4534e-01, PNorm = 98.0665, GNorm = 0.2520
Meta loss on this task batch = 3.3651e-01, Meta loss averaged over last 500 steps = 3.4527e-01, PNorm = 98.0729, GNorm = 0.2058
Meta loss on this task batch = 3.5397e-01, Meta loss averaged over last 500 steps = 3.4545e-01, PNorm = 98.0792, GNorm = 0.2211
Meta loss on this task batch = 3.8715e-01, Meta loss averaged over last 500 steps = 3.4537e-01, PNorm = 98.0858, GNorm = 0.2809
Meta loss on this task batch = 3.2001e-01, Meta loss averaged over last 500 steps = 3.4532e-01, PNorm = 98.0934, GNorm = 0.2004
Meta loss on this task batch = 3.1501e-01, Meta loss averaged over last 500 steps = 3.4519e-01, PNorm = 98.1018, GNorm = 0.2067
Meta loss on this task batch = 2.9673e-01, Meta loss averaged over last 500 steps = 3.4503e-01, PNorm = 98.1113, GNorm = 0.1859
Meta loss on this task batch = 3.1726e-01, Meta loss averaged over last 500 steps = 3.4507e-01, PNorm = 98.1220, GNorm = 0.1933
Meta loss on this task batch = 3.4435e-01, Meta loss averaged over last 500 steps = 3.4511e-01, PNorm = 98.1332, GNorm = 0.1984
Meta loss on this task batch = 3.3364e-01, Meta loss averaged over last 500 steps = 3.4506e-01, PNorm = 98.1444, GNorm = 0.2200
Meta loss on this task batch = 3.2194e-01, Meta loss averaged over last 500 steps = 3.4503e-01, PNorm = 98.1550, GNorm = 0.2116
Meta loss on this task batch = 3.5663e-01, Meta loss averaged over last 500 steps = 3.4510e-01, PNorm = 98.1628, GNorm = 0.2829
Meta loss on this task batch = 3.5596e-01, Meta loss averaged over last 500 steps = 3.4524e-01, PNorm = 98.1700, GNorm = 0.2418
Took 114.69106411933899 seconds to complete one epoch of meta training
Took 122.37554383277893 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484707
Epoch 223
Meta loss on this task batch = 3.8674e-01, Meta loss averaged over last 500 steps = 3.4521e-01, PNorm = 98.1764, GNorm = 0.2408
Meta loss on this task batch = 3.8486e-01, Meta loss averaged over last 500 steps = 3.4521e-01, PNorm = 98.1833, GNorm = 0.2115
Meta loss on this task batch = 2.8024e-01, Meta loss averaged over last 500 steps = 3.4509e-01, PNorm = 98.1912, GNorm = 0.1898
Meta loss on this task batch = 3.4853e-01, Meta loss averaged over last 500 steps = 3.4516e-01, PNorm = 98.1998, GNorm = 0.2327
Meta loss on this task batch = 3.1831e-01, Meta loss averaged over last 500 steps = 3.4508e-01, PNorm = 98.2093, GNorm = 0.2199
Meta loss on this task batch = 3.4709e-01, Meta loss averaged over last 500 steps = 3.4515e-01, PNorm = 98.2190, GNorm = 0.2505
Meta loss on this task batch = 3.2737e-01, Meta loss averaged over last 500 steps = 3.4505e-01, PNorm = 98.2295, GNorm = 0.2261
Meta loss on this task batch = 3.1998e-01, Meta loss averaged over last 500 steps = 3.4502e-01, PNorm = 98.2400, GNorm = 0.2373
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 3.4492e-01, PNorm = 98.2507, GNorm = 0.2495
Meta loss on this task batch = 3.1068e-01, Meta loss averaged over last 500 steps = 3.4486e-01, PNorm = 98.2618, GNorm = 0.1958
Meta loss on this task batch = 3.8287e-01, Meta loss averaged over last 500 steps = 3.4486e-01, PNorm = 98.2732, GNorm = 0.2496
Meta loss on this task batch = 3.5123e-01, Meta loss averaged over last 500 steps = 3.4499e-01, PNorm = 98.2836, GNorm = 0.2456
Meta loss on this task batch = 3.1246e-01, Meta loss averaged over last 500 steps = 3.4488e-01, PNorm = 98.2942, GNorm = 0.2213
Meta loss on this task batch = 3.1152e-01, Meta loss averaged over last 500 steps = 3.4491e-01, PNorm = 98.3035, GNorm = 0.2008
Meta loss on this task batch = 3.2740e-01, Meta loss averaged over last 500 steps = 3.4484e-01, PNorm = 98.3129, GNorm = 0.2076
Meta loss on this task batch = 3.5575e-01, Meta loss averaged over last 500 steps = 3.4487e-01, PNorm = 98.3221, GNorm = 0.2347
Meta loss on this task batch = 4.0487e-01, Meta loss averaged over last 500 steps = 3.4496e-01, PNorm = 98.3298, GNorm = 0.2837
Meta loss on this task batch = 3.3647e-01, Meta loss averaged over last 500 steps = 3.4486e-01, PNorm = 98.3375, GNorm = 0.2281
Meta loss on this task batch = 3.8089e-01, Meta loss averaged over last 500 steps = 3.4493e-01, PNorm = 98.3456, GNorm = 0.2328
Took 115.36863684654236 seconds to complete one epoch of meta training
Took 123.12116360664368 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482560
Epoch 224
Meta loss on this task batch = 3.4823e-01, Meta loss averaged over last 500 steps = 3.4484e-01, PNorm = 98.3548, GNorm = 0.1916
Meta loss on this task batch = 2.8801e-01, Meta loss averaged over last 500 steps = 3.4478e-01, PNorm = 98.3646, GNorm = 0.1902
Meta loss on this task batch = 3.1770e-01, Meta loss averaged over last 500 steps = 3.4469e-01, PNorm = 98.3754, GNorm = 0.1785
Meta loss on this task batch = 3.8166e-01, Meta loss averaged over last 500 steps = 3.4474e-01, PNorm = 98.3860, GNorm = 0.2191
Meta loss on this task batch = 3.3647e-01, Meta loss averaged over last 500 steps = 3.4461e-01, PNorm = 98.3974, GNorm = 0.2135
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 3.4460e-01, PNorm = 98.4081, GNorm = 0.2190
Meta loss on this task batch = 3.0253e-01, Meta loss averaged over last 500 steps = 3.4449e-01, PNorm = 98.4197, GNorm = 0.1922
Meta loss on this task batch = 3.0055e-01, Meta loss averaged over last 500 steps = 3.4435e-01, PNorm = 98.4318, GNorm = 0.1963
Meta loss on this task batch = 4.0932e-01, Meta loss averaged over last 500 steps = 3.4435e-01, PNorm = 98.4430, GNorm = 0.2563
Meta loss on this task batch = 3.5639e-01, Meta loss averaged over last 500 steps = 3.4430e-01, PNorm = 98.4544, GNorm = 0.2300
Meta loss on this task batch = 2.6105e-01, Meta loss averaged over last 500 steps = 3.4421e-01, PNorm = 98.4657, GNorm = 0.1956
Meta loss on this task batch = 2.8370e-01, Meta loss averaged over last 500 steps = 3.4417e-01, PNorm = 98.4765, GNorm = 0.1990
Meta loss on this task batch = 3.6040e-01, Meta loss averaged over last 500 steps = 3.4415e-01, PNorm = 98.4864, GNorm = 0.2719
Meta loss on this task batch = 3.6106e-01, Meta loss averaged over last 500 steps = 3.4427e-01, PNorm = 98.4969, GNorm = 0.2102
Meta loss on this task batch = 3.3056e-01, Meta loss averaged over last 500 steps = 3.4428e-01, PNorm = 98.5058, GNorm = 0.2060
Meta loss on this task batch = 3.4845e-01, Meta loss averaged over last 500 steps = 3.4421e-01, PNorm = 98.5146, GNorm = 0.2200
Meta loss on this task batch = 3.6095e-01, Meta loss averaged over last 500 steps = 3.4418e-01, PNorm = 98.5218, GNorm = 0.2163
Meta loss on this task batch = 3.2147e-01, Meta loss averaged over last 500 steps = 3.4422e-01, PNorm = 98.5282, GNorm = 0.2367
Meta loss on this task batch = 3.4702e-01, Meta loss averaged over last 500 steps = 3.4419e-01, PNorm = 98.5344, GNorm = 0.2443
Took 117.04899311065674 seconds to complete one epoch of meta training
Took 124.78540849685669 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503311
Epoch 225
Meta loss on this task batch = 3.4309e-01, Meta loss averaged over last 500 steps = 3.4419e-01, PNorm = 98.5411, GNorm = 0.1820
Meta loss on this task batch = 3.2343e-01, Meta loss averaged over last 500 steps = 3.4415e-01, PNorm = 98.5477, GNorm = 0.2232
Meta loss on this task batch = 3.0475e-01, Meta loss averaged over last 500 steps = 3.4393e-01, PNorm = 98.5554, GNorm = 0.1921
Meta loss on this task batch = 3.3488e-01, Meta loss averaged over last 500 steps = 3.4383e-01, PNorm = 98.5640, GNorm = 0.2151
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 3.4376e-01, PNorm = 98.5730, GNorm = 0.1934
Meta loss on this task batch = 3.6995e-01, Meta loss averaged over last 500 steps = 3.4382e-01, PNorm = 98.5818, GNorm = 0.2085
Meta loss on this task batch = 3.7474e-01, Meta loss averaged over last 500 steps = 3.4392e-01, PNorm = 98.5900, GNorm = 0.2109
Meta loss on this task batch = 2.5950e-01, Meta loss averaged over last 500 steps = 3.4372e-01, PNorm = 98.6001, GNorm = 0.2105
Meta loss on this task batch = 3.6431e-01, Meta loss averaged over last 500 steps = 3.4375e-01, PNorm = 98.6099, GNorm = 0.2267
Meta loss on this task batch = 3.2683e-01, Meta loss averaged over last 500 steps = 3.4361e-01, PNorm = 98.6211, GNorm = 0.2260
Meta loss on this task batch = 3.7458e-01, Meta loss averaged over last 500 steps = 3.4357e-01, PNorm = 98.6316, GNorm = 0.2459
Meta loss on this task batch = 3.5504e-01, Meta loss averaged over last 500 steps = 3.4363e-01, PNorm = 98.6413, GNorm = 0.2359
Meta loss on this task batch = 3.1748e-01, Meta loss averaged over last 500 steps = 3.4350e-01, PNorm = 98.6512, GNorm = 0.2099
Meta loss on this task batch = 3.4062e-01, Meta loss averaged over last 500 steps = 3.4350e-01, PNorm = 98.6622, GNorm = 0.2186
Meta loss on this task batch = 3.3822e-01, Meta loss averaged over last 500 steps = 3.4341e-01, PNorm = 98.6724, GNorm = 0.2294
Meta loss on this task batch = 3.7013e-01, Meta loss averaged over last 500 steps = 3.4344e-01, PNorm = 98.6806, GNorm = 0.2308
Meta loss on this task batch = 3.1256e-01, Meta loss averaged over last 500 steps = 3.4332e-01, PNorm = 98.6892, GNorm = 0.1967
Meta loss on this task batch = 2.9789e-01, Meta loss averaged over last 500 steps = 3.4320e-01, PNorm = 98.6989, GNorm = 0.1852
Meta loss on this task batch = 3.7142e-01, Meta loss averaged over last 500 steps = 3.4341e-01, PNorm = 98.7075, GNorm = 0.2884
Took 114.94410848617554 seconds to complete one epoch of meta training
Took 122.74822115898132 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505164
Epoch 226
Meta loss on this task batch = 3.9483e-01, Meta loss averaged over last 500 steps = 3.4348e-01, PNorm = 98.7152, GNorm = 0.2746
Meta loss on this task batch = 3.3863e-01, Meta loss averaged over last 500 steps = 3.4349e-01, PNorm = 98.7234, GNorm = 0.2322
Meta loss on this task batch = 3.1896e-01, Meta loss averaged over last 500 steps = 3.4334e-01, PNorm = 98.7323, GNorm = 0.1689
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.4326e-01, PNorm = 98.7417, GNorm = 0.2031
Meta loss on this task batch = 3.5177e-01, Meta loss averaged over last 500 steps = 3.4321e-01, PNorm = 98.7509, GNorm = 0.2011
Meta loss on this task batch = 3.3235e-01, Meta loss averaged over last 500 steps = 3.4332e-01, PNorm = 98.7616, GNorm = 0.1983
Meta loss on this task batch = 3.5270e-01, Meta loss averaged over last 500 steps = 3.4332e-01, PNorm = 98.7732, GNorm = 0.2148
Meta loss on this task batch = 3.8992e-01, Meta loss averaged over last 500 steps = 3.4340e-01, PNorm = 98.7831, GNorm = 0.2423
Meta loss on this task batch = 3.7480e-01, Meta loss averaged over last 500 steps = 3.4340e-01, PNorm = 98.7918, GNorm = 0.2179
Meta loss on this task batch = 3.1433e-01, Meta loss averaged over last 500 steps = 3.4331e-01, PNorm = 98.8007, GNorm = 0.2112
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 3.4314e-01, PNorm = 98.8102, GNorm = 0.2255
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 3.4298e-01, PNorm = 98.8201, GNorm = 0.1861
Meta loss on this task batch = 3.6396e-01, Meta loss averaged over last 500 steps = 3.4306e-01, PNorm = 98.8292, GNorm = 0.2155
Meta loss on this task batch = 3.4618e-01, Meta loss averaged over last 500 steps = 3.4302e-01, PNorm = 98.8391, GNorm = 0.2248
Meta loss on this task batch = 3.6256e-01, Meta loss averaged over last 500 steps = 3.4302e-01, PNorm = 98.8484, GNorm = 0.2432
Meta loss on this task batch = 3.6834e-01, Meta loss averaged over last 500 steps = 3.4323e-01, PNorm = 98.8578, GNorm = 0.2444
Meta loss on this task batch = 2.8916e-01, Meta loss averaged over last 500 steps = 3.4309e-01, PNorm = 98.8683, GNorm = 0.2042
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 3.4297e-01, PNorm = 98.8794, GNorm = 0.2138
Meta loss on this task batch = 3.6298e-01, Meta loss averaged over last 500 steps = 3.4308e-01, PNorm = 98.8891, GNorm = 0.2952
Took 113.41678071022034 seconds to complete one epoch of meta training
Took 121.68111085891724 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469100
Epoch 227
Meta loss on this task batch = 3.8692e-01, Meta loss averaged over last 500 steps = 3.4315e-01, PNorm = 98.8991, GNorm = 0.2564
Meta loss on this task batch = 3.1656e-01, Meta loss averaged over last 500 steps = 3.4309e-01, PNorm = 98.9102, GNorm = 0.2007
Meta loss on this task batch = 3.2875e-01, Meta loss averaged over last 500 steps = 3.4305e-01, PNorm = 98.9205, GNorm = 0.2318
Meta loss on this task batch = 2.3563e-01, Meta loss averaged over last 500 steps = 3.4279e-01, PNorm = 98.9314, GNorm = 0.1643
Meta loss on this task batch = 2.9833e-01, Meta loss averaged over last 500 steps = 3.4271e-01, PNorm = 98.9425, GNorm = 0.1991
Meta loss on this task batch = 3.4417e-01, Meta loss averaged over last 500 steps = 3.4270e-01, PNorm = 98.9515, GNorm = 0.2310
Meta loss on this task batch = 3.7697e-01, Meta loss averaged over last 500 steps = 3.4272e-01, PNorm = 98.9595, GNorm = 0.2385
Meta loss on this task batch = 3.3287e-01, Meta loss averaged over last 500 steps = 3.4277e-01, PNorm = 98.9683, GNorm = 0.2142
Meta loss on this task batch = 3.2826e-01, Meta loss averaged over last 500 steps = 3.4262e-01, PNorm = 98.9762, GNorm = 0.2049
Meta loss on this task batch = 3.6537e-01, Meta loss averaged over last 500 steps = 3.4274e-01, PNorm = 98.9839, GNorm = 0.2161
Meta loss on this task batch = 3.1643e-01, Meta loss averaged over last 500 steps = 3.4268e-01, PNorm = 98.9918, GNorm = 0.1967
Meta loss on this task batch = 3.7959e-01, Meta loss averaged over last 500 steps = 3.4274e-01, PNorm = 98.9996, GNorm = 0.2186
Meta loss on this task batch = 4.0497e-01, Meta loss averaged over last 500 steps = 3.4292e-01, PNorm = 99.0064, GNorm = 0.2289
Meta loss on this task batch = 3.1749e-01, Meta loss averaged over last 500 steps = 3.4279e-01, PNorm = 99.0138, GNorm = 0.1943
Meta loss on this task batch = 3.3510e-01, Meta loss averaged over last 500 steps = 3.4283e-01, PNorm = 99.0217, GNorm = 0.2053
Meta loss on this task batch = 3.4071e-01, Meta loss averaged over last 500 steps = 3.4284e-01, PNorm = 99.0306, GNorm = 0.1882
Meta loss on this task batch = 3.5962e-01, Meta loss averaged over last 500 steps = 3.4287e-01, PNorm = 99.0394, GNorm = 0.1956
Meta loss on this task batch = 3.1127e-01, Meta loss averaged over last 500 steps = 3.4275e-01, PNorm = 99.0484, GNorm = 0.2095
Meta loss on this task batch = 3.1895e-01, Meta loss averaged over last 500 steps = 3.4272e-01, PNorm = 99.0570, GNorm = 0.2126
Took 117.13637971878052 seconds to complete one epoch of meta training
Took 125.26381802558899 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464753
Epoch 228
Meta loss on this task batch = 3.5011e-01, Meta loss averaged over last 500 steps = 3.4270e-01, PNorm = 99.0659, GNorm = 0.1838
Meta loss on this task batch = 3.1956e-01, Meta loss averaged over last 500 steps = 3.4264e-01, PNorm = 99.0748, GNorm = 0.2095
Meta loss on this task batch = 3.2783e-01, Meta loss averaged over last 500 steps = 3.4255e-01, PNorm = 99.0841, GNorm = 0.1740
Meta loss on this task batch = 3.8085e-01, Meta loss averaged over last 500 steps = 3.4258e-01, PNorm = 99.0933, GNorm = 0.2323
Meta loss on this task batch = 3.7549e-01, Meta loss averaged over last 500 steps = 3.4257e-01, PNorm = 99.1014, GNorm = 0.2441
Meta loss on this task batch = 2.9611e-01, Meta loss averaged over last 500 steps = 3.4239e-01, PNorm = 99.1107, GNorm = 0.1881
Meta loss on this task batch = 3.0185e-01, Meta loss averaged over last 500 steps = 3.4223e-01, PNorm = 99.1208, GNorm = 0.1984
Meta loss on this task batch = 3.1405e-01, Meta loss averaged over last 500 steps = 3.4227e-01, PNorm = 99.1307, GNorm = 0.1937
Meta loss on this task batch = 3.5531e-01, Meta loss averaged over last 500 steps = 3.4230e-01, PNorm = 99.1398, GNorm = 0.2449
Meta loss on this task batch = 3.4733e-01, Meta loss averaged over last 500 steps = 3.4229e-01, PNorm = 99.1495, GNorm = 0.2157
Meta loss on this task batch = 3.4343e-01, Meta loss averaged over last 500 steps = 3.4223e-01, PNorm = 99.1586, GNorm = 0.2464
Meta loss on this task batch = 3.2192e-01, Meta loss averaged over last 500 steps = 3.4210e-01, PNorm = 99.1683, GNorm = 0.1984
Meta loss on this task batch = 3.3673e-01, Meta loss averaged over last 500 steps = 3.4214e-01, PNorm = 99.1787, GNorm = 0.2155
Meta loss on this task batch = 3.3592e-01, Meta loss averaged over last 500 steps = 3.4211e-01, PNorm = 99.1891, GNorm = 0.2112
Meta loss on this task batch = 3.5851e-01, Meta loss averaged over last 500 steps = 3.4219e-01, PNorm = 99.1997, GNorm = 0.1928
Meta loss on this task batch = 3.2660e-01, Meta loss averaged over last 500 steps = 3.4208e-01, PNorm = 99.2102, GNorm = 0.1906
Meta loss on this task batch = 4.0374e-01, Meta loss averaged over last 500 steps = 3.4224e-01, PNorm = 99.2201, GNorm = 0.2292
Meta loss on this task batch = 3.1819e-01, Meta loss averaged over last 500 steps = 3.4220e-01, PNorm = 99.2306, GNorm = 0.1860
Meta loss on this task batch = 2.7110e-01, Meta loss averaged over last 500 steps = 3.4212e-01, PNorm = 99.2418, GNorm = 0.2112
Took 111.67048621177673 seconds to complete one epoch of meta training
Took 119.46019744873047 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500683
Epoch 229
Meta loss on this task batch = 2.8832e-01, Meta loss averaged over last 500 steps = 3.4208e-01, PNorm = 99.2540, GNorm = 0.1910
Meta loss on this task batch = 3.2371e-01, Meta loss averaged over last 500 steps = 3.4204e-01, PNorm = 99.2661, GNorm = 0.2166
Meta loss on this task batch = 3.0269e-01, Meta loss averaged over last 500 steps = 3.4195e-01, PNorm = 99.2782, GNorm = 0.1810
Meta loss on this task batch = 3.6178e-01, Meta loss averaged over last 500 steps = 3.4199e-01, PNorm = 99.2884, GNorm = 0.2826
Meta loss on this task batch = 3.4566e-01, Meta loss averaged over last 500 steps = 3.4186e-01, PNorm = 99.2978, GNorm = 0.2210
Meta loss on this task batch = 3.1889e-01, Meta loss averaged over last 500 steps = 3.4175e-01, PNorm = 99.3082, GNorm = 0.2181
Meta loss on this task batch = 3.3179e-01, Meta loss averaged over last 500 steps = 3.4168e-01, PNorm = 99.3187, GNorm = 0.2324
Meta loss on this task batch = 3.6045e-01, Meta loss averaged over last 500 steps = 3.4172e-01, PNorm = 99.3288, GNorm = 0.2395
Meta loss on this task batch = 2.8850e-01, Meta loss averaged over last 500 steps = 3.4172e-01, PNorm = 99.3382, GNorm = 0.2093
Meta loss on this task batch = 3.1632e-01, Meta loss averaged over last 500 steps = 3.4159e-01, PNorm = 99.3469, GNorm = 0.2152
Meta loss on this task batch = 2.7676e-01, Meta loss averaged over last 500 steps = 3.4148e-01, PNorm = 99.3568, GNorm = 0.2267
Meta loss on this task batch = 3.4118e-01, Meta loss averaged over last 500 steps = 3.4144e-01, PNorm = 99.3668, GNorm = 0.2662
Meta loss on this task batch = 2.6284e-01, Meta loss averaged over last 500 steps = 3.4132e-01, PNorm = 99.3784, GNorm = 0.2440
Meta loss on this task batch = 3.6813e-01, Meta loss averaged over last 500 steps = 3.4128e-01, PNorm = 99.3886, GNorm = 0.2556
Meta loss on this task batch = 3.3652e-01, Meta loss averaged over last 500 steps = 3.4126e-01, PNorm = 99.3975, GNorm = 0.2322
Meta loss on this task batch = 3.7438e-01, Meta loss averaged over last 500 steps = 3.4128e-01, PNorm = 99.4061, GNorm = 0.2133
Meta loss on this task batch = 3.2944e-01, Meta loss averaged over last 500 steps = 3.4129e-01, PNorm = 99.4156, GNorm = 0.2269
Meta loss on this task batch = 3.4717e-01, Meta loss averaged over last 500 steps = 3.4135e-01, PNorm = 99.4254, GNorm = 0.1851
Meta loss on this task batch = 3.6359e-01, Meta loss averaged over last 500 steps = 3.4144e-01, PNorm = 99.4341, GNorm = 0.2946
Took 117.20584988594055 seconds to complete one epoch of meta training
Took 124.70324110984802 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.530389
Found better MAML checkpoint after meta validation, saving now
Epoch 230
Meta loss on this task batch = 2.9336e-01, Meta loss averaged over last 500 steps = 3.4137e-01, PNorm = 99.4437, GNorm = 0.1833
Meta loss on this task batch = 3.2160e-01, Meta loss averaged over last 500 steps = 3.4125e-01, PNorm = 99.4536, GNorm = 0.1989
Meta loss on this task batch = 3.3322e-01, Meta loss averaged over last 500 steps = 3.4132e-01, PNorm = 99.4631, GNorm = 0.2096
Meta loss on this task batch = 3.7392e-01, Meta loss averaged over last 500 steps = 3.4142e-01, PNorm = 99.4719, GNorm = 0.2462
Meta loss on this task batch = 3.4931e-01, Meta loss averaged over last 500 steps = 3.4131e-01, PNorm = 99.4807, GNorm = 0.1896
Meta loss on this task batch = 2.9357e-01, Meta loss averaged over last 500 steps = 3.4118e-01, PNorm = 99.4896, GNorm = 0.1783
Meta loss on this task batch = 3.2991e-01, Meta loss averaged over last 500 steps = 3.4107e-01, PNorm = 99.4969, GNorm = 0.2085
Meta loss on this task batch = 3.5373e-01, Meta loss averaged over last 500 steps = 3.4101e-01, PNorm = 99.5052, GNorm = 0.2261
Meta loss on this task batch = 3.1809e-01, Meta loss averaged over last 500 steps = 3.4094e-01, PNorm = 99.5139, GNorm = 0.1826
Meta loss on this task batch = 3.9493e-01, Meta loss averaged over last 500 steps = 3.4094e-01, PNorm = 99.5227, GNorm = 0.2224
Meta loss on this task batch = 3.2318e-01, Meta loss averaged over last 500 steps = 3.4089e-01, PNorm = 99.5327, GNorm = 0.1969
Meta loss on this task batch = 3.5678e-01, Meta loss averaged over last 500 steps = 3.4090e-01, PNorm = 99.5418, GNorm = 0.2576
Meta loss on this task batch = 3.0892e-01, Meta loss averaged over last 500 steps = 3.4096e-01, PNorm = 99.5520, GNorm = 0.2174
Meta loss on this task batch = 3.6559e-01, Meta loss averaged over last 500 steps = 3.4105e-01, PNorm = 99.5624, GNorm = 0.2511
Meta loss on this task batch = 3.8682e-01, Meta loss averaged over last 500 steps = 3.4120e-01, PNorm = 99.5735, GNorm = 0.2241
Meta loss on this task batch = 3.1459e-01, Meta loss averaged over last 500 steps = 3.4114e-01, PNorm = 99.5841, GNorm = 0.1955
Meta loss on this task batch = 3.4832e-01, Meta loss averaged over last 500 steps = 3.4120e-01, PNorm = 99.5940, GNorm = 0.2607
Meta loss on this task batch = 3.4144e-01, Meta loss averaged over last 500 steps = 3.4114e-01, PNorm = 99.6024, GNorm = 0.2316
Meta loss on this task batch = 3.6422e-01, Meta loss averaged over last 500 steps = 3.4121e-01, PNorm = 99.6103, GNorm = 0.2379
Took 114.75389385223389 seconds to complete one epoch of meta training
Took 122.44911742210388 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.520108
Epoch 231
Meta loss on this task batch = 4.3951e-01, Meta loss averaged over last 500 steps = 3.4142e-01, PNorm = 99.6166, GNorm = 0.2536
Meta loss on this task batch = 3.3142e-01, Meta loss averaged over last 500 steps = 3.4139e-01, PNorm = 99.6235, GNorm = 0.2112
Meta loss on this task batch = 3.4295e-01, Meta loss averaged over last 500 steps = 3.4144e-01, PNorm = 99.6310, GNorm = 0.2334
Meta loss on this task batch = 3.2052e-01, Meta loss averaged over last 500 steps = 3.4145e-01, PNorm = 99.6394, GNorm = 0.2117
Meta loss on this task batch = 3.6036e-01, Meta loss averaged over last 500 steps = 3.4133e-01, PNorm = 99.6490, GNorm = 0.2131
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 3.4133e-01, PNorm = 99.6587, GNorm = 0.2053
Meta loss on this task batch = 3.1954e-01, Meta loss averaged over last 500 steps = 3.4130e-01, PNorm = 99.6697, GNorm = 0.2172
Meta loss on this task batch = 3.5252e-01, Meta loss averaged over last 500 steps = 3.4135e-01, PNorm = 99.6811, GNorm = 0.2215
Meta loss on this task batch = 3.5332e-01, Meta loss averaged over last 500 steps = 3.4131e-01, PNorm = 99.6917, GNorm = 0.2010
Meta loss on this task batch = 2.9329e-01, Meta loss averaged over last 500 steps = 3.4109e-01, PNorm = 99.7025, GNorm = 0.2174
Meta loss on this task batch = 3.5613e-01, Meta loss averaged over last 500 steps = 3.4112e-01, PNorm = 99.7136, GNorm = 0.2045
Meta loss on this task batch = 3.3370e-01, Meta loss averaged over last 500 steps = 3.4099e-01, PNorm = 99.7248, GNorm = 0.1953
Meta loss on this task batch = 3.1469e-01, Meta loss averaged over last 500 steps = 3.4090e-01, PNorm = 99.7351, GNorm = 0.2179
Meta loss on this task batch = 2.9342e-01, Meta loss averaged over last 500 steps = 3.4091e-01, PNorm = 99.7455, GNorm = 0.2029
Meta loss on this task batch = 3.4492e-01, Meta loss averaged over last 500 steps = 3.4096e-01, PNorm = 99.7549, GNorm = 0.2239
Meta loss on this task batch = 3.2656e-01, Meta loss averaged over last 500 steps = 3.4095e-01, PNorm = 99.7644, GNorm = 0.1897
Meta loss on this task batch = 4.1069e-01, Meta loss averaged over last 500 steps = 3.4114e-01, PNorm = 99.7721, GNorm = 0.2588
Meta loss on this task batch = 3.2283e-01, Meta loss averaged over last 500 steps = 3.4112e-01, PNorm = 99.7802, GNorm = 0.1921
Meta loss on this task batch = 2.6477e-01, Meta loss averaged over last 500 steps = 3.4096e-01, PNorm = 99.7890, GNorm = 0.1967
Took 115.43101239204407 seconds to complete one epoch of meta training
Took 123.03888845443726 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496445
Epoch 232
Meta loss on this task batch = 3.1979e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 99.7987, GNorm = 0.2022
Meta loss on this task batch = 3.1620e-01, Meta loss averaged over last 500 steps = 3.4086e-01, PNorm = 99.8069, GNorm = 0.2375
Meta loss on this task batch = 3.6232e-01, Meta loss averaged over last 500 steps = 3.4091e-01, PNorm = 99.8157, GNorm = 0.2627
Meta loss on this task batch = 3.0618e-01, Meta loss averaged over last 500 steps = 3.4078e-01, PNorm = 99.8247, GNorm = 0.2042
Meta loss on this task batch = 2.9357e-01, Meta loss averaged over last 500 steps = 3.4061e-01, PNorm = 99.8339, GNorm = 0.1883
Meta loss on this task batch = 3.6684e-01, Meta loss averaged over last 500 steps = 3.4065e-01, PNorm = 99.8434, GNorm = 0.2162
Meta loss on this task batch = 3.4330e-01, Meta loss averaged over last 500 steps = 3.4063e-01, PNorm = 99.8531, GNorm = 0.2105
Meta loss on this task batch = 3.7393e-01, Meta loss averaged over last 500 steps = 3.4073e-01, PNorm = 99.8615, GNorm = 0.2551
Meta loss on this task batch = 3.2746e-01, Meta loss averaged over last 500 steps = 3.4074e-01, PNorm = 99.8694, GNorm = 0.1943
Meta loss on this task batch = 3.7812e-01, Meta loss averaged over last 500 steps = 3.4075e-01, PNorm = 99.8768, GNorm = 0.2297
Meta loss on this task batch = 3.4279e-01, Meta loss averaged over last 500 steps = 3.4080e-01, PNorm = 99.8843, GNorm = 0.2211
Meta loss on this task batch = 3.6075e-01, Meta loss averaged over last 500 steps = 3.4089e-01, PNorm = 99.8913, GNorm = 0.2228
Meta loss on this task batch = 3.5492e-01, Meta loss averaged over last 500 steps = 3.4102e-01, PNorm = 99.8983, GNorm = 0.1957
Meta loss on this task batch = 3.6003e-01, Meta loss averaged over last 500 steps = 3.4095e-01, PNorm = 99.9063, GNorm = 0.2148
Meta loss on this task batch = 3.2069e-01, Meta loss averaged over last 500 steps = 3.4088e-01, PNorm = 99.9154, GNorm = 0.2015
Meta loss on this task batch = 3.3429e-01, Meta loss averaged over last 500 steps = 3.4083e-01, PNorm = 99.9258, GNorm = 0.2046
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 3.4101e-01, PNorm = 99.9362, GNorm = 0.1914
Meta loss on this task batch = 3.3037e-01, Meta loss averaged over last 500 steps = 3.4105e-01, PNorm = 99.9472, GNorm = 0.1971
Meta loss on this task batch = 3.2660e-01, Meta loss averaged over last 500 steps = 3.4100e-01, PNorm = 99.9571, GNorm = 0.2483
Took 115.138995885849 seconds to complete one epoch of meta training
Took 122.155846118927 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480348
Epoch 233
Meta loss on this task batch = 3.0785e-01, Meta loss averaged over last 500 steps = 3.4084e-01, PNorm = 99.9677, GNorm = 0.2013
Meta loss on this task batch = 3.1581e-01, Meta loss averaged over last 500 steps = 3.4086e-01, PNorm = 99.9780, GNorm = 0.1978
Meta loss on this task batch = 2.8982e-01, Meta loss averaged over last 500 steps = 3.4070e-01, PNorm = 99.9891, GNorm = 0.1975
Meta loss on this task batch = 3.8461e-01, Meta loss averaged over last 500 steps = 3.4079e-01, PNorm = 99.9975, GNorm = 0.2634
Meta loss on this task batch = 3.4979e-01, Meta loss averaged over last 500 steps = 3.4066e-01, PNorm = 100.0045, GNorm = 0.2262
Meta loss on this task batch = 3.1979e-01, Meta loss averaged over last 500 steps = 3.4043e-01, PNorm = 100.0111, GNorm = 0.2173
Meta loss on this task batch = 3.5409e-01, Meta loss averaged over last 500 steps = 3.4034e-01, PNorm = 100.0181, GNorm = 0.2133
Meta loss on this task batch = 3.4245e-01, Meta loss averaged over last 500 steps = 3.4037e-01, PNorm = 100.0262, GNorm = 0.2001
Meta loss on this task batch = 3.5679e-01, Meta loss averaged over last 500 steps = 3.4039e-01, PNorm = 100.0336, GNorm = 0.1840
Meta loss on this task batch = 3.4653e-01, Meta loss averaged over last 500 steps = 3.4044e-01, PNorm = 100.0416, GNorm = 0.2042
Meta loss on this task batch = 3.3782e-01, Meta loss averaged over last 500 steps = 3.4049e-01, PNorm = 100.0492, GNorm = 0.2348
Meta loss on this task batch = 3.5456e-01, Meta loss averaged over last 500 steps = 3.4058e-01, PNorm = 100.0559, GNorm = 0.2124
Meta loss on this task batch = 3.3890e-01, Meta loss averaged over last 500 steps = 3.4049e-01, PNorm = 100.0616, GNorm = 0.2223
Meta loss on this task batch = 3.5101e-01, Meta loss averaged over last 500 steps = 3.4047e-01, PNorm = 100.0677, GNorm = 0.2207
Meta loss on this task batch = 3.3563e-01, Meta loss averaged over last 500 steps = 3.4045e-01, PNorm = 100.0755, GNorm = 0.1927
Meta loss on this task batch = 3.3816e-01, Meta loss averaged over last 500 steps = 3.4038e-01, PNorm = 100.0841, GNorm = 0.2356
Meta loss on this task batch = 3.2533e-01, Meta loss averaged over last 500 steps = 3.4039e-01, PNorm = 100.0942, GNorm = 0.1939
Meta loss on this task batch = 3.5927e-01, Meta loss averaged over last 500 steps = 3.4045e-01, PNorm = 100.1043, GNorm = 0.2118
Meta loss on this task batch = 3.5882e-01, Meta loss averaged over last 500 steps = 3.4050e-01, PNorm = 100.1149, GNorm = 0.2323
Took 118.27810478210449 seconds to complete one epoch of meta training
Took 125.3530945777893 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470076
Epoch 234
Meta loss on this task batch = 3.5917e-01, Meta loss averaged over last 500 steps = 3.4052e-01, PNorm = 100.1266, GNorm = 0.2416
Meta loss on this task batch = 3.5462e-01, Meta loss averaged over last 500 steps = 3.4048e-01, PNorm = 100.1367, GNorm = 0.2225
Meta loss on this task batch = 3.4146e-01, Meta loss averaged over last 500 steps = 3.4052e-01, PNorm = 100.1472, GNorm = 0.2417
Meta loss on this task batch = 2.4236e-01, Meta loss averaged over last 500 steps = 3.4035e-01, PNorm = 100.1587, GNorm = 0.2111
Meta loss on this task batch = 3.0843e-01, Meta loss averaged over last 500 steps = 3.4032e-01, PNorm = 100.1704, GNorm = 0.2054
Meta loss on this task batch = 3.4215e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 100.1805, GNorm = 0.2413
Meta loss on this task batch = 3.3286e-01, Meta loss averaged over last 500 steps = 3.3997e-01, PNorm = 100.1898, GNorm = 0.2392
Meta loss on this task batch = 3.2615e-01, Meta loss averaged over last 500 steps = 3.3982e-01, PNorm = 100.1995, GNorm = 0.2266
Meta loss on this task batch = 3.5347e-01, Meta loss averaged over last 500 steps = 3.3986e-01, PNorm = 100.2068, GNorm = 0.2504
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.3982e-01, PNorm = 100.2143, GNorm = 0.2114
Meta loss on this task batch = 3.6662e-01, Meta loss averaged over last 500 steps = 3.3991e-01, PNorm = 100.2213, GNorm = 0.2166
Meta loss on this task batch = 3.8128e-01, Meta loss averaged over last 500 steps = 3.3995e-01, PNorm = 100.2280, GNorm = 0.2576
Meta loss on this task batch = 3.7399e-01, Meta loss averaged over last 500 steps = 3.4015e-01, PNorm = 100.2344, GNorm = 0.2247
Meta loss on this task batch = 2.9006e-01, Meta loss averaged over last 500 steps = 3.3993e-01, PNorm = 100.2427, GNorm = 0.1980
Meta loss on this task batch = 3.4969e-01, Meta loss averaged over last 500 steps = 3.4007e-01, PNorm = 100.2514, GNorm = 0.2054
Meta loss on this task batch = 3.3011e-01, Meta loss averaged over last 500 steps = 3.4013e-01, PNorm = 100.2608, GNorm = 0.1836
Meta loss on this task batch = 3.3814e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 100.2704, GNorm = 0.2426
Meta loss on this task batch = 3.5026e-01, Meta loss averaged over last 500 steps = 3.4023e-01, PNorm = 100.2815, GNorm = 0.2140
Meta loss on this task batch = 3.3874e-01, Meta loss averaged over last 500 steps = 3.4032e-01, PNorm = 100.2925, GNorm = 0.2414
Took 118.2080283164978 seconds to complete one epoch of meta training
Took 125.83490777015686 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.447369
Epoch 235
Meta loss on this task batch = 3.7294e-01, Meta loss averaged over last 500 steps = 3.4040e-01, PNorm = 100.3036, GNorm = 0.2253
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.4027e-01, PNorm = 100.3144, GNorm = 0.2075
Meta loss on this task batch = 3.4844e-01, Meta loss averaged over last 500 steps = 3.4031e-01, PNorm = 100.3251, GNorm = 0.2073
Meta loss on this task batch = 3.4003e-01, Meta loss averaged over last 500 steps = 3.4024e-01, PNorm = 100.3351, GNorm = 0.2222
Meta loss on this task batch = 3.3718e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 100.3458, GNorm = 0.3010
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 3.4010e-01, PNorm = 100.3558, GNorm = 0.1923
Meta loss on this task batch = 2.9590e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 100.3654, GNorm = 0.1819
Meta loss on this task batch = 3.4560e-01, Meta loss averaged over last 500 steps = 3.4003e-01, PNorm = 100.3738, GNorm = 0.2291
Meta loss on this task batch = 3.3272e-01, Meta loss averaged over last 500 steps = 3.4002e-01, PNorm = 100.3819, GNorm = 0.2250
Meta loss on this task batch = 4.0450e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 100.3876, GNorm = 0.2928
Meta loss on this task batch = 3.8233e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 100.3931, GNorm = 0.2117
Meta loss on this task batch = 3.3808e-01, Meta loss averaged over last 500 steps = 3.4010e-01, PNorm = 100.3993, GNorm = 0.2377
Meta loss on this task batch = 3.1725e-01, Meta loss averaged over last 500 steps = 3.4006e-01, PNorm = 100.4063, GNorm = 0.2499
Meta loss on this task batch = 3.7357e-01, Meta loss averaged over last 500 steps = 3.4025e-01, PNorm = 100.4132, GNorm = 0.2100
Meta loss on this task batch = 3.4126e-01, Meta loss averaged over last 500 steps = 3.4016e-01, PNorm = 100.4207, GNorm = 0.2138
Meta loss on this task batch = 3.3545e-01, Meta loss averaged over last 500 steps = 3.4024e-01, PNorm = 100.4298, GNorm = 0.1961
Meta loss on this task batch = 2.7206e-01, Meta loss averaged over last 500 steps = 3.3998e-01, PNorm = 100.4386, GNorm = 0.1858
Meta loss on this task batch = 3.2482e-01, Meta loss averaged over last 500 steps = 3.3995e-01, PNorm = 100.4477, GNorm = 0.1806
Meta loss on this task batch = 3.2995e-01, Meta loss averaged over last 500 steps = 3.3999e-01, PNorm = 100.4570, GNorm = 0.2330
Took 116.89504623413086 seconds to complete one epoch of meta training
Took 124.59099221229553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505059
Epoch 236
Meta loss on this task batch = 2.9464e-01, Meta loss averaged over last 500 steps = 3.3995e-01, PNorm = 100.4658, GNorm = 0.1862
Meta loss on this task batch = 3.9357e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 100.4739, GNorm = 0.2712
Meta loss on this task batch = 3.6580e-01, Meta loss averaged over last 500 steps = 3.4022e-01, PNorm = 100.4816, GNorm = 0.2220
Meta loss on this task batch = 3.1183e-01, Meta loss averaged over last 500 steps = 3.4011e-01, PNorm = 100.4897, GNorm = 0.1925
Meta loss on this task batch = 3.3991e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 100.4982, GNorm = 0.2133
Meta loss on this task batch = 3.6029e-01, Meta loss averaged over last 500 steps = 3.4002e-01, PNorm = 100.5069, GNorm = 0.2191
Meta loss on this task batch = 3.7289e-01, Meta loss averaged over last 500 steps = 3.4012e-01, PNorm = 100.5151, GNorm = 0.1943
Meta loss on this task batch = 3.3399e-01, Meta loss averaged over last 500 steps = 3.4004e-01, PNorm = 100.5237, GNorm = 0.2440
Meta loss on this task batch = 3.7688e-01, Meta loss averaged over last 500 steps = 3.4009e-01, PNorm = 100.5312, GNorm = 0.1940
Meta loss on this task batch = 3.6258e-01, Meta loss averaged over last 500 steps = 3.4006e-01, PNorm = 100.5393, GNorm = 0.1958
Meta loss on this task batch = 3.4950e-01, Meta loss averaged over last 500 steps = 3.4018e-01, PNorm = 100.5480, GNorm = 0.2157
Meta loss on this task batch = 3.1116e-01, Meta loss averaged over last 500 steps = 3.4005e-01, PNorm = 100.5579, GNorm = 0.2143
Meta loss on this task batch = 3.3535e-01, Meta loss averaged over last 500 steps = 3.4006e-01, PNorm = 100.5694, GNorm = 0.2307
Meta loss on this task batch = 2.6544e-01, Meta loss averaged over last 500 steps = 3.3985e-01, PNorm = 100.5827, GNorm = 0.2146
Meta loss on this task batch = 3.5141e-01, Meta loss averaged over last 500 steps = 3.3975e-01, PNorm = 100.5958, GNorm = 0.2649
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 3.3972e-01, PNorm = 100.6095, GNorm = 0.2127
Meta loss on this task batch = 3.0818e-01, Meta loss averaged over last 500 steps = 3.3967e-01, PNorm = 100.6222, GNorm = 0.1839
Meta loss on this task batch = 3.6331e-01, Meta loss averaged over last 500 steps = 3.3971e-01, PNorm = 100.6325, GNorm = 0.2764
Meta loss on this task batch = 3.3194e-01, Meta loss averaged over last 500 steps = 3.3969e-01, PNorm = 100.6416, GNorm = 0.3382
Took 114.4198431968689 seconds to complete one epoch of meta training
Took 121.8879132270813 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.530903
Found better MAML checkpoint after meta validation, saving now
Epoch 237
Meta loss on this task batch = 3.4320e-01, Meta loss averaged over last 500 steps = 3.3973e-01, PNorm = 100.6486, GNorm = 0.2560
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 3.3969e-01, PNorm = 100.6559, GNorm = 0.2555
Meta loss on this task batch = 3.4041e-01, Meta loss averaged over last 500 steps = 3.3970e-01, PNorm = 100.6613, GNorm = 0.2876
Meta loss on this task batch = 2.8642e-01, Meta loss averaged over last 500 steps = 3.3962e-01, PNorm = 100.6678, GNorm = 0.2019
Meta loss on this task batch = 3.1458e-01, Meta loss averaged over last 500 steps = 3.3954e-01, PNorm = 100.6749, GNorm = 0.2255
Meta loss on this task batch = 3.5524e-01, Meta loss averaged over last 500 steps = 3.3948e-01, PNorm = 100.6828, GNorm = 0.2047
Meta loss on this task batch = 3.1362e-01, Meta loss averaged over last 500 steps = 3.3941e-01, PNorm = 100.6915, GNorm = 0.2008
Meta loss on this task batch = 3.1344e-01, Meta loss averaged over last 500 steps = 3.3933e-01, PNorm = 100.7002, GNorm = 0.2079
Meta loss on this task batch = 3.3672e-01, Meta loss averaged over last 500 steps = 3.3935e-01, PNorm = 100.7087, GNorm = 0.1883
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 3.3924e-01, PNorm = 100.7176, GNorm = 0.1809
Meta loss on this task batch = 3.3909e-01, Meta loss averaged over last 500 steps = 3.3923e-01, PNorm = 100.7273, GNorm = 0.2000
Meta loss on this task batch = 3.2749e-01, Meta loss averaged over last 500 steps = 3.3917e-01, PNorm = 100.7365, GNorm = 0.2064
Meta loss on this task batch = 3.2042e-01, Meta loss averaged over last 500 steps = 3.3914e-01, PNorm = 100.7464, GNorm = 0.1998
Meta loss on this task batch = 3.1446e-01, Meta loss averaged over last 500 steps = 3.3897e-01, PNorm = 100.7558, GNorm = 0.2076
Meta loss on this task batch = 3.0620e-01, Meta loss averaged over last 500 steps = 3.3886e-01, PNorm = 100.7662, GNorm = 0.1798
Meta loss on this task batch = 3.5017e-01, Meta loss averaged over last 500 steps = 3.3893e-01, PNorm = 100.7759, GNorm = 0.2054
Meta loss on this task batch = 3.9138e-01, Meta loss averaged over last 500 steps = 3.3890e-01, PNorm = 100.7850, GNorm = 0.2447
Meta loss on this task batch = 3.5666e-01, Meta loss averaged over last 500 steps = 3.3892e-01, PNorm = 100.7932, GNorm = 0.2255
Meta loss on this task batch = 3.3567e-01, Meta loss averaged over last 500 steps = 3.3881e-01, PNorm = 100.8012, GNorm = 0.2521
Took 114.1095621585846 seconds to complete one epoch of meta training
Took 121.75356554985046 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517818
Epoch 238
Meta loss on this task batch = 3.0259e-01, Meta loss averaged over last 500 steps = 3.3884e-01, PNorm = 100.8099, GNorm = 0.1864
Meta loss on this task batch = 3.7512e-01, Meta loss averaged over last 500 steps = 3.3897e-01, PNorm = 100.8178, GNorm = 0.2194
Meta loss on this task batch = 3.5891e-01, Meta loss averaged over last 500 steps = 3.3899e-01, PNorm = 100.8256, GNorm = 0.2112
Meta loss on this task batch = 3.0327e-01, Meta loss averaged over last 500 steps = 3.3904e-01, PNorm = 100.8329, GNorm = 0.2034
Meta loss on this task batch = 3.2109e-01, Meta loss averaged over last 500 steps = 3.3904e-01, PNorm = 100.8419, GNorm = 0.2043
Meta loss on this task batch = 3.3222e-01, Meta loss averaged over last 500 steps = 3.3909e-01, PNorm = 100.8516, GNorm = 0.2042
Meta loss on this task batch = 3.4384e-01, Meta loss averaged over last 500 steps = 3.3906e-01, PNorm = 100.8610, GNorm = 0.2060
Meta loss on this task batch = 3.3262e-01, Meta loss averaged over last 500 steps = 3.3908e-01, PNorm = 100.8711, GNorm = 0.2217
Meta loss on this task batch = 3.2830e-01, Meta loss averaged over last 500 steps = 3.3911e-01, PNorm = 100.8808, GNorm = 0.2132
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.3903e-01, PNorm = 100.8908, GNorm = 0.2021
Meta loss on this task batch = 3.1359e-01, Meta loss averaged over last 500 steps = 3.3906e-01, PNorm = 100.9026, GNorm = 0.2116
Meta loss on this task batch = 3.4605e-01, Meta loss averaged over last 500 steps = 3.3898e-01, PNorm = 100.9149, GNorm = 0.2319
Meta loss on this task batch = 3.3253e-01, Meta loss averaged over last 500 steps = 3.3891e-01, PNorm = 100.9265, GNorm = 0.2124
Meta loss on this task batch = 3.8254e-01, Meta loss averaged over last 500 steps = 3.3892e-01, PNorm = 100.9386, GNorm = 0.2238
Meta loss on this task batch = 3.5162e-01, Meta loss averaged over last 500 steps = 3.3894e-01, PNorm = 100.9508, GNorm = 0.2374
Meta loss on this task batch = 2.6058e-01, Meta loss averaged over last 500 steps = 3.3879e-01, PNorm = 100.9633, GNorm = 0.1868
Meta loss on this task batch = 3.7067e-01, Meta loss averaged over last 500 steps = 3.3887e-01, PNorm = 100.9743, GNorm = 0.2546
Meta loss on this task batch = 3.5201e-01, Meta loss averaged over last 500 steps = 3.3893e-01, PNorm = 100.9841, GNorm = 0.2466
Meta loss on this task batch = 3.5820e-01, Meta loss averaged over last 500 steps = 3.3901e-01, PNorm = 100.9912, GNorm = 0.2942
Took 115.37135815620422 seconds to complete one epoch of meta training
Took 123.06933975219727 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499916
Epoch 239
Meta loss on this task batch = 3.0488e-01, Meta loss averaged over last 500 steps = 3.3899e-01, PNorm = 100.9992, GNorm = 0.1839
Meta loss on this task batch = 3.4891e-01, Meta loss averaged over last 500 steps = 3.3899e-01, PNorm = 101.0068, GNorm = 0.2090
Meta loss on this task batch = 3.3523e-01, Meta loss averaged over last 500 steps = 3.3893e-01, PNorm = 101.0151, GNorm = 0.1900
Meta loss on this task batch = 3.7075e-01, Meta loss averaged over last 500 steps = 3.3900e-01, PNorm = 101.0236, GNorm = 0.2320
Meta loss on this task batch = 3.3363e-01, Meta loss averaged over last 500 steps = 3.3901e-01, PNorm = 101.0322, GNorm = 0.1939
Meta loss on this task batch = 3.6578e-01, Meta loss averaged over last 500 steps = 3.3905e-01, PNorm = 101.0418, GNorm = 0.2057
Meta loss on this task batch = 3.4800e-01, Meta loss averaged over last 500 steps = 3.3911e-01, PNorm = 101.0521, GNorm = 0.2115
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 3.3907e-01, PNorm = 101.0636, GNorm = 0.2001
Meta loss on this task batch = 3.6747e-01, Meta loss averaged over last 500 steps = 3.3908e-01, PNorm = 101.0747, GNorm = 0.1965
Meta loss on this task batch = 3.0320e-01, Meta loss averaged over last 500 steps = 3.3909e-01, PNorm = 101.0865, GNorm = 0.2414
Meta loss on this task batch = 3.3561e-01, Meta loss averaged over last 500 steps = 3.3901e-01, PNorm = 101.0979, GNorm = 0.2147
Meta loss on this task batch = 3.0182e-01, Meta loss averaged over last 500 steps = 3.3885e-01, PNorm = 101.1094, GNorm = 0.2031
Meta loss on this task batch = 3.6088e-01, Meta loss averaged over last 500 steps = 3.3896e-01, PNorm = 101.1198, GNorm = 0.2550
Meta loss on this task batch = 3.4023e-01, Meta loss averaged over last 500 steps = 3.3902e-01, PNorm = 101.1300, GNorm = 0.2171
Meta loss on this task batch = 3.1360e-01, Meta loss averaged over last 500 steps = 3.3905e-01, PNorm = 101.1400, GNorm = 0.2078
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 3.3891e-01, PNorm = 101.1495, GNorm = 0.2166
Meta loss on this task batch = 3.8796e-01, Meta loss averaged over last 500 steps = 3.3897e-01, PNorm = 101.1578, GNorm = 0.2557
Meta loss on this task batch = 3.3637e-01, Meta loss averaged over last 500 steps = 3.3892e-01, PNorm = 101.1658, GNorm = 0.2141
Meta loss on this task batch = 3.2431e-01, Meta loss averaged over last 500 steps = 3.3887e-01, PNorm = 101.1739, GNorm = 0.2684
Took 115.01109409332275 seconds to complete one epoch of meta training
Took 122.74484443664551 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500987
Epoch 240
Meta loss on this task batch = 3.3627e-01, Meta loss averaged over last 500 steps = 3.3881e-01, PNorm = 101.1815, GNorm = 0.2759
Meta loss on this task batch = 3.3863e-01, Meta loss averaged over last 500 steps = 3.3865e-01, PNorm = 101.1896, GNorm = 0.2365
Meta loss on this task batch = 3.5978e-01, Meta loss averaged over last 500 steps = 3.3880e-01, PNorm = 101.1976, GNorm = 0.2168
Meta loss on this task batch = 3.1627e-01, Meta loss averaged over last 500 steps = 3.3865e-01, PNorm = 101.2060, GNorm = 0.1914
Meta loss on this task batch = 3.4912e-01, Meta loss averaged over last 500 steps = 3.3872e-01, PNorm = 101.2140, GNorm = 0.2038
Meta loss on this task batch = 3.6262e-01, Meta loss averaged over last 500 steps = 3.3871e-01, PNorm = 101.2236, GNorm = 0.2286
Meta loss on this task batch = 3.3480e-01, Meta loss averaged over last 500 steps = 3.3877e-01, PNorm = 101.2332, GNorm = 0.2219
Meta loss on this task batch = 3.3817e-01, Meta loss averaged over last 500 steps = 3.3875e-01, PNorm = 101.2424, GNorm = 0.2127
Meta loss on this task batch = 3.6849e-01, Meta loss averaged over last 500 steps = 3.3872e-01, PNorm = 101.2521, GNorm = 0.2264
Meta loss on this task batch = 3.5661e-01, Meta loss averaged over last 500 steps = 3.3882e-01, PNorm = 101.2614, GNorm = 0.2315
Meta loss on this task batch = 3.1618e-01, Meta loss averaged over last 500 steps = 3.3882e-01, PNorm = 101.2713, GNorm = 0.1899
Meta loss on this task batch = 3.1290e-01, Meta loss averaged over last 500 steps = 3.3877e-01, PNorm = 101.2805, GNorm = 0.2378
Meta loss on this task batch = 3.4150e-01, Meta loss averaged over last 500 steps = 3.3882e-01, PNorm = 101.2896, GNorm = 0.2031
Meta loss on this task batch = 4.1816e-01, Meta loss averaged over last 500 steps = 3.3895e-01, PNorm = 101.2985, GNorm = 0.2780
Meta loss on this task batch = 2.7916e-01, Meta loss averaged over last 500 steps = 3.3883e-01, PNorm = 101.3085, GNorm = 0.2271
Meta loss on this task batch = 3.2104e-01, Meta loss averaged over last 500 steps = 3.3879e-01, PNorm = 101.3181, GNorm = 0.2147
Meta loss on this task batch = 2.8773e-01, Meta loss averaged over last 500 steps = 3.3858e-01, PNorm = 101.3271, GNorm = 0.2072
Meta loss on this task batch = 3.3944e-01, Meta loss averaged over last 500 steps = 3.3863e-01, PNorm = 101.3356, GNorm = 0.2070
Meta loss on this task batch = 3.4937e-01, Meta loss averaged over last 500 steps = 3.3865e-01, PNorm = 101.3436, GNorm = 0.2574
Took 114.0576229095459 seconds to complete one epoch of meta training
Took 121.78556513786316 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515732
Epoch 241
Meta loss on this task batch = 3.2837e-01, Meta loss averaged over last 500 steps = 3.3859e-01, PNorm = 101.3505, GNorm = 0.1950
Meta loss on this task batch = 3.3075e-01, Meta loss averaged over last 500 steps = 3.3851e-01, PNorm = 101.3578, GNorm = 0.2140
Meta loss on this task batch = 2.8012e-01, Meta loss averaged over last 500 steps = 3.3832e-01, PNorm = 101.3671, GNorm = 0.1936
Meta loss on this task batch = 4.0299e-01, Meta loss averaged over last 500 steps = 3.3847e-01, PNorm = 101.3750, GNorm = 0.2491
Meta loss on this task batch = 3.5299e-01, Meta loss averaged over last 500 steps = 3.3845e-01, PNorm = 101.3832, GNorm = 0.2058
Meta loss on this task batch = 3.6976e-01, Meta loss averaged over last 500 steps = 3.3860e-01, PNorm = 101.3914, GNorm = 0.2443
Meta loss on this task batch = 3.5956e-01, Meta loss averaged over last 500 steps = 3.3865e-01, PNorm = 101.3986, GNorm = 0.2453
Meta loss on this task batch = 3.2297e-01, Meta loss averaged over last 500 steps = 3.3847e-01, PNorm = 101.4064, GNorm = 0.1913
Meta loss on this task batch = 3.3374e-01, Meta loss averaged over last 500 steps = 3.3852e-01, PNorm = 101.4131, GNorm = 0.1990
Meta loss on this task batch = 3.3923e-01, Meta loss averaged over last 500 steps = 3.3845e-01, PNorm = 101.4207, GNorm = 0.2647
Meta loss on this task batch = 3.5358e-01, Meta loss averaged over last 500 steps = 3.3847e-01, PNorm = 101.4282, GNorm = 0.2334
Meta loss on this task batch = 3.0780e-01, Meta loss averaged over last 500 steps = 3.3842e-01, PNorm = 101.4369, GNorm = 0.1917
Meta loss on this task batch = 4.0583e-01, Meta loss averaged over last 500 steps = 3.3849e-01, PNorm = 101.4449, GNorm = 0.2322
Meta loss on this task batch = 3.1739e-01, Meta loss averaged over last 500 steps = 3.3836e-01, PNorm = 101.4534, GNorm = 0.2069
Meta loss on this task batch = 3.4655e-01, Meta loss averaged over last 500 steps = 3.3842e-01, PNorm = 101.4628, GNorm = 0.2400
Meta loss on this task batch = 2.8294e-01, Meta loss averaged over last 500 steps = 3.3833e-01, PNorm = 101.4730, GNorm = 0.1875
Meta loss on this task batch = 3.3386e-01, Meta loss averaged over last 500 steps = 3.3837e-01, PNorm = 101.4835, GNorm = 0.2294
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 3.3834e-01, PNorm = 101.4951, GNorm = 0.2005
Meta loss on this task batch = 2.7959e-01, Meta loss averaged over last 500 steps = 3.3822e-01, PNorm = 101.5085, GNorm = 0.2472
Took 115.06655502319336 seconds to complete one epoch of meta training
Took 123.04265928268433 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507782
Epoch 242
Meta loss on this task batch = 3.3979e-01, Meta loss averaged over last 500 steps = 3.3830e-01, PNorm = 101.5216, GNorm = 0.2030
Meta loss on this task batch = 3.4239e-01, Meta loss averaged over last 500 steps = 3.3828e-01, PNorm = 101.5342, GNorm = 0.2451
Meta loss on this task batch = 2.9629e-01, Meta loss averaged over last 500 steps = 3.3818e-01, PNorm = 101.5456, GNorm = 0.2682
Meta loss on this task batch = 3.7038e-01, Meta loss averaged over last 500 steps = 3.3824e-01, PNorm = 101.5572, GNorm = 0.2168
Meta loss on this task batch = 2.7077e-01, Meta loss averaged over last 500 steps = 3.3810e-01, PNorm = 101.5689, GNorm = 0.2089
Meta loss on this task batch = 3.2009e-01, Meta loss averaged over last 500 steps = 3.3808e-01, PNorm = 101.5806, GNorm = 0.1947
Meta loss on this task batch = 3.9061e-01, Meta loss averaged over last 500 steps = 3.3811e-01, PNorm = 101.5922, GNorm = 0.2282
Meta loss on this task batch = 2.8304e-01, Meta loss averaged over last 500 steps = 3.3801e-01, PNorm = 101.6037, GNorm = 0.1878
Meta loss on this task batch = 3.4488e-01, Meta loss averaged over last 500 steps = 3.3792e-01, PNorm = 101.6157, GNorm = 0.2245
Meta loss on this task batch = 3.4331e-01, Meta loss averaged over last 500 steps = 3.3792e-01, PNorm = 101.6280, GNorm = 0.2016
Meta loss on this task batch = 3.5507e-01, Meta loss averaged over last 500 steps = 3.3786e-01, PNorm = 101.6389, GNorm = 0.2478
Meta loss on this task batch = 3.5730e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 101.6487, GNorm = 0.2324
Meta loss on this task batch = 3.2553e-01, Meta loss averaged over last 500 steps = 3.3793e-01, PNorm = 101.6588, GNorm = 0.1912
Meta loss on this task batch = 3.5291e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 101.6683, GNorm = 0.1974
Meta loss on this task batch = 3.0798e-01, Meta loss averaged over last 500 steps = 3.3792e-01, PNorm = 101.6775, GNorm = 0.2111
Meta loss on this task batch = 3.1816e-01, Meta loss averaged over last 500 steps = 3.3792e-01, PNorm = 101.6865, GNorm = 0.2213
Meta loss on this task batch = 3.7901e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 101.6950, GNorm = 0.2425
Meta loss on this task batch = 3.4587e-01, Meta loss averaged over last 500 steps = 3.3799e-01, PNorm = 101.7027, GNorm = 0.2207
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 3.3799e-01, PNorm = 101.7110, GNorm = 0.2725
Took 117.33369398117065 seconds to complete one epoch of meta training
Took 124.08015632629395 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515416
Epoch 243
Meta loss on this task batch = 3.5649e-01, Meta loss averaged over last 500 steps = 3.3795e-01, PNorm = 101.7194, GNorm = 0.2353
Meta loss on this task batch = 3.0497e-01, Meta loss averaged over last 500 steps = 3.3790e-01, PNorm = 101.7274, GNorm = 0.2001
Meta loss on this task batch = 3.1653e-01, Meta loss averaged over last 500 steps = 3.3787e-01, PNorm = 101.7363, GNorm = 0.2091
Meta loss on this task batch = 3.3148e-01, Meta loss averaged over last 500 steps = 3.3781e-01, PNorm = 101.7462, GNorm = 0.2337
Meta loss on this task batch = 3.4661e-01, Meta loss averaged over last 500 steps = 3.3786e-01, PNorm = 101.7566, GNorm = 0.1924
Meta loss on this task batch = 3.1517e-01, Meta loss averaged over last 500 steps = 3.3784e-01, PNorm = 101.7680, GNorm = 0.2155
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 3.3772e-01, PNorm = 101.7793, GNorm = 0.2241
Meta loss on this task batch = 3.6425e-01, Meta loss averaged over last 500 steps = 3.3782e-01, PNorm = 101.7895, GNorm = 0.2383
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.3776e-01, PNorm = 101.8010, GNorm = 0.2311
Meta loss on this task batch = 2.8867e-01, Meta loss averaged over last 500 steps = 3.3768e-01, PNorm = 101.8125, GNorm = 0.1799
Meta loss on this task batch = 3.3294e-01, Meta loss averaged over last 500 steps = 3.3760e-01, PNorm = 101.8231, GNorm = 0.2098
Meta loss on this task batch = 3.4199e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 101.8330, GNorm = 0.2569
Meta loss on this task batch = 3.1562e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 101.8431, GNorm = 0.2150
Meta loss on this task batch = 3.7506e-01, Meta loss averaged over last 500 steps = 3.3765e-01, PNorm = 101.8524, GNorm = 0.2486
Meta loss on this task batch = 3.3935e-01, Meta loss averaged over last 500 steps = 3.3772e-01, PNorm = 101.8607, GNorm = 0.2254
Meta loss on this task batch = 3.5308e-01, Meta loss averaged over last 500 steps = 3.3771e-01, PNorm = 101.8699, GNorm = 0.2031
Meta loss on this task batch = 3.4340e-01, Meta loss averaged over last 500 steps = 3.3765e-01, PNorm = 101.8780, GNorm = 0.2122
Meta loss on this task batch = 3.3707e-01, Meta loss averaged over last 500 steps = 3.3754e-01, PNorm = 101.8869, GNorm = 0.2102
Meta loss on this task batch = 3.5407e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 101.8968, GNorm = 0.2598
Took 115.44284915924072 seconds to complete one epoch of meta training
Took 123.21257972717285 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506571
Epoch 244
Meta loss on this task batch = 3.6889e-01, Meta loss averaged over last 500 steps = 3.3773e-01, PNorm = 101.9071, GNorm = 0.2069
Meta loss on this task batch = 3.5297e-01, Meta loss averaged over last 500 steps = 3.3788e-01, PNorm = 101.9176, GNorm = 0.2153
Meta loss on this task batch = 3.8068e-01, Meta loss averaged over last 500 steps = 3.3791e-01, PNorm = 101.9278, GNorm = 0.2561
Meta loss on this task batch = 3.3147e-01, Meta loss averaged over last 500 steps = 3.3790e-01, PNorm = 101.9380, GNorm = 0.2040
Meta loss on this task batch = 3.3539e-01, Meta loss averaged over last 500 steps = 3.3782e-01, PNorm = 101.9493, GNorm = 0.2071
Meta loss on this task batch = 3.2789e-01, Meta loss averaged over last 500 steps = 3.3778e-01, PNorm = 101.9603, GNorm = 0.1918
Meta loss on this task batch = 2.8258e-01, Meta loss averaged over last 500 steps = 3.3764e-01, PNorm = 101.9712, GNorm = 0.2601
Meta loss on this task batch = 3.0157e-01, Meta loss averaged over last 500 steps = 3.3746e-01, PNorm = 101.9823, GNorm = 0.2042
Meta loss on this task batch = 3.1870e-01, Meta loss averaged over last 500 steps = 3.3744e-01, PNorm = 101.9928, GNorm = 0.2192
Meta loss on this task batch = 3.7862e-01, Meta loss averaged over last 500 steps = 3.3764e-01, PNorm = 102.0029, GNorm = 0.2156
Meta loss on this task batch = 3.1772e-01, Meta loss averaged over last 500 steps = 3.3759e-01, PNorm = 102.0125, GNorm = 0.1813
Meta loss on this task batch = 3.2292e-01, Meta loss averaged over last 500 steps = 3.3749e-01, PNorm = 102.0206, GNorm = 0.2454
Meta loss on this task batch = 3.4857e-01, Meta loss averaged over last 500 steps = 3.3755e-01, PNorm = 102.0287, GNorm = 0.2493
Meta loss on this task batch = 3.4143e-01, Meta loss averaged over last 500 steps = 3.3753e-01, PNorm = 102.0365, GNorm = 0.2357
Meta loss on this task batch = 3.2630e-01, Meta loss averaged over last 500 steps = 3.3752e-01, PNorm = 102.0447, GNorm = 0.2351
Meta loss on this task batch = 4.0582e-01, Meta loss averaged over last 500 steps = 3.3758e-01, PNorm = 102.0524, GNorm = 0.3147
Meta loss on this task batch = 3.4519e-01, Meta loss averaged over last 500 steps = 3.3745e-01, PNorm = 102.0617, GNorm = 0.2507
Meta loss on this task batch = 3.4604e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 102.0720, GNorm = 0.2264
Meta loss on this task batch = 3.2663e-01, Meta loss averaged over last 500 steps = 3.3754e-01, PNorm = 102.0829, GNorm = 0.2784
Took 115.40110182762146 seconds to complete one epoch of meta training
Took 123.50097250938416 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486055
Epoch 245
Meta loss on this task batch = 3.3433e-01, Meta loss averaged over last 500 steps = 3.3746e-01, PNorm = 102.0937, GNorm = 0.1988
Meta loss on this task batch = 3.1643e-01, Meta loss averaged over last 500 steps = 3.3744e-01, PNorm = 102.1048, GNorm = 0.2071
Meta loss on this task batch = 3.0601e-01, Meta loss averaged over last 500 steps = 3.3740e-01, PNorm = 102.1166, GNorm = 0.2232
Meta loss on this task batch = 3.8889e-01, Meta loss averaged over last 500 steps = 3.3756e-01, PNorm = 102.1275, GNorm = 0.2459
Meta loss on this task batch = 3.2556e-01, Meta loss averaged over last 500 steps = 3.3755e-01, PNorm = 102.1383, GNorm = 0.2123
Meta loss on this task batch = 2.8599e-01, Meta loss averaged over last 500 steps = 3.3746e-01, PNorm = 102.1497, GNorm = 0.1988
Meta loss on this task batch = 3.7068e-01, Meta loss averaged over last 500 steps = 3.3753e-01, PNorm = 102.1601, GNorm = 0.2164
Meta loss on this task batch = 4.0821e-01, Meta loss averaged over last 500 steps = 3.3771e-01, PNorm = 102.1693, GNorm = 0.2342
Meta loss on this task batch = 3.1259e-01, Meta loss averaged over last 500 steps = 3.3768e-01, PNorm = 102.1783, GNorm = 0.2165
Meta loss on this task batch = 3.3168e-01, Meta loss averaged over last 500 steps = 3.3775e-01, PNorm = 102.1872, GNorm = 0.2132
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 3.3765e-01, PNorm = 102.1956, GNorm = 0.2087
Meta loss on this task batch = 3.2281e-01, Meta loss averaged over last 500 steps = 3.3764e-01, PNorm = 102.2040, GNorm = 0.1860
Meta loss on this task batch = 3.1908e-01, Meta loss averaged over last 500 steps = 3.3763e-01, PNorm = 102.2123, GNorm = 0.1854
Meta loss on this task batch = 3.5941e-01, Meta loss averaged over last 500 steps = 3.3762e-01, PNorm = 102.2206, GNorm = 0.2086
Meta loss on this task batch = 3.4927e-01, Meta loss averaged over last 500 steps = 3.3777e-01, PNorm = 102.2297, GNorm = 0.2248
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 3.3758e-01, PNorm = 102.2388, GNorm = 0.2315
Meta loss on this task batch = 3.5227e-01, Meta loss averaged over last 500 steps = 3.3750e-01, PNorm = 102.2472, GNorm = 0.2274
Meta loss on this task batch = 3.3240e-01, Meta loss averaged over last 500 steps = 3.3753e-01, PNorm = 102.2549, GNorm = 0.2049
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 3.3741e-01, PNorm = 102.2627, GNorm = 0.2204
Took 116.52731943130493 seconds to complete one epoch of meta training
Took 124.77437734603882 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501411
Epoch 246
Meta loss on this task batch = 2.8597e-01, Meta loss averaged over last 500 steps = 3.3720e-01, PNorm = 102.2706, GNorm = 0.2060
Meta loss on this task batch = 2.9201e-01, Meta loss averaged over last 500 steps = 3.3710e-01, PNorm = 102.2794, GNorm = 0.2046
Meta loss on this task batch = 3.1521e-01, Meta loss averaged over last 500 steps = 3.3709e-01, PNorm = 102.2877, GNorm = 0.1930
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 3.3704e-01, PNorm = 102.2961, GNorm = 0.2239
Meta loss on this task batch = 3.7287e-01, Meta loss averaged over last 500 steps = 3.3705e-01, PNorm = 102.3044, GNorm = 0.2498
Meta loss on this task batch = 3.4707e-01, Meta loss averaged over last 500 steps = 3.3707e-01, PNorm = 102.3124, GNorm = 0.2315
Meta loss on this task batch = 3.4337e-01, Meta loss averaged over last 500 steps = 3.3700e-01, PNorm = 102.3193, GNorm = 0.2497
Meta loss on this task batch = 3.0730e-01, Meta loss averaged over last 500 steps = 3.3694e-01, PNorm = 102.3262, GNorm = 0.2279
Meta loss on this task batch = 3.3266e-01, Meta loss averaged over last 500 steps = 3.3695e-01, PNorm = 102.3336, GNorm = 0.2337
Meta loss on this task batch = 3.2214e-01, Meta loss averaged over last 500 steps = 3.3687e-01, PNorm = 102.3423, GNorm = 0.2396
Meta loss on this task batch = 3.1731e-01, Meta loss averaged over last 500 steps = 3.3673e-01, PNorm = 102.3517, GNorm = 0.2413
Meta loss on this task batch = 3.1417e-01, Meta loss averaged over last 500 steps = 3.3675e-01, PNorm = 102.3600, GNorm = 0.1949
Meta loss on this task batch = 3.2387e-01, Meta loss averaged over last 500 steps = 3.3670e-01, PNorm = 102.3670, GNorm = 0.2192
Meta loss on this task batch = 3.1369e-01, Meta loss averaged over last 500 steps = 3.3659e-01, PNorm = 102.3750, GNorm = 0.1960
Meta loss on this task batch = 3.4873e-01, Meta loss averaged over last 500 steps = 3.3656e-01, PNorm = 102.3832, GNorm = 0.1938
Meta loss on this task batch = 3.8822e-01, Meta loss averaged over last 500 steps = 3.3662e-01, PNorm = 102.3913, GNorm = 0.2833
Meta loss on this task batch = 3.6270e-01, Meta loss averaged over last 500 steps = 3.3680e-01, PNorm = 102.3992, GNorm = 0.2319
Meta loss on this task batch = 3.3019e-01, Meta loss averaged over last 500 steps = 3.3679e-01, PNorm = 102.4061, GNorm = 0.2295
Meta loss on this task batch = 3.8340e-01, Meta loss averaged over last 500 steps = 3.3695e-01, PNorm = 102.4126, GNorm = 0.2515
Took 114.11200714111328 seconds to complete one epoch of meta training
Took 121.93140888214111 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506088
Epoch 247
Meta loss on this task batch = 3.4386e-01, Meta loss averaged over last 500 steps = 3.3700e-01, PNorm = 102.4181, GNorm = 0.2106
Meta loss on this task batch = 3.2488e-01, Meta loss averaged over last 500 steps = 3.3691e-01, PNorm = 102.4242, GNorm = 0.1815
Meta loss on this task batch = 3.5401e-01, Meta loss averaged over last 500 steps = 3.3686e-01, PNorm = 102.4308, GNorm = 0.1913
Meta loss on this task batch = 3.7559e-01, Meta loss averaged over last 500 steps = 3.3692e-01, PNorm = 102.4376, GNorm = 0.2133
Meta loss on this task batch = 2.7335e-01, Meta loss averaged over last 500 steps = 3.3690e-01, PNorm = 102.4458, GNorm = 0.2149
Meta loss on this task batch = 3.3484e-01, Meta loss averaged over last 500 steps = 3.3682e-01, PNorm = 102.4542, GNorm = 0.1868
Meta loss on this task batch = 3.3799e-01, Meta loss averaged over last 500 steps = 3.3686e-01, PNorm = 102.4629, GNorm = 0.1886
Meta loss on this task batch = 3.4817e-01, Meta loss averaged over last 500 steps = 3.3689e-01, PNorm = 102.4712, GNorm = 0.2180
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 3.3693e-01, PNorm = 102.4794, GNorm = 0.2157
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 3.3675e-01, PNorm = 102.4875, GNorm = 0.1830
Meta loss on this task batch = 3.4183e-01, Meta loss averaged over last 500 steps = 3.3688e-01, PNorm = 102.4952, GNorm = 0.2215
Meta loss on this task batch = 3.2574e-01, Meta loss averaged over last 500 steps = 3.3684e-01, PNorm = 102.5035, GNorm = 0.2041
Meta loss on this task batch = 3.4855e-01, Meta loss averaged over last 500 steps = 3.3690e-01, PNorm = 102.5120, GNorm = 0.1988
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 3.3686e-01, PNorm = 102.5214, GNorm = 0.1951
Meta loss on this task batch = 3.3727e-01, Meta loss averaged over last 500 steps = 3.3680e-01, PNorm = 102.5320, GNorm = 0.1919
Meta loss on this task batch = 3.4321e-01, Meta loss averaged over last 500 steps = 3.3684e-01, PNorm = 102.5419, GNorm = 0.2046
Meta loss on this task batch = 2.9757e-01, Meta loss averaged over last 500 steps = 3.3668e-01, PNorm = 102.5524, GNorm = 0.2085
Meta loss on this task batch = 3.7439e-01, Meta loss averaged over last 500 steps = 3.3675e-01, PNorm = 102.5614, GNorm = 0.2498
Meta loss on this task batch = 3.4512e-01, Meta loss averaged over last 500 steps = 3.3669e-01, PNorm = 102.5723, GNorm = 0.2799
Took 116.27906918525696 seconds to complete one epoch of meta training
Took 123.904855966568 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500421
Epoch 248
Meta loss on this task batch = 3.5029e-01, Meta loss averaged over last 500 steps = 3.3673e-01, PNorm = 102.5813, GNorm = 0.2591
Meta loss on this task batch = 3.4654e-01, Meta loss averaged over last 500 steps = 3.3661e-01, PNorm = 102.5897, GNorm = 0.2290
Meta loss on this task batch = 3.1839e-01, Meta loss averaged over last 500 steps = 3.3653e-01, PNorm = 102.5977, GNorm = 0.1953
Meta loss on this task batch = 3.0733e-01, Meta loss averaged over last 500 steps = 3.3644e-01, PNorm = 102.6061, GNorm = 0.2470
Meta loss on this task batch = 3.2300e-01, Meta loss averaged over last 500 steps = 3.3643e-01, PNorm = 102.6144, GNorm = 0.2041
Meta loss on this task batch = 3.4998e-01, Meta loss averaged over last 500 steps = 3.3658e-01, PNorm = 102.6237, GNorm = 0.2446
Meta loss on this task batch = 3.3249e-01, Meta loss averaged over last 500 steps = 3.3655e-01, PNorm = 102.6325, GNorm = 0.2165
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 3.3659e-01, PNorm = 102.6418, GNorm = 0.2345
Meta loss on this task batch = 3.4942e-01, Meta loss averaged over last 500 steps = 3.3658e-01, PNorm = 102.6517, GNorm = 0.2085
Meta loss on this task batch = 3.1836e-01, Meta loss averaged over last 500 steps = 3.3658e-01, PNorm = 102.6631, GNorm = 0.2041
Meta loss on this task batch = 3.3270e-01, Meta loss averaged over last 500 steps = 3.3642e-01, PNorm = 102.6738, GNorm = 0.2233
Meta loss on this task batch = 3.4042e-01, Meta loss averaged over last 500 steps = 3.3638e-01, PNorm = 102.6840, GNorm = 0.2225
Meta loss on this task batch = 3.5580e-01, Meta loss averaged over last 500 steps = 3.3632e-01, PNorm = 102.6939, GNorm = 0.2380
Meta loss on this task batch = 2.9623e-01, Meta loss averaged over last 500 steps = 3.3624e-01, PNorm = 102.7025, GNorm = 0.2050
Meta loss on this task batch = 3.3364e-01, Meta loss averaged over last 500 steps = 3.3620e-01, PNorm = 102.7083, GNorm = 0.3078
Meta loss on this task batch = 3.6629e-01, Meta loss averaged over last 500 steps = 3.3616e-01, PNorm = 102.7146, GNorm = 0.2425
Meta loss on this task batch = 3.2457e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 102.7225, GNorm = 0.2141
Meta loss on this task batch = 3.2015e-01, Meta loss averaged over last 500 steps = 3.3618e-01, PNorm = 102.7313, GNorm = 0.2086
Meta loss on this task batch = 3.6418e-01, Meta loss averaged over last 500 steps = 3.3631e-01, PNorm = 102.7392, GNorm = 0.2853
Took 115.50929760932922 seconds to complete one epoch of meta training
Took 122.99946069717407 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507279
Epoch 249
Meta loss on this task batch = 3.3056e-01, Meta loss averaged over last 500 steps = 3.3634e-01, PNorm = 102.7468, GNorm = 0.2009
Meta loss on this task batch = 3.8481e-01, Meta loss averaged over last 500 steps = 3.3642e-01, PNorm = 102.7542, GNorm = 0.2148
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 3.3634e-01, PNorm = 102.7624, GNorm = 0.1936
Meta loss on this task batch = 3.0628e-01, Meta loss averaged over last 500 steps = 3.3631e-01, PNorm = 102.7716, GNorm = 0.2033
Meta loss on this task batch = 3.4040e-01, Meta loss averaged over last 500 steps = 3.3628e-01, PNorm = 102.7801, GNorm = 0.2582
Meta loss on this task batch = 3.7083e-01, Meta loss averaged over last 500 steps = 3.3631e-01, PNorm = 102.7884, GNorm = 0.2243
Meta loss on this task batch = 3.1900e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 102.7977, GNorm = 0.2409
Meta loss on this task batch = 3.1886e-01, Meta loss averaged over last 500 steps = 3.3604e-01, PNorm = 102.8077, GNorm = 0.2403
Meta loss on this task batch = 3.4582e-01, Meta loss averaged over last 500 steps = 3.3617e-01, PNorm = 102.8179, GNorm = 0.2436
Meta loss on this task batch = 2.5138e-01, Meta loss averaged over last 500 steps = 3.3598e-01, PNorm = 102.8282, GNorm = 0.2202
Meta loss on this task batch = 3.6042e-01, Meta loss averaged over last 500 steps = 3.3606e-01, PNorm = 102.8382, GNorm = 0.2750
Meta loss on this task batch = 3.5975e-01, Meta loss averaged over last 500 steps = 3.3608e-01, PNorm = 102.8473, GNorm = 0.2388
Meta loss on this task batch = 3.1765e-01, Meta loss averaged over last 500 steps = 3.3607e-01, PNorm = 102.8564, GNorm = 0.2089
Meta loss on this task batch = 3.6709e-01, Meta loss averaged over last 500 steps = 3.3616e-01, PNorm = 102.8657, GNorm = 0.2410
Meta loss on this task batch = 3.3137e-01, Meta loss averaged over last 500 steps = 3.3623e-01, PNorm = 102.8741, GNorm = 0.2100
Meta loss on this task batch = 3.5570e-01, Meta loss averaged over last 500 steps = 3.3632e-01, PNorm = 102.8816, GNorm = 0.2076
Meta loss on this task batch = 3.3398e-01, Meta loss averaged over last 500 steps = 3.3622e-01, PNorm = 102.8895, GNorm = 0.2071
Meta loss on this task batch = 3.5030e-01, Meta loss averaged over last 500 steps = 3.3622e-01, PNorm = 102.8975, GNorm = 0.2003
Meta loss on this task batch = 3.7802e-01, Meta loss averaged over last 500 steps = 3.3635e-01, PNorm = 102.9054, GNorm = 0.2656
Took 118.88702368736267 seconds to complete one epoch of meta training
Took 126.5910701751709 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492801
Epoch 250
Meta loss on this task batch = 3.4148e-01, Meta loss averaged over last 500 steps = 3.3641e-01, PNorm = 102.9140, GNorm = 0.1666
Meta loss on this task batch = 3.2753e-01, Meta loss averaged over last 500 steps = 3.3641e-01, PNorm = 102.9237, GNorm = 0.2176
Meta loss on this task batch = 3.4565e-01, Meta loss averaged over last 500 steps = 3.3639e-01, PNorm = 102.9335, GNorm = 0.1941
Meta loss on this task batch = 3.8050e-01, Meta loss averaged over last 500 steps = 3.3634e-01, PNorm = 102.9426, GNorm = 0.2314
Meta loss on this task batch = 2.9536e-01, Meta loss averaged over last 500 steps = 3.3626e-01, PNorm = 102.9527, GNorm = 0.1986
Meta loss on this task batch = 3.0489e-01, Meta loss averaged over last 500 steps = 3.3611e-01, PNorm = 102.9642, GNorm = 0.1800
Meta loss on this task batch = 3.3353e-01, Meta loss averaged over last 500 steps = 3.3608e-01, PNorm = 102.9759, GNorm = 0.1827
Meta loss on this task batch = 3.5816e-01, Meta loss averaged over last 500 steps = 3.3622e-01, PNorm = 102.9873, GNorm = 0.2860
Meta loss on this task batch = 3.2432e-01, Meta loss averaged over last 500 steps = 3.3623e-01, PNorm = 102.9982, GNorm = 0.2211
Meta loss on this task batch = 2.7318e-01, Meta loss averaged over last 500 steps = 3.3602e-01, PNorm = 103.0085, GNorm = 0.1907
Meta loss on this task batch = 3.2312e-01, Meta loss averaged over last 500 steps = 3.3599e-01, PNorm = 103.0176, GNorm = 0.2145
Meta loss on this task batch = 3.3820e-01, Meta loss averaged over last 500 steps = 3.3598e-01, PNorm = 103.0254, GNorm = 0.2409
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 3.3596e-01, PNorm = 103.0331, GNorm = 0.2115
Meta loss on this task batch = 3.6228e-01, Meta loss averaged over last 500 steps = 3.3609e-01, PNorm = 103.0403, GNorm = 0.2198
Meta loss on this task batch = 3.5161e-01, Meta loss averaged over last 500 steps = 3.3597e-01, PNorm = 103.0474, GNorm = 0.2206
Meta loss on this task batch = 3.3293e-01, Meta loss averaged over last 500 steps = 3.3592e-01, PNorm = 103.0545, GNorm = 0.1941
Meta loss on this task batch = 3.3948e-01, Meta loss averaged over last 500 steps = 3.3608e-01, PNorm = 103.0612, GNorm = 0.2192
Meta loss on this task batch = 3.5324e-01, Meta loss averaged over last 500 steps = 3.3622e-01, PNorm = 103.0678, GNorm = 0.2498
Meta loss on this task batch = 3.0509e-01, Meta loss averaged over last 500 steps = 3.3611e-01, PNorm = 103.0749, GNorm = 0.2299
Took 113.9350037574768 seconds to complete one epoch of meta training
Took 122.18201065063477 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487405
Epoch 251
Meta loss on this task batch = 3.0113e-01, Meta loss averaged over last 500 steps = 3.3599e-01, PNorm = 103.0819, GNorm = 0.2321
Meta loss on this task batch = 2.8416e-01, Meta loss averaged over last 500 steps = 3.3590e-01, PNorm = 103.0901, GNorm = 0.1902
Meta loss on this task batch = 3.4182e-01, Meta loss averaged over last 500 steps = 3.3588e-01, PNorm = 103.0976, GNorm = 0.2235
Meta loss on this task batch = 3.4270e-01, Meta loss averaged over last 500 steps = 3.3585e-01, PNorm = 103.1060, GNorm = 0.2001
Meta loss on this task batch = 3.1536e-01, Meta loss averaged over last 500 steps = 3.3584e-01, PNorm = 103.1153, GNorm = 0.2315
Meta loss on this task batch = 2.9681e-01, Meta loss averaged over last 500 steps = 3.3573e-01, PNorm = 103.1254, GNorm = 0.2095
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.3568e-01, PNorm = 103.1350, GNorm = 0.2010
Meta loss on this task batch = 3.5184e-01, Meta loss averaged over last 500 steps = 3.3574e-01, PNorm = 103.1434, GNorm = 0.2544
Meta loss on this task batch = 3.1629e-01, Meta loss averaged over last 500 steps = 3.3576e-01, PNorm = 103.1521, GNorm = 0.2387
Meta loss on this task batch = 3.6613e-01, Meta loss averaged over last 500 steps = 3.3582e-01, PNorm = 103.1602, GNorm = 0.2245
Meta loss on this task batch = 3.6786e-01, Meta loss averaged over last 500 steps = 3.3593e-01, PNorm = 103.1688, GNorm = 0.2422
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 3.3571e-01, PNorm = 103.1782, GNorm = 0.1927
Meta loss on this task batch = 3.6362e-01, Meta loss averaged over last 500 steps = 3.3569e-01, PNorm = 103.1875, GNorm = 0.2159
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 3.3577e-01, PNorm = 103.1966, GNorm = 0.1895
Meta loss on this task batch = 3.5312e-01, Meta loss averaged over last 500 steps = 3.3574e-01, PNorm = 103.2051, GNorm = 0.2235
Meta loss on this task batch = 3.4372e-01, Meta loss averaged over last 500 steps = 3.3578e-01, PNorm = 103.2136, GNorm = 0.1971
Meta loss on this task batch = 3.3446e-01, Meta loss averaged over last 500 steps = 3.3570e-01, PNorm = 103.2218, GNorm = 0.2394
Meta loss on this task batch = 3.2958e-01, Meta loss averaged over last 500 steps = 3.3565e-01, PNorm = 103.2305, GNorm = 0.2125
Meta loss on this task batch = 3.7174e-01, Meta loss averaged over last 500 steps = 3.3575e-01, PNorm = 103.2398, GNorm = 0.2440
Took 116.29601335525513 seconds to complete one epoch of meta training
Took 122.96729850769043 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485057
Epoch 252
Meta loss on this task batch = 3.3848e-01, Meta loss averaged over last 500 steps = 3.3575e-01, PNorm = 103.2492, GNorm = 0.2064
Meta loss on this task batch = 3.6779e-01, Meta loss averaged over last 500 steps = 3.3581e-01, PNorm = 103.2582, GNorm = 0.2140
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 3.3564e-01, PNorm = 103.2673, GNorm = 0.1871
Meta loss on this task batch = 3.1752e-01, Meta loss averaged over last 500 steps = 3.3565e-01, PNorm = 103.2773, GNorm = 0.1968
Meta loss on this task batch = 3.5490e-01, Meta loss averaged over last 500 steps = 3.3577e-01, PNorm = 103.2861, GNorm = 0.1879
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 3.3567e-01, PNorm = 103.2943, GNorm = 0.1851
Meta loss on this task batch = 3.4853e-01, Meta loss averaged over last 500 steps = 3.3558e-01, PNorm = 103.3033, GNorm = 0.2155
Meta loss on this task batch = 2.5679e-01, Meta loss averaged over last 500 steps = 3.3541e-01, PNorm = 103.3129, GNorm = 0.1824
Meta loss on this task batch = 3.7257e-01, Meta loss averaged over last 500 steps = 3.3552e-01, PNorm = 103.3218, GNorm = 0.2306
Meta loss on this task batch = 3.0624e-01, Meta loss averaged over last 500 steps = 3.3550e-01, PNorm = 103.3307, GNorm = 0.2192
Meta loss on this task batch = 2.8702e-01, Meta loss averaged over last 500 steps = 3.3537e-01, PNorm = 103.3397, GNorm = 0.2007
Meta loss on this task batch = 3.5523e-01, Meta loss averaged over last 500 steps = 3.3541e-01, PNorm = 103.3483, GNorm = 0.2051
Meta loss on this task batch = 3.1206e-01, Meta loss averaged over last 500 steps = 3.3533e-01, PNorm = 103.3572, GNorm = 0.2156
Meta loss on this task batch = 3.3703e-01, Meta loss averaged over last 500 steps = 3.3522e-01, PNorm = 103.3659, GNorm = 0.2191
Meta loss on this task batch = 2.9864e-01, Meta loss averaged over last 500 steps = 3.3507e-01, PNorm = 103.3756, GNorm = 0.2092
Meta loss on this task batch = 4.1694e-01, Meta loss averaged over last 500 steps = 3.3528e-01, PNorm = 103.3840, GNorm = 0.2783
Meta loss on this task batch = 3.5663e-01, Meta loss averaged over last 500 steps = 3.3538e-01, PNorm = 103.3920, GNorm = 0.2103
Meta loss on this task batch = 3.2048e-01, Meta loss averaged over last 500 steps = 3.3545e-01, PNorm = 103.3999, GNorm = 0.2703
Meta loss on this task batch = 3.4121e-01, Meta loss averaged over last 500 steps = 3.3540e-01, PNorm = 103.4080, GNorm = 0.3129
Took 115.71928691864014 seconds to complete one epoch of meta training
Took 123.69181156158447 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483269
Epoch 253
Meta loss on this task batch = 3.2842e-01, Meta loss averaged over last 500 steps = 3.3537e-01, PNorm = 103.4166, GNorm = 0.2394
Meta loss on this task batch = 3.3213e-01, Meta loss averaged over last 500 steps = 3.3531e-01, PNorm = 103.4263, GNorm = 0.2392
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 3.3513e-01, PNorm = 103.4361, GNorm = 0.1982
Meta loss on this task batch = 3.1236e-01, Meta loss averaged over last 500 steps = 3.3518e-01, PNorm = 103.4453, GNorm = 0.2380
Meta loss on this task batch = 3.4152e-01, Meta loss averaged over last 500 steps = 3.3526e-01, PNorm = 103.4549, GNorm = 0.1999
Meta loss on this task batch = 3.7500e-01, Meta loss averaged over last 500 steps = 3.3528e-01, PNorm = 103.4638, GNorm = 0.2327
Meta loss on this task batch = 3.1908e-01, Meta loss averaged over last 500 steps = 3.3515e-01, PNorm = 103.4733, GNorm = 0.2095
Meta loss on this task batch = 3.5617e-01, Meta loss averaged over last 500 steps = 3.3523e-01, PNorm = 103.4835, GNorm = 0.2254
Meta loss on this task batch = 3.8308e-01, Meta loss averaged over last 500 steps = 3.3534e-01, PNorm = 103.4927, GNorm = 0.2312
Meta loss on this task batch = 3.1044e-01, Meta loss averaged over last 500 steps = 3.3549e-01, PNorm = 103.5017, GNorm = 0.2184
Meta loss on this task batch = 3.2506e-01, Meta loss averaged over last 500 steps = 3.3554e-01, PNorm = 103.5110, GNorm = 0.1911
Meta loss on this task batch = 3.3645e-01, Meta loss averaged over last 500 steps = 3.3552e-01, PNorm = 103.5194, GNorm = 0.2642
Meta loss on this task batch = 3.2861e-01, Meta loss averaged over last 500 steps = 3.3543e-01, PNorm = 103.5278, GNorm = 0.1839
Meta loss on this task batch = 3.5234e-01, Meta loss averaged over last 500 steps = 3.3547e-01, PNorm = 103.5363, GNorm = 0.2121
Meta loss on this task batch = 3.3260e-01, Meta loss averaged over last 500 steps = 3.3547e-01, PNorm = 103.5455, GNorm = 0.2167
Meta loss on this task batch = 3.1807e-01, Meta loss averaged over last 500 steps = 3.3538e-01, PNorm = 103.5554, GNorm = 0.2037
Meta loss on this task batch = 3.2268e-01, Meta loss averaged over last 500 steps = 3.3539e-01, PNorm = 103.5661, GNorm = 0.1989
Meta loss on this task batch = 3.0475e-01, Meta loss averaged over last 500 steps = 3.3524e-01, PNorm = 103.5777, GNorm = 0.1909
Meta loss on this task batch = 3.2251e-01, Meta loss averaged over last 500 steps = 3.3508e-01, PNorm = 103.5886, GNorm = 0.2459
Took 115.59587502479553 seconds to complete one epoch of meta training
Took 123.6903440952301 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501511
Epoch 254
Meta loss on this task batch = 2.8187e-01, Meta loss averaged over last 500 steps = 3.3501e-01, PNorm = 103.6000, GNorm = 0.1893
Meta loss on this task batch = 3.5028e-01, Meta loss averaged over last 500 steps = 3.3504e-01, PNorm = 103.6110, GNorm = 0.2303
Meta loss on this task batch = 3.3659e-01, Meta loss averaged over last 500 steps = 3.3503e-01, PNorm = 103.6211, GNorm = 0.2022
Meta loss on this task batch = 3.0863e-01, Meta loss averaged over last 500 steps = 3.3493e-01, PNorm = 103.6311, GNorm = 0.2153
Meta loss on this task batch = 3.3720e-01, Meta loss averaged over last 500 steps = 3.3498e-01, PNorm = 103.6397, GNorm = 0.2499
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 3.3497e-01, PNorm = 103.6480, GNorm = 0.2138
Meta loss on this task batch = 3.0902e-01, Meta loss averaged over last 500 steps = 3.3489e-01, PNorm = 103.6576, GNorm = 0.2078
Meta loss on this task batch = 3.3691e-01, Meta loss averaged over last 500 steps = 3.3492e-01, PNorm = 103.6672, GNorm = 0.2090
Meta loss on this task batch = 3.6183e-01, Meta loss averaged over last 500 steps = 3.3499e-01, PNorm = 103.6765, GNorm = 0.2293
Meta loss on this task batch = 3.4882e-01, Meta loss averaged over last 500 steps = 3.3492e-01, PNorm = 103.6861, GNorm = 0.2217
Meta loss on this task batch = 3.6076e-01, Meta loss averaged over last 500 steps = 3.3489e-01, PNorm = 103.6962, GNorm = 0.2041
Meta loss on this task batch = 2.9010e-01, Meta loss averaged over last 500 steps = 3.3488e-01, PNorm = 103.7070, GNorm = 0.2035
Meta loss on this task batch = 2.8970e-01, Meta loss averaged over last 500 steps = 3.3486e-01, PNorm = 103.7181, GNorm = 0.2037
Meta loss on this task batch = 3.2716e-01, Meta loss averaged over last 500 steps = 3.3488e-01, PNorm = 103.7284, GNorm = 0.1817
Meta loss on this task batch = 3.3117e-01, Meta loss averaged over last 500 steps = 3.3484e-01, PNorm = 103.7377, GNorm = 0.2321
Meta loss on this task batch = 3.2476e-01, Meta loss averaged over last 500 steps = 3.3479e-01, PNorm = 103.7465, GNorm = 0.2121
Meta loss on this task batch = 3.2890e-01, Meta loss averaged over last 500 steps = 3.3476e-01, PNorm = 103.7559, GNorm = 0.2607
Meta loss on this task batch = 3.7174e-01, Meta loss averaged over last 500 steps = 3.3486e-01, PNorm = 103.7637, GNorm = 0.2338
Meta loss on this task batch = 3.0501e-01, Meta loss averaged over last 500 steps = 3.3480e-01, PNorm = 103.7708, GNorm = 0.2299
Took 115.46385073661804 seconds to complete one epoch of meta training
Took 123.30744671821594 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484972
Epoch 255
Meta loss on this task batch = 3.4354e-01, Meta loss averaged over last 500 steps = 3.3481e-01, PNorm = 103.7786, GNorm = 0.2124
Meta loss on this task batch = 3.3070e-01, Meta loss averaged over last 500 steps = 3.3476e-01, PNorm = 103.7866, GNorm = 0.2161
Meta loss on this task batch = 3.9135e-01, Meta loss averaged over last 500 steps = 3.3489e-01, PNorm = 103.7944, GNorm = 0.2148
Meta loss on this task batch = 3.5381e-01, Meta loss averaged over last 500 steps = 3.3479e-01, PNorm = 103.8023, GNorm = 0.2022
Meta loss on this task batch = 3.0705e-01, Meta loss averaged over last 500 steps = 3.3476e-01, PNorm = 103.8108, GNorm = 0.1779
Meta loss on this task batch = 3.3350e-01, Meta loss averaged over last 500 steps = 3.3489e-01, PNorm = 103.8203, GNorm = 0.2577
Meta loss on this task batch = 3.5380e-01, Meta loss averaged over last 500 steps = 3.3502e-01, PNorm = 103.8296, GNorm = 0.2247
Meta loss on this task batch = 2.8627e-01, Meta loss averaged over last 500 steps = 3.3495e-01, PNorm = 103.8395, GNorm = 0.1859
Meta loss on this task batch = 3.0926e-01, Meta loss averaged over last 500 steps = 3.3496e-01, PNorm = 103.8507, GNorm = 0.2251
Meta loss on this task batch = 3.3175e-01, Meta loss averaged over last 500 steps = 3.3490e-01, PNorm = 103.8617, GNorm = 0.2245
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 3.3486e-01, PNorm = 103.8718, GNorm = 0.2202
Meta loss on this task batch = 3.1338e-01, Meta loss averaged over last 500 steps = 3.3485e-01, PNorm = 103.8816, GNorm = 0.2477
Meta loss on this task batch = 2.9404e-01, Meta loss averaged over last 500 steps = 3.3477e-01, PNorm = 103.8906, GNorm = 0.2131
Meta loss on this task batch = 3.5179e-01, Meta loss averaged over last 500 steps = 3.3476e-01, PNorm = 103.8996, GNorm = 0.2263
Meta loss on this task batch = 3.4397e-01, Meta loss averaged over last 500 steps = 3.3487e-01, PNorm = 103.9082, GNorm = 0.2214
Meta loss on this task batch = 3.1541e-01, Meta loss averaged over last 500 steps = 3.3486e-01, PNorm = 103.9163, GNorm = 0.2134
Meta loss on this task batch = 3.0660e-01, Meta loss averaged over last 500 steps = 3.3492e-01, PNorm = 103.9239, GNorm = 0.2228
Meta loss on this task batch = 3.0145e-01, Meta loss averaged over last 500 steps = 3.3485e-01, PNorm = 103.9317, GNorm = 0.1927
Meta loss on this task batch = 2.2924e-01, Meta loss averaged over last 500 steps = 3.3478e-01, PNorm = 103.9397, GNorm = 0.2187
Took 111.94720530509949 seconds to complete one epoch of meta training
Took 119.29118037223816 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502139
Epoch 256
Meta loss on this task batch = 3.5200e-01, Meta loss averaged over last 500 steps = 3.3475e-01, PNorm = 103.9477, GNorm = 0.1957
Meta loss on this task batch = 2.9920e-01, Meta loss averaged over last 500 steps = 3.3467e-01, PNorm = 103.9561, GNorm = 0.1957
Meta loss on this task batch = 3.0201e-01, Meta loss averaged over last 500 steps = 3.3453e-01, PNorm = 103.9648, GNorm = 0.2139
Meta loss on this task batch = 3.0651e-01, Meta loss averaged over last 500 steps = 3.3448e-01, PNorm = 103.9738, GNorm = 0.1888
Meta loss on this task batch = 2.8525e-01, Meta loss averaged over last 500 steps = 3.3436e-01, PNorm = 103.9827, GNorm = 0.2036
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 3.3425e-01, PNorm = 103.9918, GNorm = 0.2179
Meta loss on this task batch = 3.2223e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 104.0008, GNorm = 0.2155
Meta loss on this task batch = 3.7514e-01, Meta loss averaged over last 500 steps = 3.3441e-01, PNorm = 104.0091, GNorm = 0.2601
Meta loss on this task batch = 3.4279e-01, Meta loss averaged over last 500 steps = 3.3443e-01, PNorm = 104.0166, GNorm = 0.2212
Meta loss on this task batch = 3.4563e-01, Meta loss averaged over last 500 steps = 3.3438e-01, PNorm = 104.0227, GNorm = 0.2325
Meta loss on this task batch = 3.3939e-01, Meta loss averaged over last 500 steps = 3.3436e-01, PNorm = 104.0284, GNorm = 0.2391
Meta loss on this task batch = 3.0695e-01, Meta loss averaged over last 500 steps = 3.3438e-01, PNorm = 104.0357, GNorm = 0.2078
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.3438e-01, PNorm = 104.0443, GNorm = 0.2189
Meta loss on this task batch = 3.3434e-01, Meta loss averaged over last 500 steps = 3.3434e-01, PNorm = 104.0532, GNorm = 0.1922
Meta loss on this task batch = 3.4315e-01, Meta loss averaged over last 500 steps = 3.3439e-01, PNorm = 104.0629, GNorm = 0.1882
Meta loss on this task batch = 3.5420e-01, Meta loss averaged over last 500 steps = 3.3431e-01, PNorm = 104.0730, GNorm = 0.2067
Meta loss on this task batch = 3.2718e-01, Meta loss averaged over last 500 steps = 3.3432e-01, PNorm = 104.0830, GNorm = 0.2024
Meta loss on this task batch = 3.6091e-01, Meta loss averaged over last 500 steps = 3.3432e-01, PNorm = 104.0948, GNorm = 0.2031
Meta loss on this task batch = 2.9666e-01, Meta loss averaged over last 500 steps = 3.3430e-01, PNorm = 104.1077, GNorm = 0.2756
Took 113.90494799613953 seconds to complete one epoch of meta training
Took 122.15215134620667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515100
Epoch 257
Meta loss on this task batch = 3.1285e-01, Meta loss averaged over last 500 steps = 3.3419e-01, PNorm = 104.1192, GNorm = 0.1997
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 3.3404e-01, PNorm = 104.1290, GNorm = 0.2462
Meta loss on this task batch = 2.8370e-01, Meta loss averaged over last 500 steps = 3.3398e-01, PNorm = 104.1402, GNorm = 0.1939
Meta loss on this task batch = 3.0406e-01, Meta loss averaged over last 500 steps = 3.3389e-01, PNorm = 104.1502, GNorm = 0.2219
Meta loss on this task batch = 2.8770e-01, Meta loss averaged over last 500 steps = 3.3379e-01, PNorm = 104.1604, GNorm = 0.2160
Meta loss on this task batch = 2.9326e-01, Meta loss averaged over last 500 steps = 3.3364e-01, PNorm = 104.1702, GNorm = 0.2099
Meta loss on this task batch = 3.7666e-01, Meta loss averaged over last 500 steps = 3.3352e-01, PNorm = 104.1789, GNorm = 0.2590
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 3.3345e-01, PNorm = 104.1873, GNorm = 0.2087
Meta loss on this task batch = 3.2673e-01, Meta loss averaged over last 500 steps = 3.3341e-01, PNorm = 104.1956, GNorm = 0.2209
Meta loss on this task batch = 2.9516e-01, Meta loss averaged over last 500 steps = 3.3336e-01, PNorm = 104.2034, GNorm = 0.2462
Meta loss on this task batch = 3.6842e-01, Meta loss averaged over last 500 steps = 3.3338e-01, PNorm = 104.2114, GNorm = 0.2389
Meta loss on this task batch = 3.8131e-01, Meta loss averaged over last 500 steps = 3.3347e-01, PNorm = 104.2203, GNorm = 0.2233
Meta loss on this task batch = 3.6479e-01, Meta loss averaged over last 500 steps = 3.3356e-01, PNorm = 104.2301, GNorm = 0.2279
Meta loss on this task batch = 3.6722e-01, Meta loss averaged over last 500 steps = 3.3359e-01, PNorm = 104.2381, GNorm = 0.2544
Meta loss on this task batch = 3.3394e-01, Meta loss averaged over last 500 steps = 3.3355e-01, PNorm = 104.2462, GNorm = 0.1992
Meta loss on this task batch = 3.5396e-01, Meta loss averaged over last 500 steps = 3.3367e-01, PNorm = 104.2533, GNorm = 0.2273
Meta loss on this task batch = 3.0872e-01, Meta loss averaged over last 500 steps = 3.3357e-01, PNorm = 104.2614, GNorm = 0.2035
Meta loss on this task batch = 3.5752e-01, Meta loss averaged over last 500 steps = 3.3362e-01, PNorm = 104.2695, GNorm = 0.2313
Meta loss on this task batch = 2.9702e-01, Meta loss averaged over last 500 steps = 3.3359e-01, PNorm = 104.2786, GNorm = 0.2041
Took 111.72071146965027 seconds to complete one epoch of meta training
Took 119.39256072044373 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505807
Epoch 258
Meta loss on this task batch = 2.9679e-01, Meta loss averaged over last 500 steps = 3.3359e-01, PNorm = 104.2886, GNorm = 0.1874
Meta loss on this task batch = 3.2399e-01, Meta loss averaged over last 500 steps = 3.3355e-01, PNorm = 104.2980, GNorm = 0.2134
Meta loss on this task batch = 3.6991e-01, Meta loss averaged over last 500 steps = 3.3364e-01, PNorm = 104.3058, GNorm = 0.2832
Meta loss on this task batch = 3.3912e-01, Meta loss averaged over last 500 steps = 3.3350e-01, PNorm = 104.3136, GNorm = 0.2200
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 3.3339e-01, PNorm = 104.3215, GNorm = 0.1578
Meta loss on this task batch = 3.3089e-01, Meta loss averaged over last 500 steps = 3.3352e-01, PNorm = 104.3301, GNorm = 0.1948
Meta loss on this task batch = 3.5471e-01, Meta loss averaged over last 500 steps = 3.3359e-01, PNorm = 104.3389, GNorm = 0.2021
Meta loss on this task batch = 2.9236e-01, Meta loss averaged over last 500 steps = 3.3354e-01, PNorm = 104.3483, GNorm = 0.1879
Meta loss on this task batch = 3.1935e-01, Meta loss averaged over last 500 steps = 3.3346e-01, PNorm = 104.3589, GNorm = 0.2114
Meta loss on this task batch = 4.0991e-01, Meta loss averaged over last 500 steps = 3.3367e-01, PNorm = 104.3683, GNorm = 0.2388
Meta loss on this task batch = 3.3396e-01, Meta loss averaged over last 500 steps = 3.3375e-01, PNorm = 104.3779, GNorm = 0.2255
Meta loss on this task batch = 2.7714e-01, Meta loss averaged over last 500 steps = 3.3357e-01, PNorm = 104.3877, GNorm = 0.2265
Meta loss on this task batch = 3.6269e-01, Meta loss averaged over last 500 steps = 3.3361e-01, PNorm = 104.3969, GNorm = 0.2436
Meta loss on this task batch = 3.6814e-01, Meta loss averaged over last 500 steps = 3.3359e-01, PNorm = 104.4054, GNorm = 0.2257
Meta loss on this task batch = 3.3269e-01, Meta loss averaged over last 500 steps = 3.3360e-01, PNorm = 104.4134, GNorm = 0.2712
Meta loss on this task batch = 3.3828e-01, Meta loss averaged over last 500 steps = 3.3352e-01, PNorm = 104.4207, GNorm = 0.2256
Meta loss on this task batch = 3.4618e-01, Meta loss averaged over last 500 steps = 3.3353e-01, PNorm = 104.4286, GNorm = 0.2653
Meta loss on this task batch = 2.7161e-01, Meta loss averaged over last 500 steps = 3.3335e-01, PNorm = 104.4378, GNorm = 0.1938
Meta loss on this task batch = 3.0887e-01, Meta loss averaged over last 500 steps = 3.3326e-01, PNorm = 104.4467, GNorm = 0.2318
Took 114.77633500099182 seconds to complete one epoch of meta training
Took 122.89582014083862 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501549
Epoch 259
Meta loss on this task batch = 3.1703e-01, Meta loss averaged over last 500 steps = 3.3317e-01, PNorm = 104.4549, GNorm = 0.2006
Meta loss on this task batch = 3.1703e-01, Meta loss averaged over last 500 steps = 3.3317e-01, PNorm = 104.4627, GNorm = 0.1928
Meta loss on this task batch = 3.3449e-01, Meta loss averaged over last 500 steps = 3.3317e-01, PNorm = 104.4698, GNorm = 0.2150
Meta loss on this task batch = 3.6527e-01, Meta loss averaged over last 500 steps = 3.3320e-01, PNorm = 104.4768, GNorm = 0.2115
Meta loss on this task batch = 4.0065e-01, Meta loss averaged over last 500 steps = 3.3334e-01, PNorm = 104.4839, GNorm = 0.2593
Meta loss on this task batch = 3.6050e-01, Meta loss averaged over last 500 steps = 3.3341e-01, PNorm = 104.4908, GNorm = 0.2616
Meta loss on this task batch = 2.8359e-01, Meta loss averaged over last 500 steps = 3.3336e-01, PNorm = 104.4968, GNorm = 0.2846
Meta loss on this task batch = 3.4640e-01, Meta loss averaged over last 500 steps = 3.3342e-01, PNorm = 104.5028, GNorm = 0.2391
Meta loss on this task batch = 3.1814e-01, Meta loss averaged over last 500 steps = 3.3348e-01, PNorm = 104.5098, GNorm = 0.2189
Meta loss on this task batch = 3.8205e-01, Meta loss averaged over last 500 steps = 3.3347e-01, PNorm = 104.5166, GNorm = 0.2127
Meta loss on this task batch = 3.4066e-01, Meta loss averaged over last 500 steps = 3.3345e-01, PNorm = 104.5234, GNorm = 0.1865
Meta loss on this task batch = 3.2060e-01, Meta loss averaged over last 500 steps = 3.3346e-01, PNorm = 104.5309, GNorm = 0.1919
Meta loss on this task batch = 3.3624e-01, Meta loss averaged over last 500 steps = 3.3342e-01, PNorm = 104.5379, GNorm = 0.2105
Meta loss on this task batch = 3.4833e-01, Meta loss averaged over last 500 steps = 3.3343e-01, PNorm = 104.5454, GNorm = 0.2203
Meta loss on this task batch = 3.2317e-01, Meta loss averaged over last 500 steps = 3.3336e-01, PNorm = 104.5535, GNorm = 0.2589
Meta loss on this task batch = 2.9684e-01, Meta loss averaged over last 500 steps = 3.3326e-01, PNorm = 104.5620, GNorm = 0.1861
Meta loss on this task batch = 3.4201e-01, Meta loss averaged over last 500 steps = 3.3327e-01, PNorm = 104.5710, GNorm = 0.2273
Meta loss on this task batch = 3.2077e-01, Meta loss averaged over last 500 steps = 3.3321e-01, PNorm = 104.5791, GNorm = 0.2069
Meta loss on this task batch = 3.1497e-01, Meta loss averaged over last 500 steps = 3.3316e-01, PNorm = 104.5874, GNorm = 0.2015
Took 113.99885535240173 seconds to complete one epoch of meta training
Took 121.69913005828857 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497711
Epoch 260
Meta loss on this task batch = 2.9852e-01, Meta loss averaged over last 500 steps = 3.3305e-01, PNorm = 104.5938, GNorm = 0.2092
Meta loss on this task batch = 3.6682e-01, Meta loss averaged over last 500 steps = 3.3311e-01, PNorm = 104.6003, GNorm = 0.2039
Meta loss on this task batch = 2.8534e-01, Meta loss averaged over last 500 steps = 3.3301e-01, PNorm = 104.6074, GNorm = 0.1846
Meta loss on this task batch = 3.5776e-01, Meta loss averaged over last 500 steps = 3.3307e-01, PNorm = 104.6155, GNorm = 0.2325
Meta loss on this task batch = 3.4997e-01, Meta loss averaged over last 500 steps = 3.3306e-01, PNorm = 104.6237, GNorm = 0.2150
Meta loss on this task batch = 2.8804e-01, Meta loss averaged over last 500 steps = 3.3291e-01, PNorm = 104.6335, GNorm = 0.2075
Meta loss on this task batch = 3.5905e-01, Meta loss averaged over last 500 steps = 3.3291e-01, PNorm = 104.6433, GNorm = 0.2179
Meta loss on this task batch = 3.1225e-01, Meta loss averaged over last 500 steps = 3.3283e-01, PNorm = 104.6534, GNorm = 0.2096
Meta loss on this task batch = 3.2588e-01, Meta loss averaged over last 500 steps = 3.3280e-01, PNorm = 104.6628, GNorm = 0.2407
Meta loss on this task batch = 3.8447e-01, Meta loss averaged over last 500 steps = 3.3308e-01, PNorm = 104.6725, GNorm = 0.2300
Meta loss on this task batch = 2.8772e-01, Meta loss averaged over last 500 steps = 3.3304e-01, PNorm = 104.6829, GNorm = 0.2119
Meta loss on this task batch = 3.9651e-01, Meta loss averaged over last 500 steps = 3.3315e-01, PNorm = 104.6923, GNorm = 0.2698
Meta loss on this task batch = 3.2732e-01, Meta loss averaged over last 500 steps = 3.3314e-01, PNorm = 104.7021, GNorm = 0.2326
Meta loss on this task batch = 3.2061e-01, Meta loss averaged over last 500 steps = 3.3313e-01, PNorm = 104.7123, GNorm = 0.1950
Meta loss on this task batch = 3.4255e-01, Meta loss averaged over last 500 steps = 3.3311e-01, PNorm = 104.7218, GNorm = 0.2198
Meta loss on this task batch = 2.9066e-01, Meta loss averaged over last 500 steps = 3.3301e-01, PNorm = 104.7326, GNorm = 0.2318
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 3.3288e-01, PNorm = 104.7428, GNorm = 0.2380
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 3.3269e-01, PNorm = 104.7525, GNorm = 0.2549
Meta loss on this task batch = 3.3458e-01, Meta loss averaged over last 500 steps = 3.3261e-01, PNorm = 104.7601, GNorm = 0.2870
Took 113.03834509849548 seconds to complete one epoch of meta training
Took 120.49747085571289 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509159
Epoch 261
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 3.3265e-01, PNorm = 104.7669, GNorm = 0.2132
Meta loss on this task batch = 3.0760e-01, Meta loss averaged over last 500 steps = 3.3257e-01, PNorm = 104.7723, GNorm = 0.2150
Meta loss on this task batch = 3.1204e-01, Meta loss averaged over last 500 steps = 3.3253e-01, PNorm = 104.7780, GNorm = 0.2373
Meta loss on this task batch = 3.5758e-01, Meta loss averaged over last 500 steps = 3.3257e-01, PNorm = 104.7833, GNorm = 0.2604
Meta loss on this task batch = 3.6327e-01, Meta loss averaged over last 500 steps = 3.3260e-01, PNorm = 104.7883, GNorm = 0.2333
Meta loss on this task batch = 3.2910e-01, Meta loss averaged over last 500 steps = 3.3258e-01, PNorm = 104.7935, GNorm = 0.2133
Meta loss on this task batch = 3.3447e-01, Meta loss averaged over last 500 steps = 3.3250e-01, PNorm = 104.7990, GNorm = 0.2035
Meta loss on this task batch = 3.3469e-01, Meta loss averaged over last 500 steps = 3.3249e-01, PNorm = 104.8063, GNorm = 0.2251
Meta loss on this task batch = 3.0394e-01, Meta loss averaged over last 500 steps = 3.3240e-01, PNorm = 104.8142, GNorm = 0.2137
Meta loss on this task batch = 3.3155e-01, Meta loss averaged over last 500 steps = 3.3238e-01, PNorm = 104.8225, GNorm = 0.2100
Meta loss on this task batch = 3.3160e-01, Meta loss averaged over last 500 steps = 3.3237e-01, PNorm = 104.8313, GNorm = 0.2082
Meta loss on this task batch = 3.1493e-01, Meta loss averaged over last 500 steps = 3.3243e-01, PNorm = 104.8404, GNorm = 0.2087
Meta loss on this task batch = 3.3783e-01, Meta loss averaged over last 500 steps = 3.3252e-01, PNorm = 104.8503, GNorm = 0.2155
Meta loss on this task batch = 3.3432e-01, Meta loss averaged over last 500 steps = 3.3249e-01, PNorm = 104.8599, GNorm = 0.2051
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 3.3243e-01, PNorm = 104.8706, GNorm = 0.2467
Meta loss on this task batch = 3.0405e-01, Meta loss averaged over last 500 steps = 3.3223e-01, PNorm = 104.8809, GNorm = 0.2074
Meta loss on this task batch = 3.7610e-01, Meta loss averaged over last 500 steps = 3.3222e-01, PNorm = 104.8902, GNorm = 0.2491
Meta loss on this task batch = 3.5124e-01, Meta loss averaged over last 500 steps = 3.3224e-01, PNorm = 104.8989, GNorm = 0.2139
Meta loss on this task batch = 3.6522e-01, Meta loss averaged over last 500 steps = 3.3234e-01, PNorm = 104.9059, GNorm = 0.3211
Took 115.79493451118469 seconds to complete one epoch of meta training
Took 123.26096153259277 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510220
Epoch 262
Meta loss on this task batch = 2.7840e-01, Meta loss averaged over last 500 steps = 3.3215e-01, PNorm = 104.9144, GNorm = 0.1857
Meta loss on this task batch = 3.8025e-01, Meta loss averaged over last 500 steps = 3.3223e-01, PNorm = 104.9237, GNorm = 0.2439
Meta loss on this task batch = 3.0718e-01, Meta loss averaged over last 500 steps = 3.3217e-01, PNorm = 104.9337, GNorm = 0.2102
Meta loss on this task batch = 3.4118e-01, Meta loss averaged over last 500 steps = 3.3231e-01, PNorm = 104.9438, GNorm = 0.2148
Meta loss on this task batch = 3.4297e-01, Meta loss averaged over last 500 steps = 3.3234e-01, PNorm = 104.9532, GNorm = 0.2411
Meta loss on this task batch = 3.3272e-01, Meta loss averaged over last 500 steps = 3.3235e-01, PNorm = 104.9628, GNorm = 0.2237
Meta loss on this task batch = 3.0927e-01, Meta loss averaged over last 500 steps = 3.3238e-01, PNorm = 104.9722, GNorm = 0.2111
Meta loss on this task batch = 3.3761e-01, Meta loss averaged over last 500 steps = 3.3227e-01, PNorm = 104.9811, GNorm = 0.2067
Meta loss on this task batch = 3.5012e-01, Meta loss averaged over last 500 steps = 3.3224e-01, PNorm = 104.9893, GNorm = 0.2122
Meta loss on this task batch = 3.4366e-01, Meta loss averaged over last 500 steps = 3.3230e-01, PNorm = 104.9960, GNorm = 0.2402
Meta loss on this task batch = 3.2639e-01, Meta loss averaged over last 500 steps = 3.3227e-01, PNorm = 105.0019, GNorm = 0.2492
Meta loss on this task batch = 3.5674e-01, Meta loss averaged over last 500 steps = 3.3227e-01, PNorm = 105.0080, GNorm = 0.2101
Meta loss on this task batch = 3.4585e-01, Meta loss averaged over last 500 steps = 3.3221e-01, PNorm = 105.0142, GNorm = 0.2264
Meta loss on this task batch = 3.3323e-01, Meta loss averaged over last 500 steps = 3.3221e-01, PNorm = 105.0216, GNorm = 0.2385
Meta loss on this task batch = 3.2997e-01, Meta loss averaged over last 500 steps = 3.3212e-01, PNorm = 105.0302, GNorm = 0.1980
Meta loss on this task batch = 3.7554e-01, Meta loss averaged over last 500 steps = 3.3214e-01, PNorm = 105.0377, GNorm = 0.2400
Meta loss on this task batch = 3.2921e-01, Meta loss averaged over last 500 steps = 3.3210e-01, PNorm = 105.0464, GNorm = 0.2042
Meta loss on this task batch = 3.6892e-01, Meta loss averaged over last 500 steps = 3.3222e-01, PNorm = 105.0541, GNorm = 0.2081
Meta loss on this task batch = 2.7507e-01, Meta loss averaged over last 500 steps = 3.3210e-01, PNorm = 105.0626, GNorm = 0.1991
Took 117.04125738143921 seconds to complete one epoch of meta training
Took 125.16441917419434 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484357
Epoch 263
Meta loss on this task batch = 4.1514e-01, Meta loss averaged over last 500 steps = 3.3240e-01, PNorm = 105.0712, GNorm = 0.2539
Meta loss on this task batch = 3.4433e-01, Meta loss averaged over last 500 steps = 3.3238e-01, PNorm = 105.0808, GNorm = 0.1868
Meta loss on this task batch = 3.3909e-01, Meta loss averaged over last 500 steps = 3.3242e-01, PNorm = 105.0911, GNorm = 0.1902
Meta loss on this task batch = 3.5570e-01, Meta loss averaged over last 500 steps = 3.3251e-01, PNorm = 105.1025, GNorm = 0.2177
Meta loss on this task batch = 2.9517e-01, Meta loss averaged over last 500 steps = 3.3237e-01, PNorm = 105.1139, GNorm = 0.1615
Meta loss on this task batch = 3.4840e-01, Meta loss averaged over last 500 steps = 3.3241e-01, PNorm = 105.1254, GNorm = 0.1927
Meta loss on this task batch = 2.6654e-01, Meta loss averaged over last 500 steps = 3.3225e-01, PNorm = 105.1374, GNorm = 0.1924
Meta loss on this task batch = 3.0365e-01, Meta loss averaged over last 500 steps = 3.3220e-01, PNorm = 105.1499, GNorm = 0.2014
Meta loss on this task batch = 3.3524e-01, Meta loss averaged over last 500 steps = 3.3219e-01, PNorm = 105.1614, GNorm = 0.2490
Meta loss on this task batch = 3.2893e-01, Meta loss averaged over last 500 steps = 3.3228e-01, PNorm = 105.1735, GNorm = 0.2206
Meta loss on this task batch = 3.4827e-01, Meta loss averaged over last 500 steps = 3.3234e-01, PNorm = 105.1848, GNorm = 0.2318
Meta loss on this task batch = 3.9430e-01, Meta loss averaged over last 500 steps = 3.3242e-01, PNorm = 105.1920, GNorm = 0.2861
Meta loss on this task batch = 3.1194e-01, Meta loss averaged over last 500 steps = 3.3242e-01, PNorm = 105.1978, GNorm = 0.2714
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 3.3238e-01, PNorm = 105.2037, GNorm = 0.2321
Meta loss on this task batch = 2.9930e-01, Meta loss averaged over last 500 steps = 3.3230e-01, PNorm = 105.2105, GNorm = 0.1900
Meta loss on this task batch = 3.1452e-01, Meta loss averaged over last 500 steps = 3.3232e-01, PNorm = 105.2168, GNorm = 0.2358
Meta loss on this task batch = 3.3064e-01, Meta loss averaged over last 500 steps = 3.3230e-01, PNorm = 105.2230, GNorm = 0.2107
Meta loss on this task batch = 3.3338e-01, Meta loss averaged over last 500 steps = 3.3231e-01, PNorm = 105.2301, GNorm = 0.1927
Meta loss on this task batch = 3.4647e-01, Meta loss averaged over last 500 steps = 3.3236e-01, PNorm = 105.2362, GNorm = 0.2506
Took 112.03520178794861 seconds to complete one epoch of meta training
Took 119.9719066619873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501337
Epoch 264
Meta loss on this task batch = 3.0525e-01, Meta loss averaged over last 500 steps = 3.3235e-01, PNorm = 105.2432, GNorm = 0.1960
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 3.3238e-01, PNorm = 105.2519, GNorm = 0.1894
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 3.3226e-01, PNorm = 105.2602, GNorm = 0.2391
Meta loss on this task batch = 3.3904e-01, Meta loss averaged over last 500 steps = 3.3215e-01, PNorm = 105.2683, GNorm = 0.2258
Meta loss on this task batch = 3.2931e-01, Meta loss averaged over last 500 steps = 3.3210e-01, PNorm = 105.2769, GNorm = 0.1979
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 3.3206e-01, PNorm = 105.2864, GNorm = 0.2581
Meta loss on this task batch = 3.3057e-01, Meta loss averaged over last 500 steps = 3.3212e-01, PNorm = 105.2961, GNorm = 0.2200
Meta loss on this task batch = 3.3187e-01, Meta loss averaged over last 500 steps = 3.3203e-01, PNorm = 105.3059, GNorm = 0.1972
Meta loss on this task batch = 3.7581e-01, Meta loss averaged over last 500 steps = 3.3207e-01, PNorm = 105.3152, GNorm = 0.2368
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 3.3204e-01, PNorm = 105.3260, GNorm = 0.2046
Meta loss on this task batch = 3.1860e-01, Meta loss averaged over last 500 steps = 3.3203e-01, PNorm = 105.3370, GNorm = 0.2253
Meta loss on this task batch = 3.5235e-01, Meta loss averaged over last 500 steps = 3.3207e-01, PNorm = 105.3469, GNorm = 0.2517
Meta loss on this task batch = 3.7007e-01, Meta loss averaged over last 500 steps = 3.3212e-01, PNorm = 105.3546, GNorm = 0.2585
Meta loss on this task batch = 3.2011e-01, Meta loss averaged over last 500 steps = 3.3210e-01, PNorm = 105.3617, GNorm = 0.2226
Meta loss on this task batch = 3.0594e-01, Meta loss averaged over last 500 steps = 3.3206e-01, PNorm = 105.3683, GNorm = 0.2105
Meta loss on this task batch = 3.6033e-01, Meta loss averaged over last 500 steps = 3.3209e-01, PNorm = 105.3745, GNorm = 0.2877
Meta loss on this task batch = 3.5326e-01, Meta loss averaged over last 500 steps = 3.3217e-01, PNorm = 105.3799, GNorm = 0.2134
Meta loss on this task batch = 3.0745e-01, Meta loss averaged over last 500 steps = 3.3209e-01, PNorm = 105.3848, GNorm = 0.2510
Meta loss on this task batch = 3.0982e-01, Meta loss averaged over last 500 steps = 3.3205e-01, PNorm = 105.3901, GNorm = 0.2308
Took 115.91907382011414 seconds to complete one epoch of meta training
Took 122.47477555274963 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508083
Epoch 265
Meta loss on this task batch = 2.8309e-01, Meta loss averaged over last 500 steps = 3.3185e-01, PNorm = 105.3967, GNorm = 0.1871
Meta loss on this task batch = 3.2064e-01, Meta loss averaged over last 500 steps = 3.3179e-01, PNorm = 105.4051, GNorm = 0.2125
Meta loss on this task batch = 3.3453e-01, Meta loss averaged over last 500 steps = 3.3194e-01, PNorm = 105.4140, GNorm = 0.2041
Meta loss on this task batch = 3.6098e-01, Meta loss averaged over last 500 steps = 3.3192e-01, PNorm = 105.4232, GNorm = 0.2076
Meta loss on this task batch = 3.4361e-01, Meta loss averaged over last 500 steps = 3.3190e-01, PNorm = 105.4335, GNorm = 0.2083
Meta loss on this task batch = 3.4526e-01, Meta loss averaged over last 500 steps = 3.3187e-01, PNorm = 105.4441, GNorm = 0.2040
Meta loss on this task batch = 2.9318e-01, Meta loss averaged over last 500 steps = 3.3185e-01, PNorm = 105.4557, GNorm = 0.1868
Meta loss on this task batch = 3.4205e-01, Meta loss averaged over last 500 steps = 3.3184e-01, PNorm = 105.4683, GNorm = 0.2108
Meta loss on this task batch = 3.5693e-01, Meta loss averaged over last 500 steps = 3.3188e-01, PNorm = 105.4804, GNorm = 0.2172
Meta loss on this task batch = 2.7856e-01, Meta loss averaged over last 500 steps = 3.3170e-01, PNorm = 105.4917, GNorm = 0.1890
Meta loss on this task batch = 3.2429e-01, Meta loss averaged over last 500 steps = 3.3168e-01, PNorm = 105.5006, GNorm = 0.2499
Meta loss on this task batch = 3.6123e-01, Meta loss averaged over last 500 steps = 3.3167e-01, PNorm = 105.5087, GNorm = 0.2604
Meta loss on this task batch = 3.1493e-01, Meta loss averaged over last 500 steps = 3.3160e-01, PNorm = 105.5173, GNorm = 0.2120
Meta loss on this task batch = 2.9109e-01, Meta loss averaged over last 500 steps = 3.3156e-01, PNorm = 105.5268, GNorm = 0.2104
Meta loss on this task batch = 3.0982e-01, Meta loss averaged over last 500 steps = 3.3145e-01, PNorm = 105.5366, GNorm = 0.2230
Meta loss on this task batch = 3.2456e-01, Meta loss averaged over last 500 steps = 3.3149e-01, PNorm = 105.5465, GNorm = 0.2167
Meta loss on this task batch = 3.5462e-01, Meta loss averaged over last 500 steps = 3.3153e-01, PNorm = 105.5564, GNorm = 0.2423
Meta loss on this task batch = 2.9019e-01, Meta loss averaged over last 500 steps = 3.3150e-01, PNorm = 105.5661, GNorm = 0.2590
Meta loss on this task batch = 3.3793e-01, Meta loss averaged over last 500 steps = 3.3146e-01, PNorm = 105.5754, GNorm = 0.2523
Took 115.00848650932312 seconds to complete one epoch of meta training
Took 122.71276473999023 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497205
Epoch 266
Meta loss on this task batch = 3.8288e-01, Meta loss averaged over last 500 steps = 3.3154e-01, PNorm = 105.5825, GNorm = 0.2591
Meta loss on this task batch = 3.3726e-01, Meta loss averaged over last 500 steps = 3.3159e-01, PNorm = 105.5889, GNorm = 0.2718
Meta loss on this task batch = 3.4061e-01, Meta loss averaged over last 500 steps = 3.3169e-01, PNorm = 105.5958, GNorm = 0.1876
Meta loss on this task batch = 3.3434e-01, Meta loss averaged over last 500 steps = 3.3159e-01, PNorm = 105.6034, GNorm = 0.1854
Meta loss on this task batch = 3.6997e-01, Meta loss averaged over last 500 steps = 3.3165e-01, PNorm = 105.6106, GNorm = 0.2492
Meta loss on this task batch = 2.9806e-01, Meta loss averaged over last 500 steps = 3.3160e-01, PNorm = 105.6179, GNorm = 0.2047
Meta loss on this task batch = 2.8842e-01, Meta loss averaged over last 500 steps = 3.3151e-01, PNorm = 105.6259, GNorm = 0.1950
Meta loss on this task batch = 3.1200e-01, Meta loss averaged over last 500 steps = 3.3145e-01, PNorm = 105.6355, GNorm = 0.2075
Meta loss on this task batch = 3.0635e-01, Meta loss averaged over last 500 steps = 3.3135e-01, PNorm = 105.6453, GNorm = 0.1821
Meta loss on this task batch = 3.4215e-01, Meta loss averaged over last 500 steps = 3.3140e-01, PNorm = 105.6552, GNorm = 0.2474
Meta loss on this task batch = 3.2715e-01, Meta loss averaged over last 500 steps = 3.3135e-01, PNorm = 105.6651, GNorm = 0.2303
Meta loss on this task batch = 2.5591e-01, Meta loss averaged over last 500 steps = 3.3114e-01, PNorm = 105.6749, GNorm = 0.2135
Meta loss on this task batch = 3.0003e-01, Meta loss averaged over last 500 steps = 3.3107e-01, PNorm = 105.6842, GNorm = 0.1822
Meta loss on this task batch = 3.2625e-01, Meta loss averaged over last 500 steps = 3.3105e-01, PNorm = 105.6932, GNorm = 0.2201
Meta loss on this task batch = 3.7203e-01, Meta loss averaged over last 500 steps = 3.3105e-01, PNorm = 105.7022, GNorm = 0.2220
Meta loss on this task batch = 3.4466e-01, Meta loss averaged over last 500 steps = 3.3103e-01, PNorm = 105.7116, GNorm = 0.2074
Meta loss on this task batch = 3.0303e-01, Meta loss averaged over last 500 steps = 3.3100e-01, PNorm = 105.7217, GNorm = 0.1869
Meta loss on this task batch = 2.5103e-01, Meta loss averaged over last 500 steps = 3.3088e-01, PNorm = 105.7321, GNorm = 0.1745
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 3.3086e-01, PNorm = 105.7422, GNorm = 0.2965
Took 114.50825428962708 seconds to complete one epoch of meta training
Took 122.26969909667969 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509275
Epoch 267
Meta loss on this task batch = 3.4743e-01, Meta loss averaged over last 500 steps = 3.3072e-01, PNorm = 105.7517, GNorm = 0.2192
Meta loss on this task batch = 3.4023e-01, Meta loss averaged over last 500 steps = 3.3084e-01, PNorm = 105.7616, GNorm = 0.1960
Meta loss on this task batch = 3.2319e-01, Meta loss averaged over last 500 steps = 3.3085e-01, PNorm = 105.7719, GNorm = 0.2347
Meta loss on this task batch = 2.9851e-01, Meta loss averaged over last 500 steps = 3.3087e-01, PNorm = 105.7823, GNorm = 0.2031
Meta loss on this task batch = 3.4198e-01, Meta loss averaged over last 500 steps = 3.3087e-01, PNorm = 105.7924, GNorm = 0.2178
Meta loss on this task batch = 3.4923e-01, Meta loss averaged over last 500 steps = 3.3087e-01, PNorm = 105.8017, GNorm = 0.2253
Meta loss on this task batch = 3.4139e-01, Meta loss averaged over last 500 steps = 3.3090e-01, PNorm = 105.8110, GNorm = 0.2205
Meta loss on this task batch = 3.2282e-01, Meta loss averaged over last 500 steps = 3.3088e-01, PNorm = 105.8193, GNorm = 0.2203
Meta loss on this task batch = 3.4713e-01, Meta loss averaged over last 500 steps = 3.3102e-01, PNorm = 105.8276, GNorm = 0.2260
Meta loss on this task batch = 2.7398e-01, Meta loss averaged over last 500 steps = 3.3076e-01, PNorm = 105.8372, GNorm = 0.2044
Meta loss on this task batch = 3.0784e-01, Meta loss averaged over last 500 steps = 3.3067e-01, PNorm = 105.8472, GNorm = 0.1941
Meta loss on this task batch = 3.1645e-01, Meta loss averaged over last 500 steps = 3.3056e-01, PNorm = 105.8571, GNorm = 0.1905
Meta loss on this task batch = 3.1088e-01, Meta loss averaged over last 500 steps = 3.3046e-01, PNorm = 105.8663, GNorm = 0.2439
Meta loss on this task batch = 3.6542e-01, Meta loss averaged over last 500 steps = 3.3055e-01, PNorm = 105.8755, GNorm = 0.2341
Meta loss on this task batch = 3.4259e-01, Meta loss averaged over last 500 steps = 3.3057e-01, PNorm = 105.8848, GNorm = 0.2479
Meta loss on this task batch = 4.1466e-01, Meta loss averaged over last 500 steps = 3.3072e-01, PNorm = 105.8931, GNorm = 0.2869
Meta loss on this task batch = 3.1411e-01, Meta loss averaged over last 500 steps = 3.3064e-01, PNorm = 105.9014, GNorm = 0.2077
Meta loss on this task batch = 3.0161e-01, Meta loss averaged over last 500 steps = 3.3063e-01, PNorm = 105.9109, GNorm = 0.2115
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 3.3038e-01, PNorm = 105.9203, GNorm = 0.2578
Took 115.30594396591187 seconds to complete one epoch of meta training
Took 123.09402441978455 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517314
Epoch 268
Meta loss on this task batch = 3.1724e-01, Meta loss averaged over last 500 steps = 3.3038e-01, PNorm = 105.9287, GNorm = 0.2194
Meta loss on this task batch = 3.6966e-01, Meta loss averaged over last 500 steps = 3.3043e-01, PNorm = 105.9371, GNorm = 0.2256
Meta loss on this task batch = 2.9814e-01, Meta loss averaged over last 500 steps = 3.3046e-01, PNorm = 105.9460, GNorm = 0.2037
Meta loss on this task batch = 3.1467e-01, Meta loss averaged over last 500 steps = 3.3042e-01, PNorm = 105.9547, GNorm = 0.1908
Meta loss on this task batch = 3.3681e-01, Meta loss averaged over last 500 steps = 3.3046e-01, PNorm = 105.9634, GNorm = 0.1937
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 3.3049e-01, PNorm = 105.9720, GNorm = 0.2121
Meta loss on this task batch = 2.8572e-01, Meta loss averaged over last 500 steps = 3.3039e-01, PNorm = 105.9803, GNorm = 0.1910
Meta loss on this task batch = 3.4670e-01, Meta loss averaged over last 500 steps = 3.3039e-01, PNorm = 105.9871, GNorm = 0.2802
Meta loss on this task batch = 3.5091e-01, Meta loss averaged over last 500 steps = 3.3050e-01, PNorm = 105.9943, GNorm = 0.2483
Meta loss on this task batch = 3.3913e-01, Meta loss averaged over last 500 steps = 3.3044e-01, PNorm = 106.0012, GNorm = 0.1985
Meta loss on this task batch = 3.0575e-01, Meta loss averaged over last 500 steps = 3.3051e-01, PNorm = 106.0091, GNorm = 0.1980
Meta loss on this task batch = 3.1518e-01, Meta loss averaged over last 500 steps = 3.3050e-01, PNorm = 106.0171, GNorm = 0.2020
Meta loss on this task batch = 3.2797e-01, Meta loss averaged over last 500 steps = 3.3038e-01, PNorm = 106.0263, GNorm = 0.2407
Meta loss on this task batch = 3.3307e-01, Meta loss averaged over last 500 steps = 3.3048e-01, PNorm = 106.0359, GNorm = 0.2113
Meta loss on this task batch = 3.4384e-01, Meta loss averaged over last 500 steps = 3.3047e-01, PNorm = 106.0449, GNorm = 0.2976
Meta loss on this task batch = 3.6623e-01, Meta loss averaged over last 500 steps = 3.3052e-01, PNorm = 106.0540, GNorm = 0.2512
Meta loss on this task batch = 3.2156e-01, Meta loss averaged over last 500 steps = 3.3045e-01, PNorm = 106.0640, GNorm = 0.2598
Meta loss on this task batch = 3.6430e-01, Meta loss averaged over last 500 steps = 3.3047e-01, PNorm = 106.0745, GNorm = 0.2318
Meta loss on this task batch = 4.0462e-01, Meta loss averaged over last 500 steps = 3.3063e-01, PNorm = 106.0828, GNorm = 0.2578
Took 113.83010649681091 seconds to complete one epoch of meta training
Took 122.2298698425293 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496722
Epoch 269
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 3.3053e-01, PNorm = 106.0910, GNorm = 0.1784
Meta loss on this task batch = 3.9995e-01, Meta loss averaged over last 500 steps = 3.3071e-01, PNorm = 106.0990, GNorm = 0.2290
Meta loss on this task batch = 3.6673e-01, Meta loss averaged over last 500 steps = 3.3081e-01, PNorm = 106.1046, GNorm = 0.2523
Meta loss on this task batch = 2.7643e-01, Meta loss averaged over last 500 steps = 3.3060e-01, PNorm = 106.1109, GNorm = 0.1771
Meta loss on this task batch = 3.2687e-01, Meta loss averaged over last 500 steps = 3.3057e-01, PNorm = 106.1166, GNorm = 0.2206
Meta loss on this task batch = 3.0094e-01, Meta loss averaged over last 500 steps = 3.3053e-01, PNorm = 106.1227, GNorm = 0.2022
Meta loss on this task batch = 3.7245e-01, Meta loss averaged over last 500 steps = 3.3056e-01, PNorm = 106.1279, GNorm = 0.2493
Meta loss on this task batch = 3.3789e-01, Meta loss averaged over last 500 steps = 3.3062e-01, PNorm = 106.1344, GNorm = 0.2076
Meta loss on this task batch = 3.6883e-01, Meta loss averaged over last 500 steps = 3.3073e-01, PNorm = 106.1423, GNorm = 0.2185
Meta loss on this task batch = 3.1173e-01, Meta loss averaged over last 500 steps = 3.3069e-01, PNorm = 106.1523, GNorm = 0.2437
Meta loss on this task batch = 3.7470e-01, Meta loss averaged over last 500 steps = 3.3074e-01, PNorm = 106.1629, GNorm = 0.2147
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 3.3067e-01, PNorm = 106.1748, GNorm = 0.2249
Meta loss on this task batch = 2.5797e-01, Meta loss averaged over last 500 steps = 3.3058e-01, PNorm = 106.1865, GNorm = 0.2470
Meta loss on this task batch = 3.1085e-01, Meta loss averaged over last 500 steps = 3.3047e-01, PNorm = 106.1990, GNorm = 0.2383
Meta loss on this task batch = 3.4979e-01, Meta loss averaged over last 500 steps = 3.3056e-01, PNorm = 106.2100, GNorm = 0.2725
Meta loss on this task batch = 3.3915e-01, Meta loss averaged over last 500 steps = 3.3066e-01, PNorm = 106.2193, GNorm = 0.2300
Meta loss on this task batch = 2.4971e-01, Meta loss averaged over last 500 steps = 3.3049e-01, PNorm = 106.2286, GNorm = 0.2027
Meta loss on this task batch = 3.3832e-01, Meta loss averaged over last 500 steps = 3.3049e-01, PNorm = 106.2383, GNorm = 0.2710
Meta loss on this task batch = 3.4669e-01, Meta loss averaged over last 500 steps = 3.3055e-01, PNorm = 106.2476, GNorm = 0.3382
Took 114.19393420219421 seconds to complete one epoch of meta training
Took 122.28703951835632 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499269
Epoch 270
Meta loss on this task batch = 3.1955e-01, Meta loss averaged over last 500 steps = 3.3044e-01, PNorm = 106.2566, GNorm = 0.2333
Meta loss on this task batch = 3.4275e-01, Meta loss averaged over last 500 steps = 3.3044e-01, PNorm = 106.2653, GNorm = 0.2642
Meta loss on this task batch = 3.5514e-01, Meta loss averaged over last 500 steps = 3.3045e-01, PNorm = 106.2730, GNorm = 0.2430
Meta loss on this task batch = 3.5400e-01, Meta loss averaged over last 500 steps = 3.3047e-01, PNorm = 106.2801, GNorm = 0.2509
Meta loss on this task batch = 2.7666e-01, Meta loss averaged over last 500 steps = 3.3035e-01, PNorm = 106.2881, GNorm = 0.1847
Meta loss on this task batch = 3.5756e-01, Meta loss averaged over last 500 steps = 3.3036e-01, PNorm = 106.2950, GNorm = 0.2509
Meta loss on this task batch = 3.2641e-01, Meta loss averaged over last 500 steps = 3.3027e-01, PNorm = 106.3024, GNorm = 0.2017
Meta loss on this task batch = 3.2554e-01, Meta loss averaged over last 500 steps = 3.3022e-01, PNorm = 106.3108, GNorm = 0.2123
Meta loss on this task batch = 3.1300e-01, Meta loss averaged over last 500 steps = 3.3008e-01, PNorm = 106.3212, GNorm = 0.2056
Meta loss on this task batch = 2.9802e-01, Meta loss averaged over last 500 steps = 3.3001e-01, PNorm = 106.3324, GNorm = 0.2569
Meta loss on this task batch = 3.4483e-01, Meta loss averaged over last 500 steps = 3.3003e-01, PNorm = 106.3428, GNorm = 0.1986
Meta loss on this task batch = 2.9479e-01, Meta loss averaged over last 500 steps = 3.2997e-01, PNorm = 106.3537, GNorm = 0.1965
Meta loss on this task batch = 3.4757e-01, Meta loss averaged over last 500 steps = 3.3010e-01, PNorm = 106.3642, GNorm = 0.2131
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 3.3007e-01, PNorm = 106.3748, GNorm = 0.1953
Meta loss on this task batch = 2.8070e-01, Meta loss averaged over last 500 steps = 3.2999e-01, PNorm = 106.3858, GNorm = 0.2139
Meta loss on this task batch = 3.2605e-01, Meta loss averaged over last 500 steps = 3.2989e-01, PNorm = 106.3966, GNorm = 0.1964
Meta loss on this task batch = 3.2493e-01, Meta loss averaged over last 500 steps = 3.2990e-01, PNorm = 106.4062, GNorm = 0.2271
Meta loss on this task batch = 3.3680e-01, Meta loss averaged over last 500 steps = 3.2993e-01, PNorm = 106.4139, GNorm = 0.2791
Meta loss on this task batch = 2.9550e-01, Meta loss averaged over last 500 steps = 3.2982e-01, PNorm = 106.4214, GNorm = 0.2347
Took 114.52819752693176 seconds to complete one epoch of meta training
Took 121.99778985977173 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494487
Epoch 271
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 3.2973e-01, PNorm = 106.4294, GNorm = 0.2371
Meta loss on this task batch = 3.1612e-01, Meta loss averaged over last 500 steps = 3.2971e-01, PNorm = 106.4370, GNorm = 0.2156
Meta loss on this task batch = 2.7428e-01, Meta loss averaged over last 500 steps = 3.2945e-01, PNorm = 106.4452, GNorm = 0.1966
Meta loss on this task batch = 3.1711e-01, Meta loss averaged over last 500 steps = 3.2939e-01, PNorm = 106.4533, GNorm = 0.2388
Meta loss on this task batch = 3.6885e-01, Meta loss averaged over last 500 steps = 3.2944e-01, PNorm = 106.4607, GNorm = 0.2308
Meta loss on this task batch = 2.9078e-01, Meta loss averaged over last 500 steps = 3.2937e-01, PNorm = 106.4686, GNorm = 0.1931
Meta loss on this task batch = 3.8176e-01, Meta loss averaged over last 500 steps = 3.2946e-01, PNorm = 106.4769, GNorm = 0.2263
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.2949e-01, PNorm = 106.4860, GNorm = 0.2067
Meta loss on this task batch = 3.5327e-01, Meta loss averaged over last 500 steps = 3.2958e-01, PNorm = 106.4952, GNorm = 0.2246
Meta loss on this task batch = 3.3575e-01, Meta loss averaged over last 500 steps = 3.2947e-01, PNorm = 106.5049, GNorm = 0.2023
Meta loss on this task batch = 3.6988e-01, Meta loss averaged over last 500 steps = 3.2956e-01, PNorm = 106.5148, GNorm = 0.2014
Meta loss on this task batch = 3.4183e-01, Meta loss averaged over last 500 steps = 3.2967e-01, PNorm = 106.5241, GNorm = 0.2098
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 3.2954e-01, PNorm = 106.5344, GNorm = 0.2018
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 3.2935e-01, PNorm = 106.5448, GNorm = 0.1833
Meta loss on this task batch = 3.0655e-01, Meta loss averaged over last 500 steps = 3.2933e-01, PNorm = 106.5550, GNorm = 0.1844
Meta loss on this task batch = 3.3843e-01, Meta loss averaged over last 500 steps = 3.2935e-01, PNorm = 106.5641, GNorm = 0.2617
Meta loss on this task batch = 3.3526e-01, Meta loss averaged over last 500 steps = 3.2942e-01, PNorm = 106.5730, GNorm = 0.2210
Meta loss on this task batch = 2.9854e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 106.5824, GNorm = 0.1722
Meta loss on this task batch = 3.3083e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 106.5897, GNorm = 0.2778
Took 110.61250805854797 seconds to complete one epoch of meta training
Took 118.09307146072388 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480151
Epoch 272
Meta loss on this task batch = 3.1736e-01, Meta loss averaged over last 500 steps = 3.2932e-01, PNorm = 106.5972, GNorm = 0.2273
Meta loss on this task batch = 3.0796e-01, Meta loss averaged over last 500 steps = 3.2923e-01, PNorm = 106.6050, GNorm = 0.1872
Meta loss on this task batch = 3.0789e-01, Meta loss averaged over last 500 steps = 3.2920e-01, PNorm = 106.6132, GNorm = 0.1857
Meta loss on this task batch = 3.5865e-01, Meta loss averaged over last 500 steps = 3.2922e-01, PNorm = 106.6203, GNorm = 0.2051
Meta loss on this task batch = 3.2908e-01, Meta loss averaged over last 500 steps = 3.2921e-01, PNorm = 106.6286, GNorm = 0.2107
Meta loss on this task batch = 3.0495e-01, Meta loss averaged over last 500 steps = 3.2926e-01, PNorm = 106.6375, GNorm = 0.2298
Meta loss on this task batch = 3.9777e-01, Meta loss averaged over last 500 steps = 3.2948e-01, PNorm = 106.6469, GNorm = 0.2502
Meta loss on this task batch = 3.4340e-01, Meta loss averaged over last 500 steps = 3.2958e-01, PNorm = 106.6549, GNorm = 0.2525
Meta loss on this task batch = 2.7904e-01, Meta loss averaged over last 500 steps = 3.2951e-01, PNorm = 106.6633, GNorm = 0.2064
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 3.2949e-01, PNorm = 106.6721, GNorm = 0.1956
Meta loss on this task batch = 3.2822e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 106.6806, GNorm = 0.2521
Meta loss on this task batch = 2.8775e-01, Meta loss averaged over last 500 steps = 3.2928e-01, PNorm = 106.6894, GNorm = 0.2069
Meta loss on this task batch = 3.8449e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 106.6966, GNorm = 0.2703
Meta loss on this task batch = 3.1761e-01, Meta loss averaged over last 500 steps = 3.2938e-01, PNorm = 106.7042, GNorm = 0.2027
Meta loss on this task batch = 3.4002e-01, Meta loss averaged over last 500 steps = 3.2940e-01, PNorm = 106.7121, GNorm = 0.2078
Meta loss on this task batch = 2.9590e-01, Meta loss averaged over last 500 steps = 3.2934e-01, PNorm = 106.7203, GNorm = 0.2043
Meta loss on this task batch = 3.1010e-01, Meta loss averaged over last 500 steps = 3.2933e-01, PNorm = 106.7275, GNorm = 0.2033
Meta loss on this task batch = 3.3217e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 106.7347, GNorm = 0.2691
Meta loss on this task batch = 3.2165e-01, Meta loss averaged over last 500 steps = 3.2936e-01, PNorm = 106.7418, GNorm = 0.2517
Took 114.88906741142273 seconds to complete one epoch of meta training
Took 122.56774139404297 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505318
Epoch 273
Meta loss on this task batch = 2.8029e-01, Meta loss averaged over last 500 steps = 3.2929e-01, PNorm = 106.7502, GNorm = 0.2112
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 3.2918e-01, PNorm = 106.7596, GNorm = 0.1978
Meta loss on this task batch = 3.5698e-01, Meta loss averaged over last 500 steps = 3.2912e-01, PNorm = 106.7692, GNorm = 0.2268
Meta loss on this task batch = 2.9664e-01, Meta loss averaged over last 500 steps = 3.2898e-01, PNorm = 106.7791, GNorm = 0.1853
Meta loss on this task batch = 3.2284e-01, Meta loss averaged over last 500 steps = 3.2897e-01, PNorm = 106.7888, GNorm = 0.2347
Meta loss on this task batch = 3.1893e-01, Meta loss averaged over last 500 steps = 3.2884e-01, PNorm = 106.7983, GNorm = 0.2961
Meta loss on this task batch = 3.2144e-01, Meta loss averaged over last 500 steps = 3.2880e-01, PNorm = 106.8071, GNorm = 0.2129
Meta loss on this task batch = 3.5143e-01, Meta loss averaged over last 500 steps = 3.2885e-01, PNorm = 106.8143, GNorm = 0.2241
Meta loss on this task batch = 3.2788e-01, Meta loss averaged over last 500 steps = 3.2880e-01, PNorm = 106.8215, GNorm = 0.1780
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 3.2869e-01, PNorm = 106.8291, GNorm = 0.2600
Meta loss on this task batch = 3.7773e-01, Meta loss averaged over last 500 steps = 3.2890e-01, PNorm = 106.8359, GNorm = 0.3112
Meta loss on this task batch = 3.1800e-01, Meta loss averaged over last 500 steps = 3.2887e-01, PNorm = 106.8431, GNorm = 0.2157
Meta loss on this task batch = 2.9429e-01, Meta loss averaged over last 500 steps = 3.2878e-01, PNorm = 106.8510, GNorm = 0.2494
Meta loss on this task batch = 3.5099e-01, Meta loss averaged over last 500 steps = 3.2879e-01, PNorm = 106.8595, GNorm = 0.2311
Meta loss on this task batch = 3.3328e-01, Meta loss averaged over last 500 steps = 3.2877e-01, PNorm = 106.8698, GNorm = 0.2447
Meta loss on this task batch = 3.6272e-01, Meta loss averaged over last 500 steps = 3.2892e-01, PNorm = 106.8796, GNorm = 0.2180
Meta loss on this task batch = 3.4614e-01, Meta loss averaged over last 500 steps = 3.2893e-01, PNorm = 106.8896, GNorm = 0.2045
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 3.2885e-01, PNorm = 106.8991, GNorm = 0.1972
Meta loss on this task batch = 3.3058e-01, Meta loss averaged over last 500 steps = 3.2881e-01, PNorm = 106.9067, GNorm = 0.3319
Took 112.17808628082275 seconds to complete one epoch of meta training
Took 119.07928538322449 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489414
Epoch 274
Meta loss on this task batch = 2.8199e-01, Meta loss averaged over last 500 steps = 3.2879e-01, PNorm = 106.9148, GNorm = 0.2034
Meta loss on this task batch = 3.1062e-01, Meta loss averaged over last 500 steps = 3.2874e-01, PNorm = 106.9235, GNorm = 0.2340
Meta loss on this task batch = 3.4003e-01, Meta loss averaged over last 500 steps = 3.2873e-01, PNorm = 106.9313, GNorm = 0.2192
Meta loss on this task batch = 3.4847e-01, Meta loss averaged over last 500 steps = 3.2883e-01, PNorm = 106.9384, GNorm = 0.2233
Meta loss on this task batch = 2.8575e-01, Meta loss averaged over last 500 steps = 3.2866e-01, PNorm = 106.9469, GNorm = 0.2243
Meta loss on this task batch = 3.0810e-01, Meta loss averaged over last 500 steps = 3.2858e-01, PNorm = 106.9564, GNorm = 0.2471
Meta loss on this task batch = 3.6711e-01, Meta loss averaged over last 500 steps = 3.2862e-01, PNorm = 106.9645, GNorm = 0.2291
Meta loss on this task batch = 3.6054e-01, Meta loss averaged over last 500 steps = 3.2864e-01, PNorm = 106.9728, GNorm = 0.2224
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 106.9802, GNorm = 0.2394
Meta loss on this task batch = 3.0772e-01, Meta loss averaged over last 500 steps = 3.2865e-01, PNorm = 106.9872, GNorm = 0.2073
Meta loss on this task batch = 3.3321e-01, Meta loss averaged over last 500 steps = 3.2867e-01, PNorm = 106.9945, GNorm = 0.2363
Meta loss on this task batch = 3.5658e-01, Meta loss averaged over last 500 steps = 3.2869e-01, PNorm = 107.0018, GNorm = 0.2247
Meta loss on this task batch = 2.9561e-01, Meta loss averaged over last 500 steps = 3.2861e-01, PNorm = 107.0089, GNorm = 0.1957
Meta loss on this task batch = 3.1780e-01, Meta loss averaged over last 500 steps = 3.2863e-01, PNorm = 107.0158, GNorm = 0.2217
Meta loss on this task batch = 2.9487e-01, Meta loss averaged over last 500 steps = 3.2853e-01, PNorm = 107.0235, GNorm = 0.2066
Meta loss on this task batch = 3.3839e-01, Meta loss averaged over last 500 steps = 3.2857e-01, PNorm = 107.0297, GNorm = 0.2421
Meta loss on this task batch = 3.1929e-01, Meta loss averaged over last 500 steps = 3.2854e-01, PNorm = 107.0356, GNorm = 0.1999
Meta loss on this task batch = 3.4416e-01, Meta loss averaged over last 500 steps = 3.2855e-01, PNorm = 107.0414, GNorm = 0.2288
Meta loss on this task batch = 3.0085e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 107.0478, GNorm = 0.2595
Took 140.00808715820312 seconds to complete one epoch of meta training
Took 148.1614158153534 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510161
Epoch 275
Meta loss on this task batch = 3.2716e-01, Meta loss averaged over last 500 steps = 3.2850e-01, PNorm = 107.0554, GNorm = 0.2089
Meta loss on this task batch = 3.4427e-01, Meta loss averaged over last 500 steps = 3.2852e-01, PNorm = 107.0627, GNorm = 0.2650
Meta loss on this task batch = 3.1991e-01, Meta loss averaged over last 500 steps = 3.2843e-01, PNorm = 107.0711, GNorm = 0.2017
Meta loss on this task batch = 3.6125e-01, Meta loss averaged over last 500 steps = 3.2850e-01, PNorm = 107.0799, GNorm = 0.2557
Meta loss on this task batch = 3.3491e-01, Meta loss averaged over last 500 steps = 3.2853e-01, PNorm = 107.0907, GNorm = 0.2328
Meta loss on this task batch = 3.0402e-01, Meta loss averaged over last 500 steps = 3.2841e-01, PNorm = 107.1018, GNorm = 0.1810
Meta loss on this task batch = 3.4579e-01, Meta loss averaged over last 500 steps = 3.2844e-01, PNorm = 107.1129, GNorm = 0.2293
Meta loss on this task batch = 2.5619e-01, Meta loss averaged over last 500 steps = 3.2818e-01, PNorm = 107.1235, GNorm = 0.1970
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 107.1333, GNorm = 0.2111
Meta loss on this task batch = 3.4533e-01, Meta loss averaged over last 500 steps = 3.2832e-01, PNorm = 107.1426, GNorm = 0.2368
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 3.2826e-01, PNorm = 107.1516, GNorm = 0.2034
Meta loss on this task batch = 3.6267e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 107.1593, GNorm = 0.2488
Meta loss on this task batch = 2.9516e-01, Meta loss averaged over last 500 steps = 3.2820e-01, PNorm = 107.1669, GNorm = 0.2189
Meta loss on this task batch = 3.6599e-01, Meta loss averaged over last 500 steps = 3.2829e-01, PNorm = 107.1745, GNorm = 0.2330
Meta loss on this task batch = 3.3858e-01, Meta loss averaged over last 500 steps = 3.2828e-01, PNorm = 107.1816, GNorm = 0.2294
Meta loss on this task batch = 3.0386e-01, Meta loss averaged over last 500 steps = 3.2838e-01, PNorm = 107.1888, GNorm = 0.2011
Meta loss on this task batch = 3.6653e-01, Meta loss averaged over last 500 steps = 3.2839e-01, PNorm = 107.1961, GNorm = 0.2248
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 3.2824e-01, PNorm = 107.2035, GNorm = 0.2123
Meta loss on this task batch = 3.1188e-01, Meta loss averaged over last 500 steps = 3.2823e-01, PNorm = 107.2125, GNorm = 0.2610
Took 114.32633566856384 seconds to complete one epoch of meta training
Took 122.25892448425293 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479417
Epoch 276
Meta loss on this task batch = 3.1765e-01, Meta loss averaged over last 500 steps = 3.2813e-01, PNorm = 107.2230, GNorm = 0.2105
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 3.2808e-01, PNorm = 107.2332, GNorm = 0.2049
Meta loss on this task batch = 3.1230e-01, Meta loss averaged over last 500 steps = 3.2799e-01, PNorm = 107.2432, GNorm = 0.2077
Meta loss on this task batch = 3.4924e-01, Meta loss averaged over last 500 steps = 3.2802e-01, PNorm = 107.2519, GNorm = 0.2590
Meta loss on this task batch = 3.5825e-01, Meta loss averaged over last 500 steps = 3.2804e-01, PNorm = 107.2601, GNorm = 0.2017
Meta loss on this task batch = 3.1413e-01, Meta loss averaged over last 500 steps = 3.2791e-01, PNorm = 107.2689, GNorm = 0.2227
Meta loss on this task batch = 3.6656e-01, Meta loss averaged over last 500 steps = 3.2796e-01, PNorm = 107.2761, GNorm = 0.2578
Meta loss on this task batch = 2.7234e-01, Meta loss averaged over last 500 steps = 3.2785e-01, PNorm = 107.2833, GNorm = 0.2103
Meta loss on this task batch = 3.1299e-01, Meta loss averaged over last 500 steps = 3.2779e-01, PNorm = 107.2907, GNorm = 0.2321
Meta loss on this task batch = 2.7583e-01, Meta loss averaged over last 500 steps = 3.2758e-01, PNorm = 107.2980, GNorm = 0.2314
Meta loss on this task batch = 3.5947e-01, Meta loss averaged over last 500 steps = 3.2770e-01, PNorm = 107.3060, GNorm = 0.2377
Meta loss on this task batch = 3.4477e-01, Meta loss averaged over last 500 steps = 3.2778e-01, PNorm = 107.3126, GNorm = 0.2391
Meta loss on this task batch = 3.1224e-01, Meta loss averaged over last 500 steps = 3.2774e-01, PNorm = 107.3194, GNorm = 0.2159
Meta loss on this task batch = 3.4118e-01, Meta loss averaged over last 500 steps = 3.2771e-01, PNorm = 107.3265, GNorm = 0.2362
Meta loss on this task batch = 3.3300e-01, Meta loss averaged over last 500 steps = 3.2772e-01, PNorm = 107.3332, GNorm = 0.2093
Meta loss on this task batch = 3.1757e-01, Meta loss averaged over last 500 steps = 3.2781e-01, PNorm = 107.3409, GNorm = 0.2082
Meta loss on this task batch = 3.4715e-01, Meta loss averaged over last 500 steps = 3.2786e-01, PNorm = 107.3489, GNorm = 0.2003
Meta loss on this task batch = 3.1865e-01, Meta loss averaged over last 500 steps = 3.2782e-01, PNorm = 107.3574, GNorm = 0.2433
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 3.2779e-01, PNorm = 107.3663, GNorm = 0.2568
Took 116.17161154747009 seconds to complete one epoch of meta training
Took 124.42026281356812 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507335
Epoch 277
Meta loss on this task batch = 3.4875e-01, Meta loss averaged over last 500 steps = 3.2776e-01, PNorm = 107.3757, GNorm = 0.2376
Meta loss on this task batch = 3.3313e-01, Meta loss averaged over last 500 steps = 3.2773e-01, PNorm = 107.3849, GNorm = 0.2374
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 107.3941, GNorm = 0.2130
Meta loss on this task batch = 3.4062e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 107.4025, GNorm = 0.2292
Meta loss on this task batch = 2.8807e-01, Meta loss averaged over last 500 steps = 3.2746e-01, PNorm = 107.4107, GNorm = 0.2042
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 3.2752e-01, PNorm = 107.4183, GNorm = 0.2327
Meta loss on this task batch = 3.5287e-01, Meta loss averaged over last 500 steps = 3.2762e-01, PNorm = 107.4247, GNorm = 0.2509
Meta loss on this task batch = 3.0949e-01, Meta loss averaged over last 500 steps = 3.2767e-01, PNorm = 107.4318, GNorm = 0.2343
Meta loss on this task batch = 2.9514e-01, Meta loss averaged over last 500 steps = 3.2758e-01, PNorm = 107.4403, GNorm = 0.2119
Meta loss on this task batch = 3.6029e-01, Meta loss averaged over last 500 steps = 3.2761e-01, PNorm = 107.4477, GNorm = 0.2324
Meta loss on this task batch = 3.0211e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 107.4553, GNorm = 0.2225
Meta loss on this task batch = 3.2400e-01, Meta loss averaged over last 500 steps = 3.2764e-01, PNorm = 107.4624, GNorm = 0.2326
Meta loss on this task batch = 4.2721e-01, Meta loss averaged over last 500 steps = 3.2786e-01, PNorm = 107.4685, GNorm = 0.2516
Meta loss on this task batch = 3.5114e-01, Meta loss averaged over last 500 steps = 3.2786e-01, PNorm = 107.4754, GNorm = 0.2149
Meta loss on this task batch = 3.3532e-01, Meta loss averaged over last 500 steps = 3.2790e-01, PNorm = 107.4826, GNorm = 0.2127
Meta loss on this task batch = 3.4477e-01, Meta loss averaged over last 500 steps = 3.2785e-01, PNorm = 107.4897, GNorm = 0.2285
Meta loss on this task batch = 2.8238e-01, Meta loss averaged over last 500 steps = 3.2768e-01, PNorm = 107.4974, GNorm = 0.1784
Meta loss on this task batch = 3.1236e-01, Meta loss averaged over last 500 steps = 3.2778e-01, PNorm = 107.5072, GNorm = 0.2167
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 3.2759e-01, PNorm = 107.5170, GNorm = 0.2461
Took 114.27952241897583 seconds to complete one epoch of meta training
Took 121.78840208053589 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464568
Epoch 278
Meta loss on this task batch = 2.8273e-01, Meta loss averaged over last 500 steps = 3.2756e-01, PNorm = 107.5278, GNorm = 0.1863
Meta loss on this task batch = 3.0701e-01, Meta loss averaged over last 500 steps = 3.2747e-01, PNorm = 107.5386, GNorm = 0.1816
Meta loss on this task batch = 3.2153e-01, Meta loss averaged over last 500 steps = 3.2742e-01, PNorm = 107.5498, GNorm = 0.2280
Meta loss on this task batch = 3.2815e-01, Meta loss averaged over last 500 steps = 3.2741e-01, PNorm = 107.5600, GNorm = 0.2182
Meta loss on this task batch = 4.0689e-01, Meta loss averaged over last 500 steps = 3.2756e-01, PNorm = 107.5679, GNorm = 0.2884
Meta loss on this task batch = 3.0227e-01, Meta loss averaged over last 500 steps = 3.2742e-01, PNorm = 107.5763, GNorm = 0.2219
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 107.5849, GNorm = 0.1867
Meta loss on this task batch = 2.9214e-01, Meta loss averaged over last 500 steps = 3.2720e-01, PNorm = 107.5924, GNorm = 0.2265
Meta loss on this task batch = 3.4624e-01, Meta loss averaged over last 500 steps = 3.2731e-01, PNorm = 107.5991, GNorm = 0.2387
Meta loss on this task batch = 3.3568e-01, Meta loss averaged over last 500 steps = 3.2735e-01, PNorm = 107.6054, GNorm = 0.2448
Meta loss on this task batch = 3.4837e-01, Meta loss averaged over last 500 steps = 3.2734e-01, PNorm = 107.6120, GNorm = 0.2402
Meta loss on this task batch = 2.7901e-01, Meta loss averaged over last 500 steps = 3.2725e-01, PNorm = 107.6198, GNorm = 0.2300
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 3.2715e-01, PNorm = 107.6281, GNorm = 0.1956
Meta loss on this task batch = 3.1914e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 107.6366, GNorm = 0.2279
Meta loss on this task batch = 3.7385e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 107.6442, GNorm = 0.2348
Meta loss on this task batch = 3.0580e-01, Meta loss averaged over last 500 steps = 3.2727e-01, PNorm = 107.6531, GNorm = 0.2173
Meta loss on this task batch = 3.5923e-01, Meta loss averaged over last 500 steps = 3.2742e-01, PNorm = 107.6620, GNorm = 0.2431
Meta loss on this task batch = 3.9602e-01, Meta loss averaged over last 500 steps = 3.2750e-01, PNorm = 107.6716, GNorm = 0.2410
Meta loss on this task batch = 3.4515e-01, Meta loss averaged over last 500 steps = 3.2756e-01, PNorm = 107.6808, GNorm = 0.2509
Took 112.46524477005005 seconds to complete one epoch of meta training
Took 119.20275521278381 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459142
Epoch 279
Meta loss on this task batch = 3.1212e-01, Meta loss averaged over last 500 steps = 3.2752e-01, PNorm = 107.6901, GNorm = 0.2244
Meta loss on this task batch = 2.4493e-01, Meta loss averaged over last 500 steps = 3.2741e-01, PNorm = 107.6996, GNorm = 0.1832
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 3.2719e-01, PNorm = 107.7091, GNorm = 0.2275
Meta loss on this task batch = 3.2077e-01, Meta loss averaged over last 500 steps = 3.2712e-01, PNorm = 107.7171, GNorm = 0.2271
Meta loss on this task batch = 3.2447e-01, Meta loss averaged over last 500 steps = 3.2713e-01, PNorm = 107.7239, GNorm = 0.2367
Meta loss on this task batch = 3.4157e-01, Meta loss averaged over last 500 steps = 3.2713e-01, PNorm = 107.7312, GNorm = 0.2147
Meta loss on this task batch = 3.9274e-01, Meta loss averaged over last 500 steps = 3.2726e-01, PNorm = 107.7368, GNorm = 0.2425
Meta loss on this task batch = 3.0140e-01, Meta loss averaged over last 500 steps = 3.2720e-01, PNorm = 107.7419, GNorm = 0.2082
Meta loss on this task batch = 2.9583e-01, Meta loss averaged over last 500 steps = 3.2723e-01, PNorm = 107.7487, GNorm = 0.2071
Meta loss on this task batch = 3.2326e-01, Meta loss averaged over last 500 steps = 3.2725e-01, PNorm = 107.7557, GNorm = 0.2050
Meta loss on this task batch = 2.8485e-01, Meta loss averaged over last 500 steps = 3.2714e-01, PNorm = 107.7637, GNorm = 0.2093
Meta loss on this task batch = 3.9580e-01, Meta loss averaged over last 500 steps = 3.2718e-01, PNorm = 107.7704, GNorm = 0.2410
Meta loss on this task batch = 3.3029e-01, Meta loss averaged over last 500 steps = 3.2720e-01, PNorm = 107.7783, GNorm = 0.2074
Meta loss on this task batch = 3.2995e-01, Meta loss averaged over last 500 steps = 3.2715e-01, PNorm = 107.7853, GNorm = 0.2330
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 3.2693e-01, PNorm = 107.7931, GNorm = 0.2012
Meta loss on this task batch = 3.5268e-01, Meta loss averaged over last 500 steps = 3.2701e-01, PNorm = 107.8003, GNorm = 0.2299
Meta loss on this task batch = 2.7462e-01, Meta loss averaged over last 500 steps = 3.2691e-01, PNorm = 107.8082, GNorm = 0.2237
Meta loss on this task batch = 3.5383e-01, Meta loss averaged over last 500 steps = 3.2694e-01, PNorm = 107.8158, GNorm = 0.2237
Meta loss on this task batch = 3.6870e-01, Meta loss averaged over last 500 steps = 3.2702e-01, PNorm = 107.8230, GNorm = 0.2665
Took 115.41648650169373 seconds to complete one epoch of meta training
Took 123.58030223846436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480655
Epoch 280
Meta loss on this task batch = 2.8667e-01, Meta loss averaged over last 500 steps = 3.2689e-01, PNorm = 107.8312, GNorm = 0.2192
Meta loss on this task batch = 2.8742e-01, Meta loss averaged over last 500 steps = 3.2680e-01, PNorm = 107.8387, GNorm = 0.2024
Meta loss on this task batch = 3.2498e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 107.8460, GNorm = 0.2272
Meta loss on this task batch = 3.2769e-01, Meta loss averaged over last 500 steps = 3.2683e-01, PNorm = 107.8533, GNorm = 0.2235
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 3.2676e-01, PNorm = 107.8621, GNorm = 0.1945
Meta loss on this task batch = 3.3252e-01, Meta loss averaged over last 500 steps = 3.2678e-01, PNorm = 107.8705, GNorm = 0.2130
Meta loss on this task batch = 2.6911e-01, Meta loss averaged over last 500 steps = 3.2676e-01, PNorm = 107.8783, GNorm = 0.2252
Meta loss on this task batch = 3.4807e-01, Meta loss averaged over last 500 steps = 3.2675e-01, PNorm = 107.8851, GNorm = 0.2247
Meta loss on this task batch = 3.2634e-01, Meta loss averaged over last 500 steps = 3.2673e-01, PNorm = 107.8923, GNorm = 0.2438
Meta loss on this task batch = 3.9908e-01, Meta loss averaged over last 500 steps = 3.2691e-01, PNorm = 107.8984, GNorm = 0.2661
Meta loss on this task batch = 3.5394e-01, Meta loss averaged over last 500 steps = 3.2695e-01, PNorm = 107.9053, GNorm = 0.2106
Meta loss on this task batch = 3.6103e-01, Meta loss averaged over last 500 steps = 3.2704e-01, PNorm = 107.9113, GNorm = 0.2156
Meta loss on this task batch = 3.0261e-01, Meta loss averaged over last 500 steps = 3.2703e-01, PNorm = 107.9175, GNorm = 0.2163
Meta loss on this task batch = 3.4946e-01, Meta loss averaged over last 500 steps = 3.2705e-01, PNorm = 107.9242, GNorm = 0.2473
Meta loss on this task batch = 3.7025e-01, Meta loss averaged over last 500 steps = 3.2707e-01, PNorm = 107.9315, GNorm = 0.2063
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 3.2702e-01, PNorm = 107.9394, GNorm = 0.2029
Meta loss on this task batch = 3.0273e-01, Meta loss averaged over last 500 steps = 3.2691e-01, PNorm = 107.9475, GNorm = 0.1991
Meta loss on this task batch = 3.4382e-01, Meta loss averaged over last 500 steps = 3.2701e-01, PNorm = 107.9560, GNorm = 0.1754
Meta loss on this task batch = 3.3482e-01, Meta loss averaged over last 500 steps = 3.2710e-01, PNorm = 107.9651, GNorm = 0.2347
Took 112.83696699142456 seconds to complete one epoch of meta training
Took 120.53734016418457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494277
Epoch 281
Meta loss on this task batch = 3.6345e-01, Meta loss averaged over last 500 steps = 3.2718e-01, PNorm = 107.9742, GNorm = 0.2057
Meta loss on this task batch = 2.7101e-01, Meta loss averaged over last 500 steps = 3.2706e-01, PNorm = 107.9847, GNorm = 0.1930
Meta loss on this task batch = 2.5886e-01, Meta loss averaged over last 500 steps = 3.2692e-01, PNorm = 107.9957, GNorm = 0.1970
Meta loss on this task batch = 3.7028e-01, Meta loss averaged over last 500 steps = 3.2701e-01, PNorm = 108.0057, GNorm = 0.2364
Meta loss on this task batch = 2.8684e-01, Meta loss averaged over last 500 steps = 3.2684e-01, PNorm = 108.0156, GNorm = 0.2034
Meta loss on this task batch = 3.3106e-01, Meta loss averaged over last 500 steps = 3.2689e-01, PNorm = 108.0252, GNorm = 0.2134
Meta loss on this task batch = 3.7121e-01, Meta loss averaged over last 500 steps = 3.2694e-01, PNorm = 108.0338, GNorm = 0.2576
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 108.0429, GNorm = 0.1989
Meta loss on this task batch = 2.6587e-01, Meta loss averaged over last 500 steps = 3.2657e-01, PNorm = 108.0531, GNorm = 0.2306
Meta loss on this task batch = 3.0193e-01, Meta loss averaged over last 500 steps = 3.2647e-01, PNorm = 108.0630, GNorm = 0.2651
Meta loss on this task batch = 3.3642e-01, Meta loss averaged over last 500 steps = 3.2653e-01, PNorm = 108.0717, GNorm = 0.2380
Meta loss on this task batch = 3.1124e-01, Meta loss averaged over last 500 steps = 3.2648e-01, PNorm = 108.0794, GNorm = 0.2119
Meta loss on this task batch = 3.1484e-01, Meta loss averaged over last 500 steps = 3.2640e-01, PNorm = 108.0866, GNorm = 0.2175
Meta loss on this task batch = 3.3216e-01, Meta loss averaged over last 500 steps = 3.2650e-01, PNorm = 108.0934, GNorm = 0.2374
Meta loss on this task batch = 3.5122e-01, Meta loss averaged over last 500 steps = 3.2658e-01, PNorm = 108.1002, GNorm = 0.2246
Meta loss on this task batch = 3.1273e-01, Meta loss averaged over last 500 steps = 3.2654e-01, PNorm = 108.1079, GNorm = 0.2413
Meta loss on this task batch = 3.3944e-01, Meta loss averaged over last 500 steps = 3.2657e-01, PNorm = 108.1154, GNorm = 0.2340
Meta loss on this task batch = 3.5235e-01, Meta loss averaged over last 500 steps = 3.2665e-01, PNorm = 108.1221, GNorm = 0.2269
Meta loss on this task batch = 3.3831e-01, Meta loss averaged over last 500 steps = 3.2674e-01, PNorm = 108.1290, GNorm = 0.2541
Took 113.90323090553284 seconds to complete one epoch of meta training
Took 121.81807470321655 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489870
Epoch 282
Meta loss on this task batch = 3.1599e-01, Meta loss averaged over last 500 steps = 3.2666e-01, PNorm = 108.1362, GNorm = 0.2333
Meta loss on this task batch = 3.3730e-01, Meta loss averaged over last 500 steps = 3.2665e-01, PNorm = 108.1433, GNorm = 0.1960
Meta loss on this task batch = 2.9641e-01, Meta loss averaged over last 500 steps = 3.2661e-01, PNorm = 108.1513, GNorm = 0.1924
Meta loss on this task batch = 3.0101e-01, Meta loss averaged over last 500 steps = 3.2660e-01, PNorm = 108.1605, GNorm = 0.2208
Meta loss on this task batch = 3.3322e-01, Meta loss averaged over last 500 steps = 3.2667e-01, PNorm = 108.1692, GNorm = 0.2276
Meta loss on this task batch = 3.3627e-01, Meta loss averaged over last 500 steps = 3.2688e-01, PNorm = 108.1776, GNorm = 0.2295
Meta loss on this task batch = 3.2125e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 108.1853, GNorm = 0.2002
Meta loss on this task batch = 3.4299e-01, Meta loss averaged over last 500 steps = 3.2691e-01, PNorm = 108.1931, GNorm = 0.2102
Meta loss on this task batch = 2.7434e-01, Meta loss averaged over last 500 steps = 3.2685e-01, PNorm = 108.2025, GNorm = 0.2171
Meta loss on this task batch = 3.5591e-01, Meta loss averaged over last 500 steps = 3.2695e-01, PNorm = 108.2119, GNorm = 0.2445
Meta loss on this task batch = 3.1514e-01, Meta loss averaged over last 500 steps = 3.2701e-01, PNorm = 108.2209, GNorm = 0.2148
Meta loss on this task batch = 3.2845e-01, Meta loss averaged over last 500 steps = 3.2705e-01, PNorm = 108.2291, GNorm = 0.2389
Meta loss on this task batch = 2.8325e-01, Meta loss averaged over last 500 steps = 3.2697e-01, PNorm = 108.2372, GNorm = 0.2033
Meta loss on this task batch = 3.3836e-01, Meta loss averaged over last 500 steps = 3.2690e-01, PNorm = 108.2443, GNorm = 0.2385
Meta loss on this task batch = 3.0500e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 108.2512, GNorm = 0.2423
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 3.2676e-01, PNorm = 108.2583, GNorm = 0.2016
Meta loss on this task batch = 3.1458e-01, Meta loss averaged over last 500 steps = 3.2671e-01, PNorm = 108.2660, GNorm = 0.2332
Meta loss on this task batch = 3.3123e-01, Meta loss averaged over last 500 steps = 3.2676e-01, PNorm = 108.2730, GNorm = 0.2195
Meta loss on this task batch = 3.3142e-01, Meta loss averaged over last 500 steps = 3.2676e-01, PNorm = 108.2810, GNorm = 0.2499
Took 111.86124396324158 seconds to complete one epoch of meta training
Took 119.91446185112 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506889
Epoch 283
Meta loss on this task batch = 3.6092e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 108.2875, GNorm = 0.2403
Meta loss on this task batch = 3.0935e-01, Meta loss averaged over last 500 steps = 3.2675e-01, PNorm = 108.2945, GNorm = 0.2089
Meta loss on this task batch = 3.3012e-01, Meta loss averaged over last 500 steps = 3.2670e-01, PNorm = 108.3025, GNorm = 0.2159
Meta loss on this task batch = 3.3758e-01, Meta loss averaged over last 500 steps = 3.2672e-01, PNorm = 108.3108, GNorm = 0.2291
Meta loss on this task batch = 2.9300e-01, Meta loss averaged over last 500 steps = 3.2659e-01, PNorm = 108.3192, GNorm = 0.2520
Meta loss on this task batch = 3.5034e-01, Meta loss averaged over last 500 steps = 3.2669e-01, PNorm = 108.3276, GNorm = 0.2252
Meta loss on this task batch = 3.6211e-01, Meta loss averaged over last 500 steps = 3.2679e-01, PNorm = 108.3356, GNorm = 0.2439
Meta loss on this task batch = 3.6334e-01, Meta loss averaged over last 500 steps = 3.2690e-01, PNorm = 108.3435, GNorm = 0.2182
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 3.2696e-01, PNorm = 108.3519, GNorm = 0.2094
Meta loss on this task batch = 3.5559e-01, Meta loss averaged over last 500 steps = 3.2706e-01, PNorm = 108.3596, GNorm = 0.2234
Meta loss on this task batch = 2.9580e-01, Meta loss averaged over last 500 steps = 3.2708e-01, PNorm = 108.3685, GNorm = 0.1979
Meta loss on this task batch = 3.4317e-01, Meta loss averaged over last 500 steps = 3.2718e-01, PNorm = 108.3772, GNorm = 0.2163
Meta loss on this task batch = 3.3052e-01, Meta loss averaged over last 500 steps = 3.2708e-01, PNorm = 108.3858, GNorm = 0.2116
Meta loss on this task batch = 3.6778e-01, Meta loss averaged over last 500 steps = 3.2723e-01, PNorm = 108.3937, GNorm = 0.2429
Meta loss on this task batch = 3.3165e-01, Meta loss averaged over last 500 steps = 3.2724e-01, PNorm = 108.4029, GNorm = 0.2438
Meta loss on this task batch = 2.8358e-01, Meta loss averaged over last 500 steps = 3.2722e-01, PNorm = 108.4122, GNorm = 0.1921
Meta loss on this task batch = 3.0660e-01, Meta loss averaged over last 500 steps = 3.2709e-01, PNorm = 108.4210, GNorm = 0.2550
Meta loss on this task batch = 3.4832e-01, Meta loss averaged over last 500 steps = 3.2703e-01, PNorm = 108.4285, GNorm = 0.2427
Meta loss on this task batch = 2.4905e-01, Meta loss averaged over last 500 steps = 3.2679e-01, PNorm = 108.4361, GNorm = 0.2427
Took 115.13807559013367 seconds to complete one epoch of meta training
Took 122.77747440338135 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493871
Epoch 284
Meta loss on this task batch = 3.2000e-01, Meta loss averaged over last 500 steps = 3.2670e-01, PNorm = 108.4434, GNorm = 0.2143
Meta loss on this task batch = 3.3367e-01, Meta loss averaged over last 500 steps = 3.2670e-01, PNorm = 108.4502, GNorm = 0.2095
Meta loss on this task batch = 3.4721e-01, Meta loss averaged over last 500 steps = 3.2669e-01, PNorm = 108.4562, GNorm = 0.2365
Meta loss on this task batch = 2.8086e-01, Meta loss averaged over last 500 steps = 3.2663e-01, PNorm = 108.4630, GNorm = 0.2445
Meta loss on this task batch = 3.9271e-01, Meta loss averaged over last 500 steps = 3.2670e-01, PNorm = 108.4700, GNorm = 0.2340
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 3.2674e-01, PNorm = 108.4774, GNorm = 0.2424
Meta loss on this task batch = 2.8324e-01, Meta loss averaged over last 500 steps = 3.2671e-01, PNorm = 108.4847, GNorm = 0.2437
Meta loss on this task batch = 3.4834e-01, Meta loss averaged over last 500 steps = 3.2676e-01, PNorm = 108.4912, GNorm = 0.2332
Meta loss on this task batch = 3.5801e-01, Meta loss averaged over last 500 steps = 3.2673e-01, PNorm = 108.4990, GNorm = 0.2362
Meta loss on this task batch = 3.8044e-01, Meta loss averaged over last 500 steps = 3.2682e-01, PNorm = 108.5069, GNorm = 0.2660
Meta loss on this task batch = 3.4085e-01, Meta loss averaged over last 500 steps = 3.2696e-01, PNorm = 108.5154, GNorm = 0.2184
Meta loss on this task batch = 3.4025e-01, Meta loss averaged over last 500 steps = 3.2698e-01, PNorm = 108.5246, GNorm = 0.2200
Meta loss on this task batch = 3.2086e-01, Meta loss averaged over last 500 steps = 3.2691e-01, PNorm = 108.5340, GNorm = 0.2003
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 3.2696e-01, PNorm = 108.5436, GNorm = 0.2064
Meta loss on this task batch = 2.7922e-01, Meta loss averaged over last 500 steps = 3.2688e-01, PNorm = 108.5521, GNorm = 0.2059
Meta loss on this task batch = 2.4825e-01, Meta loss averaged over last 500 steps = 3.2656e-01, PNorm = 108.5607, GNorm = 0.1821
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 3.2646e-01, PNorm = 108.5688, GNorm = 0.2342
Meta loss on this task batch = 2.7883e-01, Meta loss averaged over last 500 steps = 3.2647e-01, PNorm = 108.5761, GNorm = 0.2098
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.2639e-01, PNorm = 108.5832, GNorm = 0.2769
Took 113.93792796134949 seconds to complete one epoch of meta training
Took 122.06069803237915 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519421
Epoch 285
Meta loss on this task batch = 3.4275e-01, Meta loss averaged over last 500 steps = 3.2634e-01, PNorm = 108.5906, GNorm = 0.2389
Meta loss on this task batch = 2.7085e-01, Meta loss averaged over last 500 steps = 3.2621e-01, PNorm = 108.5988, GNorm = 0.1997
Meta loss on this task batch = 3.2380e-01, Meta loss averaged over last 500 steps = 3.2618e-01, PNorm = 108.6071, GNorm = 0.2453
Meta loss on this task batch = 3.4162e-01, Meta loss averaged over last 500 steps = 3.2617e-01, PNorm = 108.6157, GNorm = 0.2098
Meta loss on this task batch = 3.5817e-01, Meta loss averaged over last 500 steps = 3.2635e-01, PNorm = 108.6241, GNorm = 0.2244
Meta loss on this task batch = 3.1881e-01, Meta loss averaged over last 500 steps = 3.2637e-01, PNorm = 108.6326, GNorm = 0.2046
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 3.2634e-01, PNorm = 108.6406, GNorm = 0.1917
Meta loss on this task batch = 3.1440e-01, Meta loss averaged over last 500 steps = 3.2634e-01, PNorm = 108.6500, GNorm = 0.2163
Meta loss on this task batch = 3.7371e-01, Meta loss averaged over last 500 steps = 3.2642e-01, PNorm = 108.6594, GNorm = 0.2511
Meta loss on this task batch = 3.5938e-01, Meta loss averaged over last 500 steps = 3.2641e-01, PNorm = 108.6686, GNorm = 0.2455
Meta loss on this task batch = 3.1339e-01, Meta loss averaged over last 500 steps = 3.2623e-01, PNorm = 108.6778, GNorm = 0.1994
Meta loss on this task batch = 3.6991e-01, Meta loss averaged over last 500 steps = 3.2625e-01, PNorm = 108.6866, GNorm = 0.2601
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 3.2629e-01, PNorm = 108.6958, GNorm = 0.2127
Meta loss on this task batch = 3.2650e-01, Meta loss averaged over last 500 steps = 3.2625e-01, PNorm = 108.7062, GNorm = 0.2041
Meta loss on this task batch = 3.0571e-01, Meta loss averaged over last 500 steps = 3.2623e-01, PNorm = 108.7162, GNorm = 0.2700
Meta loss on this task batch = 2.8033e-01, Meta loss averaged over last 500 steps = 3.2603e-01, PNorm = 108.7269, GNorm = 0.1974
Meta loss on this task batch = 3.3790e-01, Meta loss averaged over last 500 steps = 3.2602e-01, PNorm = 108.7372, GNorm = 0.2203
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 3.2597e-01, PNorm = 108.7466, GNorm = 0.2532
Meta loss on this task batch = 2.9944e-01, Meta loss averaged over last 500 steps = 3.2590e-01, PNorm = 108.7566, GNorm = 0.2252
Took 114.19059228897095 seconds to complete one epoch of meta training
Took 121.93154859542847 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517164
Epoch 286
Meta loss on this task batch = 3.6406e-01, Meta loss averaged over last 500 steps = 3.2593e-01, PNorm = 108.7661, GNorm = 0.2186
Meta loss on this task batch = 2.8778e-01, Meta loss averaged over last 500 steps = 3.2586e-01, PNorm = 108.7755, GNorm = 0.1964
Meta loss on this task batch = 3.4636e-01, Meta loss averaged over last 500 steps = 3.2596e-01, PNorm = 108.7838, GNorm = 0.2286
Meta loss on this task batch = 4.2503e-01, Meta loss averaged over last 500 steps = 3.2612e-01, PNorm = 108.7894, GNorm = 0.2624
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 3.2611e-01, PNorm = 108.7956, GNorm = 0.2136
Meta loss on this task batch = 2.4384e-01, Meta loss averaged over last 500 steps = 3.2597e-01, PNorm = 108.8031, GNorm = 0.1616
Meta loss on this task batch = 3.8742e-01, Meta loss averaged over last 500 steps = 3.2614e-01, PNorm = 108.8096, GNorm = 0.2191
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 3.2596e-01, PNorm = 108.8166, GNorm = 0.2029
Meta loss on this task batch = 3.2954e-01, Meta loss averaged over last 500 steps = 3.2605e-01, PNorm = 108.8238, GNorm = 0.1960
Meta loss on this task batch = 3.0702e-01, Meta loss averaged over last 500 steps = 3.2594e-01, PNorm = 108.8320, GNorm = 0.2104
Meta loss on this task batch = 3.0089e-01, Meta loss averaged over last 500 steps = 3.2585e-01, PNorm = 108.8415, GNorm = 0.1980
Meta loss on this task batch = 3.1820e-01, Meta loss averaged over last 500 steps = 3.2591e-01, PNorm = 108.8518, GNorm = 0.1930
Meta loss on this task batch = 3.4410e-01, Meta loss averaged over last 500 steps = 3.2588e-01, PNorm = 108.8620, GNorm = 0.2145
Meta loss on this task batch = 3.9845e-01, Meta loss averaged over last 500 steps = 3.2605e-01, PNorm = 108.8721, GNorm = 0.2338
Meta loss on this task batch = 3.2078e-01, Meta loss averaged over last 500 steps = 3.2604e-01, PNorm = 108.8818, GNorm = 0.2166
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 3.2588e-01, PNorm = 108.8918, GNorm = 0.2330
Meta loss on this task batch = 3.3482e-01, Meta loss averaged over last 500 steps = 3.2597e-01, PNorm = 108.9005, GNorm = 0.2352
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 3.2571e-01, PNorm = 108.9104, GNorm = 0.2011
Meta loss on this task batch = 3.1540e-01, Meta loss averaged over last 500 steps = 3.2569e-01, PNorm = 108.9192, GNorm = 0.2703
Took 117.27840948104858 seconds to complete one epoch of meta training
Took 124.84687352180481 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507635
Epoch 287
Meta loss on this task batch = 3.3039e-01, Meta loss averaged over last 500 steps = 3.2571e-01, PNorm = 108.9272, GNorm = 0.2436
Meta loss on this task batch = 3.4098e-01, Meta loss averaged over last 500 steps = 3.2571e-01, PNorm = 108.9353, GNorm = 0.2105
Meta loss on this task batch = 3.1751e-01, Meta loss averaged over last 500 steps = 3.2576e-01, PNorm = 108.9437, GNorm = 0.2226
Meta loss on this task batch = 3.3654e-01, Meta loss averaged over last 500 steps = 3.2583e-01, PNorm = 108.9520, GNorm = 0.2027
Meta loss on this task batch = 3.0587e-01, Meta loss averaged over last 500 steps = 3.2587e-01, PNorm = 108.9598, GNorm = 0.2214
Meta loss on this task batch = 2.6986e-01, Meta loss averaged over last 500 steps = 3.2574e-01, PNorm = 108.9685, GNorm = 0.2178
Meta loss on this task batch = 2.7116e-01, Meta loss averaged over last 500 steps = 3.2566e-01, PNorm = 108.9759, GNorm = 0.2137
Meta loss on this task batch = 2.9259e-01, Meta loss averaged over last 500 steps = 3.2563e-01, PNorm = 108.9823, GNorm = 0.2108
Meta loss on this task batch = 3.0689e-01, Meta loss averaged over last 500 steps = 3.2562e-01, PNorm = 108.9877, GNorm = 0.2507
Meta loss on this task batch = 3.4613e-01, Meta loss averaged over last 500 steps = 3.2560e-01, PNorm = 108.9923, GNorm = 0.2219
Meta loss on this task batch = 3.0838e-01, Meta loss averaged over last 500 steps = 3.2549e-01, PNorm = 108.9976, GNorm = 0.2164
Meta loss on this task batch = 3.1672e-01, Meta loss averaged over last 500 steps = 3.2547e-01, PNorm = 109.0038, GNorm = 0.2490
Meta loss on this task batch = 3.4398e-01, Meta loss averaged over last 500 steps = 3.2548e-01, PNorm = 109.0102, GNorm = 0.2220
Meta loss on this task batch = 3.3152e-01, Meta loss averaged over last 500 steps = 3.2548e-01, PNorm = 109.0178, GNorm = 0.2232
Meta loss on this task batch = 3.1898e-01, Meta loss averaged over last 500 steps = 3.2551e-01, PNorm = 109.0260, GNorm = 0.2441
Meta loss on this task batch = 3.2803e-01, Meta loss averaged over last 500 steps = 3.2550e-01, PNorm = 109.0345, GNorm = 0.2016
Meta loss on this task batch = 3.0921e-01, Meta loss averaged over last 500 steps = 3.2546e-01, PNorm = 109.0433, GNorm = 0.2212
Meta loss on this task batch = 3.5915e-01, Meta loss averaged over last 500 steps = 3.2554e-01, PNorm = 109.0527, GNorm = 0.2507
Meta loss on this task batch = 3.7396e-01, Meta loss averaged over last 500 steps = 3.2562e-01, PNorm = 109.0623, GNorm = 0.2606
Took 114.73229122161865 seconds to complete one epoch of meta training
Took 122.17556428909302 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508390
Epoch 288
Meta loss on this task batch = 3.3963e-01, Meta loss averaged over last 500 steps = 3.2563e-01, PNorm = 109.0704, GNorm = 0.2275
Meta loss on this task batch = 2.8328e-01, Meta loss averaged over last 500 steps = 3.2559e-01, PNorm = 109.0786, GNorm = 0.2251
Meta loss on this task batch = 3.4737e-01, Meta loss averaged over last 500 steps = 3.2568e-01, PNorm = 109.0860, GNorm = 0.2091
Meta loss on this task batch = 3.3038e-01, Meta loss averaged over last 500 steps = 3.2559e-01, PNorm = 109.0923, GNorm = 0.2249
Meta loss on this task batch = 3.0177e-01, Meta loss averaged over last 500 steps = 3.2549e-01, PNorm = 109.0987, GNorm = 0.1929
Meta loss on this task batch = 2.7279e-01, Meta loss averaged over last 500 steps = 3.2530e-01, PNorm = 109.1056, GNorm = 0.2048
Meta loss on this task batch = 3.4627e-01, Meta loss averaged over last 500 steps = 3.2544e-01, PNorm = 109.1126, GNorm = 0.2665
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 3.2526e-01, PNorm = 109.1201, GNorm = 0.2718
Meta loss on this task batch = 3.1984e-01, Meta loss averaged over last 500 steps = 3.2528e-01, PNorm = 109.1273, GNorm = 0.2355
Meta loss on this task batch = 2.7798e-01, Meta loss averaged over last 500 steps = 3.2515e-01, PNorm = 109.1351, GNorm = 0.1983
Meta loss on this task batch = 3.0622e-01, Meta loss averaged over last 500 steps = 3.2508e-01, PNorm = 109.1433, GNorm = 0.2107
Meta loss on this task batch = 3.2491e-01, Meta loss averaged over last 500 steps = 3.2507e-01, PNorm = 109.1510, GNorm = 0.2438
Meta loss on this task batch = 3.7817e-01, Meta loss averaged over last 500 steps = 3.2520e-01, PNorm = 109.1578, GNorm = 0.2537
Meta loss on this task batch = 3.2755e-01, Meta loss averaged over last 500 steps = 3.2518e-01, PNorm = 109.1663, GNorm = 0.2719
Meta loss on this task batch = 3.7262e-01, Meta loss averaged over last 500 steps = 3.2523e-01, PNorm = 109.1747, GNorm = 0.2772
Meta loss on this task batch = 3.5899e-01, Meta loss averaged over last 500 steps = 3.2526e-01, PNorm = 109.1829, GNorm = 0.2407
Meta loss on this task batch = 2.9653e-01, Meta loss averaged over last 500 steps = 3.2520e-01, PNorm = 109.1916, GNorm = 0.2167
Meta loss on this task batch = 3.3363e-01, Meta loss averaged over last 500 steps = 3.2515e-01, PNorm = 109.2001, GNorm = 0.2012
Meta loss on this task batch = 3.0592e-01, Meta loss averaged over last 500 steps = 3.2507e-01, PNorm = 109.2080, GNorm = 0.2568
Took 112.24797582626343 seconds to complete one epoch of meta training
Took 119.65663480758667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504578
Epoch 289
Meta loss on this task batch = 3.4731e-01, Meta loss averaged over last 500 steps = 3.2510e-01, PNorm = 109.2152, GNorm = 0.2391
Meta loss on this task batch = 3.7994e-01, Meta loss averaged over last 500 steps = 3.2520e-01, PNorm = 109.2218, GNorm = 0.2623
Meta loss on this task batch = 2.9533e-01, Meta loss averaged over last 500 steps = 3.2504e-01, PNorm = 109.2298, GNorm = 0.2457
Meta loss on this task batch = 2.8767e-01, Meta loss averaged over last 500 steps = 3.2496e-01, PNorm = 109.2378, GNorm = 0.1892
Meta loss on this task batch = 3.3033e-01, Meta loss averaged over last 500 steps = 3.2488e-01, PNorm = 109.2460, GNorm = 0.1855
Meta loss on this task batch = 2.7724e-01, Meta loss averaged over last 500 steps = 3.2488e-01, PNorm = 109.2538, GNorm = 0.1918
Meta loss on this task batch = 3.1630e-01, Meta loss averaged over last 500 steps = 3.2469e-01, PNorm = 109.2619, GNorm = 0.1972
Meta loss on this task batch = 3.7856e-01, Meta loss averaged over last 500 steps = 3.2476e-01, PNorm = 109.2694, GNorm = 0.2339
Meta loss on this task batch = 2.8795e-01, Meta loss averaged over last 500 steps = 3.2465e-01, PNorm = 109.2768, GNorm = 0.2205
Meta loss on this task batch = 3.5348e-01, Meta loss averaged over last 500 steps = 3.2465e-01, PNorm = 109.2833, GNorm = 0.2386
Meta loss on this task batch = 2.9763e-01, Meta loss averaged over last 500 steps = 3.2465e-01, PNorm = 109.2909, GNorm = 0.2047
Meta loss on this task batch = 3.8186e-01, Meta loss averaged over last 500 steps = 3.2472e-01, PNorm = 109.2983, GNorm = 0.2590
Meta loss on this task batch = 2.8782e-01, Meta loss averaged over last 500 steps = 3.2476e-01, PNorm = 109.3061, GNorm = 0.2188
Meta loss on this task batch = 3.5211e-01, Meta loss averaged over last 500 steps = 3.2486e-01, PNorm = 109.3131, GNorm = 0.2289
Meta loss on this task batch = 3.0543e-01, Meta loss averaged over last 500 steps = 3.2480e-01, PNorm = 109.3214, GNorm = 0.2137
Meta loss on this task batch = 2.8169e-01, Meta loss averaged over last 500 steps = 3.2471e-01, PNorm = 109.3310, GNorm = 0.2297
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 3.2465e-01, PNorm = 109.3409, GNorm = 0.2425
Meta loss on this task batch = 3.2821e-01, Meta loss averaged over last 500 steps = 3.2452e-01, PNorm = 109.3517, GNorm = 0.2867
Meta loss on this task batch = 2.4227e-01, Meta loss averaged over last 500 steps = 3.2438e-01, PNorm = 109.3626, GNorm = 0.2451
Took 113.26323580741882 seconds to complete one epoch of meta training
Took 120.96904611587524 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497627
Epoch 290
Meta loss on this task batch = 3.4030e-01, Meta loss averaged over last 500 steps = 3.2447e-01, PNorm = 109.3719, GNorm = 0.2384
Meta loss on this task batch = 3.0201e-01, Meta loss averaged over last 500 steps = 3.2448e-01, PNorm = 109.3800, GNorm = 0.2184
Meta loss on this task batch = 3.0146e-01, Meta loss averaged over last 500 steps = 3.2445e-01, PNorm = 109.3871, GNorm = 0.1890
Meta loss on this task batch = 2.9612e-01, Meta loss averaged over last 500 steps = 3.2438e-01, PNorm = 109.3934, GNorm = 0.2126
Meta loss on this task batch = 3.0777e-01, Meta loss averaged over last 500 steps = 3.2433e-01, PNorm = 109.3981, GNorm = 0.2723
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 3.2426e-01, PNorm = 109.4034, GNorm = 0.2002
Meta loss on this task batch = 3.2404e-01, Meta loss averaged over last 500 steps = 3.2430e-01, PNorm = 109.4081, GNorm = 0.2386
Meta loss on this task batch = 3.5398e-01, Meta loss averaged over last 500 steps = 3.2436e-01, PNorm = 109.4131, GNorm = 0.2141
Meta loss on this task batch = 3.2767e-01, Meta loss averaged over last 500 steps = 3.2444e-01, PNorm = 109.4181, GNorm = 0.2480
Meta loss on this task batch = 3.1604e-01, Meta loss averaged over last 500 steps = 3.2439e-01, PNorm = 109.4243, GNorm = 0.2130
Meta loss on this task batch = 3.5162e-01, Meta loss averaged over last 500 steps = 3.2444e-01, PNorm = 109.4309, GNorm = 0.2139
Meta loss on this task batch = 2.4819e-01, Meta loss averaged over last 500 steps = 3.2430e-01, PNorm = 109.4386, GNorm = 0.1972
Meta loss on this task batch = 3.7491e-01, Meta loss averaged over last 500 steps = 3.2439e-01, PNorm = 109.4454, GNorm = 0.2290
Meta loss on this task batch = 3.7310e-01, Meta loss averaged over last 500 steps = 3.2447e-01, PNorm = 109.4528, GNorm = 0.2459
Meta loss on this task batch = 3.1820e-01, Meta loss averaged over last 500 steps = 3.2435e-01, PNorm = 109.4603, GNorm = 0.2247
Meta loss on this task batch = 2.9613e-01, Meta loss averaged over last 500 steps = 3.2437e-01, PNorm = 109.4688, GNorm = 0.2113
Meta loss on this task batch = 3.2069e-01, Meta loss averaged over last 500 steps = 3.2437e-01, PNorm = 109.4767, GNorm = 0.2564
Meta loss on this task batch = 3.5389e-01, Meta loss averaged over last 500 steps = 3.2438e-01, PNorm = 109.4835, GNorm = 0.2299
Meta loss on this task batch = 3.3825e-01, Meta loss averaged over last 500 steps = 3.2431e-01, PNorm = 109.4903, GNorm = 0.2683
Took 114.71708297729492 seconds to complete one epoch of meta training
Took 122.64148569107056 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505069
Epoch 291
Meta loss on this task batch = 3.4373e-01, Meta loss averaged over last 500 steps = 3.2436e-01, PNorm = 109.4966, GNorm = 0.2335
Meta loss on this task batch = 3.1400e-01, Meta loss averaged over last 500 steps = 3.2438e-01, PNorm = 109.5033, GNorm = 0.1969
Meta loss on this task batch = 2.9230e-01, Meta loss averaged over last 500 steps = 3.2424e-01, PNorm = 109.5107, GNorm = 0.1868
Meta loss on this task batch = 3.3198e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 109.5189, GNorm = 0.2102
Meta loss on this task batch = 3.3099e-01, Meta loss averaged over last 500 steps = 3.2424e-01, PNorm = 109.5274, GNorm = 0.2366
Meta loss on this task batch = 3.3397e-01, Meta loss averaged over last 500 steps = 3.2429e-01, PNorm = 109.5359, GNorm = 0.2266
Meta loss on this task batch = 3.3147e-01, Meta loss averaged over last 500 steps = 3.2439e-01, PNorm = 109.5444, GNorm = 0.2154
Meta loss on this task batch = 3.0164e-01, Meta loss averaged over last 500 steps = 3.2435e-01, PNorm = 109.5536, GNorm = 0.1929
Meta loss on this task batch = 3.1162e-01, Meta loss averaged over last 500 steps = 3.2431e-01, PNorm = 109.5621, GNorm = 0.2031
Meta loss on this task batch = 2.6414e-01, Meta loss averaged over last 500 steps = 3.2411e-01, PNorm = 109.5713, GNorm = 0.1879
Meta loss on this task batch = 3.4462e-01, Meta loss averaged over last 500 steps = 3.2411e-01, PNorm = 109.5806, GNorm = 0.2540
Meta loss on this task batch = 3.1670e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 109.5903, GNorm = 0.1925
Meta loss on this task batch = 3.0274e-01, Meta loss averaged over last 500 steps = 3.2408e-01, PNorm = 109.5998, GNorm = 0.2090
Meta loss on this task batch = 3.2666e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 109.6092, GNorm = 0.2165
Meta loss on this task batch = 3.2043e-01, Meta loss averaged over last 500 steps = 3.2397e-01, PNorm = 109.6193, GNorm = 0.2056
Meta loss on this task batch = 3.1048e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 109.6283, GNorm = 0.2214
Meta loss on this task batch = 3.1099e-01, Meta loss averaged over last 500 steps = 3.2401e-01, PNorm = 109.6376, GNorm = 0.2060
Meta loss on this task batch = 3.3668e-01, Meta loss averaged over last 500 steps = 3.2396e-01, PNorm = 109.6471, GNorm = 0.2361
Meta loss on this task batch = 3.5451e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 109.6556, GNorm = 0.2609
Took 113.82703852653503 seconds to complete one epoch of meta training
Took 121.45190024375916 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499155
Epoch 292
Meta loss on this task batch = 3.2851e-01, Meta loss averaged over last 500 steps = 3.2411e-01, PNorm = 109.6643, GNorm = 0.2295
Meta loss on this task batch = 3.1554e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 109.6735, GNorm = 0.2443
Meta loss on this task batch = 3.4648e-01, Meta loss averaged over last 500 steps = 3.2417e-01, PNorm = 109.6815, GNorm = 0.2098
Meta loss on this task batch = 3.2241e-01, Meta loss averaged over last 500 steps = 3.2410e-01, PNorm = 109.6900, GNorm = 0.2034
Meta loss on this task batch = 3.0148e-01, Meta loss averaged over last 500 steps = 3.2413e-01, PNorm = 109.6985, GNorm = 0.2035
Meta loss on this task batch = 2.9701e-01, Meta loss averaged over last 500 steps = 3.2405e-01, PNorm = 109.7072, GNorm = 0.1714
Meta loss on this task batch = 3.4005e-01, Meta loss averaged over last 500 steps = 3.2396e-01, PNorm = 109.7159, GNorm = 0.2358
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 109.7240, GNorm = 0.2191
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 3.2387e-01, PNorm = 109.7331, GNorm = 0.2033
Meta loss on this task batch = 3.2887e-01, Meta loss averaged over last 500 steps = 3.2386e-01, PNorm = 109.7416, GNorm = 0.2310
Meta loss on this task batch = 3.5484e-01, Meta loss averaged over last 500 steps = 3.2383e-01, PNorm = 109.7497, GNorm = 0.2052
Meta loss on this task batch = 3.0640e-01, Meta loss averaged over last 500 steps = 3.2385e-01, PNorm = 109.7581, GNorm = 0.2098
Meta loss on this task batch = 3.0448e-01, Meta loss averaged over last 500 steps = 3.2388e-01, PNorm = 109.7669, GNorm = 0.2050
Meta loss on this task batch = 3.5753e-01, Meta loss averaged over last 500 steps = 3.2397e-01, PNorm = 109.7756, GNorm = 0.2164
Meta loss on this task batch = 3.4398e-01, Meta loss averaged over last 500 steps = 3.2405e-01, PNorm = 109.7843, GNorm = 0.1910
Meta loss on this task batch = 2.9003e-01, Meta loss averaged over last 500 steps = 3.2394e-01, PNorm = 109.7937, GNorm = 0.2010
Meta loss on this task batch = 3.1052e-01, Meta loss averaged over last 500 steps = 3.2391e-01, PNorm = 109.8031, GNorm = 0.2045
Meta loss on this task batch = 3.1444e-01, Meta loss averaged over last 500 steps = 3.2403e-01, PNorm = 109.8107, GNorm = 0.2397
Meta loss on this task batch = 3.7842e-01, Meta loss averaged over last 500 steps = 3.2418e-01, PNorm = 109.8181, GNorm = 0.2734
Took 113.89669132232666 seconds to complete one epoch of meta training
Took 121.71924781799316 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479494
Epoch 293
Meta loss on this task batch = 3.3587e-01, Meta loss averaged over last 500 steps = 3.2420e-01, PNorm = 109.8251, GNorm = 0.2552
Meta loss on this task batch = 2.9136e-01, Meta loss averaged over last 500 steps = 3.2404e-01, PNorm = 109.8322, GNorm = 0.1986
Meta loss on this task batch = 3.3104e-01, Meta loss averaged over last 500 steps = 3.2401e-01, PNorm = 109.8389, GNorm = 0.2379
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 109.8455, GNorm = 0.2078
Meta loss on this task batch = 3.2316e-01, Meta loss averaged over last 500 steps = 3.2412e-01, PNorm = 109.8522, GNorm = 0.1979
Meta loss on this task batch = 3.8337e-01, Meta loss averaged over last 500 steps = 3.2423e-01, PNorm = 109.8586, GNorm = 0.2237
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 3.2410e-01, PNorm = 109.8661, GNorm = 0.2005
Meta loss on this task batch = 2.4009e-01, Meta loss averaged over last 500 steps = 3.2390e-01, PNorm = 109.8759, GNorm = 0.2153
Meta loss on this task batch = 2.8094e-01, Meta loss averaged over last 500 steps = 3.2382e-01, PNorm = 109.8859, GNorm = 0.2004
Meta loss on this task batch = 3.7944e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 109.8941, GNorm = 0.2630
Meta loss on this task batch = 3.4806e-01, Meta loss averaged over last 500 steps = 3.2399e-01, PNorm = 109.9005, GNorm = 0.2885
Meta loss on this task batch = 3.4402e-01, Meta loss averaged over last 500 steps = 3.2398e-01, PNorm = 109.9061, GNorm = 0.2701
Meta loss on this task batch = 3.2272e-01, Meta loss averaged over last 500 steps = 3.2395e-01, PNorm = 109.9115, GNorm = 0.2424
Meta loss on this task batch = 3.0884e-01, Meta loss averaged over last 500 steps = 3.2392e-01, PNorm = 109.9173, GNorm = 0.2079
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 3.2373e-01, PNorm = 109.9228, GNorm = 0.2325
Meta loss on this task batch = 3.5275e-01, Meta loss averaged over last 500 steps = 3.2388e-01, PNorm = 109.9291, GNorm = 0.2444
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 3.2387e-01, PNorm = 109.9355, GNorm = 0.2438
Meta loss on this task batch = 3.9439e-01, Meta loss averaged over last 500 steps = 3.2403e-01, PNorm = 109.9405, GNorm = 0.2578
Meta loss on this task batch = 3.2444e-01, Meta loss averaged over last 500 steps = 3.2406e-01, PNorm = 109.9453, GNorm = 0.2577
Took 114.66349864006042 seconds to complete one epoch of meta training
Took 122.44400978088379 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472902
Epoch 294
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 3.2393e-01, PNorm = 109.9511, GNorm = 0.1965
Meta loss on this task batch = 3.0205e-01, Meta loss averaged over last 500 steps = 3.2385e-01, PNorm = 109.9585, GNorm = 0.1970
Meta loss on this task batch = 3.2369e-01, Meta loss averaged over last 500 steps = 3.2367e-01, PNorm = 109.9666, GNorm = 0.2317
Meta loss on this task batch = 3.0705e-01, Meta loss averaged over last 500 steps = 3.2365e-01, PNorm = 109.9756, GNorm = 0.2064
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 3.2367e-01, PNorm = 109.9842, GNorm = 0.2257
Meta loss on this task batch = 3.2241e-01, Meta loss averaged over last 500 steps = 3.2375e-01, PNorm = 109.9924, GNorm = 0.2312
Meta loss on this task batch = 3.2108e-01, Meta loss averaged over last 500 steps = 3.2375e-01, PNorm = 110.0012, GNorm = 0.1828
Meta loss on this task batch = 3.1780e-01, Meta loss averaged over last 500 steps = 3.2365e-01, PNorm = 110.0092, GNorm = 0.2343
Meta loss on this task batch = 3.1629e-01, Meta loss averaged over last 500 steps = 3.2369e-01, PNorm = 110.0168, GNorm = 0.2115
Meta loss on this task batch = 3.1528e-01, Meta loss averaged over last 500 steps = 3.2369e-01, PNorm = 110.0243, GNorm = 0.2183
Meta loss on this task batch = 3.5832e-01, Meta loss averaged over last 500 steps = 3.2373e-01, PNorm = 110.0313, GNorm = 0.2371
Meta loss on this task batch = 3.1367e-01, Meta loss averaged over last 500 steps = 3.2376e-01, PNorm = 110.0392, GNorm = 0.2262
Meta loss on this task batch = 3.2491e-01, Meta loss averaged over last 500 steps = 3.2384e-01, PNorm = 110.0463, GNorm = 0.2278
Meta loss on this task batch = 2.9201e-01, Meta loss averaged over last 500 steps = 3.2373e-01, PNorm = 110.0548, GNorm = 0.2202
Meta loss on this task batch = 3.0397e-01, Meta loss averaged over last 500 steps = 3.2364e-01, PNorm = 110.0633, GNorm = 0.2083
Meta loss on this task batch = 3.1600e-01, Meta loss averaged over last 500 steps = 3.2359e-01, PNorm = 110.0716, GNorm = 0.2402
Meta loss on this task batch = 3.8116e-01, Meta loss averaged over last 500 steps = 3.2374e-01, PNorm = 110.0785, GNorm = 0.2745
Meta loss on this task batch = 3.5369e-01, Meta loss averaged over last 500 steps = 3.2382e-01, PNorm = 110.0852, GNorm = 0.2346
Meta loss on this task batch = 3.4182e-01, Meta loss averaged over last 500 steps = 3.2385e-01, PNorm = 110.0913, GNorm = 0.3048
Took 115.68905353546143 seconds to complete one epoch of meta training
Took 122.72235870361328 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468323
Epoch 295
Meta loss on this task batch = 2.8765e-01, Meta loss averaged over last 500 steps = 3.2376e-01, PNorm = 110.0974, GNorm = 0.2298
Meta loss on this task batch = 2.7808e-01, Meta loss averaged over last 500 steps = 3.2363e-01, PNorm = 110.1047, GNorm = 0.1985
Meta loss on this task batch = 3.7785e-01, Meta loss averaged over last 500 steps = 3.2365e-01, PNorm = 110.1115, GNorm = 0.2570
Meta loss on this task batch = 2.9981e-01, Meta loss averaged over last 500 steps = 3.2361e-01, PNorm = 110.1192, GNorm = 0.1907
Meta loss on this task batch = 3.0991e-01, Meta loss averaged over last 500 steps = 3.2350e-01, PNorm = 110.1282, GNorm = 0.1958
Meta loss on this task batch = 4.1460e-01, Meta loss averaged over last 500 steps = 3.2352e-01, PNorm = 110.1362, GNorm = 0.2322
Meta loss on this task batch = 3.7032e-01, Meta loss averaged over last 500 steps = 3.2365e-01, PNorm = 110.1447, GNorm = 0.2221
Meta loss on this task batch = 2.8967e-01, Meta loss averaged over last 500 steps = 3.2343e-01, PNorm = 110.1532, GNorm = 0.2365
Meta loss on this task batch = 3.9169e-01, Meta loss averaged over last 500 steps = 3.2348e-01, PNorm = 110.1614, GNorm = 0.2587
Meta loss on this task batch = 2.9096e-01, Meta loss averaged over last 500 steps = 3.2351e-01, PNorm = 110.1700, GNorm = 0.1922
Meta loss on this task batch = 2.5906e-01, Meta loss averaged over last 500 steps = 3.2337e-01, PNorm = 110.1793, GNorm = 0.1775
Meta loss on this task batch = 2.8631e-01, Meta loss averaged over last 500 steps = 3.2334e-01, PNorm = 110.1880, GNorm = 0.1997
Meta loss on this task batch = 2.6699e-01, Meta loss averaged over last 500 steps = 3.2313e-01, PNorm = 110.1956, GNorm = 0.2401
Meta loss on this task batch = 3.3794e-01, Meta loss averaged over last 500 steps = 3.2313e-01, PNorm = 110.2039, GNorm = 0.2125
Meta loss on this task batch = 3.4589e-01, Meta loss averaged over last 500 steps = 3.2309e-01, PNorm = 110.2117, GNorm = 0.1992
Meta loss on this task batch = 3.2567e-01, Meta loss averaged over last 500 steps = 3.2311e-01, PNorm = 110.2191, GNorm = 0.2052
Meta loss on this task batch = 3.3448e-01, Meta loss averaged over last 500 steps = 3.2303e-01, PNorm = 110.2265, GNorm = 0.2188
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.2309e-01, PNorm = 110.2345, GNorm = 0.1938
Meta loss on this task batch = 2.4974e-01, Meta loss averaged over last 500 steps = 3.2308e-01, PNorm = 110.2440, GNorm = 0.2273
Took 114.19974684715271 seconds to complete one epoch of meta training
Took 122.037264585495 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475199
Epoch 296
Meta loss on this task batch = 3.1876e-01, Meta loss averaged over last 500 steps = 3.2309e-01, PNorm = 110.2527, GNorm = 0.2383
Meta loss on this task batch = 2.4506e-01, Meta loss averaged over last 500 steps = 3.2288e-01, PNorm = 110.2617, GNorm = 0.1820
Meta loss on this task batch = 2.9254e-01, Meta loss averaged over last 500 steps = 3.2279e-01, PNorm = 110.2705, GNorm = 0.2327
Meta loss on this task batch = 3.3102e-01, Meta loss averaged over last 500 steps = 3.2295e-01, PNorm = 110.2788, GNorm = 0.2340
Meta loss on this task batch = 3.8645e-01, Meta loss averaged over last 500 steps = 3.2305e-01, PNorm = 110.2863, GNorm = 0.2793
Meta loss on this task batch = 3.2372e-01, Meta loss averaged over last 500 steps = 3.2300e-01, PNorm = 110.2928, GNorm = 0.2339
Meta loss on this task batch = 3.3343e-01, Meta loss averaged over last 500 steps = 3.2303e-01, PNorm = 110.2980, GNorm = 0.2520
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 3.2296e-01, PNorm = 110.3034, GNorm = 0.2036
Meta loss on this task batch = 3.1558e-01, Meta loss averaged over last 500 steps = 3.2288e-01, PNorm = 110.3098, GNorm = 0.2001
Meta loss on this task batch = 3.1733e-01, Meta loss averaged over last 500 steps = 3.2281e-01, PNorm = 110.3165, GNorm = 0.1938
Meta loss on this task batch = 3.2432e-01, Meta loss averaged over last 500 steps = 3.2290e-01, PNorm = 110.3226, GNorm = 0.2784
Meta loss on this task batch = 3.2851e-01, Meta loss averaged over last 500 steps = 3.2285e-01, PNorm = 110.3296, GNorm = 0.2232
Meta loss on this task batch = 3.1008e-01, Meta loss averaged over last 500 steps = 3.2281e-01, PNorm = 110.3377, GNorm = 0.2252
Meta loss on this task batch = 2.8363e-01, Meta loss averaged over last 500 steps = 3.2273e-01, PNorm = 110.3465, GNorm = 0.2250
Meta loss on this task batch = 3.8540e-01, Meta loss averaged over last 500 steps = 3.2287e-01, PNorm = 110.3547, GNorm = 0.2279
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 3.2279e-01, PNorm = 110.3638, GNorm = 0.1937
Meta loss on this task batch = 3.0089e-01, Meta loss averaged over last 500 steps = 3.2270e-01, PNorm = 110.3733, GNorm = 0.2016
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 3.2268e-01, PNorm = 110.3823, GNorm = 0.1921
Meta loss on this task batch = 3.4164e-01, Meta loss averaged over last 500 steps = 3.2267e-01, PNorm = 110.3905, GNorm = 0.2512
Took 113.85444211959839 seconds to complete one epoch of meta training
Took 121.67348599433899 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502141
Epoch 297
Meta loss on this task batch = 2.7788e-01, Meta loss averaged over last 500 steps = 3.2265e-01, PNorm = 110.3988, GNorm = 0.2134
Meta loss on this task batch = 3.2152e-01, Meta loss averaged over last 500 steps = 3.2273e-01, PNorm = 110.4070, GNorm = 0.2052
Meta loss on this task batch = 2.6748e-01, Meta loss averaged over last 500 steps = 3.2262e-01, PNorm = 110.4152, GNorm = 0.2052
Meta loss on this task batch = 3.8200e-01, Meta loss averaged over last 500 steps = 3.2273e-01, PNorm = 110.4224, GNorm = 0.3081
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 3.2270e-01, PNorm = 110.4289, GNorm = 0.2430
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 3.2280e-01, PNorm = 110.4357, GNorm = 0.2245
Meta loss on this task batch = 3.0385e-01, Meta loss averaged over last 500 steps = 3.2282e-01, PNorm = 110.4432, GNorm = 0.2077
Meta loss on this task batch = 3.1860e-01, Meta loss averaged over last 500 steps = 3.2282e-01, PNorm = 110.4505, GNorm = 0.2338
Meta loss on this task batch = 3.6995e-01, Meta loss averaged over last 500 steps = 3.2302e-01, PNorm = 110.4591, GNorm = 0.2953
Meta loss on this task batch = 3.2550e-01, Meta loss averaged over last 500 steps = 3.2303e-01, PNorm = 110.4682, GNorm = 0.2046
Meta loss on this task batch = 3.4720e-01, Meta loss averaged over last 500 steps = 3.2299e-01, PNorm = 110.4764, GNorm = 0.2488
Meta loss on this task batch = 3.1933e-01, Meta loss averaged over last 500 steps = 3.2305e-01, PNorm = 110.4851, GNorm = 0.2119
Meta loss on this task batch = 3.4030e-01, Meta loss averaged over last 500 steps = 3.2296e-01, PNorm = 110.4922, GNorm = 0.2160
Meta loss on this task batch = 2.8640e-01, Meta loss averaged over last 500 steps = 3.2288e-01, PNorm = 110.4999, GNorm = 0.2118
Meta loss on this task batch = 3.4536e-01, Meta loss averaged over last 500 steps = 3.2286e-01, PNorm = 110.5074, GNorm = 0.2147
Meta loss on this task batch = 2.8453e-01, Meta loss averaged over last 500 steps = 3.2276e-01, PNorm = 110.5150, GNorm = 0.2149
Meta loss on this task batch = 2.8695e-01, Meta loss averaged over last 500 steps = 3.2259e-01, PNorm = 110.5227, GNorm = 0.2263
Meta loss on this task batch = 3.4263e-01, Meta loss averaged over last 500 steps = 3.2260e-01, PNorm = 110.5310, GNorm = 0.2310
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 3.2259e-01, PNorm = 110.5403, GNorm = 0.2738
Took 115.19566607475281 seconds to complete one epoch of meta training
Took 123.50065326690674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511691
Epoch 298
Meta loss on this task batch = 4.0643e-01, Meta loss averaged over last 500 steps = 3.2278e-01, PNorm = 110.5486, GNorm = 0.2285
Meta loss on this task batch = 2.8105e-01, Meta loss averaged over last 500 steps = 3.2272e-01, PNorm = 110.5569, GNorm = 0.1741
Meta loss on this task batch = 3.1392e-01, Meta loss averaged over last 500 steps = 3.2268e-01, PNorm = 110.5652, GNorm = 0.2553
Meta loss on this task batch = 3.2522e-01, Meta loss averaged over last 500 steps = 3.2266e-01, PNorm = 110.5732, GNorm = 0.2120
Meta loss on this task batch = 3.1548e-01, Meta loss averaged over last 500 steps = 3.2269e-01, PNorm = 110.5814, GNorm = 0.1841
Meta loss on this task batch = 3.0921e-01, Meta loss averaged over last 500 steps = 3.2265e-01, PNorm = 110.5889, GNorm = 0.2244
Meta loss on this task batch = 3.3002e-01, Meta loss averaged over last 500 steps = 3.2267e-01, PNorm = 110.5951, GNorm = 0.2199
Meta loss on this task batch = 3.4477e-01, Meta loss averaged over last 500 steps = 3.2274e-01, PNorm = 110.6004, GNorm = 0.2781
Meta loss on this task batch = 2.9672e-01, Meta loss averaged over last 500 steps = 3.2272e-01, PNorm = 110.6072, GNorm = 0.2182
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 3.2264e-01, PNorm = 110.6143, GNorm = 0.2509
Meta loss on this task batch = 3.1419e-01, Meta loss averaged over last 500 steps = 3.2261e-01, PNorm = 110.6221, GNorm = 0.2318
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 3.2257e-01, PNorm = 110.6294, GNorm = 0.1991
Meta loss on this task batch = 2.6537e-01, Meta loss averaged over last 500 steps = 3.2230e-01, PNorm = 110.6379, GNorm = 0.2019
Meta loss on this task batch = 3.6342e-01, Meta loss averaged over last 500 steps = 3.2234e-01, PNorm = 110.6474, GNorm = 0.2182
Meta loss on this task batch = 3.3595e-01, Meta loss averaged over last 500 steps = 3.2246e-01, PNorm = 110.6561, GNorm = 0.2134
Meta loss on this task batch = 2.6996e-01, Meta loss averaged over last 500 steps = 3.2243e-01, PNorm = 110.6646, GNorm = 0.2154
Meta loss on this task batch = 2.6269e-01, Meta loss averaged over last 500 steps = 3.2230e-01, PNorm = 110.6735, GNorm = 0.2196
Meta loss on this task batch = 3.2665e-01, Meta loss averaged over last 500 steps = 3.2238e-01, PNorm = 110.6827, GNorm = 0.2228
Meta loss on this task batch = 3.3121e-01, Meta loss averaged over last 500 steps = 3.2227e-01, PNorm = 110.6910, GNorm = 0.2685
Took 116.6731526851654 seconds to complete one epoch of meta training
Took 123.39346432685852 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494563
Epoch 299
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 3.2220e-01, PNorm = 110.6993, GNorm = 0.2544
Meta loss on this task batch = 2.7675e-01, Meta loss averaged over last 500 steps = 3.2207e-01, PNorm = 110.7066, GNorm = 0.2174
Meta loss on this task batch = 3.1790e-01, Meta loss averaged over last 500 steps = 3.2212e-01, PNorm = 110.7130, GNorm = 0.1965
Meta loss on this task batch = 3.0585e-01, Meta loss averaged over last 500 steps = 3.2211e-01, PNorm = 110.7199, GNorm = 0.2338
Meta loss on this task batch = 3.4797e-01, Meta loss averaged over last 500 steps = 3.2214e-01, PNorm = 110.7270, GNorm = 0.2927
Meta loss on this task batch = 3.2328e-01, Meta loss averaged over last 500 steps = 3.2214e-01, PNorm = 110.7334, GNorm = 0.2453
Meta loss on this task batch = 3.0464e-01, Meta loss averaged over last 500 steps = 3.2219e-01, PNorm = 110.7396, GNorm = 0.2387
Meta loss on this task batch = 2.9463e-01, Meta loss averaged over last 500 steps = 3.2220e-01, PNorm = 110.7462, GNorm = 0.2064
Meta loss on this task batch = 3.5939e-01, Meta loss averaged over last 500 steps = 3.2220e-01, PNorm = 110.7526, GNorm = 0.2362
Meta loss on this task batch = 3.2050e-01, Meta loss averaged over last 500 steps = 3.2225e-01, PNorm = 110.7594, GNorm = 0.2081
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 3.2226e-01, PNorm = 110.7658, GNorm = 0.2047
Meta loss on this task batch = 3.0982e-01, Meta loss averaged over last 500 steps = 3.2224e-01, PNorm = 110.7730, GNorm = 0.2114
Meta loss on this task batch = 3.1194e-01, Meta loss averaged over last 500 steps = 3.2222e-01, PNorm = 110.7808, GNorm = 0.2226
Meta loss on this task batch = 3.0363e-01, Meta loss averaged over last 500 steps = 3.2213e-01, PNorm = 110.7893, GNorm = 0.2313
Meta loss on this task batch = 3.5549e-01, Meta loss averaged over last 500 steps = 3.2218e-01, PNorm = 110.7984, GNorm = 0.2289
Meta loss on this task batch = 2.9617e-01, Meta loss averaged over last 500 steps = 3.2213e-01, PNorm = 110.8079, GNorm = 0.1886
Meta loss on this task batch = 3.6439e-01, Meta loss averaged over last 500 steps = 3.2210e-01, PNorm = 110.8176, GNorm = 0.2833
Meta loss on this task batch = 3.1396e-01, Meta loss averaged over last 500 steps = 3.2209e-01, PNorm = 110.8260, GNorm = 0.2453
Meta loss on this task batch = 2.7623e-01, Meta loss averaged over last 500 steps = 3.2206e-01, PNorm = 110.8335, GNorm = 0.2550
Took 114.96666049957275 seconds to complete one epoch of meta training
Took 122.53935933113098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510826
Epoch 300
Meta loss on this task batch = 3.4060e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 110.8410, GNorm = 0.2164
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 3.2195e-01, PNorm = 110.8488, GNorm = 0.2092
Meta loss on this task batch = 3.2302e-01, Meta loss averaged over last 500 steps = 3.2187e-01, PNorm = 110.8572, GNorm = 0.2612
Meta loss on this task batch = 3.4313e-01, Meta loss averaged over last 500 steps = 3.2187e-01, PNorm = 110.8653, GNorm = 0.2304
Meta loss on this task batch = 3.6898e-01, Meta loss averaged over last 500 steps = 3.2204e-01, PNorm = 110.8726, GNorm = 0.2458
Meta loss on this task batch = 3.2930e-01, Meta loss averaged over last 500 steps = 3.2203e-01, PNorm = 110.8806, GNorm = 0.2309
Meta loss on this task batch = 3.1151e-01, Meta loss averaged over last 500 steps = 3.2209e-01, PNorm = 110.8882, GNorm = 0.2109
Meta loss on this task batch = 3.3832e-01, Meta loss averaged over last 500 steps = 3.2215e-01, PNorm = 110.8955, GNorm = 0.2314
Meta loss on this task batch = 2.7348e-01, Meta loss averaged over last 500 steps = 3.2201e-01, PNorm = 110.9034, GNorm = 0.2088
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 3.2184e-01, PNorm = 110.9119, GNorm = 0.1806
Meta loss on this task batch = 3.3490e-01, Meta loss averaged over last 500 steps = 3.2194e-01, PNorm = 110.9204, GNorm = 0.2290
Meta loss on this task batch = 3.5999e-01, Meta loss averaged over last 500 steps = 3.2204e-01, PNorm = 110.9281, GNorm = 0.2370
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 3.2194e-01, PNorm = 110.9358, GNorm = 0.2058
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 3.2181e-01, PNorm = 110.9443, GNorm = 0.1826
Meta loss on this task batch = 2.8985e-01, Meta loss averaged over last 500 steps = 3.2175e-01, PNorm = 110.9537, GNorm = 0.2194
Meta loss on this task batch = 3.4736e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 110.9624, GNorm = 0.2536
Meta loss on this task batch = 3.5419e-01, Meta loss averaged over last 500 steps = 3.2187e-01, PNorm = 110.9691, GNorm = 0.2705
Meta loss on this task batch = 2.3905e-01, Meta loss averaged over last 500 steps = 3.2163e-01, PNorm = 110.9771, GNorm = 0.1957
Meta loss on this task batch = 4.0124e-01, Meta loss averaged over last 500 steps = 3.2184e-01, PNorm = 110.9857, GNorm = 0.3748
Took 113.24547600746155 seconds to complete one epoch of meta training
Took 120.81991600990295 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504531
Epoch 301
Meta loss on this task batch = 3.1154e-01, Meta loss averaged over last 500 steps = 3.2183e-01, PNorm = 110.9949, GNorm = 0.2272
Meta loss on this task batch = 2.9524e-01, Meta loss averaged over last 500 steps = 3.2183e-01, PNorm = 111.0042, GNorm = 0.2059
Meta loss on this task batch = 3.3534e-01, Meta loss averaged over last 500 steps = 3.2182e-01, PNorm = 111.0136, GNorm = 0.2084
Meta loss on this task batch = 3.5777e-01, Meta loss averaged over last 500 steps = 3.2190e-01, PNorm = 111.0229, GNorm = 0.2304
Meta loss on this task batch = 3.1424e-01, Meta loss averaged over last 500 steps = 3.2184e-01, PNorm = 111.0340, GNorm = 0.2345
Meta loss on this task batch = 2.5461e-01, Meta loss averaged over last 500 steps = 3.2175e-01, PNorm = 111.0453, GNorm = 0.1850
Meta loss on this task batch = 2.8294e-01, Meta loss averaged over last 500 steps = 3.2166e-01, PNorm = 111.0568, GNorm = 0.2286
Meta loss on this task batch = 2.8580e-01, Meta loss averaged over last 500 steps = 3.2154e-01, PNorm = 111.0683, GNorm = 0.1914
Meta loss on this task batch = 3.4126e-01, Meta loss averaged over last 500 steps = 3.2159e-01, PNorm = 111.0784, GNorm = 0.2817
Meta loss on this task batch = 2.8360e-01, Meta loss averaged over last 500 steps = 3.2143e-01, PNorm = 111.0878, GNorm = 0.2165
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.2140e-01, PNorm = 111.0972, GNorm = 0.2025
Meta loss on this task batch = 3.1511e-01, Meta loss averaged over last 500 steps = 3.2142e-01, PNorm = 111.1057, GNorm = 0.2296
Meta loss on this task batch = 3.5735e-01, Meta loss averaged over last 500 steps = 3.2145e-01, PNorm = 111.1131, GNorm = 0.2397
Meta loss on this task batch = 3.2709e-01, Meta loss averaged over last 500 steps = 3.2159e-01, PNorm = 111.1200, GNorm = 0.2243
Meta loss on this task batch = 3.4866e-01, Meta loss averaged over last 500 steps = 3.2164e-01, PNorm = 111.1273, GNorm = 0.2133
Meta loss on this task batch = 3.1307e-01, Meta loss averaged over last 500 steps = 3.2158e-01, PNorm = 111.1348, GNorm = 0.2420
Meta loss on this task batch = 3.7045e-01, Meta loss averaged over last 500 steps = 3.2169e-01, PNorm = 111.1421, GNorm = 0.2482
Meta loss on this task batch = 2.9464e-01, Meta loss averaged over last 500 steps = 3.2155e-01, PNorm = 111.1515, GNorm = 0.2198
Meta loss on this task batch = 2.8965e-01, Meta loss averaged over last 500 steps = 3.2154e-01, PNorm = 111.1614, GNorm = 0.2387
Took 113.45944738388062 seconds to complete one epoch of meta training
Took 120.3971037864685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492377
Epoch 302
Meta loss on this task batch = 3.3880e-01, Meta loss averaged over last 500 steps = 3.2149e-01, PNorm = 111.1725, GNorm = 0.2530
Meta loss on this task batch = 3.0115e-01, Meta loss averaged over last 500 steps = 3.2141e-01, PNorm = 111.1834, GNorm = 0.2186
Meta loss on this task batch = 2.9174e-01, Meta loss averaged over last 500 steps = 3.2139e-01, PNorm = 111.1942, GNorm = 0.2253
Meta loss on this task batch = 2.6998e-01, Meta loss averaged over last 500 steps = 3.2120e-01, PNorm = 111.2046, GNorm = 0.1900
Meta loss on this task batch = 3.2234e-01, Meta loss averaged over last 500 steps = 3.2127e-01, PNorm = 111.2147, GNorm = 0.2174
Meta loss on this task batch = 3.7913e-01, Meta loss averaged over last 500 steps = 3.2141e-01, PNorm = 111.2230, GNorm = 0.2663
Meta loss on this task batch = 3.4476e-01, Meta loss averaged over last 500 steps = 3.2146e-01, PNorm = 111.2312, GNorm = 0.2292
Meta loss on this task batch = 2.7245e-01, Meta loss averaged over last 500 steps = 3.2140e-01, PNorm = 111.2384, GNorm = 0.2214
Meta loss on this task batch = 3.0012e-01, Meta loss averaged over last 500 steps = 3.2137e-01, PNorm = 111.2447, GNorm = 0.2468
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.2128e-01, PNorm = 111.2502, GNorm = 0.2516
Meta loss on this task batch = 3.1689e-01, Meta loss averaged over last 500 steps = 3.2119e-01, PNorm = 111.2556, GNorm = 0.2234
Meta loss on this task batch = 3.2798e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 111.2606, GNorm = 0.2476
Meta loss on this task batch = 3.6556e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 111.2655, GNorm = 0.2371
Meta loss on this task batch = 3.2970e-01, Meta loss averaged over last 500 steps = 3.2133e-01, PNorm = 111.2720, GNorm = 0.2339
Meta loss on this task batch = 3.7735e-01, Meta loss averaged over last 500 steps = 3.2146e-01, PNorm = 111.2784, GNorm = 0.2677
Meta loss on this task batch = 3.0101e-01, Meta loss averaged over last 500 steps = 3.2151e-01, PNorm = 111.2854, GNorm = 0.2422
Meta loss on this task batch = 3.1658e-01, Meta loss averaged over last 500 steps = 3.2143e-01, PNorm = 111.2929, GNorm = 0.2162
Meta loss on this task batch = 3.3246e-01, Meta loss averaged over last 500 steps = 3.2140e-01, PNorm = 111.3020, GNorm = 0.2173
Meta loss on this task batch = 3.1633e-01, Meta loss averaged over last 500 steps = 3.2141e-01, PNorm = 111.3117, GNorm = 0.2796
Took 113.98472547531128 seconds to complete one epoch of meta training
Took 121.34914350509644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517150
Epoch 303
Meta loss on this task batch = 2.4622e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 111.3227, GNorm = 0.1956
Meta loss on this task batch = 3.0112e-01, Meta loss averaged over last 500 steps = 3.2116e-01, PNorm = 111.3331, GNorm = 0.1935
Meta loss on this task batch = 3.3353e-01, Meta loss averaged over last 500 steps = 3.2119e-01, PNorm = 111.3433, GNorm = 0.2132
Meta loss on this task batch = 2.9258e-01, Meta loss averaged over last 500 steps = 3.2108e-01, PNorm = 111.3532, GNorm = 0.1784
Meta loss on this task batch = 3.7642e-01, Meta loss averaged over last 500 steps = 3.2119e-01, PNorm = 111.3605, GNorm = 0.2366
Meta loss on this task batch = 3.4923e-01, Meta loss averaged over last 500 steps = 3.2134e-01, PNorm = 111.3674, GNorm = 0.2044
Meta loss on this task batch = 3.4507e-01, Meta loss averaged over last 500 steps = 3.2133e-01, PNorm = 111.3738, GNorm = 0.2413
Meta loss on this task batch = 2.5796e-01, Meta loss averaged over last 500 steps = 3.2118e-01, PNorm = 111.3804, GNorm = 0.1859
Meta loss on this task batch = 3.0652e-01, Meta loss averaged over last 500 steps = 3.2126e-01, PNorm = 111.3879, GNorm = 0.2035
Meta loss on this task batch = 3.3646e-01, Meta loss averaged over last 500 steps = 3.2126e-01, PNorm = 111.3958, GNorm = 0.1976
Meta loss on this task batch = 3.4980e-01, Meta loss averaged over last 500 steps = 3.2138e-01, PNorm = 111.4037, GNorm = 0.2297
Meta loss on this task batch = 2.4940e-01, Meta loss averaged over last 500 steps = 3.2122e-01, PNorm = 111.4124, GNorm = 0.1759
Meta loss on this task batch = 3.3205e-01, Meta loss averaged over last 500 steps = 3.2117e-01, PNorm = 111.4216, GNorm = 0.2029
Meta loss on this task batch = 2.9320e-01, Meta loss averaged over last 500 steps = 3.2114e-01, PNorm = 111.4302, GNorm = 0.2222
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 3.2114e-01, PNorm = 111.4390, GNorm = 0.1916
Meta loss on this task batch = 3.1409e-01, Meta loss averaged over last 500 steps = 3.2105e-01, PNorm = 111.4486, GNorm = 0.2203
Meta loss on this task batch = 3.5687e-01, Meta loss averaged over last 500 steps = 3.2116e-01, PNorm = 111.4575, GNorm = 0.2313
Meta loss on this task batch = 3.1356e-01, Meta loss averaged over last 500 steps = 3.2114e-01, PNorm = 111.4656, GNorm = 0.2134
Meta loss on this task batch = 3.2881e-01, Meta loss averaged over last 500 steps = 3.2094e-01, PNorm = 111.4736, GNorm = 0.2610
Took 146.95515489578247 seconds to complete one epoch of meta training
Took 154.78680205345154 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513780
Epoch 304
Meta loss on this task batch = 2.7346e-01, Meta loss averaged over last 500 steps = 3.2079e-01, PNorm = 111.4816, GNorm = 0.1888
Meta loss on this task batch = 3.8655e-01, Meta loss averaged over last 500 steps = 3.2089e-01, PNorm = 111.4887, GNorm = 0.2734
Meta loss on this task batch = 2.8039e-01, Meta loss averaged over last 500 steps = 3.2076e-01, PNorm = 111.4947, GNorm = 0.2365
Meta loss on this task batch = 3.0014e-01, Meta loss averaged over last 500 steps = 3.2080e-01, PNorm = 111.5013, GNorm = 0.2019
Meta loss on this task batch = 3.4323e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 111.5073, GNorm = 0.2320
Meta loss on this task batch = 3.3762e-01, Meta loss averaged over last 500 steps = 3.2100e-01, PNorm = 111.5127, GNorm = 0.2438
Meta loss on this task batch = 3.5057e-01, Meta loss averaged over last 500 steps = 3.2114e-01, PNorm = 111.5168, GNorm = 0.2376
Meta loss on this task batch = 2.6690e-01, Meta loss averaged over last 500 steps = 3.2106e-01, PNorm = 111.5228, GNorm = 0.1991
Meta loss on this task batch = 3.4390e-01, Meta loss averaged over last 500 steps = 3.2110e-01, PNorm = 111.5285, GNorm = 0.2440
Meta loss on this task batch = 3.1134e-01, Meta loss averaged over last 500 steps = 3.2107e-01, PNorm = 111.5349, GNorm = 0.2388
Meta loss on this task batch = 3.2051e-01, Meta loss averaged over last 500 steps = 3.2090e-01, PNorm = 111.5416, GNorm = 0.2019
Meta loss on this task batch = 3.0532e-01, Meta loss averaged over last 500 steps = 3.2090e-01, PNorm = 111.5491, GNorm = 0.2089
Meta loss on this task batch = 2.8027e-01, Meta loss averaged over last 500 steps = 3.2086e-01, PNorm = 111.5571, GNorm = 0.2084
Meta loss on this task batch = 3.2648e-01, Meta loss averaged over last 500 steps = 3.2093e-01, PNorm = 111.5649, GNorm = 0.2484
Meta loss on this task batch = 3.2749e-01, Meta loss averaged over last 500 steps = 3.2089e-01, PNorm = 111.5738, GNorm = 0.2120
Meta loss on this task batch = 3.0915e-01, Meta loss averaged over last 500 steps = 3.2084e-01, PNorm = 111.5829, GNorm = 0.2219
Meta loss on this task batch = 3.3302e-01, Meta loss averaged over last 500 steps = 3.2081e-01, PNorm = 111.5912, GNorm = 0.2119
Meta loss on this task batch = 3.2870e-01, Meta loss averaged over last 500 steps = 3.2091e-01, PNorm = 111.6000, GNorm = 0.2457
Meta loss on this task batch = 3.1701e-01, Meta loss averaged over last 500 steps = 3.2095e-01, PNorm = 111.6084, GNorm = 0.2554
Took 115.47835111618042 seconds to complete one epoch of meta training
Took 122.84210467338562 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495340
Epoch 305
Meta loss on this task batch = 2.5534e-01, Meta loss averaged over last 500 steps = 3.2082e-01, PNorm = 111.6176, GNorm = 0.1798
Meta loss on this task batch = 3.0275e-01, Meta loss averaged over last 500 steps = 3.2068e-01, PNorm = 111.6259, GNorm = 0.2079
Meta loss on this task batch = 3.1343e-01, Meta loss averaged over last 500 steps = 3.2070e-01, PNorm = 111.6330, GNorm = 0.2428
Meta loss on this task batch = 2.9239e-01, Meta loss averaged over last 500 steps = 3.2056e-01, PNorm = 111.6404, GNorm = 0.2103
Meta loss on this task batch = 3.9777e-01, Meta loss averaged over last 500 steps = 3.2057e-01, PNorm = 111.6487, GNorm = 0.2461
Meta loss on this task batch = 2.8376e-01, Meta loss averaged over last 500 steps = 3.2044e-01, PNorm = 111.6572, GNorm = 0.2135
Meta loss on this task batch = 3.1505e-01, Meta loss averaged over last 500 steps = 3.2045e-01, PNorm = 111.6641, GNorm = 0.2805
Meta loss on this task batch = 3.2417e-01, Meta loss averaged over last 500 steps = 3.2061e-01, PNorm = 111.6711, GNorm = 0.2358
Meta loss on this task batch = 3.2098e-01, Meta loss averaged over last 500 steps = 3.2063e-01, PNorm = 111.6789, GNorm = 0.2443
Meta loss on this task batch = 3.5919e-01, Meta loss averaged over last 500 steps = 3.2071e-01, PNorm = 111.6874, GNorm = 0.2405
Meta loss on this task batch = 3.0074e-01, Meta loss averaged over last 500 steps = 3.2066e-01, PNorm = 111.6966, GNorm = 0.2456
Meta loss on this task batch = 3.7001e-01, Meta loss averaged over last 500 steps = 3.2072e-01, PNorm = 111.7049, GNorm = 0.2323
Meta loss on this task batch = 3.6097e-01, Meta loss averaged over last 500 steps = 3.2065e-01, PNorm = 111.7130, GNorm = 0.2516
Meta loss on this task batch = 3.0014e-01, Meta loss averaged over last 500 steps = 3.2065e-01, PNorm = 111.7219, GNorm = 0.1963
Meta loss on this task batch = 2.5296e-01, Meta loss averaged over last 500 steps = 3.2057e-01, PNorm = 111.7316, GNorm = 0.1863
Meta loss on this task batch = 2.7745e-01, Meta loss averaged over last 500 steps = 3.2047e-01, PNorm = 111.7421, GNorm = 0.1915
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 3.2054e-01, PNorm = 111.7519, GNorm = 0.2253
Meta loss on this task batch = 3.5351e-01, Meta loss averaged over last 500 steps = 3.2045e-01, PNorm = 111.7618, GNorm = 0.2105
Meta loss on this task batch = 3.4117e-01, Meta loss averaged over last 500 steps = 3.2048e-01, PNorm = 111.7715, GNorm = 0.2513
Took 111.98561835289001 seconds to complete one epoch of meta training
Took 119.48669338226318 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496060
Epoch 306
Meta loss on this task batch = 3.2955e-01, Meta loss averaged over last 500 steps = 3.2047e-01, PNorm = 111.7799, GNorm = 0.2060
Meta loss on this task batch = 3.2839e-01, Meta loss averaged over last 500 steps = 3.2059e-01, PNorm = 111.7878, GNorm = 0.2273
Meta loss on this task batch = 2.8007e-01, Meta loss averaged over last 500 steps = 3.2044e-01, PNorm = 111.7965, GNorm = 0.2070
Meta loss on this task batch = 2.8211e-01, Meta loss averaged over last 500 steps = 3.2046e-01, PNorm = 111.8044, GNorm = 0.2191
Meta loss on this task batch = 2.6058e-01, Meta loss averaged over last 500 steps = 3.2027e-01, PNorm = 111.8128, GNorm = 0.2039
Meta loss on this task batch = 3.1113e-01, Meta loss averaged over last 500 steps = 3.2015e-01, PNorm = 111.8217, GNorm = 0.2480
Meta loss on this task batch = 3.1514e-01, Meta loss averaged over last 500 steps = 3.2021e-01, PNorm = 111.8313, GNorm = 0.2173
Meta loss on this task batch = 3.4311e-01, Meta loss averaged over last 500 steps = 3.2032e-01, PNorm = 111.8408, GNorm = 0.2361
Meta loss on this task batch = 3.3432e-01, Meta loss averaged over last 500 steps = 3.2034e-01, PNorm = 111.8509, GNorm = 0.2341
Meta loss on this task batch = 3.5737e-01, Meta loss averaged over last 500 steps = 3.2040e-01, PNorm = 111.8601, GNorm = 0.2512
Meta loss on this task batch = 3.3622e-01, Meta loss averaged over last 500 steps = 3.2053e-01, PNorm = 111.8693, GNorm = 0.2724
Meta loss on this task batch = 3.0029e-01, Meta loss averaged over last 500 steps = 3.2046e-01, PNorm = 111.8783, GNorm = 0.2296
Meta loss on this task batch = 3.2730e-01, Meta loss averaged over last 500 steps = 3.2058e-01, PNorm = 111.8863, GNorm = 0.2431
Meta loss on this task batch = 3.2895e-01, Meta loss averaged over last 500 steps = 3.2054e-01, PNorm = 111.8918, GNorm = 0.2855
Meta loss on this task batch = 3.4630e-01, Meta loss averaged over last 500 steps = 3.2058e-01, PNorm = 111.8963, GNorm = 0.2751
Meta loss on this task batch = 3.0285e-01, Meta loss averaged over last 500 steps = 3.2039e-01, PNorm = 111.9021, GNorm = 0.2171
Meta loss on this task batch = 3.2698e-01, Meta loss averaged over last 500 steps = 3.2034e-01, PNorm = 111.9064, GNorm = 0.2269
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 3.2011e-01, PNorm = 111.9109, GNorm = 0.1815
Meta loss on this task batch = 3.9264e-01, Meta loss averaged over last 500 steps = 3.2029e-01, PNorm = 111.9140, GNorm = 0.2959
Took 114.09842729568481 seconds to complete one epoch of meta training
Took 122.50262784957886 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488104
Epoch 307
Meta loss on this task batch = 2.7447e-01, Meta loss averaged over last 500 steps = 3.2014e-01, PNorm = 111.9183, GNorm = 0.2127
Meta loss on this task batch = 3.0482e-01, Meta loss averaged over last 500 steps = 3.2001e-01, PNorm = 111.9243, GNorm = 0.2803
Meta loss on this task batch = 3.0083e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 111.9317, GNorm = 0.2213
Meta loss on this task batch = 3.6067e-01, Meta loss averaged over last 500 steps = 3.2008e-01, PNorm = 111.9398, GNorm = 0.2203
Meta loss on this task batch = 3.2603e-01, Meta loss averaged over last 500 steps = 3.2004e-01, PNorm = 111.9482, GNorm = 0.2141
Meta loss on this task batch = 3.1137e-01, Meta loss averaged over last 500 steps = 3.1999e-01, PNorm = 111.9580, GNorm = 0.2198
Meta loss on this task batch = 3.4801e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 111.9681, GNorm = 0.2750
Meta loss on this task batch = 3.4486e-01, Meta loss averaged over last 500 steps = 3.2011e-01, PNorm = 111.9781, GNorm = 0.2263
Meta loss on this task batch = 3.2042e-01, Meta loss averaged over last 500 steps = 3.2023e-01, PNorm = 111.9884, GNorm = 0.2142
Meta loss on this task batch = 3.8627e-01, Meta loss averaged over last 500 steps = 3.2027e-01, PNorm = 111.9972, GNorm = 0.2615
Meta loss on this task batch = 3.0326e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 112.0057, GNorm = 0.2334
Meta loss on this task batch = 2.8044e-01, Meta loss averaged over last 500 steps = 3.2020e-01, PNorm = 112.0149, GNorm = 0.1945
Meta loss on this task batch = 3.3412e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 112.0239, GNorm = 0.2255
Meta loss on this task batch = 3.6187e-01, Meta loss averaged over last 500 steps = 3.2031e-01, PNorm = 112.0323, GNorm = 0.2238
Meta loss on this task batch = 3.2309e-01, Meta loss averaged over last 500 steps = 3.2042e-01, PNorm = 112.0395, GNorm = 0.2243
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 3.2038e-01, PNorm = 112.0473, GNorm = 0.1836
Meta loss on this task batch = 2.9459e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 112.0549, GNorm = 0.2094
Meta loss on this task batch = 3.1127e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 112.0627, GNorm = 0.2017
Meta loss on this task batch = 3.1874e-01, Meta loss averaged over last 500 steps = 3.2030e-01, PNorm = 112.0709, GNorm = 0.2605
Took 114.50052285194397 seconds to complete one epoch of meta training
Took 122.45114779472351 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494684
Epoch 308
Meta loss on this task batch = 3.3459e-01, Meta loss averaged over last 500 steps = 3.2031e-01, PNorm = 112.0785, GNorm = 0.2020
Meta loss on this task batch = 3.3311e-01, Meta loss averaged over last 500 steps = 3.2027e-01, PNorm = 112.0861, GNorm = 0.2017
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 3.2023e-01, PNorm = 112.0953, GNorm = 0.2301
Meta loss on this task batch = 2.8140e-01, Meta loss averaged over last 500 steps = 3.2012e-01, PNorm = 112.1039, GNorm = 0.2529
Meta loss on this task batch = 2.9491e-01, Meta loss averaged over last 500 steps = 3.2000e-01, PNorm = 112.1127, GNorm = 0.2110
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 112.1225, GNorm = 0.2242
Meta loss on this task batch = 3.6541e-01, Meta loss averaged over last 500 steps = 3.2003e-01, PNorm = 112.1312, GNorm = 0.2659
Meta loss on this task batch = 3.2692e-01, Meta loss averaged over last 500 steps = 3.2001e-01, PNorm = 112.1403, GNorm = 0.2097
Meta loss on this task batch = 3.2657e-01, Meta loss averaged over last 500 steps = 3.2007e-01, PNorm = 112.1488, GNorm = 0.2163
Meta loss on this task batch = 3.2067e-01, Meta loss averaged over last 500 steps = 3.2011e-01, PNorm = 112.1570, GNorm = 0.2347
Meta loss on this task batch = 3.0035e-01, Meta loss averaged over last 500 steps = 3.2005e-01, PNorm = 112.1655, GNorm = 0.2119
Meta loss on this task batch = 2.9626e-01, Meta loss averaged over last 500 steps = 3.1997e-01, PNorm = 112.1745, GNorm = 0.2139
Meta loss on this task batch = 3.0937e-01, Meta loss averaged over last 500 steps = 3.1994e-01, PNorm = 112.1831, GNorm = 0.2108
Meta loss on this task batch = 3.1528e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 112.1918, GNorm = 0.2027
Meta loss on this task batch = 2.8197e-01, Meta loss averaged over last 500 steps = 3.1990e-01, PNorm = 112.2011, GNorm = 0.2130
Meta loss on this task batch = 3.4868e-01, Meta loss averaged over last 500 steps = 3.1989e-01, PNorm = 112.2098, GNorm = 0.2543
Meta loss on this task batch = 3.1793e-01, Meta loss averaged over last 500 steps = 3.1990e-01, PNorm = 112.2182, GNorm = 0.2205
Meta loss on this task batch = 2.8374e-01, Meta loss averaged over last 500 steps = 3.1981e-01, PNorm = 112.2267, GNorm = 0.2148
Meta loss on this task batch = 3.4458e-01, Meta loss averaged over last 500 steps = 3.1993e-01, PNorm = 112.2358, GNorm = 0.2527
Took 118.4862551689148 seconds to complete one epoch of meta training
Took 126.16128420829773 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486728
Epoch 309
Meta loss on this task batch = 3.5582e-01, Meta loss averaged over last 500 steps = 3.1996e-01, PNorm = 112.2437, GNorm = 0.2273
Meta loss on this task batch = 2.4965e-01, Meta loss averaged over last 500 steps = 3.1985e-01, PNorm = 112.2531, GNorm = 0.1795
Meta loss on this task batch = 2.5023e-01, Meta loss averaged over last 500 steps = 3.1972e-01, PNorm = 112.2630, GNorm = 0.2179
Meta loss on this task batch = 3.0845e-01, Meta loss averaged over last 500 steps = 3.1971e-01, PNorm = 112.2735, GNorm = 0.2130
Meta loss on this task batch = 3.5160e-01, Meta loss averaged over last 500 steps = 3.1975e-01, PNorm = 112.2834, GNorm = 0.2201
Meta loss on this task batch = 3.6350e-01, Meta loss averaged over last 500 steps = 3.1982e-01, PNorm = 112.2927, GNorm = 0.2525
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 3.1971e-01, PNorm = 112.3014, GNorm = 0.2144
Meta loss on this task batch = 2.8543e-01, Meta loss averaged over last 500 steps = 3.1966e-01, PNorm = 112.3103, GNorm = 0.1851
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.1959e-01, PNorm = 112.3197, GNorm = 0.1940
Meta loss on this task batch = 3.6442e-01, Meta loss averaged over last 500 steps = 3.1965e-01, PNorm = 112.3276, GNorm = 0.2494
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 3.1977e-01, PNorm = 112.3344, GNorm = 0.2095
Meta loss on this task batch = 2.9548e-01, Meta loss averaged over last 500 steps = 3.1966e-01, PNorm = 112.3409, GNorm = 0.2439
Meta loss on this task batch = 3.7253e-01, Meta loss averaged over last 500 steps = 3.1968e-01, PNorm = 112.3477, GNorm = 0.2047
Meta loss on this task batch = 3.3510e-01, Meta loss averaged over last 500 steps = 3.1963e-01, PNorm = 112.3539, GNorm = 0.2242
Meta loss on this task batch = 2.9777e-01, Meta loss averaged over last 500 steps = 3.1959e-01, PNorm = 112.3617, GNorm = 0.1997
Meta loss on this task batch = 3.4500e-01, Meta loss averaged over last 500 steps = 3.1957e-01, PNorm = 112.3700, GNorm = 0.2449
Meta loss on this task batch = 2.9838e-01, Meta loss averaged over last 500 steps = 3.1958e-01, PNorm = 112.3777, GNorm = 0.2239
Meta loss on this task batch = 3.0851e-01, Meta loss averaged over last 500 steps = 3.1951e-01, PNorm = 112.3858, GNorm = 0.1973
Meta loss on this task batch = 3.1106e-01, Meta loss averaged over last 500 steps = 3.1947e-01, PNorm = 112.3943, GNorm = 0.2684
Took 112.60153579711914 seconds to complete one epoch of meta training
Took 121.13703465461731 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478119
Epoch 310
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 3.1929e-01, PNorm = 112.4038, GNorm = 0.2360
Meta loss on this task batch = 3.0023e-01, Meta loss averaged over last 500 steps = 3.1923e-01, PNorm = 112.4134, GNorm = 0.2074
Meta loss on this task batch = 3.2399e-01, Meta loss averaged over last 500 steps = 3.1931e-01, PNorm = 112.4229, GNorm = 0.2308
Meta loss on this task batch = 3.3014e-01, Meta loss averaged over last 500 steps = 3.1936e-01, PNorm = 112.4331, GNorm = 0.2137
Meta loss on this task batch = 3.0703e-01, Meta loss averaged over last 500 steps = 3.1928e-01, PNorm = 112.4427, GNorm = 0.2150
Meta loss on this task batch = 3.1002e-01, Meta loss averaged over last 500 steps = 3.1940e-01, PNorm = 112.4517, GNorm = 0.2630
Meta loss on this task batch = 2.8730e-01, Meta loss averaged over last 500 steps = 3.1933e-01, PNorm = 112.4610, GNorm = 0.2131
Meta loss on this task batch = 3.4170e-01, Meta loss averaged over last 500 steps = 3.1935e-01, PNorm = 112.4691, GNorm = 0.2575
Meta loss on this task batch = 3.1359e-01, Meta loss averaged over last 500 steps = 3.1928e-01, PNorm = 112.4770, GNorm = 0.2183
Meta loss on this task batch = 3.4192e-01, Meta loss averaged over last 500 steps = 3.1940e-01, PNorm = 112.4846, GNorm = 0.2338
Meta loss on this task batch = 2.8713e-01, Meta loss averaged over last 500 steps = 3.1919e-01, PNorm = 112.4919, GNorm = 0.2153
Meta loss on this task batch = 3.4547e-01, Meta loss averaged over last 500 steps = 3.1925e-01, PNorm = 112.4991, GNorm = 0.2598
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 3.1923e-01, PNorm = 112.5070, GNorm = 0.2242
Meta loss on this task batch = 2.6929e-01, Meta loss averaged over last 500 steps = 3.1908e-01, PNorm = 112.5153, GNorm = 0.2266
Meta loss on this task batch = 2.7041e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 112.5218, GNorm = 0.2351
Meta loss on this task batch = 3.7904e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 112.5269, GNorm = 0.2877
Meta loss on this task batch = 3.6483e-01, Meta loss averaged over last 500 steps = 3.1895e-01, PNorm = 112.5311, GNorm = 0.2529
Meta loss on this task batch = 3.1469e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 112.5358, GNorm = 0.1922
Meta loss on this task batch = 3.3340e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 112.5418, GNorm = 0.2496
Took 113.39091110229492 seconds to complete one epoch of meta training
Took 121.08583450317383 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471282
Epoch 311
Meta loss on this task batch = 2.4543e-01, Meta loss averaged over last 500 steps = 3.1877e-01, PNorm = 112.5499, GNorm = 0.2074
Meta loss on this task batch = 3.0577e-01, Meta loss averaged over last 500 steps = 3.1882e-01, PNorm = 112.5591, GNorm = 0.1889
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 3.1894e-01, PNorm = 112.5683, GNorm = 0.2073
Meta loss on this task batch = 3.2801e-01, Meta loss averaged over last 500 steps = 3.1903e-01, PNorm = 112.5775, GNorm = 0.1985
Meta loss on this task batch = 3.7291e-01, Meta loss averaged over last 500 steps = 3.1922e-01, PNorm = 112.5864, GNorm = 0.2310
Meta loss on this task batch = 3.2244e-01, Meta loss averaged over last 500 steps = 3.1922e-01, PNorm = 112.5954, GNorm = 0.2001
Meta loss on this task batch = 3.6273e-01, Meta loss averaged over last 500 steps = 3.1926e-01, PNorm = 112.6043, GNorm = 0.2204
Meta loss on this task batch = 3.6142e-01, Meta loss averaged over last 500 steps = 3.1944e-01, PNorm = 112.6126, GNorm = 0.2306
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 3.1941e-01, PNorm = 112.6211, GNorm = 0.2275
Meta loss on this task batch = 3.2215e-01, Meta loss averaged over last 500 steps = 3.1937e-01, PNorm = 112.6293, GNorm = 0.2168
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 3.1924e-01, PNorm = 112.6372, GNorm = 0.2149
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 3.1912e-01, PNorm = 112.6454, GNorm = 0.1979
Meta loss on this task batch = 3.0023e-01, Meta loss averaged over last 500 steps = 3.1910e-01, PNorm = 112.6536, GNorm = 0.2286
Meta loss on this task batch = 3.7064e-01, Meta loss averaged over last 500 steps = 3.1922e-01, PNorm = 112.6606, GNorm = 0.2304
Meta loss on this task batch = 3.2983e-01, Meta loss averaged over last 500 steps = 3.1913e-01, PNorm = 112.6681, GNorm = 0.2043
Meta loss on this task batch = 3.0074e-01, Meta loss averaged over last 500 steps = 3.1901e-01, PNorm = 112.6760, GNorm = 0.1867
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 3.1898e-01, PNorm = 112.6842, GNorm = 0.2527
Meta loss on this task batch = 3.1821e-01, Meta loss averaged over last 500 steps = 3.1887e-01, PNorm = 112.6924, GNorm = 0.1937
Meta loss on this task batch = 3.1539e-01, Meta loss averaged over last 500 steps = 3.1889e-01, PNorm = 112.7009, GNorm = 0.2784
Took 114.66196632385254 seconds to complete one epoch of meta training
Took 122.7753837108612 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492868
Epoch 312
Meta loss on this task batch = 3.2927e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 112.7091, GNorm = 0.2273
Meta loss on this task batch = 3.1690e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 112.7182, GNorm = 0.2781
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 112.7275, GNorm = 0.2040
Meta loss on this task batch = 3.7347e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 112.7358, GNorm = 0.2362
Meta loss on this task batch = 3.4298e-01, Meta loss averaged over last 500 steps = 3.1909e-01, PNorm = 112.7442, GNorm = 0.2554
Meta loss on this task batch = 2.8044e-01, Meta loss averaged over last 500 steps = 3.1905e-01, PNorm = 112.7540, GNorm = 0.2189
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 112.7637, GNorm = 0.2440
Meta loss on this task batch = 3.2792e-01, Meta loss averaged over last 500 steps = 3.1907e-01, PNorm = 112.7728, GNorm = 0.2306
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 3.1900e-01, PNorm = 112.7809, GNorm = 0.2703
Meta loss on this task batch = 3.7726e-01, Meta loss averaged over last 500 steps = 3.1891e-01, PNorm = 112.7887, GNorm = 0.2642
Meta loss on this task batch = 3.1917e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 112.7959, GNorm = 0.2198
Meta loss on this task batch = 3.5675e-01, Meta loss averaged over last 500 steps = 3.1914e-01, PNorm = 112.8028, GNorm = 0.2398
Meta loss on this task batch = 3.7048e-01, Meta loss averaged over last 500 steps = 3.1911e-01, PNorm = 112.8089, GNorm = 0.2176
Meta loss on this task batch = 3.0401e-01, Meta loss averaged over last 500 steps = 3.1917e-01, PNorm = 112.8164, GNorm = 0.1926
Meta loss on this task batch = 3.3919e-01, Meta loss averaged over last 500 steps = 3.1919e-01, PNorm = 112.8240, GNorm = 0.2194
Meta loss on this task batch = 2.9172e-01, Meta loss averaged over last 500 steps = 3.1916e-01, PNorm = 112.8327, GNorm = 0.2110
Meta loss on this task batch = 2.5992e-01, Meta loss averaged over last 500 steps = 3.1908e-01, PNorm = 112.8415, GNorm = 0.1851
Meta loss on this task batch = 3.3530e-01, Meta loss averaged over last 500 steps = 3.1911e-01, PNorm = 112.8487, GNorm = 0.2419
Meta loss on this task batch = 2.2176e-01, Meta loss averaged over last 500 steps = 3.1887e-01, PNorm = 112.8575, GNorm = 0.2501
Took 116.27794075012207 seconds to complete one epoch of meta training
Took 123.79737758636475 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505646
Epoch 313
Meta loss on this task batch = 3.0773e-01, Meta loss averaged over last 500 steps = 3.1868e-01, PNorm = 112.8667, GNorm = 0.1957
Meta loss on this task batch = 3.0369e-01, Meta loss averaged over last 500 steps = 3.1865e-01, PNorm = 112.8765, GNorm = 0.2000
Meta loss on this task batch = 3.4037e-01, Meta loss averaged over last 500 steps = 3.1872e-01, PNorm = 112.8844, GNorm = 0.2379
Meta loss on this task batch = 3.0237e-01, Meta loss averaged over last 500 steps = 3.1866e-01, PNorm = 112.8932, GNorm = 0.2030
Meta loss on this task batch = 3.0466e-01, Meta loss averaged over last 500 steps = 3.1873e-01, PNorm = 112.9018, GNorm = 0.2115
Meta loss on this task batch = 3.5255e-01, Meta loss averaged over last 500 steps = 3.1881e-01, PNorm = 112.9097, GNorm = 0.2353
Meta loss on this task batch = 4.0574e-01, Meta loss averaged over last 500 steps = 3.1896e-01, PNorm = 112.9151, GNorm = 0.2965
Meta loss on this task batch = 3.0970e-01, Meta loss averaged over last 500 steps = 3.1890e-01, PNorm = 112.9203, GNorm = 0.2468
Meta loss on this task batch = 2.9201e-01, Meta loss averaged over last 500 steps = 3.1884e-01, PNorm = 112.9265, GNorm = 0.2323
Meta loss on this task batch = 3.6128e-01, Meta loss averaged over last 500 steps = 3.1889e-01, PNorm = 112.9323, GNorm = 0.2277
Meta loss on this task batch = 2.8247e-01, Meta loss averaged over last 500 steps = 3.1885e-01, PNorm = 112.9387, GNorm = 0.2043
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 112.9459, GNorm = 0.2340
Meta loss on this task batch = 2.8450e-01, Meta loss averaged over last 500 steps = 3.1895e-01, PNorm = 112.9546, GNorm = 0.2135
Meta loss on this task batch = 2.7366e-01, Meta loss averaged over last 500 steps = 3.1891e-01, PNorm = 112.9644, GNorm = 0.2068
Meta loss on this task batch = 3.4613e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 112.9740, GNorm = 0.2212
Meta loss on this task batch = 2.9817e-01, Meta loss averaged over last 500 steps = 3.1889e-01, PNorm = 112.9833, GNorm = 0.2146
Meta loss on this task batch = 3.5895e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 112.9915, GNorm = 0.2594
Meta loss on this task batch = 2.6397e-01, Meta loss averaged over last 500 steps = 3.1889e-01, PNorm = 113.0003, GNorm = 0.1801
Meta loss on this task batch = 2.8828e-01, Meta loss averaged over last 500 steps = 3.1878e-01, PNorm = 113.0084, GNorm = 0.2303
Took 114.577707529068 seconds to complete one epoch of meta training
Took 122.3238205909729 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500477
Epoch 314
Meta loss on this task batch = 3.2613e-01, Meta loss averaged over last 500 steps = 3.1877e-01, PNorm = 113.0160, GNorm = 0.2469
Meta loss on this task batch = 2.6651e-01, Meta loss averaged over last 500 steps = 3.1866e-01, PNorm = 113.0241, GNorm = 0.2092
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 3.1866e-01, PNorm = 113.0304, GNorm = 0.2242
Meta loss on this task batch = 2.8614e-01, Meta loss averaged over last 500 steps = 3.1861e-01, PNorm = 113.0360, GNorm = 0.2140
Meta loss on this task batch = 3.2600e-01, Meta loss averaged over last 500 steps = 3.1854e-01, PNorm = 113.0418, GNorm = 0.1957
Meta loss on this task batch = 3.1631e-01, Meta loss averaged over last 500 steps = 3.1843e-01, PNorm = 113.0488, GNorm = 0.2212
Meta loss on this task batch = 2.9922e-01, Meta loss averaged over last 500 steps = 3.1835e-01, PNorm = 113.0562, GNorm = 0.2019
Meta loss on this task batch = 3.7170e-01, Meta loss averaged over last 500 steps = 3.1853e-01, PNorm = 113.0631, GNorm = 0.2223
Meta loss on this task batch = 3.3199e-01, Meta loss averaged over last 500 steps = 3.1849e-01, PNorm = 113.0698, GNorm = 0.2143
Meta loss on this task batch = 3.1595e-01, Meta loss averaged over last 500 steps = 3.1847e-01, PNorm = 113.0752, GNorm = 0.2585
Meta loss on this task batch = 3.1575e-01, Meta loss averaged over last 500 steps = 3.1849e-01, PNorm = 113.0810, GNorm = 0.2496
Meta loss on this task batch = 3.5953e-01, Meta loss averaged over last 500 steps = 3.1867e-01, PNorm = 113.0857, GNorm = 0.2241
Meta loss on this task batch = 3.2249e-01, Meta loss averaged over last 500 steps = 3.1862e-01, PNorm = 113.0910, GNorm = 0.2014
Meta loss on this task batch = 2.9214e-01, Meta loss averaged over last 500 steps = 3.1863e-01, PNorm = 113.0973, GNorm = 0.2005
Meta loss on this task batch = 3.1473e-01, Meta loss averaged over last 500 steps = 3.1862e-01, PNorm = 113.1044, GNorm = 0.2395
Meta loss on this task batch = 3.3799e-01, Meta loss averaged over last 500 steps = 3.1874e-01, PNorm = 113.1132, GNorm = 0.2163
Meta loss on this task batch = 3.3000e-01, Meta loss averaged over last 500 steps = 3.1879e-01, PNorm = 113.1222, GNorm = 0.2204
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 3.1874e-01, PNorm = 113.1308, GNorm = 0.2052
Meta loss on this task batch = 3.3623e-01, Meta loss averaged over last 500 steps = 3.1865e-01, PNorm = 113.1405, GNorm = 0.2667
Took 110.71861410140991 seconds to complete one epoch of meta training
Took 118.56073212623596 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501726
Epoch 315
Meta loss on this task batch = 3.2534e-01, Meta loss averaged over last 500 steps = 3.1865e-01, PNorm = 113.1497, GNorm = 0.2714
Meta loss on this task batch = 3.4780e-01, Meta loss averaged over last 500 steps = 3.1860e-01, PNorm = 113.1578, GNorm = 0.2452
Meta loss on this task batch = 3.1937e-01, Meta loss averaged over last 500 steps = 3.1852e-01, PNorm = 113.1658, GNorm = 0.2082
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.1860e-01, PNorm = 113.1734, GNorm = 0.2501
Meta loss on this task batch = 2.7816e-01, Meta loss averaged over last 500 steps = 3.1849e-01, PNorm = 113.1819, GNorm = 0.1973
Meta loss on this task batch = 3.4385e-01, Meta loss averaged over last 500 steps = 3.1856e-01, PNorm = 113.1889, GNorm = 0.2751
Meta loss on this task batch = 3.5034e-01, Meta loss averaged over last 500 steps = 3.1857e-01, PNorm = 113.1956, GNorm = 0.2240
Meta loss on this task batch = 2.5949e-01, Meta loss averaged over last 500 steps = 3.1833e-01, PNorm = 113.2025, GNorm = 0.2028
Meta loss on this task batch = 3.1621e-01, Meta loss averaged over last 500 steps = 3.1837e-01, PNorm = 113.2096, GNorm = 0.2549
Meta loss on this task batch = 2.9863e-01, Meta loss averaged over last 500 steps = 3.1839e-01, PNorm = 113.2161, GNorm = 0.2290
Meta loss on this task batch = 3.5406e-01, Meta loss averaged over last 500 steps = 3.1844e-01, PNorm = 113.2231, GNorm = 0.2292
Meta loss on this task batch = 2.7491e-01, Meta loss averaged over last 500 steps = 3.1844e-01, PNorm = 113.2302, GNorm = 0.2429
Meta loss on this task batch = 3.4548e-01, Meta loss averaged over last 500 steps = 3.1849e-01, PNorm = 113.2376, GNorm = 0.2296
Meta loss on this task batch = 3.2335e-01, Meta loss averaged over last 500 steps = 3.1838e-01, PNorm = 113.2461, GNorm = 0.2324
Meta loss on this task batch = 3.1121e-01, Meta loss averaged over last 500 steps = 3.1843e-01, PNorm = 113.2544, GNorm = 0.2359
Meta loss on this task batch = 3.4725e-01, Meta loss averaged over last 500 steps = 3.1842e-01, PNorm = 113.2607, GNorm = 0.2503
Meta loss on this task batch = 3.4172e-01, Meta loss averaged over last 500 steps = 3.1851e-01, PNorm = 113.2667, GNorm = 0.2318
Meta loss on this task batch = 3.0152e-01, Meta loss averaged over last 500 steps = 3.1835e-01, PNorm = 113.2730, GNorm = 0.1956
Meta loss on this task batch = 2.6477e-01, Meta loss averaged over last 500 steps = 3.1830e-01, PNorm = 113.2800, GNorm = 0.2169
Took 149.2672119140625 seconds to complete one epoch of meta training
Took 156.8904242515564 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483961
Epoch 316
Meta loss on this task batch = 2.7364e-01, Meta loss averaged over last 500 steps = 3.1814e-01, PNorm = 113.2860, GNorm = 0.2090
Meta loss on this task batch = 2.9972e-01, Meta loss averaged over last 500 steps = 3.1813e-01, PNorm = 113.2923, GNorm = 0.2151
Meta loss on this task batch = 3.8666e-01, Meta loss averaged over last 500 steps = 3.1834e-01, PNorm = 113.2984, GNorm = 0.1987
Meta loss on this task batch = 3.4341e-01, Meta loss averaged over last 500 steps = 3.1839e-01, PNorm = 113.3049, GNorm = 0.1906
Meta loss on this task batch = 3.1185e-01, Meta loss averaged over last 500 steps = 3.1835e-01, PNorm = 113.3123, GNorm = 0.1974
Meta loss on this task batch = 4.0761e-01, Meta loss averaged over last 500 steps = 3.1868e-01, PNorm = 113.3188, GNorm = 0.2828
Meta loss on this task batch = 3.3535e-01, Meta loss averaged over last 500 steps = 3.1868e-01, PNorm = 113.3259, GNorm = 0.2105
Meta loss on this task batch = 3.5482e-01, Meta loss averaged over last 500 steps = 3.1878e-01, PNorm = 113.3336, GNorm = 0.2312
Meta loss on this task batch = 3.4851e-01, Meta loss averaged over last 500 steps = 3.1887e-01, PNorm = 113.3412, GNorm = 0.2217
Meta loss on this task batch = 3.5280e-01, Meta loss averaged over last 500 steps = 3.1899e-01, PNorm = 113.3486, GNorm = 0.2122
Meta loss on this task batch = 3.2281e-01, Meta loss averaged over last 500 steps = 3.1902e-01, PNorm = 113.3557, GNorm = 0.2284
Meta loss on this task batch = 2.6235e-01, Meta loss averaged over last 500 steps = 3.1892e-01, PNorm = 113.3627, GNorm = 0.2049
Meta loss on this task batch = 2.8713e-01, Meta loss averaged over last 500 steps = 3.1885e-01, PNorm = 113.3697, GNorm = 0.2183
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 3.1869e-01, PNorm = 113.3773, GNorm = 0.2122
Meta loss on this task batch = 2.9384e-01, Meta loss averaged over last 500 steps = 3.1863e-01, PNorm = 113.3851, GNorm = 0.2273
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 3.1857e-01, PNorm = 113.3916, GNorm = 0.1980
Meta loss on this task batch = 2.9010e-01, Meta loss averaged over last 500 steps = 3.1845e-01, PNorm = 113.3981, GNorm = 0.2510
Meta loss on this task batch = 2.8304e-01, Meta loss averaged over last 500 steps = 3.1852e-01, PNorm = 113.4048, GNorm = 0.2130
Meta loss on this task batch = 3.8633e-01, Meta loss averaged over last 500 steps = 3.1854e-01, PNorm = 113.4103, GNorm = 0.3097
Took 114.38980793952942 seconds to complete one epoch of meta training
Took 121.8947765827179 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488748
Epoch 317
Meta loss on this task batch = 3.3192e-01, Meta loss averaged over last 500 steps = 3.1846e-01, PNorm = 113.4168, GNorm = 0.2417
Meta loss on this task batch = 2.9018e-01, Meta loss averaged over last 500 steps = 3.1840e-01, PNorm = 113.4238, GNorm = 0.2127
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 3.1835e-01, PNorm = 113.4320, GNorm = 0.2043
Meta loss on this task batch = 3.0021e-01, Meta loss averaged over last 500 steps = 3.1831e-01, PNorm = 113.4402, GNorm = 0.2115
Meta loss on this task batch = 3.0140e-01, Meta loss averaged over last 500 steps = 3.1821e-01, PNorm = 113.4478, GNorm = 0.2484
Meta loss on this task batch = 3.3561e-01, Meta loss averaged over last 500 steps = 3.1820e-01, PNorm = 113.4547, GNorm = 0.2470
Meta loss on this task batch = 2.8999e-01, Meta loss averaged over last 500 steps = 3.1810e-01, PNorm = 113.4630, GNorm = 0.2152
Meta loss on this task batch = 2.8501e-01, Meta loss averaged over last 500 steps = 3.1804e-01, PNorm = 113.4719, GNorm = 0.1887
Meta loss on this task batch = 2.6783e-01, Meta loss averaged over last 500 steps = 3.1799e-01, PNorm = 113.4801, GNorm = 0.2048
Meta loss on this task batch = 3.0816e-01, Meta loss averaged over last 500 steps = 3.1794e-01, PNorm = 113.4881, GNorm = 0.2135
Meta loss on this task batch = 3.5017e-01, Meta loss averaged over last 500 steps = 3.1798e-01, PNorm = 113.4953, GNorm = 0.2536
Meta loss on this task batch = 3.1729e-01, Meta loss averaged over last 500 steps = 3.1795e-01, PNorm = 113.5024, GNorm = 0.2349
Meta loss on this task batch = 3.2517e-01, Meta loss averaged over last 500 steps = 3.1793e-01, PNorm = 113.5091, GNorm = 0.2213
Meta loss on this task batch = 3.7370e-01, Meta loss averaged over last 500 steps = 3.1808e-01, PNorm = 113.5149, GNorm = 0.2682
Meta loss on this task batch = 2.7446e-01, Meta loss averaged over last 500 steps = 3.1800e-01, PNorm = 113.5211, GNorm = 0.2004
Meta loss on this task batch = 3.6392e-01, Meta loss averaged over last 500 steps = 3.1820e-01, PNorm = 113.5261, GNorm = 0.2299
Meta loss on this task batch = 3.1729e-01, Meta loss averaged over last 500 steps = 3.1815e-01, PNorm = 113.5319, GNorm = 0.2326
Meta loss on this task batch = 2.8045e-01, Meta loss averaged over last 500 steps = 3.1808e-01, PNorm = 113.5376, GNorm = 0.1977
Meta loss on this task batch = 3.7854e-01, Meta loss averaged over last 500 steps = 3.1823e-01, PNorm = 113.5432, GNorm = 0.3014
Took 114.16375946998596 seconds to complete one epoch of meta training
Took 120.75844216346741 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501700
Epoch 318
Meta loss on this task batch = 3.2990e-01, Meta loss averaged over last 500 steps = 3.1823e-01, PNorm = 113.5498, GNorm = 0.2022
Meta loss on this task batch = 3.5479e-01, Meta loss averaged over last 500 steps = 3.1830e-01, PNorm = 113.5583, GNorm = 0.2208
Meta loss on this task batch = 2.8552e-01, Meta loss averaged over last 500 steps = 3.1825e-01, PNorm = 113.5677, GNorm = 0.1902
Meta loss on this task batch = 2.9220e-01, Meta loss averaged over last 500 steps = 3.1821e-01, PNorm = 113.5784, GNorm = 0.1810
Meta loss on this task batch = 2.6900e-01, Meta loss averaged over last 500 steps = 3.1808e-01, PNorm = 113.5897, GNorm = 0.1824
Meta loss on this task batch = 3.1247e-01, Meta loss averaged over last 500 steps = 3.1800e-01, PNorm = 113.6021, GNorm = 0.2342
Meta loss on this task batch = 3.2535e-01, Meta loss averaged over last 500 steps = 3.1799e-01, PNorm = 113.6133, GNorm = 0.2404
Meta loss on this task batch = 3.3921e-01, Meta loss averaged over last 500 steps = 3.1804e-01, PNorm = 113.6239, GNorm = 0.2232
Meta loss on this task batch = 3.4484e-01, Meta loss averaged over last 500 steps = 3.1803e-01, PNorm = 113.6323, GNorm = 0.2422
Meta loss on this task batch = 2.6428e-01, Meta loss averaged over last 500 steps = 3.1792e-01, PNorm = 113.6411, GNorm = 0.1952
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 3.1793e-01, PNorm = 113.6498, GNorm = 0.2218
Meta loss on this task batch = 2.5531e-01, Meta loss averaged over last 500 steps = 3.1784e-01, PNorm = 113.6589, GNorm = 0.1991
Meta loss on this task batch = 3.4384e-01, Meta loss averaged over last 500 steps = 3.1785e-01, PNorm = 113.6670, GNorm = 0.2318
Meta loss on this task batch = 3.5040e-01, Meta loss averaged over last 500 steps = 3.1786e-01, PNorm = 113.6737, GNorm = 0.2497
Meta loss on this task batch = 3.4561e-01, Meta loss averaged over last 500 steps = 3.1798e-01, PNorm = 113.6807, GNorm = 0.2529
Meta loss on this task batch = 3.4760e-01, Meta loss averaged over last 500 steps = 3.1801e-01, PNorm = 113.6862, GNorm = 0.2208
Meta loss on this task batch = 2.7669e-01, Meta loss averaged over last 500 steps = 3.1786e-01, PNorm = 113.6929, GNorm = 0.2082
Meta loss on this task batch = 3.2549e-01, Meta loss averaged over last 500 steps = 3.1790e-01, PNorm = 113.6992, GNorm = 0.2169
Meta loss on this task batch = 2.6843e-01, Meta loss averaged over last 500 steps = 3.1782e-01, PNorm = 113.7053, GNorm = 0.2406
Took 116.44208574295044 seconds to complete one epoch of meta training
Took 123.92220878601074 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507311
Epoch 319
Meta loss on this task batch = 2.8426e-01, Meta loss averaged over last 500 steps = 3.1768e-01, PNorm = 113.7122, GNorm = 0.2072
Meta loss on this task batch = 3.8585e-01, Meta loss averaged over last 500 steps = 3.1776e-01, PNorm = 113.7183, GNorm = 0.2670
Meta loss on this task batch = 3.0205e-01, Meta loss averaged over last 500 steps = 3.1779e-01, PNorm = 113.7259, GNorm = 0.2310
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 3.1776e-01, PNorm = 113.7346, GNorm = 0.2212
Meta loss on this task batch = 2.8513e-01, Meta loss averaged over last 500 steps = 3.1770e-01, PNorm = 113.7439, GNorm = 0.1893
Meta loss on this task batch = 3.3275e-01, Meta loss averaged over last 500 steps = 3.1761e-01, PNorm = 113.7526, GNorm = 0.2426
Meta loss on this task batch = 3.3718e-01, Meta loss averaged over last 500 steps = 3.1761e-01, PNorm = 113.7603, GNorm = 0.2204
Meta loss on this task batch = 3.0000e-01, Meta loss averaged over last 500 steps = 3.1763e-01, PNorm = 113.7674, GNorm = 0.2330
Meta loss on this task batch = 3.0077e-01, Meta loss averaged over last 500 steps = 3.1757e-01, PNorm = 113.7749, GNorm = 0.2143
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 3.1755e-01, PNorm = 113.7826, GNorm = 0.1984
Meta loss on this task batch = 2.5798e-01, Meta loss averaged over last 500 steps = 3.1742e-01, PNorm = 113.7896, GNorm = 0.2204
Meta loss on this task batch = 2.8927e-01, Meta loss averaged over last 500 steps = 3.1723e-01, PNorm = 113.7974, GNorm = 0.2128
Meta loss on this task batch = 3.5593e-01, Meta loss averaged over last 500 steps = 3.1737e-01, PNorm = 113.8057, GNorm = 0.2553
Meta loss on this task batch = 3.0147e-01, Meta loss averaged over last 500 steps = 3.1749e-01, PNorm = 113.8140, GNorm = 0.2281
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 3.1749e-01, PNorm = 113.8220, GNorm = 0.2418
Meta loss on this task batch = 3.3612e-01, Meta loss averaged over last 500 steps = 3.1741e-01, PNorm = 113.8296, GNorm = 0.2976
Meta loss on this task batch = 3.9481e-01, Meta loss averaged over last 500 steps = 3.1750e-01, PNorm = 113.8344, GNorm = 0.3187
Meta loss on this task batch = 3.6160e-01, Meta loss averaged over last 500 steps = 3.1754e-01, PNorm = 113.8382, GNorm = 0.2453
Meta loss on this task batch = 3.1275e-01, Meta loss averaged over last 500 steps = 3.1752e-01, PNorm = 113.8439, GNorm = 0.2758
Took 123.03516960144043 seconds to complete one epoch of meta training
Took 131.35065531730652 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.524909
Epoch 320
Meta loss on this task batch = 2.7241e-01, Meta loss averaged over last 500 steps = 3.1744e-01, PNorm = 113.8503, GNorm = 0.2028
Meta loss on this task batch = 3.1772e-01, Meta loss averaged over last 500 steps = 3.1758e-01, PNorm = 113.8571, GNorm = 0.2199
Meta loss on this task batch = 3.2544e-01, Meta loss averaged over last 500 steps = 3.1752e-01, PNorm = 113.8643, GNorm = 0.2018
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 3.1757e-01, PNorm = 113.8723, GNorm = 0.2044
Meta loss on this task batch = 2.6054e-01, Meta loss averaged over last 500 steps = 3.1731e-01, PNorm = 113.8805, GNorm = 0.1891
Meta loss on this task batch = 3.4195e-01, Meta loss averaged over last 500 steps = 3.1734e-01, PNorm = 113.8890, GNorm = 0.2563
Meta loss on this task batch = 2.5653e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 113.8994, GNorm = 0.2198
Meta loss on this task batch = 2.7727e-01, Meta loss averaged over last 500 steps = 3.1720e-01, PNorm = 113.9108, GNorm = 0.2021
Meta loss on this task batch = 3.7017e-01, Meta loss averaged over last 500 steps = 3.1729e-01, PNorm = 113.9220, GNorm = 0.2247
Meta loss on this task batch = 3.1999e-01, Meta loss averaged over last 500 steps = 3.1732e-01, PNorm = 113.9323, GNorm = 0.2138
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 3.1731e-01, PNorm = 113.9417, GNorm = 0.2088
Meta loss on this task batch = 3.9203e-01, Meta loss averaged over last 500 steps = 3.1745e-01, PNorm = 113.9492, GNorm = 0.2577
Meta loss on this task batch = 3.0196e-01, Meta loss averaged over last 500 steps = 3.1742e-01, PNorm = 113.9561, GNorm = 0.2299
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 3.1741e-01, PNorm = 113.9635, GNorm = 0.1966
Meta loss on this task batch = 2.5293e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 113.9721, GNorm = 0.2083
Meta loss on this task batch = 3.0200e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 113.9801, GNorm = 0.2001
Meta loss on this task batch = 3.3850e-01, Meta loss averaged over last 500 steps = 3.1722e-01, PNorm = 113.9872, GNorm = 0.2405
Meta loss on this task batch = 3.3472e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 113.9945, GNorm = 0.2003
Meta loss on this task batch = 4.0639e-01, Meta loss averaged over last 500 steps = 3.1742e-01, PNorm = 113.9998, GNorm = 0.3177
Took 240.89860820770264 seconds to complete one epoch of meta training
Took 248.41753244400024 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493285
Epoch 321
Meta loss on this task batch = 2.7462e-01, Meta loss averaged over last 500 steps = 3.1739e-01, PNorm = 114.0063, GNorm = 0.1973
Meta loss on this task batch = 2.7551e-01, Meta loss averaged over last 500 steps = 3.1733e-01, PNorm = 114.0132, GNorm = 0.2095
Meta loss on this task batch = 3.5544e-01, Meta loss averaged over last 500 steps = 3.1741e-01, PNorm = 114.0199, GNorm = 0.2341
Meta loss on this task batch = 3.0545e-01, Meta loss averaged over last 500 steps = 3.1726e-01, PNorm = 114.0276, GNorm = 0.2197
Meta loss on this task batch = 3.0766e-01, Meta loss averaged over last 500 steps = 3.1716e-01, PNorm = 114.0353, GNorm = 0.1885
Meta loss on this task batch = 3.1370e-01, Meta loss averaged over last 500 steps = 3.1711e-01, PNorm = 114.0432, GNorm = 0.2432
Meta loss on this task batch = 3.1595e-01, Meta loss averaged over last 500 steps = 3.1716e-01, PNorm = 114.0507, GNorm = 0.2359
Meta loss on this task batch = 3.3877e-01, Meta loss averaged over last 500 steps = 3.1729e-01, PNorm = 114.0584, GNorm = 0.2603
Meta loss on this task batch = 3.8860e-01, Meta loss averaged over last 500 steps = 3.1731e-01, PNorm = 114.0656, GNorm = 0.2539
Meta loss on this task batch = 3.0922e-01, Meta loss averaged over last 500 steps = 3.1733e-01, PNorm = 114.0728, GNorm = 0.1902
Meta loss on this task batch = 3.0827e-01, Meta loss averaged over last 500 steps = 3.1732e-01, PNorm = 114.0808, GNorm = 0.2036
Meta loss on this task batch = 3.2482e-01, Meta loss averaged over last 500 steps = 3.1714e-01, PNorm = 114.0894, GNorm = 0.2501
Meta loss on this task batch = 3.6359e-01, Meta loss averaged over last 500 steps = 3.1713e-01, PNorm = 114.0973, GNorm = 0.2273
Meta loss on this task batch = 2.6670e-01, Meta loss averaged over last 500 steps = 3.1708e-01, PNorm = 114.1070, GNorm = 0.2025
Meta loss on this task batch = 2.4581e-01, Meta loss averaged over last 500 steps = 3.1679e-01, PNorm = 114.1181, GNorm = 0.1965
Meta loss on this task batch = 3.2630e-01, Meta loss averaged over last 500 steps = 3.1686e-01, PNorm = 114.1287, GNorm = 0.2513
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 3.1694e-01, PNorm = 114.1374, GNorm = 0.2290
Meta loss on this task batch = 3.6421e-01, Meta loss averaged over last 500 steps = 3.1710e-01, PNorm = 114.1445, GNorm = 0.2655
Meta loss on this task batch = 3.3794e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 114.1515, GNorm = 0.2863
Took 247.6802215576172 seconds to complete one epoch of meta training
Took 254.7225697040558 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503728
Epoch 322
Meta loss on this task batch = 3.5974e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 114.1584, GNorm = 0.2011
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 3.1715e-01, PNorm = 114.1651, GNorm = 0.1946
Meta loss on this task batch = 3.4407e-01, Meta loss averaged over last 500 steps = 3.1719e-01, PNorm = 114.1710, GNorm = 0.2194
Meta loss on this task batch = 3.3537e-01, Meta loss averaged over last 500 steps = 3.1719e-01, PNorm = 114.1759, GNorm = 0.2302
Meta loss on this task batch = 2.7204e-01, Meta loss averaged over last 500 steps = 3.1712e-01, PNorm = 114.1809, GNorm = 0.2038
Meta loss on this task batch = 3.3562e-01, Meta loss averaged over last 500 steps = 3.1729e-01, PNorm = 114.1873, GNorm = 0.2250
Meta loss on this task batch = 3.1383e-01, Meta loss averaged over last 500 steps = 3.1728e-01, PNorm = 114.1932, GNorm = 0.1972
Meta loss on this task batch = 3.0158e-01, Meta loss averaged over last 500 steps = 3.1739e-01, PNorm = 114.1995, GNorm = 0.2151
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 3.1736e-01, PNorm = 114.2070, GNorm = 0.2089
Meta loss on this task batch = 2.7004e-01, Meta loss averaged over last 500 steps = 3.1724e-01, PNorm = 114.2153, GNorm = 0.1934
Meta loss on this task batch = 3.1737e-01, Meta loss averaged over last 500 steps = 3.1710e-01, PNorm = 114.2243, GNorm = 0.2187
Meta loss on this task batch = 3.5995e-01, Meta loss averaged over last 500 steps = 3.1717e-01, PNorm = 114.2322, GNorm = 0.2158
Meta loss on this task batch = 2.9013e-01, Meta loss averaged over last 500 steps = 3.1708e-01, PNorm = 114.2413, GNorm = 0.1997
Meta loss on this task batch = 2.9974e-01, Meta loss averaged over last 500 steps = 3.1707e-01, PNorm = 114.2507, GNorm = 0.2246
Meta loss on this task batch = 2.7024e-01, Meta loss averaged over last 500 steps = 3.1698e-01, PNorm = 114.2611, GNorm = 0.2359
Meta loss on this task batch = 2.5314e-01, Meta loss averaged over last 500 steps = 3.1685e-01, PNorm = 114.2710, GNorm = 0.1860
Meta loss on this task batch = 3.7030e-01, Meta loss averaged over last 500 steps = 3.1694e-01, PNorm = 114.2778, GNorm = 0.2946
Meta loss on this task batch = 3.2677e-01, Meta loss averaged over last 500 steps = 3.1694e-01, PNorm = 114.2843, GNorm = 0.2244
Meta loss on this task batch = 3.6258e-01, Meta loss averaged over last 500 steps = 3.1704e-01, PNorm = 114.2908, GNorm = 0.2559
Took 127.63902163505554 seconds to complete one epoch of meta training
Took 135.90453267097473 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498685
Epoch 323
Meta loss on this task batch = 3.3363e-01, Meta loss averaged over last 500 steps = 3.1714e-01, PNorm = 114.2983, GNorm = 0.2399
Meta loss on this task batch = 3.3577e-01, Meta loss averaged over last 500 steps = 3.1704e-01, PNorm = 114.3058, GNorm = 0.2336
Meta loss on this task batch = 3.0338e-01, Meta loss averaged over last 500 steps = 3.1713e-01, PNorm = 114.3140, GNorm = 0.2068
Meta loss on this task batch = 2.7557e-01, Meta loss averaged over last 500 steps = 3.1708e-01, PNorm = 114.3226, GNorm = 0.1949
Meta loss on this task batch = 3.2534e-01, Meta loss averaged over last 500 steps = 3.1717e-01, PNorm = 114.3313, GNorm = 0.2214
Meta loss on this task batch = 3.3056e-01, Meta loss averaged over last 500 steps = 3.1715e-01, PNorm = 114.3399, GNorm = 0.2393
Meta loss on this task batch = 3.5336e-01, Meta loss averaged over last 500 steps = 3.1730e-01, PNorm = 114.3485, GNorm = 0.2275
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 3.1725e-01, PNorm = 114.3576, GNorm = 0.2086
Meta loss on this task batch = 3.1553e-01, Meta loss averaged over last 500 steps = 3.1735e-01, PNorm = 114.3665, GNorm = 0.1928
Meta loss on this task batch = 2.6095e-01, Meta loss averaged over last 500 steps = 3.1711e-01, PNorm = 114.3746, GNorm = 0.2213
Meta loss on this task batch = 2.0340e-01, Meta loss averaged over last 500 steps = 3.1687e-01, PNorm = 114.3843, GNorm = 0.1912
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 3.1676e-01, PNorm = 114.3930, GNorm = 0.2349
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 3.1673e-01, PNorm = 114.4005, GNorm = 0.2624
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 114.4082, GNorm = 0.2245
Meta loss on this task batch = 2.8161e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 114.4148, GNorm = 0.2497
Meta loss on this task batch = 3.3261e-01, Meta loss averaged over last 500 steps = 3.1646e-01, PNorm = 114.4213, GNorm = 0.2367
Meta loss on this task batch = 3.1937e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 114.4274, GNorm = 0.2099
Meta loss on this task batch = 4.1592e-01, Meta loss averaged over last 500 steps = 3.1660e-01, PNorm = 114.4316, GNorm = 0.2641
Meta loss on this task batch = 3.3105e-01, Meta loss averaged over last 500 steps = 3.1658e-01, PNorm = 114.4357, GNorm = 0.2755
Took 116.35721588134766 seconds to complete one epoch of meta training
Took 123.95756006240845 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498973
Epoch 324
Meta loss on this task batch = 2.7645e-01, Meta loss averaged over last 500 steps = 3.1656e-01, PNorm = 114.4413, GNorm = 0.2215
Meta loss on this task batch = 2.4242e-01, Meta loss averaged over last 500 steps = 3.1636e-01, PNorm = 114.4484, GNorm = 0.1858
Meta loss on this task batch = 3.3459e-01, Meta loss averaged over last 500 steps = 3.1646e-01, PNorm = 114.4567, GNorm = 0.2320
Meta loss on this task batch = 3.1714e-01, Meta loss averaged over last 500 steps = 3.1652e-01, PNorm = 114.4653, GNorm = 0.2215
Meta loss on this task batch = 3.4973e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 114.4743, GNorm = 0.2038
Meta loss on this task batch = 3.1085e-01, Meta loss averaged over last 500 steps = 3.1656e-01, PNorm = 114.4837, GNorm = 0.2015
Meta loss on this task batch = 3.6456e-01, Meta loss averaged over last 500 steps = 3.1647e-01, PNorm = 114.4926, GNorm = 0.2096
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 114.5011, GNorm = 0.2002
Meta loss on this task batch = 2.7319e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 114.5100, GNorm = 0.2013
Meta loss on this task batch = 3.3993e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 114.5185, GNorm = 0.2288
Meta loss on this task batch = 3.6073e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 114.5258, GNorm = 0.2477
Meta loss on this task batch = 3.1124e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 114.5326, GNorm = 0.2309
Meta loss on this task batch = 3.2626e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 114.5400, GNorm = 0.1999
Meta loss on this task batch = 3.6171e-01, Meta loss averaged over last 500 steps = 3.1656e-01, PNorm = 114.5468, GNorm = 0.2351
Meta loss on this task batch = 2.8220e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 114.5547, GNorm = 0.2043
Meta loss on this task batch = 3.1319e-01, Meta loss averaged over last 500 steps = 3.1652e-01, PNorm = 114.5633, GNorm = 0.2343
Meta loss on this task batch = 3.3889e-01, Meta loss averaged over last 500 steps = 3.1657e-01, PNorm = 114.5720, GNorm = 0.2085
Meta loss on this task batch = 3.0357e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 114.5807, GNorm = 0.1931
Meta loss on this task batch = 2.9767e-01, Meta loss averaged over last 500 steps = 3.1668e-01, PNorm = 114.5893, GNorm = 0.2279
Took 115.84731531143188 seconds to complete one epoch of meta training
Took 123.60553765296936 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505629
Epoch 325
Meta loss on this task batch = 3.2793e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 114.5983, GNorm = 0.1994
Meta loss on this task batch = 3.2406e-01, Meta loss averaged over last 500 steps = 3.1658e-01, PNorm = 114.6060, GNorm = 0.2407
Meta loss on this task batch = 3.1414e-01, Meta loss averaged over last 500 steps = 3.1667e-01, PNorm = 114.6131, GNorm = 0.2369
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 3.1678e-01, PNorm = 114.6207, GNorm = 0.2142
Meta loss on this task batch = 3.5914e-01, Meta loss averaged over last 500 steps = 3.1685e-01, PNorm = 114.6260, GNorm = 0.2906
Meta loss on this task batch = 3.2968e-01, Meta loss averaged over last 500 steps = 3.1684e-01, PNorm = 114.6314, GNorm = 0.2045
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 3.1685e-01, PNorm = 114.6369, GNorm = 0.1998
Meta loss on this task batch = 3.2862e-01, Meta loss averaged over last 500 steps = 3.1696e-01, PNorm = 114.6431, GNorm = 0.1982
Meta loss on this task batch = 3.4500e-01, Meta loss averaged over last 500 steps = 3.1701e-01, PNorm = 114.6508, GNorm = 0.2447
Meta loss on this task batch = 3.2537e-01, Meta loss averaged over last 500 steps = 3.1705e-01, PNorm = 114.6582, GNorm = 0.2256
Meta loss on this task batch = 2.9852e-01, Meta loss averaged over last 500 steps = 3.1695e-01, PNorm = 114.6669, GNorm = 0.1925
Meta loss on this task batch = 3.3250e-01, Meta loss averaged over last 500 steps = 3.1697e-01, PNorm = 114.6763, GNorm = 0.1997
Meta loss on this task batch = 3.2532e-01, Meta loss averaged over last 500 steps = 3.1701e-01, PNorm = 114.6861, GNorm = 0.2278
Meta loss on this task batch = 3.0439e-01, Meta loss averaged over last 500 steps = 3.1703e-01, PNorm = 114.6949, GNorm = 0.2057
Meta loss on this task batch = 2.8719e-01, Meta loss averaged over last 500 steps = 3.1689e-01, PNorm = 114.7035, GNorm = 0.2026
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 3.1690e-01, PNorm = 114.7115, GNorm = 0.2159
Meta loss on this task batch = 3.2937e-01, Meta loss averaged over last 500 steps = 3.1690e-01, PNorm = 114.7191, GNorm = 0.2346
Meta loss on this task batch = 3.1665e-01, Meta loss averaged over last 500 steps = 3.1691e-01, PNorm = 114.7262, GNorm = 0.2458
Meta loss on this task batch = 3.0775e-01, Meta loss averaged over last 500 steps = 3.1691e-01, PNorm = 114.7338, GNorm = 0.2593
Took 111.57817029953003 seconds to complete one epoch of meta training
Took 119.42197895050049 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505230
Epoch 326
Meta loss on this task batch = 2.5993e-01, Meta loss averaged over last 500 steps = 3.1682e-01, PNorm = 114.7420, GNorm = 0.2146
Meta loss on this task batch = 3.0809e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 114.7507, GNorm = 0.2003
Meta loss on this task batch = 2.7756e-01, Meta loss averaged over last 500 steps = 3.1669e-01, PNorm = 114.7596, GNorm = 0.2363
Meta loss on this task batch = 3.3454e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 114.7657, GNorm = 0.2869
Meta loss on this task batch = 3.5810e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 114.7718, GNorm = 0.2364
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 3.1670e-01, PNorm = 114.7778, GNorm = 0.2240
Meta loss on this task batch = 3.6865e-01, Meta loss averaged over last 500 steps = 3.1676e-01, PNorm = 114.7831, GNorm = 0.2283
Meta loss on this task batch = 3.5828e-01, Meta loss averaged over last 500 steps = 3.1689e-01, PNorm = 114.7889, GNorm = 0.2558
Meta loss on this task batch = 3.4478e-01, Meta loss averaged over last 500 steps = 3.1693e-01, PNorm = 114.7948, GNorm = 0.2120
Meta loss on this task batch = 2.9559e-01, Meta loss averaged over last 500 steps = 3.1684e-01, PNorm = 114.8004, GNorm = 0.2311
Meta loss on this task batch = 3.1163e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 114.8073, GNorm = 0.2056
Meta loss on this task batch = 3.2929e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 114.8146, GNorm = 0.2340
Meta loss on this task batch = 3.0903e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 114.8222, GNorm = 0.2196
Meta loss on this task batch = 2.8401e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 114.8303, GNorm = 0.1884
Meta loss on this task batch = 2.7320e-01, Meta loss averaged over last 500 steps = 3.1661e-01, PNorm = 114.8395, GNorm = 0.1794
Meta loss on this task batch = 2.9002e-01, Meta loss averaged over last 500 steps = 3.1666e-01, PNorm = 114.8490, GNorm = 0.2069
Meta loss on this task batch = 3.1192e-01, Meta loss averaged over last 500 steps = 3.1662e-01, PNorm = 114.8590, GNorm = 0.2174
Meta loss on this task batch = 3.9657e-01, Meta loss averaged over last 500 steps = 3.1669e-01, PNorm = 114.8672, GNorm = 0.2674
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 3.1664e-01, PNorm = 114.8756, GNorm = 0.2408
Took 115.01713156700134 seconds to complete one epoch of meta training
Took 122.73227643966675 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516557
Epoch 327
Meta loss on this task batch = 3.1258e-01, Meta loss averaged over last 500 steps = 3.1667e-01, PNorm = 114.8836, GNorm = 0.2029
Meta loss on this task batch = 3.1715e-01, Meta loss averaged over last 500 steps = 3.1672e-01, PNorm = 114.8912, GNorm = 0.2163
Meta loss on this task batch = 3.1892e-01, Meta loss averaged over last 500 steps = 3.1667e-01, PNorm = 114.8983, GNorm = 0.2025
Meta loss on this task batch = 3.3471e-01, Meta loss averaged over last 500 steps = 3.1663e-01, PNorm = 114.9043, GNorm = 0.2306
Meta loss on this task batch = 3.0890e-01, Meta loss averaged over last 500 steps = 3.1677e-01, PNorm = 114.9106, GNorm = 0.2181
Meta loss on this task batch = 2.8024e-01, Meta loss averaged over last 500 steps = 3.1652e-01, PNorm = 114.9174, GNorm = 0.2004
Meta loss on this task batch = 2.8721e-01, Meta loss averaged over last 500 steps = 3.1648e-01, PNorm = 114.9243, GNorm = 0.2248
Meta loss on this task batch = 3.2320e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 114.9321, GNorm = 0.1971
Meta loss on this task batch = 2.5821e-01, Meta loss averaged over last 500 steps = 3.1638e-01, PNorm = 114.9410, GNorm = 0.2089
Meta loss on this task batch = 2.9505e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 114.9502, GNorm = 0.2020
Meta loss on this task batch = 3.1709e-01, Meta loss averaged over last 500 steps = 3.1626e-01, PNorm = 114.9591, GNorm = 0.2335
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 3.1640e-01, PNorm = 114.9676, GNorm = 0.1968
Meta loss on this task batch = 3.1178e-01, Meta loss averaged over last 500 steps = 3.1646e-01, PNorm = 114.9780, GNorm = 0.2390
Meta loss on this task batch = 3.1545e-01, Meta loss averaged over last 500 steps = 3.1652e-01, PNorm = 114.9876, GNorm = 0.2307
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 114.9974, GNorm = 0.2094
Meta loss on this task batch = 3.4686e-01, Meta loss averaged over last 500 steps = 3.1655e-01, PNorm = 115.0065, GNorm = 0.2339
Meta loss on this task batch = 3.1104e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 115.0148, GNorm = 0.2352
Meta loss on this task batch = 3.1427e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 115.0224, GNorm = 0.2473
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 3.1643e-01, PNorm = 115.0311, GNorm = 0.2762
Took 113.33645915985107 seconds to complete one epoch of meta training
Took 120.97376012802124 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510151
Epoch 328
Meta loss on this task batch = 3.1614e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 115.0384, GNorm = 0.2513
Meta loss on this task batch = 3.4883e-01, Meta loss averaged over last 500 steps = 3.1641e-01, PNorm = 115.0455, GNorm = 0.2507
Meta loss on this task batch = 3.6449e-01, Meta loss averaged over last 500 steps = 3.1651e-01, PNorm = 115.0532, GNorm = 0.2992
Meta loss on this task batch = 3.0960e-01, Meta loss averaged over last 500 steps = 3.1639e-01, PNorm = 115.0602, GNorm = 0.2125
Meta loss on this task batch = 3.5686e-01, Meta loss averaged over last 500 steps = 3.1651e-01, PNorm = 115.0674, GNorm = 0.2472
Meta loss on this task batch = 3.0521e-01, Meta loss averaged over last 500 steps = 3.1654e-01, PNorm = 115.0747, GNorm = 0.1955
Meta loss on this task batch = 3.1831e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 115.0812, GNorm = 0.2274
Meta loss on this task batch = 2.8234e-01, Meta loss averaged over last 500 steps = 3.1647e-01, PNorm = 115.0887, GNorm = 0.1814
Meta loss on this task batch = 3.0965e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 115.0961, GNorm = 0.2208
Meta loss on this task batch = 2.8307e-01, Meta loss averaged over last 500 steps = 3.1653e-01, PNorm = 115.1048, GNorm = 0.1929
Meta loss on this task batch = 3.1016e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 115.1130, GNorm = 0.2247
Meta loss on this task batch = 3.3640e-01, Meta loss averaged over last 500 steps = 3.1642e-01, PNorm = 115.1210, GNorm = 0.2276
Meta loss on this task batch = 3.0982e-01, Meta loss averaged over last 500 steps = 3.1635e-01, PNorm = 115.1290, GNorm = 0.2248
Meta loss on this task batch = 2.9562e-01, Meta loss averaged over last 500 steps = 3.1639e-01, PNorm = 115.1376, GNorm = 0.2235
Meta loss on this task batch = 3.2748e-01, Meta loss averaged over last 500 steps = 3.1645e-01, PNorm = 115.1457, GNorm = 0.2125
Meta loss on this task batch = 3.2217e-01, Meta loss averaged over last 500 steps = 3.1649e-01, PNorm = 115.1542, GNorm = 0.2178
Meta loss on this task batch = 3.2238e-01, Meta loss averaged over last 500 steps = 3.1650e-01, PNorm = 115.1619, GNorm = 0.2922
Meta loss on this task batch = 2.9845e-01, Meta loss averaged over last 500 steps = 3.1644e-01, PNorm = 115.1693, GNorm = 0.2005
Meta loss on this task batch = 2.4815e-01, Meta loss averaged over last 500 steps = 3.1621e-01, PNorm = 115.1761, GNorm = 0.2663
Took 115.82028865814209 seconds to complete one epoch of meta training
Took 123.70774984359741 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502007
Epoch 329
Meta loss on this task batch = 3.3698e-01, Meta loss averaged over last 500 steps = 3.1622e-01, PNorm = 115.1832, GNorm = 0.2185
Meta loss on this task batch = 3.0784e-01, Meta loss averaged over last 500 steps = 3.1608e-01, PNorm = 115.1908, GNorm = 0.2566
Meta loss on this task batch = 3.2816e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 115.1988, GNorm = 0.2540
Meta loss on this task batch = 3.6539e-01, Meta loss averaged over last 500 steps = 3.1624e-01, PNorm = 115.2068, GNorm = 0.2458
Meta loss on this task batch = 2.6584e-01, Meta loss averaged over last 500 steps = 3.1610e-01, PNorm = 115.2156, GNorm = 0.2255
Meta loss on this task batch = 2.9961e-01, Meta loss averaged over last 500 steps = 3.1607e-01, PNorm = 115.2244, GNorm = 0.2154
Meta loss on this task batch = 2.8119e-01, Meta loss averaged over last 500 steps = 3.1614e-01, PNorm = 115.2343, GNorm = 0.1998
Meta loss on this task batch = 3.5687e-01, Meta loss averaged over last 500 steps = 3.1625e-01, PNorm = 115.2433, GNorm = 0.2275
Meta loss on this task batch = 2.8255e-01, Meta loss averaged over last 500 steps = 3.1615e-01, PNorm = 115.2523, GNorm = 0.2322
Meta loss on this task batch = 2.8461e-01, Meta loss averaged over last 500 steps = 3.1613e-01, PNorm = 115.2621, GNorm = 0.2000
Meta loss on this task batch = 3.6655e-01, Meta loss averaged over last 500 steps = 3.1611e-01, PNorm = 115.2692, GNorm = 0.2588
Meta loss on this task batch = 3.2344e-01, Meta loss averaged over last 500 steps = 3.1606e-01, PNorm = 115.2758, GNorm = 0.2262
Meta loss on this task batch = 3.3557e-01, Meta loss averaged over last 500 steps = 3.1604e-01, PNorm = 115.2819, GNorm = 0.2129
Meta loss on this task batch = 2.7409e-01, Meta loss averaged over last 500 steps = 3.1608e-01, PNorm = 115.2883, GNorm = 0.2032
Meta loss on this task batch = 3.1005e-01, Meta loss averaged over last 500 steps = 3.1608e-01, PNorm = 115.2952, GNorm = 0.2115
Meta loss on this task batch = 3.2698e-01, Meta loss averaged over last 500 steps = 3.1606e-01, PNorm = 115.3009, GNorm = 0.2656
Meta loss on this task batch = 2.8266e-01, Meta loss averaged over last 500 steps = 3.1593e-01, PNorm = 115.3066, GNorm = 0.2078
Meta loss on this task batch = 3.2852e-01, Meta loss averaged over last 500 steps = 3.1609e-01, PNorm = 115.3119, GNorm = 0.2094
Meta loss on this task batch = 3.3100e-01, Meta loss averaged over last 500 steps = 3.1609e-01, PNorm = 115.3174, GNorm = 0.2251
Took 112.60416603088379 seconds to complete one epoch of meta training
Took 120.4590973854065 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495670
Epoch 330
Meta loss on this task batch = 3.1304e-01, Meta loss averaged over last 500 steps = 3.1613e-01, PNorm = 115.3235, GNorm = 0.2103
Meta loss on this task batch = 3.2386e-01, Meta loss averaged over last 500 steps = 3.1618e-01, PNorm = 115.3297, GNorm = 0.2386
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 3.1612e-01, PNorm = 115.3367, GNorm = 0.2169
Meta loss on this task batch = 2.9628e-01, Meta loss averaged over last 500 steps = 3.1600e-01, PNorm = 115.3453, GNorm = 0.2354
Meta loss on this task batch = 3.2717e-01, Meta loss averaged over last 500 steps = 3.1603e-01, PNorm = 115.3541, GNorm = 0.2337
Meta loss on this task batch = 3.0382e-01, Meta loss averaged over last 500 steps = 3.1598e-01, PNorm = 115.3629, GNorm = 0.2184
Meta loss on this task batch = 2.8728e-01, Meta loss averaged over last 500 steps = 3.1601e-01, PNorm = 115.3723, GNorm = 0.1941
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 3.1578e-01, PNorm = 115.3824, GNorm = 0.2059
Meta loss on this task batch = 3.5490e-01, Meta loss averaged over last 500 steps = 3.1593e-01, PNorm = 115.3930, GNorm = 0.2147
Meta loss on this task batch = 3.4474e-01, Meta loss averaged over last 500 steps = 3.1602e-01, PNorm = 115.4035, GNorm = 0.2327
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 3.1591e-01, PNorm = 115.4145, GNorm = 0.2105
Meta loss on this task batch = 2.9661e-01, Meta loss averaged over last 500 steps = 3.1583e-01, PNorm = 115.4250, GNorm = 0.2384
Meta loss on this task batch = 2.8207e-01, Meta loss averaged over last 500 steps = 3.1569e-01, PNorm = 115.4354, GNorm = 0.2098
Meta loss on this task batch = 3.2813e-01, Meta loss averaged over last 500 steps = 3.1581e-01, PNorm = 115.4448, GNorm = 0.2415
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.1572e-01, PNorm = 115.4520, GNorm = 0.2780
Meta loss on this task batch = 3.6447e-01, Meta loss averaged over last 500 steps = 3.1583e-01, PNorm = 115.4575, GNorm = 0.2926
Meta loss on this task batch = 3.0659e-01, Meta loss averaged over last 500 steps = 3.1580e-01, PNorm = 115.4621, GNorm = 0.2160
Meta loss on this task batch = 3.4241e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 115.4669, GNorm = 0.2121
Meta loss on this task batch = 2.8628e-01, Meta loss averaged over last 500 steps = 3.1589e-01, PNorm = 115.4734, GNorm = 0.2640
Took 115.53623032569885 seconds to complete one epoch of meta training
Took 123.46031546592712 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493770
Epoch 331
Meta loss on this task batch = 3.0204e-01, Meta loss averaged over last 500 steps = 3.1584e-01, PNorm = 115.4807, GNorm = 0.2533
Meta loss on this task batch = 3.3052e-01, Meta loss averaged over last 500 steps = 3.1585e-01, PNorm = 115.4885, GNorm = 0.2160
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 3.1588e-01, PNorm = 115.4954, GNorm = 0.2383
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 3.1578e-01, PNorm = 115.5027, GNorm = 0.2054
Meta loss on this task batch = 3.2045e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 115.5102, GNorm = 0.2132
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 3.1569e-01, PNorm = 115.5182, GNorm = 0.2393
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 3.1577e-01, PNorm = 115.5268, GNorm = 0.2189
Meta loss on this task batch = 3.3096e-01, Meta loss averaged over last 500 steps = 3.1583e-01, PNorm = 115.5345, GNorm = 0.2213
Meta loss on this task batch = 2.6030e-01, Meta loss averaged over last 500 steps = 3.1572e-01, PNorm = 115.5417, GNorm = 0.2059
Meta loss on this task batch = 3.3750e-01, Meta loss averaged over last 500 steps = 3.1581e-01, PNorm = 115.5483, GNorm = 0.2219
Meta loss on this task batch = 3.3492e-01, Meta loss averaged over last 500 steps = 3.1569e-01, PNorm = 115.5557, GNorm = 0.2540
Meta loss on this task batch = 3.2940e-01, Meta loss averaged over last 500 steps = 3.1578e-01, PNorm = 115.5632, GNorm = 0.2215
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 3.1570e-01, PNorm = 115.5710, GNorm = 0.1987
Meta loss on this task batch = 3.2346e-01, Meta loss averaged over last 500 steps = 3.1570e-01, PNorm = 115.5800, GNorm = 0.2605
Meta loss on this task batch = 2.8893e-01, Meta loss averaged over last 500 steps = 3.1564e-01, PNorm = 115.5895, GNorm = 0.2477
Meta loss on this task batch = 3.2377e-01, Meta loss averaged over last 500 steps = 3.1557e-01, PNorm = 115.5992, GNorm = 0.2140
Meta loss on this task batch = 3.3845e-01, Meta loss averaged over last 500 steps = 3.1564e-01, PNorm = 115.6087, GNorm = 0.2214
Meta loss on this task batch = 2.9540e-01, Meta loss averaged over last 500 steps = 3.1549e-01, PNorm = 115.6167, GNorm = 0.2509
Meta loss on this task batch = 3.9073e-01, Meta loss averaged over last 500 steps = 3.1555e-01, PNorm = 115.6227, GNorm = 0.2963
Took 113.25476932525635 seconds to complete one epoch of meta training
Took 120.60425615310669 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482001
Epoch 332
Meta loss on this task batch = 3.4963e-01, Meta loss averaged over last 500 steps = 3.1565e-01, PNorm = 115.6285, GNorm = 0.2282
Meta loss on this task batch = 2.9171e-01, Meta loss averaged over last 500 steps = 3.1573e-01, PNorm = 115.6338, GNorm = 0.2138
Meta loss on this task batch = 3.0399e-01, Meta loss averaged over last 500 steps = 3.1578e-01, PNorm = 115.6401, GNorm = 0.1753
Meta loss on this task batch = 2.8277e-01, Meta loss averaged over last 500 steps = 3.1571e-01, PNorm = 115.6457, GNorm = 0.1933
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 3.1563e-01, PNorm = 115.6501, GNorm = 0.2509
Meta loss on this task batch = 2.8741e-01, Meta loss averaged over last 500 steps = 3.1552e-01, PNorm = 115.6549, GNorm = 0.2122
Meta loss on this task batch = 3.9316e-01, Meta loss averaged over last 500 steps = 3.1565e-01, PNorm = 115.6594, GNorm = 0.2285
Meta loss on this task batch = 4.3301e-01, Meta loss averaged over last 500 steps = 3.1586e-01, PNorm = 115.6636, GNorm = 0.3404
Meta loss on this task batch = 3.0664e-01, Meta loss averaged over last 500 steps = 3.1591e-01, PNorm = 115.6700, GNorm = 0.2783
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 3.1598e-01, PNorm = 115.6785, GNorm = 0.2313
Meta loss on this task batch = 3.2587e-01, Meta loss averaged over last 500 steps = 3.1611e-01, PNorm = 115.6873, GNorm = 0.1969
Meta loss on this task batch = 2.8207e-01, Meta loss averaged over last 500 steps = 3.1605e-01, PNorm = 115.6966, GNorm = 0.1913
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 3.1598e-01, PNorm = 115.7067, GNorm = 0.2028
Meta loss on this task batch = 2.6681e-01, Meta loss averaged over last 500 steps = 3.1583e-01, PNorm = 115.7169, GNorm = 0.2396
Meta loss on this task batch = 3.5000e-01, Meta loss averaged over last 500 steps = 3.1586e-01, PNorm = 115.7263, GNorm = 0.2612
Meta loss on this task batch = 3.3479e-01, Meta loss averaged over last 500 steps = 3.1581e-01, PNorm = 115.7356, GNorm = 0.2360
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 3.1571e-01, PNorm = 115.7437, GNorm = 0.2279
Meta loss on this task batch = 3.2075e-01, Meta loss averaged over last 500 steps = 3.1575e-01, PNorm = 115.7515, GNorm = 0.2494
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 3.1568e-01, PNorm = 115.7591, GNorm = 0.2383
Took 116.87745451927185 seconds to complete one epoch of meta training
Took 124.43511939048767 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517189
Epoch 333
Meta loss on this task batch = 2.9373e-01, Meta loss averaged over last 500 steps = 3.1561e-01, PNorm = 115.7658, GNorm = 0.2003
Meta loss on this task batch = 3.2044e-01, Meta loss averaged over last 500 steps = 3.1556e-01, PNorm = 115.7730, GNorm = 0.2272
Meta loss on this task batch = 3.1377e-01, Meta loss averaged over last 500 steps = 3.1558e-01, PNorm = 115.7805, GNorm = 0.2057
Meta loss on this task batch = 2.9975e-01, Meta loss averaged over last 500 steps = 3.1553e-01, PNorm = 115.7879, GNorm = 0.1988
Meta loss on this task batch = 3.5006e-01, Meta loss averaged over last 500 steps = 3.1573e-01, PNorm = 115.7950, GNorm = 0.2381
Meta loss on this task batch = 2.9038e-01, Meta loss averaged over last 500 steps = 3.1553e-01, PNorm = 115.8021, GNorm = 0.2433
Meta loss on this task batch = 2.6786e-01, Meta loss averaged over last 500 steps = 3.1551e-01, PNorm = 115.8086, GNorm = 0.2146
Meta loss on this task batch = 3.3971e-01, Meta loss averaged over last 500 steps = 3.1558e-01, PNorm = 115.8146, GNorm = 0.2442
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 3.1557e-01, PNorm = 115.8218, GNorm = 0.2107
Meta loss on this task batch = 3.2113e-01, Meta loss averaged over last 500 steps = 3.1550e-01, PNorm = 115.8286, GNorm = 0.2470
Meta loss on this task batch = 3.3071e-01, Meta loss averaged over last 500 steps = 3.1551e-01, PNorm = 115.8356, GNorm = 0.2454
Meta loss on this task batch = 3.1732e-01, Meta loss averaged over last 500 steps = 3.1552e-01, PNorm = 115.8426, GNorm = 0.2245
Meta loss on this task batch = 3.2229e-01, Meta loss averaged over last 500 steps = 3.1547e-01, PNorm = 115.8483, GNorm = 0.2479
Meta loss on this task batch = 3.2957e-01, Meta loss averaged over last 500 steps = 3.1544e-01, PNorm = 115.8538, GNorm = 0.2254
Meta loss on this task batch = 3.0604e-01, Meta loss averaged over last 500 steps = 3.1541e-01, PNorm = 115.8596, GNorm = 0.2115
Meta loss on this task batch = 2.9181e-01, Meta loss averaged over last 500 steps = 3.1522e-01, PNorm = 115.8664, GNorm = 0.2162
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 3.1517e-01, PNorm = 115.8736, GNorm = 0.2594
Meta loss on this task batch = 2.9184e-01, Meta loss averaged over last 500 steps = 3.1520e-01, PNorm = 115.8820, GNorm = 0.2394
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 115.8904, GNorm = 0.2697
Took 146.45006442070007 seconds to complete one epoch of meta training
Took 154.39177656173706 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496201
Epoch 334
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 115.8992, GNorm = 0.2278
Meta loss on this task batch = 3.4746e-01, Meta loss averaged over last 500 steps = 3.1510e-01, PNorm = 115.9084, GNorm = 0.2346
Meta loss on this task batch = 3.0645e-01, Meta loss averaged over last 500 steps = 3.1516e-01, PNorm = 115.9178, GNorm = 0.2171
Meta loss on this task batch = 2.6063e-01, Meta loss averaged over last 500 steps = 3.1509e-01, PNorm = 115.9277, GNorm = 0.2046
Meta loss on this task batch = 2.6941e-01, Meta loss averaged over last 500 steps = 3.1500e-01, PNorm = 115.9377, GNorm = 0.1901
Meta loss on this task batch = 2.7736e-01, Meta loss averaged over last 500 steps = 3.1492e-01, PNorm = 115.9462, GNorm = 0.2132
Meta loss on this task batch = 3.7719e-01, Meta loss averaged over last 500 steps = 3.1501e-01, PNorm = 115.9518, GNorm = 0.2763
Meta loss on this task batch = 3.1199e-01, Meta loss averaged over last 500 steps = 3.1496e-01, PNorm = 115.9562, GNorm = 0.2766
Meta loss on this task batch = 3.4578e-01, Meta loss averaged over last 500 steps = 3.1507e-01, PNorm = 115.9617, GNorm = 0.2013
Meta loss on this task batch = 2.8754e-01, Meta loss averaged over last 500 steps = 3.1508e-01, PNorm = 115.9680, GNorm = 0.1909
Meta loss on this task batch = 3.1175e-01, Meta loss averaged over last 500 steps = 3.1512e-01, PNorm = 115.9750, GNorm = 0.2507
Meta loss on this task batch = 2.7838e-01, Meta loss averaged over last 500 steps = 3.1506e-01, PNorm = 115.9825, GNorm = 0.2886
Meta loss on this task batch = 3.6118e-01, Meta loss averaged over last 500 steps = 3.1505e-01, PNorm = 115.9891, GNorm = 0.2168
Meta loss on this task batch = 3.1582e-01, Meta loss averaged over last 500 steps = 3.1503e-01, PNorm = 115.9962, GNorm = 0.2316
Meta loss on this task batch = 3.4900e-01, Meta loss averaged over last 500 steps = 3.1508e-01, PNorm = 116.0038, GNorm = 0.2110
Meta loss on this task batch = 3.1725e-01, Meta loss averaged over last 500 steps = 3.1507e-01, PNorm = 116.0115, GNorm = 0.1977
Meta loss on this task batch = 3.4102e-01, Meta loss averaged over last 500 steps = 3.1515e-01, PNorm = 116.0193, GNorm = 0.2129
Meta loss on this task batch = 3.7791e-01, Meta loss averaged over last 500 steps = 3.1531e-01, PNorm = 116.0273, GNorm = 0.2434
Meta loss on this task batch = 2.5107e-01, Meta loss averaged over last 500 steps = 3.1520e-01, PNorm = 116.0355, GNorm = 0.2318
Took 114.18938779830933 seconds to complete one epoch of meta training
Took 122.10582709312439 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512548
Epoch 335
Meta loss on this task batch = 3.1777e-01, Meta loss averaged over last 500 steps = 3.1520e-01, PNorm = 116.0435, GNorm = 0.2274
Meta loss on this task batch = 3.9073e-01, Meta loss averaged over last 500 steps = 3.1542e-01, PNorm = 116.0501, GNorm = 0.2341
Meta loss on this task batch = 3.2836e-01, Meta loss averaged over last 500 steps = 3.1538e-01, PNorm = 116.0562, GNorm = 0.2287
Meta loss on this task batch = 3.4005e-01, Meta loss averaged over last 500 steps = 3.1542e-01, PNorm = 116.0621, GNorm = 0.2369
Meta loss on this task batch = 2.7702e-01, Meta loss averaged over last 500 steps = 3.1541e-01, PNorm = 116.0690, GNorm = 0.1948
Meta loss on this task batch = 2.7994e-01, Meta loss averaged over last 500 steps = 3.1528e-01, PNorm = 116.0763, GNorm = 0.1875
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 3.1519e-01, PNorm = 116.0839, GNorm = 0.2012
Meta loss on this task batch = 2.9664e-01, Meta loss averaged over last 500 steps = 3.1528e-01, PNorm = 116.0918, GNorm = 0.2099
Meta loss on this task batch = 3.1562e-01, Meta loss averaged over last 500 steps = 3.1541e-01, PNorm = 116.0987, GNorm = 0.2423
Meta loss on this task batch = 3.2278e-01, Meta loss averaged over last 500 steps = 3.1544e-01, PNorm = 116.1052, GNorm = 0.2528
Meta loss on this task batch = 3.2781e-01, Meta loss averaged over last 500 steps = 3.1540e-01, PNorm = 116.1114, GNorm = 0.2534
Meta loss on this task batch = 3.0752e-01, Meta loss averaged over last 500 steps = 3.1528e-01, PNorm = 116.1186, GNorm = 0.2103
Meta loss on this task batch = 3.2922e-01, Meta loss averaged over last 500 steps = 3.1533e-01, PNorm = 116.1266, GNorm = 0.2092
Meta loss on this task batch = 2.7530e-01, Meta loss averaged over last 500 steps = 3.1531e-01, PNorm = 116.1345, GNorm = 0.2054
Meta loss on this task batch = 3.0060e-01, Meta loss averaged over last 500 steps = 3.1532e-01, PNorm = 116.1425, GNorm = 0.2010
Meta loss on this task batch = 2.8188e-01, Meta loss averaged over last 500 steps = 3.1515e-01, PNorm = 116.1502, GNorm = 0.2153
Meta loss on this task batch = 2.8435e-01, Meta loss averaged over last 500 steps = 3.1501e-01, PNorm = 116.1585, GNorm = 0.2056
Meta loss on this task batch = 2.8864e-01, Meta loss averaged over last 500 steps = 3.1500e-01, PNorm = 116.1672, GNorm = 0.1959
Meta loss on this task batch = 3.2346e-01, Meta loss averaged over last 500 steps = 3.1490e-01, PNorm = 116.1744, GNorm = 0.2781
Took 112.70961999893188 seconds to complete one epoch of meta training
Took 120.34836673736572 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508851
Epoch 336
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 3.1481e-01, PNorm = 116.1826, GNorm = 0.2013
Meta loss on this task batch = 2.9690e-01, Meta loss averaged over last 500 steps = 3.1481e-01, PNorm = 116.1907, GNorm = 0.2196
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 3.1471e-01, PNorm = 116.1987, GNorm = 0.2117
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 116.2073, GNorm = 0.2361
Meta loss on this task batch = 3.3112e-01, Meta loss averaged over last 500 steps = 3.1476e-01, PNorm = 116.2166, GNorm = 0.2740
Meta loss on this task batch = 3.2092e-01, Meta loss averaged over last 500 steps = 3.1478e-01, PNorm = 116.2252, GNorm = 0.2312
Meta loss on this task batch = 3.6022e-01, Meta loss averaged over last 500 steps = 3.1494e-01, PNorm = 116.2332, GNorm = 0.2487
Meta loss on this task batch = 2.6457e-01, Meta loss averaged over last 500 steps = 3.1487e-01, PNorm = 116.2407, GNorm = 0.2022
Meta loss on this task batch = 2.5703e-01, Meta loss averaged over last 500 steps = 3.1473e-01, PNorm = 116.2490, GNorm = 0.2387
Meta loss on this task batch = 3.1737e-01, Meta loss averaged over last 500 steps = 3.1471e-01, PNorm = 116.2572, GNorm = 0.2248
Meta loss on this task batch = 3.1300e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 116.2655, GNorm = 0.2157
Meta loss on this task batch = 3.2165e-01, Meta loss averaged over last 500 steps = 3.1474e-01, PNorm = 116.2733, GNorm = 0.2100
Meta loss on this task batch = 3.2933e-01, Meta loss averaged over last 500 steps = 3.1483e-01, PNorm = 116.2816, GNorm = 0.2128
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 3.1471e-01, PNorm = 116.2897, GNorm = 0.1892
Meta loss on this task batch = 3.1571e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 116.2974, GNorm = 0.2129
Meta loss on this task batch = 3.0571e-01, Meta loss averaged over last 500 steps = 3.1464e-01, PNorm = 116.3054, GNorm = 0.2066
Meta loss on this task batch = 2.9701e-01, Meta loss averaged over last 500 steps = 3.1466e-01, PNorm = 116.3126, GNorm = 0.2331
Meta loss on this task batch = 3.1597e-01, Meta loss averaged over last 500 steps = 3.1460e-01, PNorm = 116.3197, GNorm = 0.2348
Meta loss on this task batch = 2.9580e-01, Meta loss averaged over last 500 steps = 3.1465e-01, PNorm = 116.3264, GNorm = 0.2635
Took 114.53307843208313 seconds to complete one epoch of meta training
Took 121.99023985862732 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519808
Epoch 337
Meta loss on this task batch = 3.0215e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 116.3331, GNorm = 0.2203
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 3.1483e-01, PNorm = 116.3404, GNorm = 0.2370
Meta loss on this task batch = 3.7298e-01, Meta loss averaged over last 500 steps = 3.1481e-01, PNorm = 116.3467, GNorm = 0.2451
Meta loss on this task batch = 3.1561e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 116.3532, GNorm = 0.2232
Meta loss on this task batch = 2.9157e-01, Meta loss averaged over last 500 steps = 3.1467e-01, PNorm = 116.3599, GNorm = 0.2240
Meta loss on this task batch = 3.1271e-01, Meta loss averaged over last 500 steps = 3.1463e-01, PNorm = 116.3667, GNorm = 0.2158
Meta loss on this task batch = 2.7288e-01, Meta loss averaged over last 500 steps = 3.1468e-01, PNorm = 116.3718, GNorm = 0.2188
Meta loss on this task batch = 3.2398e-01, Meta loss averaged over last 500 steps = 3.1472e-01, PNorm = 116.3761, GNorm = 0.2267
Meta loss on this task batch = 2.8801e-01, Meta loss averaged over last 500 steps = 3.1468e-01, PNorm = 116.3817, GNorm = 0.1744
Meta loss on this task batch = 3.5648e-01, Meta loss averaged over last 500 steps = 3.1474e-01, PNorm = 116.3863, GNorm = 0.2364
Meta loss on this task batch = 2.6723e-01, Meta loss averaged over last 500 steps = 3.1453e-01, PNorm = 116.3924, GNorm = 0.1958
Meta loss on this task batch = 2.7325e-01, Meta loss averaged over last 500 steps = 3.1443e-01, PNorm = 116.3999, GNorm = 0.2050
Meta loss on this task batch = 2.8830e-01, Meta loss averaged over last 500 steps = 3.1428e-01, PNorm = 116.4086, GNorm = 0.1975
Meta loss on this task batch = 3.2151e-01, Meta loss averaged over last 500 steps = 3.1420e-01, PNorm = 116.4177, GNorm = 0.2140
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 3.1416e-01, PNorm = 116.4273, GNorm = 0.2470
Meta loss on this task batch = 3.2755e-01, Meta loss averaged over last 500 steps = 3.1418e-01, PNorm = 116.4348, GNorm = 0.2232
Meta loss on this task batch = 3.6650e-01, Meta loss averaged over last 500 steps = 3.1432e-01, PNorm = 116.4415, GNorm = 0.2596
Meta loss on this task batch = 3.6436e-01, Meta loss averaged over last 500 steps = 3.1453e-01, PNorm = 116.4469, GNorm = 0.2354
Meta loss on this task batch = 3.4573e-01, Meta loss averaged over last 500 steps = 3.1463e-01, PNorm = 116.4526, GNorm = 0.2958
Took 113.14026689529419 seconds to complete one epoch of meta training
Took 120.6481397151947 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500840
Epoch 338
Meta loss on this task batch = 2.6919e-01, Meta loss averaged over last 500 steps = 3.1442e-01, PNorm = 116.4591, GNorm = 0.2071
Meta loss on this task batch = 3.2069e-01, Meta loss averaged over last 500 steps = 3.1440e-01, PNorm = 116.4668, GNorm = 0.2453
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 3.1443e-01, PNorm = 116.4746, GNorm = 0.2153
Meta loss on this task batch = 2.6854e-01, Meta loss averaged over last 500 steps = 3.1438e-01, PNorm = 116.4827, GNorm = 0.2005
Meta loss on this task batch = 3.5106e-01, Meta loss averaged over last 500 steps = 3.1444e-01, PNorm = 116.4908, GNorm = 0.2232
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 3.1436e-01, PNorm = 116.4984, GNorm = 0.2224
Meta loss on this task batch = 2.8557e-01, Meta loss averaged over last 500 steps = 3.1427e-01, PNorm = 116.5077, GNorm = 0.2203
Meta loss on this task batch = 3.2297e-01, Meta loss averaged over last 500 steps = 3.1428e-01, PNorm = 116.5166, GNorm = 0.2344
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 3.1431e-01, PNorm = 116.5261, GNorm = 0.2347
Meta loss on this task batch = 3.1239e-01, Meta loss averaged over last 500 steps = 3.1419e-01, PNorm = 116.5354, GNorm = 0.2474
Meta loss on this task batch = 3.9989e-01, Meta loss averaged over last 500 steps = 3.1431e-01, PNorm = 116.5438, GNorm = 0.2685
Meta loss on this task batch = 2.7615e-01, Meta loss averaged over last 500 steps = 3.1430e-01, PNorm = 116.5523, GNorm = 0.2077
Meta loss on this task batch = 3.2185e-01, Meta loss averaged over last 500 steps = 3.1427e-01, PNorm = 116.5604, GNorm = 0.2230
Meta loss on this task batch = 2.8973e-01, Meta loss averaged over last 500 steps = 3.1419e-01, PNorm = 116.5681, GNorm = 0.2303
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 3.1409e-01, PNorm = 116.5765, GNorm = 0.1815
Meta loss on this task batch = 2.6984e-01, Meta loss averaged over last 500 steps = 3.1388e-01, PNorm = 116.5848, GNorm = 0.1939
Meta loss on this task batch = 3.1131e-01, Meta loss averaged over last 500 steps = 3.1386e-01, PNorm = 116.5930, GNorm = 0.2286
Meta loss on this task batch = 3.2806e-01, Meta loss averaged over last 500 steps = 3.1381e-01, PNorm = 116.6014, GNorm = 0.2190
Meta loss on this task batch = 3.3568e-01, Meta loss averaged over last 500 steps = 3.1374e-01, PNorm = 116.6095, GNorm = 0.2669
Took 115.06284832954407 seconds to complete one epoch of meta training
Took 122.45664405822754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.520419
Epoch 339
Meta loss on this task batch = 3.5559e-01, Meta loss averaged over last 500 steps = 3.1384e-01, PNorm = 116.6177, GNorm = 0.2589
Meta loss on this task batch = 3.0016e-01, Meta loss averaged over last 500 steps = 3.1376e-01, PNorm = 116.6253, GNorm = 0.1952
Meta loss on this task batch = 3.1941e-01, Meta loss averaged over last 500 steps = 3.1382e-01, PNorm = 116.6320, GNorm = 0.2431
Meta loss on this task batch = 3.4430e-01, Meta loss averaged over last 500 steps = 3.1399e-01, PNorm = 116.6373, GNorm = 0.2257
Meta loss on this task batch = 3.3033e-01, Meta loss averaged over last 500 steps = 3.1398e-01, PNorm = 116.6424, GNorm = 0.2051
Meta loss on this task batch = 3.2061e-01, Meta loss averaged over last 500 steps = 3.1417e-01, PNorm = 116.6482, GNorm = 0.2071
Meta loss on this task batch = 3.0001e-01, Meta loss averaged over last 500 steps = 3.1416e-01, PNorm = 116.6542, GNorm = 0.1793
Meta loss on this task batch = 2.8137e-01, Meta loss averaged over last 500 steps = 3.1411e-01, PNorm = 116.6613, GNorm = 0.1763
Meta loss on this task batch = 3.0147e-01, Meta loss averaged over last 500 steps = 3.1404e-01, PNorm = 116.6689, GNorm = 0.2125
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 3.1401e-01, PNorm = 116.6779, GNorm = 0.1959
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 3.1397e-01, PNorm = 116.6879, GNorm = 0.1966
Meta loss on this task batch = 3.1635e-01, Meta loss averaged over last 500 steps = 3.1390e-01, PNorm = 116.6970, GNorm = 0.2501
Meta loss on this task batch = 2.6049e-01, Meta loss averaged over last 500 steps = 3.1361e-01, PNorm = 116.7073, GNorm = 0.1781
Meta loss on this task batch = 3.2329e-01, Meta loss averaged over last 500 steps = 3.1364e-01, PNorm = 116.7190, GNorm = 0.2568
Meta loss on this task batch = 3.3425e-01, Meta loss averaged over last 500 steps = 3.1372e-01, PNorm = 116.7301, GNorm = 0.2210
Meta loss on this task batch = 3.3903e-01, Meta loss averaged over last 500 steps = 3.1368e-01, PNorm = 116.7403, GNorm = 0.3002
Meta loss on this task batch = 3.3105e-01, Meta loss averaged over last 500 steps = 3.1377e-01, PNorm = 116.7494, GNorm = 0.2932
Meta loss on this task batch = 2.7450e-01, Meta loss averaged over last 500 steps = 3.1371e-01, PNorm = 116.7582, GNorm = 0.2307
Meta loss on this task batch = 3.4484e-01, Meta loss averaged over last 500 steps = 3.1383e-01, PNorm = 116.7659, GNorm = 0.2709
Took 114.22966933250427 seconds to complete one epoch of meta training
Took 120.97862887382507 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509860
Epoch 340
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 3.1383e-01, PNorm = 116.7722, GNorm = 0.2443
Meta loss on this task batch = 2.9355e-01, Meta loss averaged over last 500 steps = 3.1373e-01, PNorm = 116.7792, GNorm = 0.2020
Meta loss on this task batch = 3.8246e-01, Meta loss averaged over last 500 steps = 3.1390e-01, PNorm = 116.7841, GNorm = 0.2564
Meta loss on this task batch = 3.0772e-01, Meta loss averaged over last 500 steps = 3.1379e-01, PNorm = 116.7890, GNorm = 0.2404
Meta loss on this task batch = 3.3688e-01, Meta loss averaged over last 500 steps = 3.1394e-01, PNorm = 116.7937, GNorm = 0.2550
Meta loss on this task batch = 3.2869e-01, Meta loss averaged over last 500 steps = 3.1402e-01, PNorm = 116.7975, GNorm = 0.2210
Meta loss on this task batch = 3.2483e-01, Meta loss averaged over last 500 steps = 3.1402e-01, PNorm = 116.8010, GNorm = 0.2437
Meta loss on this task batch = 2.5839e-01, Meta loss averaged over last 500 steps = 3.1400e-01, PNorm = 116.8054, GNorm = 0.1834
Meta loss on this task batch = 3.2751e-01, Meta loss averaged over last 500 steps = 3.1401e-01, PNorm = 116.8106, GNorm = 0.2200
Meta loss on this task batch = 3.7620e-01, Meta loss averaged over last 500 steps = 3.1419e-01, PNorm = 116.8161, GNorm = 0.2639
Meta loss on this task batch = 3.5400e-01, Meta loss averaged over last 500 steps = 3.1424e-01, PNorm = 116.8215, GNorm = 0.2314
Meta loss on this task batch = 3.1037e-01, Meta loss averaged over last 500 steps = 3.1423e-01, PNorm = 116.8287, GNorm = 0.2179
Meta loss on this task batch = 2.9727e-01, Meta loss averaged over last 500 steps = 3.1423e-01, PNorm = 116.8369, GNorm = 0.1953
Meta loss on this task batch = 2.2755e-01, Meta loss averaged over last 500 steps = 3.1394e-01, PNorm = 116.8469, GNorm = 0.1976
Meta loss on this task batch = 3.3257e-01, Meta loss averaged over last 500 steps = 3.1394e-01, PNorm = 116.8570, GNorm = 0.2138
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 3.1385e-01, PNorm = 116.8672, GNorm = 0.1897
Meta loss on this task batch = 3.2385e-01, Meta loss averaged over last 500 steps = 3.1387e-01, PNorm = 116.8773, GNorm = 0.1879
Meta loss on this task batch = 2.4114e-01, Meta loss averaged over last 500 steps = 3.1363e-01, PNorm = 116.8881, GNorm = 0.1865
Meta loss on this task batch = 3.5337e-01, Meta loss averaged over last 500 steps = 3.1370e-01, PNorm = 116.8983, GNorm = 0.2813
Took 115.40235757827759 seconds to complete one epoch of meta training
Took 123.22820663452148 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490779
Epoch 341
Meta loss on this task batch = 3.0134e-01, Meta loss averaged over last 500 steps = 3.1371e-01, PNorm = 116.9073, GNorm = 0.2335
Meta loss on this task batch = 3.0892e-01, Meta loss averaged over last 500 steps = 3.1370e-01, PNorm = 116.9161, GNorm = 0.2198
Meta loss on this task batch = 3.0829e-01, Meta loss averaged over last 500 steps = 3.1364e-01, PNorm = 116.9236, GNorm = 0.2304
Meta loss on this task batch = 3.4903e-01, Meta loss averaged over last 500 steps = 3.1368e-01, PNorm = 116.9313, GNorm = 0.2613
Meta loss on this task batch = 3.6177e-01, Meta loss averaged over last 500 steps = 3.1380e-01, PNorm = 116.9368, GNorm = 0.2600
Meta loss on this task batch = 3.6574e-01, Meta loss averaged over last 500 steps = 3.1386e-01, PNorm = 116.9407, GNorm = 0.2720
Meta loss on this task batch = 2.3425e-01, Meta loss averaged over last 500 steps = 3.1368e-01, PNorm = 116.9454, GNorm = 0.2064
Meta loss on this task batch = 2.9968e-01, Meta loss averaged over last 500 steps = 3.1358e-01, PNorm = 116.9516, GNorm = 0.2264
Meta loss on this task batch = 2.6153e-01, Meta loss averaged over last 500 steps = 3.1347e-01, PNorm = 116.9570, GNorm = 0.2194
Meta loss on this task batch = 2.9914e-01, Meta loss averaged over last 500 steps = 3.1339e-01, PNorm = 116.9631, GNorm = 0.2116
Meta loss on this task batch = 3.3065e-01, Meta loss averaged over last 500 steps = 3.1350e-01, PNorm = 116.9680, GNorm = 0.2403
Meta loss on this task batch = 2.2105e-01, Meta loss averaged over last 500 steps = 3.1325e-01, PNorm = 116.9746, GNorm = 0.1866
Meta loss on this task batch = 3.4660e-01, Meta loss averaged over last 500 steps = 3.1325e-01, PNorm = 116.9813, GNorm = 0.2434
Meta loss on this task batch = 4.0197e-01, Meta loss averaged over last 500 steps = 3.1353e-01, PNorm = 116.9882, GNorm = 0.2895
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 3.1345e-01, PNorm = 116.9956, GNorm = 0.1882
Meta loss on this task batch = 3.6329e-01, Meta loss averaged over last 500 steps = 3.1358e-01, PNorm = 117.0030, GNorm = 0.2169
Meta loss on this task batch = 3.5461e-01, Meta loss averaged over last 500 steps = 3.1358e-01, PNorm = 117.0105, GNorm = 0.2390
Meta loss on this task batch = 2.6174e-01, Meta loss averaged over last 500 steps = 3.1355e-01, PNorm = 117.0196, GNorm = 0.2080
Meta loss on this task batch = 3.5343e-01, Meta loss averaged over last 500 steps = 3.1357e-01, PNorm = 117.0287, GNorm = 0.2668
Took 116.39823651313782 seconds to complete one epoch of meta training
Took 124.64052653312683 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486657
Epoch 342
Meta loss on this task batch = 3.1122e-01, Meta loss averaged over last 500 steps = 3.1354e-01, PNorm = 117.0380, GNorm = 0.2586
Meta loss on this task batch = 3.0040e-01, Meta loss averaged over last 500 steps = 3.1352e-01, PNorm = 117.0458, GNorm = 0.2399
Meta loss on this task batch = 3.1949e-01, Meta loss averaged over last 500 steps = 3.1347e-01, PNorm = 117.0535, GNorm = 0.2144
Meta loss on this task batch = 2.7483e-01, Meta loss averaged over last 500 steps = 3.1333e-01, PNorm = 117.0615, GNorm = 0.1854
Meta loss on this task batch = 3.0673e-01, Meta loss averaged over last 500 steps = 3.1334e-01, PNorm = 117.0699, GNorm = 0.2059
Meta loss on this task batch = 2.7476e-01, Meta loss averaged over last 500 steps = 3.1336e-01, PNorm = 117.0792, GNorm = 0.2019
Meta loss on this task batch = 3.1600e-01, Meta loss averaged over last 500 steps = 3.1345e-01, PNorm = 117.0886, GNorm = 0.2052
Meta loss on this task batch = 3.6756e-01, Meta loss averaged over last 500 steps = 3.1358e-01, PNorm = 117.0973, GNorm = 0.2582
Meta loss on this task batch = 2.8854e-01, Meta loss averaged over last 500 steps = 3.1339e-01, PNorm = 117.1071, GNorm = 0.2114
Meta loss on this task batch = 2.3899e-01, Meta loss averaged over last 500 steps = 3.1318e-01, PNorm = 117.1172, GNorm = 0.1853
Meta loss on this task batch = 2.9162e-01, Meta loss averaged over last 500 steps = 3.1314e-01, PNorm = 117.1271, GNorm = 0.2278
Meta loss on this task batch = 2.9661e-01, Meta loss averaged over last 500 steps = 3.1292e-01, PNorm = 117.1370, GNorm = 0.2330
Meta loss on this task batch = 2.8684e-01, Meta loss averaged over last 500 steps = 3.1282e-01, PNorm = 117.1460, GNorm = 0.2382
Meta loss on this task batch = 2.8254e-01, Meta loss averaged over last 500 steps = 3.1268e-01, PNorm = 117.1531, GNorm = 0.2568
Meta loss on this task batch = 3.3783e-01, Meta loss averaged over last 500 steps = 3.1265e-01, PNorm = 117.1586, GNorm = 0.2373
Meta loss on this task batch = 3.2322e-01, Meta loss averaged over last 500 steps = 3.1259e-01, PNorm = 117.1649, GNorm = 0.2233
Meta loss on this task batch = 3.2886e-01, Meta loss averaged over last 500 steps = 3.1261e-01, PNorm = 117.1728, GNorm = 0.2262
Meta loss on this task batch = 3.6479e-01, Meta loss averaged over last 500 steps = 3.1281e-01, PNorm = 117.1815, GNorm = 0.2464
Meta loss on this task batch = 3.0212e-01, Meta loss averaged over last 500 steps = 3.1284e-01, PNorm = 117.1911, GNorm = 0.2731
Took 116.42757415771484 seconds to complete one epoch of meta training
Took 124.38319110870361 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476851
Epoch 343
Meta loss on this task batch = 3.6582e-01, Meta loss averaged over last 500 steps = 3.1302e-01, PNorm = 117.1991, GNorm = 0.2836
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 3.1300e-01, PNorm = 117.2075, GNorm = 0.2284
Meta loss on this task batch = 2.6941e-01, Meta loss averaged over last 500 steps = 3.1296e-01, PNorm = 117.2167, GNorm = 0.2043
Meta loss on this task batch = 3.1782e-01, Meta loss averaged over last 500 steps = 3.1302e-01, PNorm = 117.2259, GNorm = 0.2320
Meta loss on this task batch = 3.3677e-01, Meta loss averaged over last 500 steps = 3.1312e-01, PNorm = 117.2353, GNorm = 0.2125
Meta loss on this task batch = 2.6759e-01, Meta loss averaged over last 500 steps = 3.1289e-01, PNorm = 117.2446, GNorm = 0.2090
Meta loss on this task batch = 2.8125e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 117.2544, GNorm = 0.2205
Meta loss on this task batch = 3.4689e-01, Meta loss averaged over last 500 steps = 3.1290e-01, PNorm = 117.2630, GNorm = 0.2677
Meta loss on this task batch = 2.9426e-01, Meta loss averaged over last 500 steps = 3.1294e-01, PNorm = 117.2702, GNorm = 0.2423
Meta loss on this task batch = 2.8096e-01, Meta loss averaged over last 500 steps = 3.1291e-01, PNorm = 117.2763, GNorm = 0.2253
Meta loss on this task batch = 3.3031e-01, Meta loss averaged over last 500 steps = 3.1296e-01, PNorm = 117.2822, GNorm = 0.2378
Meta loss on this task batch = 3.5543e-01, Meta loss averaged over last 500 steps = 3.1300e-01, PNorm = 117.2871, GNorm = 0.2390
Meta loss on this task batch = 3.3091e-01, Meta loss averaged over last 500 steps = 3.1309e-01, PNorm = 117.2928, GNorm = 0.2252
Meta loss on this task batch = 2.5663e-01, Meta loss averaged over last 500 steps = 3.1303e-01, PNorm = 117.2991, GNorm = 0.2023
Meta loss on this task batch = 3.4193e-01, Meta loss averaged over last 500 steps = 3.1318e-01, PNorm = 117.3051, GNorm = 0.2037
Meta loss on this task batch = 3.6585e-01, Meta loss averaged over last 500 steps = 3.1329e-01, PNorm = 117.3105, GNorm = 0.2809
Meta loss on this task batch = 3.5702e-01, Meta loss averaged over last 500 steps = 3.1331e-01, PNorm = 117.3163, GNorm = 0.2075
Meta loss on this task batch = 3.0237e-01, Meta loss averaged over last 500 steps = 3.1328e-01, PNorm = 117.3228, GNorm = 0.1998
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 3.1332e-01, PNorm = 117.3289, GNorm = 0.2971
Took 112.49212121963501 seconds to complete one epoch of meta training
Took 121.10165071487427 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462894
Epoch 344
Meta loss on this task batch = 2.6904e-01, Meta loss averaged over last 500 steps = 3.1311e-01, PNorm = 117.3372, GNorm = 0.2085
Meta loss on this task batch = 2.9593e-01, Meta loss averaged over last 500 steps = 3.1315e-01, PNorm = 117.3458, GNorm = 0.2076
Meta loss on this task batch = 3.1381e-01, Meta loss averaged over last 500 steps = 3.1305e-01, PNorm = 117.3542, GNorm = 0.2144
Meta loss on this task batch = 3.0654e-01, Meta loss averaged over last 500 steps = 3.1303e-01, PNorm = 117.3628, GNorm = 0.1990
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 3.1304e-01, PNorm = 117.3724, GNorm = 0.1856
Meta loss on this task batch = 3.2760e-01, Meta loss averaged over last 500 steps = 3.1294e-01, PNorm = 117.3817, GNorm = 0.2554
Meta loss on this task batch = 2.6865e-01, Meta loss averaged over last 500 steps = 3.1282e-01, PNorm = 117.3913, GNorm = 0.2118
Meta loss on this task batch = 3.1444e-01, Meta loss averaged over last 500 steps = 3.1274e-01, PNorm = 117.4002, GNorm = 0.2250
Meta loss on this task batch = 3.2684e-01, Meta loss averaged over last 500 steps = 3.1282e-01, PNorm = 117.4083, GNorm = 0.2557
Meta loss on this task batch = 3.1762e-01, Meta loss averaged over last 500 steps = 3.1287e-01, PNorm = 117.4156, GNorm = 0.2186
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 3.1289e-01, PNorm = 117.4216, GNorm = 0.2547
Meta loss on this task batch = 3.8379e-01, Meta loss averaged over last 500 steps = 3.1303e-01, PNorm = 117.4283, GNorm = 0.2440
Meta loss on this task batch = 2.5509e-01, Meta loss averaged over last 500 steps = 3.1289e-01, PNorm = 117.4362, GNorm = 0.2046
Meta loss on this task batch = 2.8664e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 117.4453, GNorm = 0.1988
Meta loss on this task batch = 3.4195e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 117.4539, GNorm = 0.2456
Meta loss on this task batch = 3.5693e-01, Meta loss averaged over last 500 steps = 3.1296e-01, PNorm = 117.4620, GNorm = 0.2628
Meta loss on this task batch = 3.3659e-01, Meta loss averaged over last 500 steps = 3.1302e-01, PNorm = 117.4702, GNorm = 0.2011
Meta loss on this task batch = 2.3162e-01, Meta loss averaged over last 500 steps = 3.1297e-01, PNorm = 117.4789, GNorm = 0.2101
Meta loss on this task batch = 3.9826e-01, Meta loss averaged over last 500 steps = 3.1308e-01, PNorm = 117.4875, GNorm = 0.2762
Took 115.28882002830505 seconds to complete one epoch of meta training
Took 121.92281746864319 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473843
Epoch 345
Meta loss on this task batch = 3.7590e-01, Meta loss averaged over last 500 steps = 3.1313e-01, PNorm = 117.4955, GNorm = 0.2463
Meta loss on this task batch = 3.1702e-01, Meta loss averaged over last 500 steps = 3.1308e-01, PNorm = 117.5042, GNorm = 0.2140
Meta loss on this task batch = 3.3722e-01, Meta loss averaged over last 500 steps = 3.1306e-01, PNorm = 117.5118, GNorm = 0.2290
Meta loss on this task batch = 3.2962e-01, Meta loss averaged over last 500 steps = 3.1316e-01, PNorm = 117.5194, GNorm = 0.2229
Meta loss on this task batch = 2.7579e-01, Meta loss averaged over last 500 steps = 3.1306e-01, PNorm = 117.5271, GNorm = 0.1936
Meta loss on this task batch = 2.9309e-01, Meta loss averaged over last 500 steps = 3.1311e-01, PNorm = 117.5342, GNorm = 0.2012
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 3.1310e-01, PNorm = 117.5403, GNorm = 0.2509
Meta loss on this task batch = 2.9222e-01, Meta loss averaged over last 500 steps = 3.1291e-01, PNorm = 117.5456, GNorm = 0.2189
Meta loss on this task batch = 3.0035e-01, Meta loss averaged over last 500 steps = 3.1291e-01, PNorm = 117.5521, GNorm = 0.2180
Meta loss on this task batch = 2.9191e-01, Meta loss averaged over last 500 steps = 3.1289e-01, PNorm = 117.5588, GNorm = 0.2091
Meta loss on this task batch = 2.9809e-01, Meta loss averaged over last 500 steps = 3.1292e-01, PNorm = 117.5662, GNorm = 0.2045
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 3.1287e-01, PNorm = 117.5749, GNorm = 0.2514
Meta loss on this task batch = 3.3828e-01, Meta loss averaged over last 500 steps = 3.1287e-01, PNorm = 117.5836, GNorm = 0.2341
Meta loss on this task batch = 3.4145e-01, Meta loss averaged over last 500 steps = 3.1295e-01, PNorm = 117.5922, GNorm = 0.2042
Meta loss on this task batch = 3.2212e-01, Meta loss averaged over last 500 steps = 3.1299e-01, PNorm = 117.6011, GNorm = 0.2057
Meta loss on this task batch = 3.2927e-01, Meta loss averaged over last 500 steps = 3.1310e-01, PNorm = 117.6107, GNorm = 0.2265
Meta loss on this task batch = 3.4650e-01, Meta loss averaged over last 500 steps = 3.1328e-01, PNorm = 117.6197, GNorm = 0.2532
Meta loss on this task batch = 2.8929e-01, Meta loss averaged over last 500 steps = 3.1328e-01, PNorm = 117.6284, GNorm = 0.2280
Meta loss on this task batch = 3.4261e-01, Meta loss averaged over last 500 steps = 3.1325e-01, PNorm = 117.6357, GNorm = 0.2954
Took 116.11201429367065 seconds to complete one epoch of meta training
Took 123.22602081298828 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473509
Epoch 346
Meta loss on this task batch = 2.7715e-01, Meta loss averaged over last 500 steps = 3.1320e-01, PNorm = 117.6424, GNorm = 0.2594
Meta loss on this task batch = 2.7821e-01, Meta loss averaged over last 500 steps = 3.1319e-01, PNorm = 117.6499, GNorm = 0.2127
Meta loss on this task batch = 2.9826e-01, Meta loss averaged over last 500 steps = 3.1312e-01, PNorm = 117.6573, GNorm = 0.2428
Meta loss on this task batch = 3.0412e-01, Meta loss averaged over last 500 steps = 3.1294e-01, PNorm = 117.6630, GNorm = 0.2710
Meta loss on this task batch = 2.8674e-01, Meta loss averaged over last 500 steps = 3.1279e-01, PNorm = 117.6703, GNorm = 0.2241
Meta loss on this task batch = 3.0824e-01, Meta loss averaged over last 500 steps = 3.1278e-01, PNorm = 117.6781, GNorm = 0.2195
Meta loss on this task batch = 3.5925e-01, Meta loss averaged over last 500 steps = 3.1295e-01, PNorm = 117.6861, GNorm = 0.2558
Meta loss on this task batch = 3.0657e-01, Meta loss averaged over last 500 steps = 3.1293e-01, PNorm = 117.6948, GNorm = 0.2197
Meta loss on this task batch = 2.4753e-01, Meta loss averaged over last 500 steps = 3.1277e-01, PNorm = 117.7052, GNorm = 0.2329
Meta loss on this task batch = 3.1371e-01, Meta loss averaged over last 500 steps = 3.1274e-01, PNorm = 117.7159, GNorm = 0.2260
Meta loss on this task batch = 3.9329e-01, Meta loss averaged over last 500 steps = 3.1301e-01, PNorm = 117.7264, GNorm = 0.2630
Meta loss on this task batch = 2.9071e-01, Meta loss averaged over last 500 steps = 3.1291e-01, PNorm = 117.7360, GNorm = 0.1996
Meta loss on this task batch = 3.2426e-01, Meta loss averaged over last 500 steps = 3.1304e-01, PNorm = 117.7463, GNorm = 0.2149
Meta loss on this task batch = 2.5620e-01, Meta loss averaged over last 500 steps = 3.1300e-01, PNorm = 117.7567, GNorm = 0.2034
Meta loss on this task batch = 2.9724e-01, Meta loss averaged over last 500 steps = 3.1285e-01, PNorm = 117.7659, GNorm = 0.2071
Meta loss on this task batch = 3.2648e-01, Meta loss averaged over last 500 steps = 3.1287e-01, PNorm = 117.7746, GNorm = 0.2585
Meta loss on this task batch = 3.3911e-01, Meta loss averaged over last 500 steps = 3.1293e-01, PNorm = 117.7833, GNorm = 0.1968
Meta loss on this task batch = 3.3075e-01, Meta loss averaged over last 500 steps = 3.1281e-01, PNorm = 117.7907, GNorm = 0.2237
Meta loss on this task batch = 3.5306e-01, Meta loss averaged over last 500 steps = 3.1291e-01, PNorm = 117.7971, GNorm = 0.2594
Took 114.6096613407135 seconds to complete one epoch of meta training
Took 122.36727547645569 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487245
Epoch 347
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 3.1283e-01, PNorm = 117.8040, GNorm = 0.2025
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 3.1290e-01, PNorm = 117.8112, GNorm = 0.2072
Meta loss on this task batch = 3.1792e-01, Meta loss averaged over last 500 steps = 3.1293e-01, PNorm = 117.8178, GNorm = 0.2069
Meta loss on this task batch = 3.0533e-01, Meta loss averaged over last 500 steps = 3.1286e-01, PNorm = 117.8247, GNorm = 0.1946
Meta loss on this task batch = 3.3084e-01, Meta loss averaged over last 500 steps = 3.1285e-01, PNorm = 117.8323, GNorm = 0.2204
Meta loss on this task batch = 2.9060e-01, Meta loss averaged over last 500 steps = 3.1262e-01, PNorm = 117.8400, GNorm = 0.2259
Meta loss on this task batch = 3.1171e-01, Meta loss averaged over last 500 steps = 3.1270e-01, PNorm = 117.8477, GNorm = 0.2095
Meta loss on this task batch = 2.8602e-01, Meta loss averaged over last 500 steps = 3.1272e-01, PNorm = 117.8559, GNorm = 0.2516
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 3.1256e-01, PNorm = 117.8647, GNorm = 0.2066
Meta loss on this task batch = 3.2453e-01, Meta loss averaged over last 500 steps = 3.1260e-01, PNorm = 117.8726, GNorm = 0.2089
Meta loss on this task batch = 2.9886e-01, Meta loss averaged over last 500 steps = 3.1258e-01, PNorm = 117.8775, GNorm = 0.3002
Meta loss on this task batch = 2.9271e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 117.8831, GNorm = 0.2218
Meta loss on this task batch = 3.7633e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 117.8883, GNorm = 0.3060
Meta loss on this task batch = 3.6006e-01, Meta loss averaged over last 500 steps = 3.1270e-01, PNorm = 117.8918, GNorm = 0.2951
Meta loss on this task batch = 3.0771e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 117.8967, GNorm = 0.2264
Meta loss on this task batch = 2.6975e-01, Meta loss averaged over last 500 steps = 3.1246e-01, PNorm = 117.9026, GNorm = 0.2352
Meta loss on this task batch = 2.7993e-01, Meta loss averaged over last 500 steps = 3.1241e-01, PNorm = 117.9094, GNorm = 0.2159
Meta loss on this task batch = 3.1416e-01, Meta loss averaged over last 500 steps = 3.1239e-01, PNorm = 117.9157, GNorm = 0.2064
Meta loss on this task batch = 3.9146e-01, Meta loss averaged over last 500 steps = 3.1244e-01, PNorm = 117.9213, GNorm = 0.3110
Took 114.9614531993866 seconds to complete one epoch of meta training
Took 121.73482584953308 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500034
Epoch 348
Meta loss on this task batch = 3.2425e-01, Meta loss averaged over last 500 steps = 3.1256e-01, PNorm = 117.9268, GNorm = 0.2261
Meta loss on this task batch = 3.2188e-01, Meta loss averaged over last 500 steps = 3.1271e-01, PNorm = 117.9322, GNorm = 0.2163
Meta loss on this task batch = 2.8785e-01, Meta loss averaged over last 500 steps = 3.1263e-01, PNorm = 117.9377, GNorm = 0.2196
Meta loss on this task batch = 3.3754e-01, Meta loss averaged over last 500 steps = 3.1271e-01, PNorm = 117.9440, GNorm = 0.2507
Meta loss on this task batch = 3.5544e-01, Meta loss averaged over last 500 steps = 3.1269e-01, PNorm = 117.9504, GNorm = 0.2392
Meta loss on this task batch = 3.2852e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 117.9569, GNorm = 0.2060
Meta loss on this task batch = 2.8349e-01, Meta loss averaged over last 500 steps = 3.1252e-01, PNorm = 117.9640, GNorm = 0.1823
Meta loss on this task batch = 3.4138e-01, Meta loss averaged over last 500 steps = 3.1265e-01, PNorm = 117.9721, GNorm = 0.2097
Meta loss on this task batch = 2.4691e-01, Meta loss averaged over last 500 steps = 3.1245e-01, PNorm = 117.9809, GNorm = 0.1804
Meta loss on this task batch = 3.1714e-01, Meta loss averaged over last 500 steps = 3.1242e-01, PNorm = 117.9903, GNorm = 0.2069
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 3.1247e-01, PNorm = 117.9993, GNorm = 0.2156
Meta loss on this task batch = 2.6399e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 118.0087, GNorm = 0.2239
Meta loss on this task batch = 2.6207e-01, Meta loss averaged over last 500 steps = 3.1223e-01, PNorm = 118.0181, GNorm = 0.2150
Meta loss on this task batch = 3.4426e-01, Meta loss averaged over last 500 steps = 3.1231e-01, PNorm = 118.0260, GNorm = 0.2817
Meta loss on this task batch = 2.9247e-01, Meta loss averaged over last 500 steps = 3.1234e-01, PNorm = 118.0325, GNorm = 0.2872
Meta loss on this task batch = 3.7209e-01, Meta loss averaged over last 500 steps = 3.1255e-01, PNorm = 118.0389, GNorm = 0.2381
Meta loss on this task batch = 3.0513e-01, Meta loss averaged over last 500 steps = 3.1252e-01, PNorm = 118.0453, GNorm = 0.2077
Meta loss on this task batch = 3.0089e-01, Meta loss averaged over last 500 steps = 3.1241e-01, PNorm = 118.0512, GNorm = 0.2338
Meta loss on this task batch = 3.1458e-01, Meta loss averaged over last 500 steps = 3.1246e-01, PNorm = 118.0583, GNorm = 0.2403
Took 115.05490970611572 seconds to complete one epoch of meta training
Took 122.8381404876709 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494644
Epoch 349
Meta loss on this task batch = 2.9460e-01, Meta loss averaged over last 500 steps = 3.1245e-01, PNorm = 118.0646, GNorm = 0.2157
Meta loss on this task batch = 2.8523e-01, Meta loss averaged over last 500 steps = 3.1248e-01, PNorm = 118.0709, GNorm = 0.1973
Meta loss on this task batch = 3.6364e-01, Meta loss averaged over last 500 steps = 3.1270e-01, PNorm = 118.0770, GNorm = 0.2356
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 3.1247e-01, PNorm = 118.0839, GNorm = 0.1999
Meta loss on this task batch = 2.2403e-01, Meta loss averaged over last 500 steps = 3.1226e-01, PNorm = 118.0922, GNorm = 0.1769
Meta loss on this task batch = 3.4794e-01, Meta loss averaged over last 500 steps = 3.1224e-01, PNorm = 118.1004, GNorm = 0.2422
Meta loss on this task batch = 3.3483e-01, Meta loss averaged over last 500 steps = 3.1224e-01, PNorm = 118.1085, GNorm = 0.2239
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 3.1213e-01, PNorm = 118.1172, GNorm = 0.2248
Meta loss on this task batch = 3.3188e-01, Meta loss averaged over last 500 steps = 3.1218e-01, PNorm = 118.1256, GNorm = 0.2421
Meta loss on this task batch = 3.1428e-01, Meta loss averaged over last 500 steps = 3.1226e-01, PNorm = 118.1333, GNorm = 0.2215
Meta loss on this task batch = 2.7071e-01, Meta loss averaged over last 500 steps = 3.1215e-01, PNorm = 118.1416, GNorm = 0.1955
Meta loss on this task batch = 2.7954e-01, Meta loss averaged over last 500 steps = 3.1205e-01, PNorm = 118.1503, GNorm = 0.2080
Meta loss on this task batch = 3.4304e-01, Meta loss averaged over last 500 steps = 3.1203e-01, PNorm = 118.1575, GNorm = 0.2695
Meta loss on this task batch = 3.1130e-01, Meta loss averaged over last 500 steps = 3.1205e-01, PNorm = 118.1639, GNorm = 0.2741
Meta loss on this task batch = 3.3749e-01, Meta loss averaged over last 500 steps = 3.1210e-01, PNorm = 118.1689, GNorm = 0.2638
Meta loss on this task batch = 3.2687e-01, Meta loss averaged over last 500 steps = 3.1223e-01, PNorm = 118.1742, GNorm = 0.2624
Meta loss on this task batch = 3.5823e-01, Meta loss averaged over last 500 steps = 3.1254e-01, PNorm = 118.1794, GNorm = 0.2560
Meta loss on this task batch = 3.2345e-01, Meta loss averaged over last 500 steps = 3.1260e-01, PNorm = 118.1852, GNorm = 0.2051
Meta loss on this task batch = 2.3912e-01, Meta loss averaged over last 500 steps = 3.1250e-01, PNorm = 118.1920, GNorm = 0.2460
Took 114.182368516922 seconds to complete one epoch of meta training
Took 122.23379397392273 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485075
Epoch 350
Meta loss on this task batch = 3.4815e-01, Meta loss averaged over last 500 steps = 3.1266e-01, PNorm = 118.1991, GNorm = 0.2519
Meta loss on this task batch = 3.3634e-01, Meta loss averaged over last 500 steps = 3.1277e-01, PNorm = 118.2074, GNorm = 0.2371
Meta loss on this task batch = 2.9448e-01, Meta loss averaged over last 500 steps = 3.1270e-01, PNorm = 118.2167, GNorm = 0.2184
Meta loss on this task batch = 3.0723e-01, Meta loss averaged over last 500 steps = 3.1267e-01, PNorm = 118.2257, GNorm = 0.2329
Meta loss on this task batch = 3.0085e-01, Meta loss averaged over last 500 steps = 3.1244e-01, PNorm = 118.2333, GNorm = 0.2199
Meta loss on this task batch = 2.7480e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 118.2413, GNorm = 0.1999
Meta loss on this task batch = 2.7543e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 118.2481, GNorm = 0.2352
Meta loss on this task batch = 2.8729e-01, Meta loss averaged over last 500 steps = 3.1242e-01, PNorm = 118.2551, GNorm = 0.2250
Meta loss on this task batch = 2.9179e-01, Meta loss averaged over last 500 steps = 3.1233e-01, PNorm = 118.2614, GNorm = 0.2247
Meta loss on this task batch = 2.9126e-01, Meta loss averaged over last 500 steps = 3.1228e-01, PNorm = 118.2673, GNorm = 0.2311
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 3.1224e-01, PNorm = 118.2725, GNorm = 0.2657
Meta loss on this task batch = 2.7597e-01, Meta loss averaged over last 500 steps = 3.1217e-01, PNorm = 118.2785, GNorm = 0.2040
Meta loss on this task batch = 3.5129e-01, Meta loss averaged over last 500 steps = 3.1214e-01, PNorm = 118.2840, GNorm = 0.2658
Meta loss on this task batch = 2.9342e-01, Meta loss averaged over last 500 steps = 3.1215e-01, PNorm = 118.2896, GNorm = 0.2194
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 3.1217e-01, PNorm = 118.2955, GNorm = 0.2216
Meta loss on this task batch = 3.3751e-01, Meta loss averaged over last 500 steps = 3.1216e-01, PNorm = 118.3018, GNorm = 0.2564
Meta loss on this task batch = 3.1507e-01, Meta loss averaged over last 500 steps = 3.1207e-01, PNorm = 118.3082, GNorm = 0.2269
Meta loss on this task batch = 3.3520e-01, Meta loss averaged over last 500 steps = 3.1212e-01, PNorm = 118.3138, GNorm = 0.2476
Meta loss on this task batch = 3.2640e-01, Meta loss averaged over last 500 steps = 3.1212e-01, PNorm = 118.3192, GNorm = 0.2837
Took 113.3721022605896 seconds to complete one epoch of meta training
Took 120.87675762176514 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500590
Epoch 351
Meta loss on this task batch = 3.2516e-01, Meta loss averaged over last 500 steps = 3.1205e-01, PNorm = 118.3255, GNorm = 0.2436
Meta loss on this task batch = 3.1992e-01, Meta loss averaged over last 500 steps = 3.1212e-01, PNorm = 118.3329, GNorm = 0.2349
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 3.1204e-01, PNorm = 118.3405, GNorm = 0.1997
Meta loss on this task batch = 2.4631e-01, Meta loss averaged over last 500 steps = 3.1185e-01, PNorm = 118.3486, GNorm = 0.1904
Meta loss on this task batch = 2.8963e-01, Meta loss averaged over last 500 steps = 3.1183e-01, PNorm = 118.3571, GNorm = 0.2112
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 3.1174e-01, PNorm = 118.3658, GNorm = 0.2066
Meta loss on this task batch = 2.5968e-01, Meta loss averaged over last 500 steps = 3.1160e-01, PNorm = 118.3737, GNorm = 0.2094
Meta loss on this task batch = 3.2408e-01, Meta loss averaged over last 500 steps = 3.1160e-01, PNorm = 118.3816, GNorm = 0.2232
Meta loss on this task batch = 3.7993e-01, Meta loss averaged over last 500 steps = 3.1173e-01, PNorm = 118.3874, GNorm = 0.2718
Meta loss on this task batch = 3.4511e-01, Meta loss averaged over last 500 steps = 3.1179e-01, PNorm = 118.3930, GNorm = 0.2377
Meta loss on this task batch = 3.3440e-01, Meta loss averaged over last 500 steps = 3.1174e-01, PNorm = 118.3983, GNorm = 0.2583
Meta loss on this task batch = 3.5309e-01, Meta loss averaged over last 500 steps = 3.1179e-01, PNorm = 118.4043, GNorm = 0.2667
Meta loss on this task batch = 2.6857e-01, Meta loss averaged over last 500 steps = 3.1175e-01, PNorm = 118.4107, GNorm = 0.2342
Meta loss on this task batch = 3.4521e-01, Meta loss averaged over last 500 steps = 3.1178e-01, PNorm = 118.4165, GNorm = 0.2346
Meta loss on this task batch = 3.2253e-01, Meta loss averaged over last 500 steps = 3.1174e-01, PNorm = 118.4223, GNorm = 0.2299
Meta loss on this task batch = 3.1371e-01, Meta loss averaged over last 500 steps = 3.1171e-01, PNorm = 118.4287, GNorm = 0.2188
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 3.1175e-01, PNorm = 118.4352, GNorm = 0.2272
Meta loss on this task batch = 3.5077e-01, Meta loss averaged over last 500 steps = 3.1179e-01, PNorm = 118.4412, GNorm = 0.2674
Meta loss on this task batch = 2.9450e-01, Meta loss averaged over last 500 steps = 3.1173e-01, PNorm = 118.4476, GNorm = 0.2395
Took 115.70192408561707 seconds to complete one epoch of meta training
Took 123.58429503440857 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499790
Epoch 352
Meta loss on this task batch = 2.9482e-01, Meta loss averaged over last 500 steps = 3.1171e-01, PNorm = 118.4559, GNorm = 0.2130
Meta loss on this task batch = 3.0520e-01, Meta loss averaged over last 500 steps = 3.1175e-01, PNorm = 118.4650, GNorm = 0.1954
Meta loss on this task batch = 3.0751e-01, Meta loss averaged over last 500 steps = 3.1171e-01, PNorm = 118.4731, GNorm = 0.1923
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 3.1156e-01, PNorm = 118.4819, GNorm = 0.1907
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 3.1153e-01, PNorm = 118.4911, GNorm = 0.2147
Meta loss on this task batch = 3.4734e-01, Meta loss averaged over last 500 steps = 3.1161e-01, PNorm = 118.4994, GNorm = 0.2525
Meta loss on this task batch = 2.7227e-01, Meta loss averaged over last 500 steps = 3.1164e-01, PNorm = 118.5085, GNorm = 0.1940
Meta loss on this task batch = 2.2869e-01, Meta loss averaged over last 500 steps = 3.1148e-01, PNorm = 118.5172, GNorm = 0.1950
Meta loss on this task batch = 3.2779e-01, Meta loss averaged over last 500 steps = 3.1158e-01, PNorm = 118.5248, GNorm = 0.2222
Meta loss on this task batch = 2.9695e-01, Meta loss averaged over last 500 steps = 3.1150e-01, PNorm = 118.5313, GNorm = 0.2076
Meta loss on this task batch = 2.9486e-01, Meta loss averaged over last 500 steps = 3.1138e-01, PNorm = 118.5386, GNorm = 0.2202
Meta loss on this task batch = 3.5094e-01, Meta loss averaged over last 500 steps = 3.1154e-01, PNorm = 118.5447, GNorm = 0.2433
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 3.1141e-01, PNorm = 118.5506, GNorm = 0.2340
Meta loss on this task batch = 3.0174e-01, Meta loss averaged over last 500 steps = 3.1130e-01, PNorm = 118.5568, GNorm = 0.2331
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 3.1121e-01, PNorm = 118.5632, GNorm = 0.2281
Meta loss on this task batch = 3.4730e-01, Meta loss averaged over last 500 steps = 3.1132e-01, PNorm = 118.5693, GNorm = 0.2319
Meta loss on this task batch = 2.8810e-01, Meta loss averaged over last 500 steps = 3.1127e-01, PNorm = 118.5764, GNorm = 0.2101
Meta loss on this task batch = 3.2534e-01, Meta loss averaged over last 500 steps = 3.1126e-01, PNorm = 118.5844, GNorm = 0.2184
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 3.1125e-01, PNorm = 118.5925, GNorm = 0.2389
Took 114.92653965950012 seconds to complete one epoch of meta training
Took 123.01874661445618 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507056
Epoch 353
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 3.1126e-01, PNorm = 118.6003, GNorm = 0.2276
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 3.1130e-01, PNorm = 118.6079, GNorm = 0.2092
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 3.1127e-01, PNorm = 118.6164, GNorm = 0.1951
Meta loss on this task batch = 3.5428e-01, Meta loss averaged over last 500 steps = 3.1136e-01, PNorm = 118.6254, GNorm = 0.2284
Meta loss on this task batch = 3.1416e-01, Meta loss averaged over last 500 steps = 3.1119e-01, PNorm = 118.6349, GNorm = 0.2260
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 3.1122e-01, PNorm = 118.6446, GNorm = 0.2106
Meta loss on this task batch = 3.0477e-01, Meta loss averaged over last 500 steps = 3.1120e-01, PNorm = 118.6545, GNorm = 0.1953
Meta loss on this task batch = 3.2153e-01, Meta loss averaged over last 500 steps = 3.1121e-01, PNorm = 118.6642, GNorm = 0.2022
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 3.1119e-01, PNorm = 118.6740, GNorm = 0.2083
Meta loss on this task batch = 3.3572e-01, Meta loss averaged over last 500 steps = 3.1119e-01, PNorm = 118.6831, GNorm = 0.2037
Meta loss on this task batch = 2.4848e-01, Meta loss averaged over last 500 steps = 3.1107e-01, PNorm = 118.6928, GNorm = 0.2004
Meta loss on this task batch = 2.9481e-01, Meta loss averaged over last 500 steps = 3.1110e-01, PNorm = 118.7031, GNorm = 0.2117
Meta loss on this task batch = 3.0211e-01, Meta loss averaged over last 500 steps = 3.1113e-01, PNorm = 118.7128, GNorm = 0.2108
Meta loss on this task batch = 2.6988e-01, Meta loss averaged over last 500 steps = 3.1102e-01, PNorm = 118.7218, GNorm = 0.2327
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 3.1115e-01, PNorm = 118.7299, GNorm = 0.2394
Meta loss on this task batch = 3.4308e-01, Meta loss averaged over last 500 steps = 3.1125e-01, PNorm = 118.7373, GNorm = 0.2452
Meta loss on this task batch = 2.6741e-01, Meta loss averaged over last 500 steps = 3.1115e-01, PNorm = 118.7447, GNorm = 0.2219
Meta loss on this task batch = 2.5414e-01, Meta loss averaged over last 500 steps = 3.1100e-01, PNorm = 118.7527, GNorm = 0.2320
Meta loss on this task batch = 3.7439e-01, Meta loss averaged over last 500 steps = 3.1113e-01, PNorm = 118.7595, GNorm = 0.3000
Took 113.5707004070282 seconds to complete one epoch of meta training
Took 121.48404049873352 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494178
Epoch 354
Meta loss on this task batch = 3.3178e-01, Meta loss averaged over last 500 steps = 3.1116e-01, PNorm = 118.7652, GNorm = 0.2566
Meta loss on this task batch = 3.4131e-01, Meta loss averaged over last 500 steps = 3.1125e-01, PNorm = 118.7723, GNorm = 0.2295
Meta loss on this task batch = 2.1396e-01, Meta loss averaged over last 500 steps = 3.1099e-01, PNorm = 118.7806, GNorm = 0.1603
Meta loss on this task batch = 3.1378e-01, Meta loss averaged over last 500 steps = 3.1099e-01, PNorm = 118.7893, GNorm = 0.2250
Meta loss on this task batch = 3.4912e-01, Meta loss averaged over last 500 steps = 3.1106e-01, PNorm = 118.7976, GNorm = 0.2605
Meta loss on this task batch = 3.2910e-01, Meta loss averaged over last 500 steps = 3.1111e-01, PNorm = 118.8055, GNorm = 0.2392
Meta loss on this task batch = 3.1047e-01, Meta loss averaged over last 500 steps = 3.1110e-01, PNorm = 118.8133, GNorm = 0.2264
Meta loss on this task batch = 2.7912e-01, Meta loss averaged over last 500 steps = 3.1096e-01, PNorm = 118.8205, GNorm = 0.2536
Meta loss on this task batch = 2.9069e-01, Meta loss averaged over last 500 steps = 3.1082e-01, PNorm = 118.8281, GNorm = 0.2098
Meta loss on this task batch = 2.9070e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 118.8354, GNorm = 0.2045
Meta loss on this task batch = 2.8706e-01, Meta loss averaged over last 500 steps = 3.1064e-01, PNorm = 118.8427, GNorm = 0.1821
Meta loss on this task batch = 3.7588e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 118.8494, GNorm = 0.2744
Meta loss on this task batch = 2.8451e-01, Meta loss averaged over last 500 steps = 3.1071e-01, PNorm = 118.8549, GNorm = 0.2097
Meta loss on this task batch = 3.1341e-01, Meta loss averaged over last 500 steps = 3.1077e-01, PNorm = 118.8601, GNorm = 0.2462
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 118.8657, GNorm = 0.2035
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 3.1074e-01, PNorm = 118.8715, GNorm = 0.2271
Meta loss on this task batch = 2.9628e-01, Meta loss averaged over last 500 steps = 3.1072e-01, PNorm = 118.8780, GNorm = 0.2248
Meta loss on this task batch = 3.2532e-01, Meta loss averaged over last 500 steps = 3.1069e-01, PNorm = 118.8853, GNorm = 0.2202
Meta loss on this task batch = 3.3632e-01, Meta loss averaged over last 500 steps = 3.1075e-01, PNorm = 118.8921, GNorm = 0.3049
Took 115.15133762359619 seconds to complete one epoch of meta training
Took 123.2230863571167 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481677
Epoch 355
Meta loss on this task batch = 2.8791e-01, Meta loss averaged over last 500 steps = 3.1073e-01, PNorm = 118.9006, GNorm = 0.2270
Meta loss on this task batch = 3.7745e-01, Meta loss averaged over last 500 steps = 3.1083e-01, PNorm = 118.9083, GNorm = 0.2451
Meta loss on this task batch = 2.7935e-01, Meta loss averaged over last 500 steps = 3.1074e-01, PNorm = 118.9152, GNorm = 0.2178
Meta loss on this task batch = 3.2740e-01, Meta loss averaged over last 500 steps = 3.1075e-01, PNorm = 118.9222, GNorm = 0.1958
Meta loss on this task batch = 3.1798e-01, Meta loss averaged over last 500 steps = 3.1079e-01, PNorm = 118.9294, GNorm = 0.2091
Meta loss on this task batch = 2.3965e-01, Meta loss averaged over last 500 steps = 3.1078e-01, PNorm = 118.9363, GNorm = 0.1889
Meta loss on this task batch = 2.7326e-01, Meta loss averaged over last 500 steps = 3.1065e-01, PNorm = 118.9417, GNorm = 0.2259
Meta loss on this task batch = 2.2469e-01, Meta loss averaged over last 500 steps = 3.1048e-01, PNorm = 118.9487, GNorm = 0.2006
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 118.9546, GNorm = 0.2438
Meta loss on this task batch = 3.2519e-01, Meta loss averaged over last 500 steps = 3.1027e-01, PNorm = 118.9600, GNorm = 0.2510
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 118.9644, GNorm = 0.2470
Meta loss on this task batch = 3.0405e-01, Meta loss averaged over last 500 steps = 3.1037e-01, PNorm = 118.9691, GNorm = 0.2162
Meta loss on this task batch = 3.0923e-01, Meta loss averaged over last 500 steps = 3.1043e-01, PNorm = 118.9742, GNorm = 0.2136
Meta loss on this task batch = 3.4653e-01, Meta loss averaged over last 500 steps = 3.1041e-01, PNorm = 118.9794, GNorm = 0.2432
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 3.1037e-01, PNorm = 118.9852, GNorm = 0.2037
Meta loss on this task batch = 3.4203e-01, Meta loss averaged over last 500 steps = 3.1048e-01, PNorm = 118.9909, GNorm = 0.2826
Meta loss on this task batch = 2.9759e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 118.9972, GNorm = 0.2134
Meta loss on this task batch = 3.3119e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 119.0040, GNorm = 0.2250
Meta loss on this task batch = 3.2598e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 119.0108, GNorm = 0.2789
Took 115.53446173667908 seconds to complete one epoch of meta training
Took 123.45993995666504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521103
Epoch 356
Meta loss on this task batch = 2.9948e-01, Meta loss averaged over last 500 steps = 3.1039e-01, PNorm = 119.0176, GNorm = 0.2362
Meta loss on this task batch = 3.0574e-01, Meta loss averaged over last 500 steps = 3.1038e-01, PNorm = 119.0246, GNorm = 0.2329
Meta loss on this task batch = 2.9623e-01, Meta loss averaged over last 500 steps = 3.1032e-01, PNorm = 119.0320, GNorm = 0.2217
Meta loss on this task batch = 2.8979e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 119.0395, GNorm = 0.1907
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 119.0471, GNorm = 0.1932
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 3.1022e-01, PNorm = 119.0550, GNorm = 0.2019
Meta loss on this task batch = 3.4241e-01, Meta loss averaged over last 500 steps = 3.1028e-01, PNorm = 119.0631, GNorm = 0.2256
Meta loss on this task batch = 2.8811e-01, Meta loss averaged over last 500 steps = 3.1020e-01, PNorm = 119.0709, GNorm = 0.1979
Meta loss on this task batch = 3.2532e-01, Meta loss averaged over last 500 steps = 3.1028e-01, PNorm = 119.0781, GNorm = 0.2295
Meta loss on this task batch = 3.3490e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 119.0867, GNorm = 0.2325
Meta loss on this task batch = 2.7329e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 119.0964, GNorm = 0.1915
Meta loss on this task batch = 3.3187e-01, Meta loss averaged over last 500 steps = 3.1031e-01, PNorm = 119.1052, GNorm = 0.2282
Meta loss on this task batch = 3.5344e-01, Meta loss averaged over last 500 steps = 3.1044e-01, PNorm = 119.1139, GNorm = 0.2267
Meta loss on this task batch = 2.3632e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 119.1234, GNorm = 0.1862
Meta loss on this task batch = 3.2000e-01, Meta loss averaged over last 500 steps = 3.1029e-01, PNorm = 119.1328, GNorm = 0.2072
Meta loss on this task batch = 3.2568e-01, Meta loss averaged over last 500 steps = 3.1026e-01, PNorm = 119.1423, GNorm = 0.2359
Meta loss on this task batch = 2.8140e-01, Meta loss averaged over last 500 steps = 3.1025e-01, PNorm = 119.1504, GNorm = 0.2159
Meta loss on this task batch = 3.2473e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 119.1587, GNorm = 0.2941
Meta loss on this task batch = 3.1258e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 119.1668, GNorm = 0.2745
Took 116.67054724693298 seconds to complete one epoch of meta training
Took 124.91176557540894 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513809
Epoch 357
Meta loss on this task batch = 2.8211e-01, Meta loss averaged over last 500 steps = 3.1027e-01, PNorm = 119.1754, GNorm = 0.2217
Meta loss on this task batch = 2.8163e-01, Meta loss averaged over last 500 steps = 3.1023e-01, PNorm = 119.1833, GNorm = 0.2036
Meta loss on this task batch = 3.6852e-01, Meta loss averaged over last 500 steps = 3.1024e-01, PNorm = 119.1904, GNorm = 0.2420
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 3.1016e-01, PNorm = 119.1973, GNorm = 0.2163
Meta loss on this task batch = 3.2534e-01, Meta loss averaged over last 500 steps = 3.1013e-01, PNorm = 119.2030, GNorm = 0.2670
Meta loss on this task batch = 3.5340e-01, Meta loss averaged over last 500 steps = 3.1026e-01, PNorm = 119.2088, GNorm = 0.2305
Meta loss on this task batch = 2.5725e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 119.2147, GNorm = 0.2090
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 3.1009e-01, PNorm = 119.2198, GNorm = 0.2121
Meta loss on this task batch = 2.8400e-01, Meta loss averaged over last 500 steps = 3.1000e-01, PNorm = 119.2264, GNorm = 0.1980
Meta loss on this task batch = 3.3598e-01, Meta loss averaged over last 500 steps = 3.1010e-01, PNorm = 119.2336, GNorm = 0.2322
Meta loss on this task batch = 2.9663e-01, Meta loss averaged over last 500 steps = 3.1006e-01, PNorm = 119.2407, GNorm = 0.2078
Meta loss on this task batch = 2.8309e-01, Meta loss averaged over last 500 steps = 3.1006e-01, PNorm = 119.2470, GNorm = 0.2018
Meta loss on this task batch = 3.3173e-01, Meta loss averaged over last 500 steps = 3.1014e-01, PNorm = 119.2528, GNorm = 0.2292
Meta loss on this task batch = 2.8813e-01, Meta loss averaged over last 500 steps = 3.1005e-01, PNorm = 119.2595, GNorm = 0.2212
Meta loss on this task batch = 3.1318e-01, Meta loss averaged over last 500 steps = 3.1016e-01, PNorm = 119.2655, GNorm = 0.2313
Meta loss on this task batch = 3.4236e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 119.2714, GNorm = 0.2223
Meta loss on this task batch = 3.3462e-01, Meta loss averaged over last 500 steps = 3.1017e-01, PNorm = 119.2771, GNorm = 0.2517
Meta loss on this task batch = 3.4358e-01, Meta loss averaged over last 500 steps = 3.1020e-01, PNorm = 119.2833, GNorm = 0.2447
Meta loss on this task batch = 2.8360e-01, Meta loss averaged over last 500 steps = 3.1021e-01, PNorm = 119.2915, GNorm = 0.2803
Took 113.38548612594604 seconds to complete one epoch of meta training
Took 120.9192168712616 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483087
Epoch 358
Meta loss on this task batch = 3.7270e-01, Meta loss averaged over last 500 steps = 3.1030e-01, PNorm = 119.3006, GNorm = 0.3073
Meta loss on this task batch = 3.0726e-01, Meta loss averaged over last 500 steps = 3.1034e-01, PNorm = 119.3102, GNorm = 0.2297
Meta loss on this task batch = 3.3427e-01, Meta loss averaged over last 500 steps = 3.1036e-01, PNorm = 119.3182, GNorm = 0.2435
Meta loss on this task batch = 2.6993e-01, Meta loss averaged over last 500 steps = 3.1022e-01, PNorm = 119.3263, GNorm = 0.2059
Meta loss on this task batch = 3.1330e-01, Meta loss averaged over last 500 steps = 3.1026e-01, PNorm = 119.3336, GNorm = 0.2298
Meta loss on this task batch = 3.1314e-01, Meta loss averaged over last 500 steps = 3.1010e-01, PNorm = 119.3403, GNorm = 0.3073
Meta loss on this task batch = 2.3141e-01, Meta loss averaged over last 500 steps = 3.0987e-01, PNorm = 119.3474, GNorm = 0.2139
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 3.0985e-01, PNorm = 119.3550, GNorm = 0.2359
Meta loss on this task batch = 2.8596e-01, Meta loss averaged over last 500 steps = 3.0982e-01, PNorm = 119.3623, GNorm = 0.2142
Meta loss on this task batch = 3.1663e-01, Meta loss averaged over last 500 steps = 3.0988e-01, PNorm = 119.3692, GNorm = 0.2343
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 3.0989e-01, PNorm = 119.3755, GNorm = 0.2333
Meta loss on this task batch = 2.9421e-01, Meta loss averaged over last 500 steps = 3.0990e-01, PNorm = 119.3807, GNorm = 0.2450
Meta loss on this task batch = 2.9911e-01, Meta loss averaged over last 500 steps = 3.0971e-01, PNorm = 119.3867, GNorm = 0.2020
Meta loss on this task batch = 3.4786e-01, Meta loss averaged over last 500 steps = 3.0954e-01, PNorm = 119.3931, GNorm = 0.2457
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 3.0948e-01, PNorm = 119.3999, GNorm = 0.2168
Meta loss on this task batch = 3.5702e-01, Meta loss averaged over last 500 steps = 3.0957e-01, PNorm = 119.4063, GNorm = 0.2599
Meta loss on this task batch = 3.1159e-01, Meta loss averaged over last 500 steps = 3.0954e-01, PNorm = 119.4136, GNorm = 0.2409
Meta loss on this task batch = 3.3017e-01, Meta loss averaged over last 500 steps = 3.0964e-01, PNorm = 119.4205, GNorm = 0.2655
Meta loss on this task batch = 2.2784e-01, Meta loss averaged over last 500 steps = 3.0953e-01, PNorm = 119.4289, GNorm = 0.2754
Took 118.32285213470459 seconds to complete one epoch of meta training
Took 126.74526500701904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504694
Epoch 359
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 3.0961e-01, PNorm = 119.4371, GNorm = 0.1987
Meta loss on this task batch = 3.2015e-01, Meta loss averaged over last 500 steps = 3.0955e-01, PNorm = 119.4450, GNorm = 0.2056
Meta loss on this task batch = 3.3085e-01, Meta loss averaged over last 500 steps = 3.0954e-01, PNorm = 119.4531, GNorm = 0.2089
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 3.0955e-01, PNorm = 119.4617, GNorm = 0.1830
Meta loss on this task batch = 3.5468e-01, Meta loss averaged over last 500 steps = 3.0962e-01, PNorm = 119.4699, GNorm = 0.2089
Meta loss on this task batch = 2.6483e-01, Meta loss averaged over last 500 steps = 3.0956e-01, PNorm = 119.4784, GNorm = 0.1934
Meta loss on this task batch = 3.3376e-01, Meta loss averaged over last 500 steps = 3.0964e-01, PNorm = 119.4855, GNorm = 0.2413
Meta loss on this task batch = 3.0844e-01, Meta loss averaged over last 500 steps = 3.0962e-01, PNorm = 119.4925, GNorm = 0.1925
Meta loss on this task batch = 2.9568e-01, Meta loss averaged over last 500 steps = 3.0958e-01, PNorm = 119.4986, GNorm = 0.2218
Meta loss on this task batch = 2.8554e-01, Meta loss averaged over last 500 steps = 3.0955e-01, PNorm = 119.5057, GNorm = 0.2082
Meta loss on this task batch = 2.9141e-01, Meta loss averaged over last 500 steps = 3.0944e-01, PNorm = 119.5140, GNorm = 0.2219
Meta loss on this task batch = 3.0379e-01, Meta loss averaged over last 500 steps = 3.0946e-01, PNorm = 119.5221, GNorm = 0.2565
Meta loss on this task batch = 2.9440e-01, Meta loss averaged over last 500 steps = 3.0952e-01, PNorm = 119.5304, GNorm = 0.2135
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 3.0945e-01, PNorm = 119.5389, GNorm = 0.2460
Meta loss on this task batch = 2.9508e-01, Meta loss averaged over last 500 steps = 3.0944e-01, PNorm = 119.5472, GNorm = 0.2354
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 3.0936e-01, PNorm = 119.5558, GNorm = 0.2549
Meta loss on this task batch = 3.1701e-01, Meta loss averaged over last 500 steps = 3.0934e-01, PNorm = 119.5643, GNorm = 0.2388
Meta loss on this task batch = 3.1254e-01, Meta loss averaged over last 500 steps = 3.0933e-01, PNorm = 119.5721, GNorm = 0.2533
Meta loss on this task batch = 2.5393e-01, Meta loss averaged over last 500 steps = 3.0919e-01, PNorm = 119.5792, GNorm = 0.2568
Took 111.52727627754211 seconds to complete one epoch of meta training
Took 119.48997569084167 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496416
Epoch 360
Meta loss on this task batch = 2.7030e-01, Meta loss averaged over last 500 steps = 3.0907e-01, PNorm = 119.5864, GNorm = 0.2007
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 3.0902e-01, PNorm = 119.5941, GNorm = 0.2510
Meta loss on this task batch = 3.1684e-01, Meta loss averaged over last 500 steps = 3.0907e-01, PNorm = 119.6014, GNorm = 0.2486
Meta loss on this task batch = 3.0976e-01, Meta loss averaged over last 500 steps = 3.0912e-01, PNorm = 119.6080, GNorm = 0.2556
Meta loss on this task batch = 3.0277e-01, Meta loss averaged over last 500 steps = 3.0915e-01, PNorm = 119.6131, GNorm = 0.2462
Meta loss on this task batch = 3.6842e-01, Meta loss averaged over last 500 steps = 3.0925e-01, PNorm = 119.6173, GNorm = 0.2660
Meta loss on this task batch = 2.6809e-01, Meta loss averaged over last 500 steps = 3.0917e-01, PNorm = 119.6221, GNorm = 0.2068
Meta loss on this task batch = 3.5798e-01, Meta loss averaged over last 500 steps = 3.0919e-01, PNorm = 119.6260, GNorm = 0.2453
Meta loss on this task batch = 3.4903e-01, Meta loss averaged over last 500 steps = 3.0927e-01, PNorm = 119.6294, GNorm = 0.2631
Meta loss on this task batch = 2.7734e-01, Meta loss averaged over last 500 steps = 3.0931e-01, PNorm = 119.6338, GNorm = 0.2071
Meta loss on this task batch = 3.0045e-01, Meta loss averaged over last 500 steps = 3.0937e-01, PNorm = 119.6392, GNorm = 0.1894
Meta loss on this task batch = 3.6336e-01, Meta loss averaged over last 500 steps = 3.0954e-01, PNorm = 119.6459, GNorm = 0.3156
Meta loss on this task batch = 3.0981e-01, Meta loss averaged over last 500 steps = 3.0940e-01, PNorm = 119.6545, GNorm = 0.2723
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 3.0939e-01, PNorm = 119.6639, GNorm = 0.2224
Meta loss on this task batch = 3.6082e-01, Meta loss averaged over last 500 steps = 3.0942e-01, PNorm = 119.6729, GNorm = 0.2519
Meta loss on this task batch = 2.8724e-01, Meta loss averaged over last 500 steps = 3.0942e-01, PNorm = 119.6828, GNorm = 0.2208
Meta loss on this task batch = 3.3114e-01, Meta loss averaged over last 500 steps = 3.0946e-01, PNorm = 119.6925, GNorm = 0.2067
Meta loss on this task batch = 2.6741e-01, Meta loss averaged over last 500 steps = 3.0944e-01, PNorm = 119.7015, GNorm = 0.2284
Meta loss on this task batch = 3.5534e-01, Meta loss averaged over last 500 steps = 3.0942e-01, PNorm = 119.7084, GNorm = 0.3128
Took 116.28043913841248 seconds to complete one epoch of meta training
Took 123.9478235244751 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504666
Epoch 361
Meta loss on this task batch = 2.9649e-01, Meta loss averaged over last 500 steps = 3.0939e-01, PNorm = 119.7152, GNorm = 0.2180
Meta loss on this task batch = 3.0293e-01, Meta loss averaged over last 500 steps = 3.0929e-01, PNorm = 119.7225, GNorm = 0.2161
Meta loss on this task batch = 2.7945e-01, Meta loss averaged over last 500 steps = 3.0922e-01, PNorm = 119.7297, GNorm = 0.2097
Meta loss on this task batch = 3.3406e-01, Meta loss averaged over last 500 steps = 3.0920e-01, PNorm = 119.7369, GNorm = 0.2203
Meta loss on this task batch = 3.9204e-01, Meta loss averaged over last 500 steps = 3.0923e-01, PNorm = 119.7430, GNorm = 0.2345
Meta loss on this task batch = 3.2191e-01, Meta loss averaged over last 500 steps = 3.0937e-01, PNorm = 119.7488, GNorm = 0.2332
Meta loss on this task batch = 2.6690e-01, Meta loss averaged over last 500 steps = 3.0927e-01, PNorm = 119.7554, GNorm = 0.1987
Meta loss on this task batch = 2.0027e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 119.7632, GNorm = 0.1953
Meta loss on this task batch = 2.6207e-01, Meta loss averaged over last 500 steps = 3.0876e-01, PNorm = 119.7708, GNorm = 0.2029
Meta loss on this task batch = 2.8094e-01, Meta loss averaged over last 500 steps = 3.0864e-01, PNorm = 119.7791, GNorm = 0.1937
Meta loss on this task batch = 2.9639e-01, Meta loss averaged over last 500 steps = 3.0868e-01, PNorm = 119.7886, GNorm = 0.2203
Meta loss on this task batch = 3.1918e-01, Meta loss averaged over last 500 steps = 3.0876e-01, PNorm = 119.7971, GNorm = 0.2362
Meta loss on this task batch = 2.9491e-01, Meta loss averaged over last 500 steps = 3.0873e-01, PNorm = 119.8055, GNorm = 0.2089
Meta loss on this task batch = 2.9572e-01, Meta loss averaged over last 500 steps = 3.0873e-01, PNorm = 119.8134, GNorm = 0.2137
Meta loss on this task batch = 3.2533e-01, Meta loss averaged over last 500 steps = 3.0875e-01, PNorm = 119.8206, GNorm = 0.2193
Meta loss on this task batch = 2.8941e-01, Meta loss averaged over last 500 steps = 3.0868e-01, PNorm = 119.8287, GNorm = 0.2002
Meta loss on this task batch = 3.5950e-01, Meta loss averaged over last 500 steps = 3.0874e-01, PNorm = 119.8357, GNorm = 0.2543
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 3.0874e-01, PNorm = 119.8428, GNorm = 0.2077
Meta loss on this task batch = 3.5916e-01, Meta loss averaged over last 500 steps = 3.0880e-01, PNorm = 119.8500, GNorm = 0.2894
Took 116.30861830711365 seconds to complete one epoch of meta training
Took 123.83772039413452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508582
Epoch 362
Meta loss on this task batch = 2.9398e-01, Meta loss averaged over last 500 steps = 3.0884e-01, PNorm = 119.8572, GNorm = 0.2200
Meta loss on this task batch = 3.0658e-01, Meta loss averaged over last 500 steps = 3.0885e-01, PNorm = 119.8647, GNorm = 0.2042
Meta loss on this task batch = 3.3441e-01, Meta loss averaged over last 500 steps = 3.0896e-01, PNorm = 119.8731, GNorm = 0.2001
Meta loss on this task batch = 3.2543e-01, Meta loss averaged over last 500 steps = 3.0904e-01, PNorm = 119.8815, GNorm = 0.2171
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 3.0908e-01, PNorm = 119.8901, GNorm = 0.2177
Meta loss on this task batch = 2.7723e-01, Meta loss averaged over last 500 steps = 3.0899e-01, PNorm = 119.8987, GNorm = 0.2163
Meta loss on this task batch = 3.1509e-01, Meta loss averaged over last 500 steps = 3.0903e-01, PNorm = 119.9071, GNorm = 0.2387
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 3.0897e-01, PNorm = 119.9158, GNorm = 0.1967
Meta loss on this task batch = 3.3241e-01, Meta loss averaged over last 500 steps = 3.0905e-01, PNorm = 119.9223, GNorm = 0.2644
Meta loss on this task batch = 2.7822e-01, Meta loss averaged over last 500 steps = 3.0900e-01, PNorm = 119.9299, GNorm = 0.2162
Meta loss on this task batch = 3.3480e-01, Meta loss averaged over last 500 steps = 3.0901e-01, PNorm = 119.9365, GNorm = 0.2359
Meta loss on this task batch = 3.5941e-01, Meta loss averaged over last 500 steps = 3.0908e-01, PNorm = 119.9418, GNorm = 0.2570
Meta loss on this task batch = 2.6881e-01, Meta loss averaged over last 500 steps = 3.0890e-01, PNorm = 119.9478, GNorm = 0.2175
Meta loss on this task batch = 3.7659e-01, Meta loss averaged over last 500 steps = 3.0912e-01, PNorm = 119.9530, GNorm = 0.2456
Meta loss on this task batch = 2.5885e-01, Meta loss averaged over last 500 steps = 3.0913e-01, PNorm = 119.9581, GNorm = 0.2338
Meta loss on this task batch = 2.9644e-01, Meta loss averaged over last 500 steps = 3.0909e-01, PNorm = 119.9639, GNorm = 0.2185
Meta loss on this task batch = 3.2826e-01, Meta loss averaged over last 500 steps = 3.0912e-01, PNorm = 119.9706, GNorm = 0.2150
Meta loss on this task batch = 3.1463e-01, Meta loss averaged over last 500 steps = 3.0910e-01, PNorm = 119.9782, GNorm = 0.1962
Meta loss on this task batch = 3.0707e-01, Meta loss averaged over last 500 steps = 3.0906e-01, PNorm = 119.9860, GNorm = 0.2601
Took 113.55217576026917 seconds to complete one epoch of meta training
Took 121.43535709381104 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494379
Epoch 363
Meta loss on this task batch = 3.0638e-01, Meta loss averaged over last 500 steps = 3.0910e-01, PNorm = 119.9940, GNorm = 0.2182
Meta loss on this task batch = 3.0065e-01, Meta loss averaged over last 500 steps = 3.0907e-01, PNorm = 120.0008, GNorm = 0.2542
Meta loss on this task batch = 2.8561e-01, Meta loss averaged over last 500 steps = 3.0903e-01, PNorm = 120.0083, GNorm = 0.1980
Meta loss on this task batch = 2.9581e-01, Meta loss averaged over last 500 steps = 3.0903e-01, PNorm = 120.0163, GNorm = 0.2180
Meta loss on this task batch = 3.5582e-01, Meta loss averaged over last 500 steps = 3.0911e-01, PNorm = 120.0243, GNorm = 0.2525
Meta loss on this task batch = 3.0017e-01, Meta loss averaged over last 500 steps = 3.0912e-01, PNorm = 120.0330, GNorm = 0.2257
Meta loss on this task batch = 3.0051e-01, Meta loss averaged over last 500 steps = 3.0912e-01, PNorm = 120.0412, GNorm = 0.2247
Meta loss on this task batch = 3.2680e-01, Meta loss averaged over last 500 steps = 3.0912e-01, PNorm = 120.0493, GNorm = 0.2274
Meta loss on this task batch = 2.6272e-01, Meta loss averaged over last 500 steps = 3.0890e-01, PNorm = 120.0578, GNorm = 0.1994
Meta loss on this task batch = 2.9435e-01, Meta loss averaged over last 500 steps = 3.0886e-01, PNorm = 120.0647, GNorm = 0.3116
Meta loss on this task batch = 3.2827e-01, Meta loss averaged over last 500 steps = 3.0893e-01, PNorm = 120.0703, GNorm = 0.2258
Meta loss on this task batch = 3.0680e-01, Meta loss averaged over last 500 steps = 3.0892e-01, PNorm = 120.0759, GNorm = 0.2451
Meta loss on this task batch = 2.4110e-01, Meta loss averaged over last 500 steps = 3.0885e-01, PNorm = 120.0819, GNorm = 0.1804
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 3.0882e-01, PNorm = 120.0888, GNorm = 0.2328
Meta loss on this task batch = 3.2287e-01, Meta loss averaged over last 500 steps = 3.0889e-01, PNorm = 120.0957, GNorm = 0.2057
Meta loss on this task batch = 2.8785e-01, Meta loss averaged over last 500 steps = 3.0875e-01, PNorm = 120.1031, GNorm = 0.2201
Meta loss on this task batch = 3.1289e-01, Meta loss averaged over last 500 steps = 3.0884e-01, PNorm = 120.1112, GNorm = 0.2768
Meta loss on this task batch = 3.1868e-01, Meta loss averaged over last 500 steps = 3.0893e-01, PNorm = 120.1181, GNorm = 0.2265
Meta loss on this task batch = 2.5791e-01, Meta loss averaged over last 500 steps = 3.0887e-01, PNorm = 120.1251, GNorm = 0.2320
Took 113.9495198726654 seconds to complete one epoch of meta training
Took 121.44393587112427 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509889
Epoch 364
Meta loss on this task batch = 2.8022e-01, Meta loss averaged over last 500 steps = 3.0879e-01, PNorm = 120.1330, GNorm = 0.2098
Meta loss on this task batch = 2.8348e-01, Meta loss averaged over last 500 steps = 3.0878e-01, PNorm = 120.1401, GNorm = 0.2104
Meta loss on this task batch = 3.6629e-01, Meta loss averaged over last 500 steps = 3.0885e-01, PNorm = 120.1465, GNorm = 0.2321
Meta loss on this task batch = 2.6907e-01, Meta loss averaged over last 500 steps = 3.0866e-01, PNorm = 120.1534, GNorm = 0.1851
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 3.0848e-01, PNorm = 120.1618, GNorm = 0.2280
Meta loss on this task batch = 2.9793e-01, Meta loss averaged over last 500 steps = 3.0839e-01, PNorm = 120.1707, GNorm = 0.2465
Meta loss on this task batch = 2.7348e-01, Meta loss averaged over last 500 steps = 3.0840e-01, PNorm = 120.1797, GNorm = 0.2074
Meta loss on this task batch = 2.9244e-01, Meta loss averaged over last 500 steps = 3.0834e-01, PNorm = 120.1880, GNorm = 0.2057
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 3.0835e-01, PNorm = 120.1967, GNorm = 0.1922
Meta loss on this task batch = 3.2696e-01, Meta loss averaged over last 500 steps = 3.0846e-01, PNorm = 120.2036, GNorm = 0.2524
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 3.0832e-01, PNorm = 120.2118, GNorm = 0.1938
Meta loss on this task batch = 2.9365e-01, Meta loss averaged over last 500 steps = 3.0836e-01, PNorm = 120.2207, GNorm = 0.2242
Meta loss on this task batch = 3.3322e-01, Meta loss averaged over last 500 steps = 3.0846e-01, PNorm = 120.2277, GNorm = 0.2509
Meta loss on this task batch = 3.2394e-01, Meta loss averaged over last 500 steps = 3.0846e-01, PNorm = 120.2340, GNorm = 0.2575
Meta loss on this task batch = 3.0537e-01, Meta loss averaged over last 500 steps = 3.0848e-01, PNorm = 120.2405, GNorm = 0.2054
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 3.0842e-01, PNorm = 120.2461, GNorm = 0.2170
Meta loss on this task batch = 2.7004e-01, Meta loss averaged over last 500 steps = 3.0816e-01, PNorm = 120.2522, GNorm = 0.1905
Meta loss on this task batch = 3.7413e-01, Meta loss averaged over last 500 steps = 3.0836e-01, PNorm = 120.2578, GNorm = 0.2366
Meta loss on this task batch = 2.7182e-01, Meta loss averaged over last 500 steps = 3.0826e-01, PNorm = 120.2659, GNorm = 0.2615
Took 116.39004707336426 seconds to complete one epoch of meta training
Took 124.24409365653992 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496607
Epoch 365
Meta loss on this task batch = 2.5781e-01, Meta loss averaged over last 500 steps = 3.0820e-01, PNorm = 120.2740, GNorm = 0.2002
Meta loss on this task batch = 3.2582e-01, Meta loss averaged over last 500 steps = 3.0832e-01, PNorm = 120.2815, GNorm = 0.2130
Meta loss on this task batch = 3.8355e-01, Meta loss averaged over last 500 steps = 3.0855e-01, PNorm = 120.2882, GNorm = 0.2451
Meta loss on this task batch = 2.3914e-01, Meta loss averaged over last 500 steps = 3.0841e-01, PNorm = 120.2951, GNorm = 0.1878
Meta loss on this task batch = 3.1783e-01, Meta loss averaged over last 500 steps = 3.0839e-01, PNorm = 120.3020, GNorm = 0.2144
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 3.0835e-01, PNorm = 120.3076, GNorm = 0.2213
Meta loss on this task batch = 2.3949e-01, Meta loss averaged over last 500 steps = 3.0812e-01, PNorm = 120.3154, GNorm = 0.2267
Meta loss on this task batch = 2.7894e-01, Meta loss averaged over last 500 steps = 3.0807e-01, PNorm = 120.3226, GNorm = 0.2199
Meta loss on this task batch = 3.7939e-01, Meta loss averaged over last 500 steps = 3.0819e-01, PNorm = 120.3296, GNorm = 0.2642
Meta loss on this task batch = 3.1854e-01, Meta loss averaged over last 500 steps = 3.0814e-01, PNorm = 120.3374, GNorm = 0.2332
Meta loss on this task batch = 3.0115e-01, Meta loss averaged over last 500 steps = 3.0808e-01, PNorm = 120.3453, GNorm = 0.1970
Meta loss on this task batch = 2.8747e-01, Meta loss averaged over last 500 steps = 3.0802e-01, PNorm = 120.3538, GNorm = 0.2120
Meta loss on this task batch = 3.7592e-01, Meta loss averaged over last 500 steps = 3.0817e-01, PNorm = 120.3617, GNorm = 0.2558
Meta loss on this task batch = 2.6009e-01, Meta loss averaged over last 500 steps = 3.0813e-01, PNorm = 120.3703, GNorm = 0.1760
Meta loss on this task batch = 3.2397e-01, Meta loss averaged over last 500 steps = 3.0817e-01, PNorm = 120.3790, GNorm = 0.2657
Meta loss on this task batch = 3.1240e-01, Meta loss averaged over last 500 steps = 3.0822e-01, PNorm = 120.3885, GNorm = 0.2213
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 3.0823e-01, PNorm = 120.3976, GNorm = 0.2294
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 3.0817e-01, PNorm = 120.4069, GNorm = 0.2216
Meta loss on this task batch = 2.6030e-01, Meta loss averaged over last 500 steps = 3.0817e-01, PNorm = 120.4151, GNorm = 0.2634
Took 119.25388312339783 seconds to complete one epoch of meta training
Took 126.12407422065735 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490562
Epoch 366
Meta loss on this task batch = 2.8854e-01, Meta loss averaged over last 500 steps = 3.0810e-01, PNorm = 120.4238, GNorm = 0.2075
Meta loss on this task batch = 2.9228e-01, Meta loss averaged over last 500 steps = 3.0801e-01, PNorm = 120.4329, GNorm = 0.2602
Meta loss on this task batch = 3.2504e-01, Meta loss averaged over last 500 steps = 3.0798e-01, PNorm = 120.4419, GNorm = 0.2251
Meta loss on this task batch = 3.1226e-01, Meta loss averaged over last 500 steps = 3.0795e-01, PNorm = 120.4502, GNorm = 0.2484
Meta loss on this task batch = 2.7007e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 120.4581, GNorm = 0.2215
Meta loss on this task batch = 2.5549e-01, Meta loss averaged over last 500 steps = 3.0776e-01, PNorm = 120.4666, GNorm = 0.2354
Meta loss on this task batch = 3.5830e-01, Meta loss averaged over last 500 steps = 3.0792e-01, PNorm = 120.4750, GNorm = 0.2671
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 3.0782e-01, PNorm = 120.4834, GNorm = 0.2121
Meta loss on this task batch = 3.0934e-01, Meta loss averaged over last 500 steps = 3.0768e-01, PNorm = 120.4924, GNorm = 0.2134
Meta loss on this task batch = 3.1337e-01, Meta loss averaged over last 500 steps = 3.0769e-01, PNorm = 120.5021, GNorm = 0.2538
Meta loss on this task batch = 2.4830e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 120.5111, GNorm = 0.2094
Meta loss on this task batch = 3.6811e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 120.5192, GNorm = 0.2557
Meta loss on this task batch = 3.1399e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 120.5273, GNorm = 0.2402
Meta loss on this task batch = 3.7094e-01, Meta loss averaged over last 500 steps = 3.0779e-01, PNorm = 120.5340, GNorm = 0.2791
Meta loss on this task batch = 2.8951e-01, Meta loss averaged over last 500 steps = 3.0772e-01, PNorm = 120.5410, GNorm = 0.2186
Meta loss on this task batch = 2.9872e-01, Meta loss averaged over last 500 steps = 3.0756e-01, PNorm = 120.5485, GNorm = 0.2358
Meta loss on this task batch = 3.1254e-01, Meta loss averaged over last 500 steps = 3.0748e-01, PNorm = 120.5559, GNorm = 0.2238
Meta loss on this task batch = 3.1270e-01, Meta loss averaged over last 500 steps = 3.0749e-01, PNorm = 120.5636, GNorm = 0.2089
Meta loss on this task batch = 2.8240e-01, Meta loss averaged over last 500 steps = 3.0746e-01, PNorm = 120.5711, GNorm = 0.2353
Took 114.83038640022278 seconds to complete one epoch of meta training
Took 122.13205814361572 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506998
Epoch 367
Meta loss on this task batch = 3.4847e-01, Meta loss averaged over last 500 steps = 3.0770e-01, PNorm = 120.5769, GNorm = 0.2352
Meta loss on this task batch = 2.9423e-01, Meta loss averaged over last 500 steps = 3.0762e-01, PNorm = 120.5825, GNorm = 0.1972
Meta loss on this task batch = 2.4646e-01, Meta loss averaged over last 500 steps = 3.0757e-01, PNorm = 120.5891, GNorm = 0.1958
Meta loss on this task batch = 3.3883e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 120.5957, GNorm = 0.2160
Meta loss on this task batch = 3.5379e-01, Meta loss averaged over last 500 steps = 3.0782e-01, PNorm = 120.6017, GNorm = 0.2488
Meta loss on this task batch = 2.6980e-01, Meta loss averaged over last 500 steps = 3.0765e-01, PNorm = 120.6078, GNorm = 0.2049
Meta loss on this task batch = 2.9680e-01, Meta loss averaged over last 500 steps = 3.0764e-01, PNorm = 120.6131, GNorm = 0.2216
Meta loss on this task batch = 2.8396e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 120.6185, GNorm = 0.2178
Meta loss on this task batch = 2.8076e-01, Meta loss averaged over last 500 steps = 3.0754e-01, PNorm = 120.6241, GNorm = 0.2210
Meta loss on this task batch = 3.8431e-01, Meta loss averaged over last 500 steps = 3.0761e-01, PNorm = 120.6298, GNorm = 0.2441
Meta loss on this task batch = 3.4531e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 120.6351, GNorm = 0.2116
Meta loss on this task batch = 3.4452e-01, Meta loss averaged over last 500 steps = 3.0753e-01, PNorm = 120.6393, GNorm = 0.2293
Meta loss on this task batch = 3.2685e-01, Meta loss averaged over last 500 steps = 3.0772e-01, PNorm = 120.6434, GNorm = 0.2338
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 3.0766e-01, PNorm = 120.6484, GNorm = 0.1844
Meta loss on this task batch = 3.1981e-01, Meta loss averaged over last 500 steps = 3.0778e-01, PNorm = 120.6532, GNorm = 0.2292
Meta loss on this task batch = 3.1428e-01, Meta loss averaged over last 500 steps = 3.0781e-01, PNorm = 120.6586, GNorm = 0.2309
Meta loss on this task batch = 3.4218e-01, Meta loss averaged over last 500 steps = 3.0783e-01, PNorm = 120.6642, GNorm = 0.2275
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 3.0794e-01, PNorm = 120.6708, GNorm = 0.1902
Meta loss on this task batch = 2.8043e-01, Meta loss averaged over last 500 steps = 3.0781e-01, PNorm = 120.6781, GNorm = 0.2567
Took 114.79628157615662 seconds to complete one epoch of meta training
Took 122.7948887348175 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481467
Epoch 368
Meta loss on this task batch = 3.2936e-01, Meta loss averaged over last 500 steps = 3.0766e-01, PNorm = 120.6851, GNorm = 0.2318
Meta loss on this task batch = 3.5717e-01, Meta loss averaged over last 500 steps = 3.0783e-01, PNorm = 120.6924, GNorm = 0.2507
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.0770e-01, PNorm = 120.7001, GNorm = 0.2090
Meta loss on this task batch = 3.4834e-01, Meta loss averaged over last 500 steps = 3.0768e-01, PNorm = 120.7085, GNorm = 0.2076
Meta loss on this task batch = 2.5600e-01, Meta loss averaged over last 500 steps = 3.0767e-01, PNorm = 120.7166, GNorm = 0.2228
Meta loss on this task batch = 3.3003e-01, Meta loss averaged over last 500 steps = 3.0763e-01, PNorm = 120.7258, GNorm = 0.2434
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 3.0756e-01, PNorm = 120.7350, GNorm = 0.2071
Meta loss on this task batch = 3.1060e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 120.7446, GNorm = 0.2248
Meta loss on this task batch = 3.1524e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 120.7526, GNorm = 0.2184
Meta loss on this task batch = 3.2565e-01, Meta loss averaged over last 500 steps = 3.0768e-01, PNorm = 120.7611, GNorm = 0.2402
Meta loss on this task batch = 2.7859e-01, Meta loss averaged over last 500 steps = 3.0762e-01, PNorm = 120.7697, GNorm = 0.2102
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 3.0772e-01, PNorm = 120.7781, GNorm = 0.2623
Meta loss on this task batch = 2.0883e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 120.7862, GNorm = 0.2034
Meta loss on this task batch = 3.0977e-01, Meta loss averaged over last 500 steps = 3.0739e-01, PNorm = 120.7937, GNorm = 0.2369
Meta loss on this task batch = 2.6589e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 120.8011, GNorm = 0.1825
Meta loss on this task batch = 3.1644e-01, Meta loss averaged over last 500 steps = 3.0750e-01, PNorm = 120.8080, GNorm = 0.2795
Meta loss on this task batch = 2.9880e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 120.8156, GNorm = 0.2152
Meta loss on this task batch = 3.0141e-01, Meta loss averaged over last 500 steps = 3.0753e-01, PNorm = 120.8233, GNorm = 0.2059
Meta loss on this task batch = 3.2165e-01, Meta loss averaged over last 500 steps = 3.0760e-01, PNorm = 120.8296, GNorm = 0.2836
Took 114.94275283813477 seconds to complete one epoch of meta training
Took 122.53379130363464 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481631
Epoch 369
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 3.0759e-01, PNorm = 120.8362, GNorm = 0.2116
Meta loss on this task batch = 3.3267e-01, Meta loss averaged over last 500 steps = 3.0758e-01, PNorm = 120.8427, GNorm = 0.2265
Meta loss on this task batch = 2.8967e-01, Meta loss averaged over last 500 steps = 3.0751e-01, PNorm = 120.8500, GNorm = 0.2102
Meta loss on this task batch = 3.3317e-01, Meta loss averaged over last 500 steps = 3.0752e-01, PNorm = 120.8572, GNorm = 0.2399
Meta loss on this task batch = 2.7712e-01, Meta loss averaged over last 500 steps = 3.0734e-01, PNorm = 120.8646, GNorm = 0.2105
Meta loss on this task batch = 2.8208e-01, Meta loss averaged over last 500 steps = 3.0730e-01, PNorm = 120.8724, GNorm = 0.2248
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 3.0713e-01, PNorm = 120.8809, GNorm = 0.2284
Meta loss on this task batch = 2.9818e-01, Meta loss averaged over last 500 steps = 3.0716e-01, PNorm = 120.8901, GNorm = 0.1955
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 120.8993, GNorm = 0.2230
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 120.9087, GNorm = 0.2471
Meta loss on this task batch = 2.5435e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 120.9194, GNorm = 0.2381
Meta loss on this task batch = 3.0473e-01, Meta loss averaged over last 500 steps = 3.0710e-01, PNorm = 120.9307, GNorm = 0.3344
Meta loss on this task batch = 3.2381e-01, Meta loss averaged over last 500 steps = 3.0719e-01, PNorm = 120.9410, GNorm = 0.2268
Meta loss on this task batch = 3.1530e-01, Meta loss averaged over last 500 steps = 3.0713e-01, PNorm = 120.9512, GNorm = 0.2424
Meta loss on this task batch = 3.6004e-01, Meta loss averaged over last 500 steps = 3.0726e-01, PNorm = 120.9594, GNorm = 0.2816
Meta loss on this task batch = 3.2967e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 120.9692, GNorm = 0.2527
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 3.0735e-01, PNorm = 120.9779, GNorm = 0.2337
Meta loss on this task batch = 3.0124e-01, Meta loss averaged over last 500 steps = 3.0724e-01, PNorm = 120.9860, GNorm = 0.2320
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 3.0713e-01, PNorm = 120.9944, GNorm = 0.2451
Took 114.22258687019348 seconds to complete one epoch of meta training
Took 122.26586246490479 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488729
Epoch 370
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 3.0726e-01, PNorm = 121.0038, GNorm = 0.1968
Meta loss on this task batch = 2.6106e-01, Meta loss averaged over last 500 steps = 3.0710e-01, PNorm = 121.0129, GNorm = 0.2079
Meta loss on this task batch = 3.2902e-01, Meta loss averaged over last 500 steps = 3.0703e-01, PNorm = 121.0212, GNorm = 0.2520
Meta loss on this task batch = 3.0977e-01, Meta loss averaged over last 500 steps = 3.0693e-01, PNorm = 121.0296, GNorm = 0.2302
Meta loss on this task batch = 3.6396e-01, Meta loss averaged over last 500 steps = 3.0705e-01, PNorm = 121.0374, GNorm = 0.2991
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 3.0693e-01, PNorm = 121.0463, GNorm = 0.1953
Meta loss on this task batch = 3.0760e-01, Meta loss averaged over last 500 steps = 3.0701e-01, PNorm = 121.0550, GNorm = 0.2059
Meta loss on this task batch = 3.1365e-01, Meta loss averaged over last 500 steps = 3.0704e-01, PNorm = 121.0634, GNorm = 0.2090
Meta loss on this task batch = 3.4564e-01, Meta loss averaged over last 500 steps = 3.0711e-01, PNorm = 121.0724, GNorm = 0.2235
Meta loss on this task batch = 2.4752e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 121.0813, GNorm = 0.2071
Meta loss on this task batch = 2.4504e-01, Meta loss averaged over last 500 steps = 3.0690e-01, PNorm = 121.0910, GNorm = 0.1948
Meta loss on this task batch = 3.0387e-01, Meta loss averaged over last 500 steps = 3.0686e-01, PNorm = 121.1003, GNorm = 0.2869
Meta loss on this task batch = 3.3765e-01, Meta loss averaged over last 500 steps = 3.0699e-01, PNorm = 121.1088, GNorm = 0.2249
Meta loss on this task batch = 3.0748e-01, Meta loss averaged over last 500 steps = 3.0698e-01, PNorm = 121.1155, GNorm = 0.2880
Meta loss on this task batch = 3.0519e-01, Meta loss averaged over last 500 steps = 3.0694e-01, PNorm = 121.1221, GNorm = 0.2049
Meta loss on this task batch = 2.4642e-01, Meta loss averaged over last 500 steps = 3.0680e-01, PNorm = 121.1293, GNorm = 0.2410
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 3.0679e-01, PNorm = 121.1365, GNorm = 0.2150
Meta loss on this task batch = 3.6975e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 121.1430, GNorm = 0.2306
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 3.0679e-01, PNorm = 121.1496, GNorm = 0.2317
Took 114.72861862182617 seconds to complete one epoch of meta training
Took 122.3853120803833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460270
Epoch 371
Meta loss on this task batch = 3.2560e-01, Meta loss averaged over last 500 steps = 3.0687e-01, PNorm = 121.1562, GNorm = 0.2209
Meta loss on this task batch = 2.9368e-01, Meta loss averaged over last 500 steps = 3.0677e-01, PNorm = 121.1623, GNorm = 0.2705
Meta loss on this task batch = 2.4029e-01, Meta loss averaged over last 500 steps = 3.0654e-01, PNorm = 121.1691, GNorm = 0.2040
Meta loss on this task batch = 2.9942e-01, Meta loss averaged over last 500 steps = 3.0646e-01, PNorm = 121.1767, GNorm = 0.2322
Meta loss on this task batch = 3.0042e-01, Meta loss averaged over last 500 steps = 3.0660e-01, PNorm = 121.1835, GNorm = 0.2205
Meta loss on this task batch = 3.8911e-01, Meta loss averaged over last 500 steps = 3.0658e-01, PNorm = 121.1884, GNorm = 0.2802
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 121.1950, GNorm = 0.2324
Meta loss on this task batch = 3.1825e-01, Meta loss averaged over last 500 steps = 3.0640e-01, PNorm = 121.2021, GNorm = 0.2323
Meta loss on this task batch = 2.7854e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 121.2101, GNorm = 0.2352
Meta loss on this task batch = 2.8376e-01, Meta loss averaged over last 500 steps = 3.0619e-01, PNorm = 121.2173, GNorm = 0.2210
Meta loss on this task batch = 3.2116e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 121.2247, GNorm = 0.2299
Meta loss on this task batch = 3.4822e-01, Meta loss averaged over last 500 steps = 3.0639e-01, PNorm = 121.2317, GNorm = 0.2409
Meta loss on this task batch = 3.0697e-01, Meta loss averaged over last 500 steps = 3.0645e-01, PNorm = 121.2394, GNorm = 0.2117
Meta loss on this task batch = 2.9608e-01, Meta loss averaged over last 500 steps = 3.0646e-01, PNorm = 121.2474, GNorm = 0.2141
Meta loss on this task batch = 3.1071e-01, Meta loss averaged over last 500 steps = 3.0648e-01, PNorm = 121.2555, GNorm = 0.1957
Meta loss on this task batch = 3.0886e-01, Meta loss averaged over last 500 steps = 3.0652e-01, PNorm = 121.2615, GNorm = 0.2257
Meta loss on this task batch = 3.0577e-01, Meta loss averaged over last 500 steps = 3.0653e-01, PNorm = 121.2654, GNorm = 0.2412
Meta loss on this task batch = 2.9349e-01, Meta loss averaged over last 500 steps = 3.0651e-01, PNorm = 121.2699, GNorm = 0.1988
Meta loss on this task batch = 3.7113e-01, Meta loss averaged over last 500 steps = 3.0657e-01, PNorm = 121.2748, GNorm = 0.2735
Took 115.05094265937805 seconds to complete one epoch of meta training
Took 122.41500091552734 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511267
Epoch 372
Meta loss on this task batch = 3.5985e-01, Meta loss averaged over last 500 steps = 3.0661e-01, PNorm = 121.2796, GNorm = 0.2191
Meta loss on this task batch = 3.5216e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 121.2850, GNorm = 0.2105
Meta loss on this task batch = 3.1514e-01, Meta loss averaged over last 500 steps = 3.0664e-01, PNorm = 121.2912, GNorm = 0.2029
Meta loss on this task batch = 2.6972e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 121.2984, GNorm = 0.1785
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 3.0649e-01, PNorm = 121.3064, GNorm = 0.1976
Meta loss on this task batch = 3.1859e-01, Meta loss averaged over last 500 steps = 3.0644e-01, PNorm = 121.3148, GNorm = 0.1994
Meta loss on this task batch = 3.5795e-01, Meta loss averaged over last 500 steps = 3.0661e-01, PNorm = 121.3233, GNorm = 0.2218
Meta loss on this task batch = 3.0945e-01, Meta loss averaged over last 500 steps = 3.0667e-01, PNorm = 121.3322, GNorm = 0.2534
Meta loss on this task batch = 3.2218e-01, Meta loss averaged over last 500 steps = 3.0672e-01, PNorm = 121.3405, GNorm = 0.2223
Meta loss on this task batch = 2.9856e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 121.3485, GNorm = 0.2337
Meta loss on this task batch = 2.8085e-01, Meta loss averaged over last 500 steps = 3.0669e-01, PNorm = 121.3558, GNorm = 0.2129
Meta loss on this task batch = 3.1579e-01, Meta loss averaged over last 500 steps = 3.0671e-01, PNorm = 121.3625, GNorm = 0.2620
Meta loss on this task batch = 3.1290e-01, Meta loss averaged over last 500 steps = 3.0662e-01, PNorm = 121.3683, GNorm = 0.2272
Meta loss on this task batch = 3.1275e-01, Meta loss averaged over last 500 steps = 3.0663e-01, PNorm = 121.3730, GNorm = 0.2277
Meta loss on this task batch = 2.5015e-01, Meta loss averaged over last 500 steps = 3.0663e-01, PNorm = 121.3789, GNorm = 0.1955
Meta loss on this task batch = 3.0708e-01, Meta loss averaged over last 500 steps = 3.0662e-01, PNorm = 121.3836, GNorm = 0.2576
Meta loss on this task batch = 2.9328e-01, Meta loss averaged over last 500 steps = 3.0642e-01, PNorm = 121.3896, GNorm = 0.2395
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 3.0641e-01, PNorm = 121.3958, GNorm = 0.1954
Meta loss on this task batch = 2.4829e-01, Meta loss averaged over last 500 steps = 3.0626e-01, PNorm = 121.4022, GNorm = 0.2750
Took 116.36892342567444 seconds to complete one epoch of meta training
Took 124.4267189502716 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490990
Epoch 373
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 3.0628e-01, PNorm = 121.4098, GNorm = 0.2106
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 3.0624e-01, PNorm = 121.4179, GNorm = 0.2225
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 3.0612e-01, PNorm = 121.4264, GNorm = 0.1918
Meta loss on this task batch = 2.6188e-01, Meta loss averaged over last 500 steps = 3.0597e-01, PNorm = 121.4356, GNorm = 0.1974
Meta loss on this task batch = 2.7929e-01, Meta loss averaged over last 500 steps = 3.0587e-01, PNorm = 121.4458, GNorm = 0.2371
Meta loss on this task batch = 3.1439e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 121.4549, GNorm = 0.2458
Meta loss on this task batch = 2.9367e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 121.4630, GNorm = 0.2815
Meta loss on this task batch = 2.6900e-01, Meta loss averaged over last 500 steps = 3.0579e-01, PNorm = 121.4712, GNorm = 0.2388
Meta loss on this task batch = 3.0761e-01, Meta loss averaged over last 500 steps = 3.0577e-01, PNorm = 121.4783, GNorm = 0.2321
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 121.4857, GNorm = 0.2270
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 121.4930, GNorm = 0.2353
Meta loss on this task batch = 3.6297e-01, Meta loss averaged over last 500 steps = 3.0578e-01, PNorm = 121.5011, GNorm = 0.2500
Meta loss on this task batch = 2.5102e-01, Meta loss averaged over last 500 steps = 3.0566e-01, PNorm = 121.5093, GNorm = 0.2136
Meta loss on this task batch = 3.2287e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 121.5176, GNorm = 0.2288
Meta loss on this task batch = 3.2344e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 121.5263, GNorm = 0.2358
Meta loss on this task batch = 2.8511e-01, Meta loss averaged over last 500 steps = 3.0575e-01, PNorm = 121.5345, GNorm = 0.2289
Meta loss on this task batch = 3.3235e-01, Meta loss averaged over last 500 steps = 3.0582e-01, PNorm = 121.5421, GNorm = 0.2207
Meta loss on this task batch = 3.3289e-01, Meta loss averaged over last 500 steps = 3.0590e-01, PNorm = 121.5503, GNorm = 0.2423
Meta loss on this task batch = 3.4050e-01, Meta loss averaged over last 500 steps = 3.0583e-01, PNorm = 121.5575, GNorm = 0.3145
Took 116.87510251998901 seconds to complete one epoch of meta training
Took 123.29822373390198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507605
Epoch 374
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 121.5641, GNorm = 0.1955
Meta loss on this task batch = 2.6485e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 121.5708, GNorm = 0.2033
Meta loss on this task batch = 3.6315e-01, Meta loss averaged over last 500 steps = 3.0574e-01, PNorm = 121.5764, GNorm = 0.2309
Meta loss on this task batch = 2.2299e-01, Meta loss averaged over last 500 steps = 3.0563e-01, PNorm = 121.5818, GNorm = 0.1964
Meta loss on this task batch = 3.1676e-01, Meta loss averaged over last 500 steps = 3.0564e-01, PNorm = 121.5878, GNorm = 0.2383
Meta loss on this task batch = 3.0027e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 121.5954, GNorm = 0.2272
Meta loss on this task batch = 3.1109e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 121.6023, GNorm = 0.2091
Meta loss on this task batch = 3.3326e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 121.6095, GNorm = 0.2406
Meta loss on this task batch = 2.9996e-01, Meta loss averaged over last 500 steps = 3.0547e-01, PNorm = 121.6164, GNorm = 0.2150
Meta loss on this task batch = 2.6999e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 121.6236, GNorm = 0.1913
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 3.0520e-01, PNorm = 121.6327, GNorm = 0.2066
Meta loss on this task batch = 3.3192e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 121.6415, GNorm = 0.2215
Meta loss on this task batch = 3.1469e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 121.6496, GNorm = 0.2285
Meta loss on this task batch = 3.4376e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 121.6573, GNorm = 0.2321
Meta loss on this task batch = 2.7650e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 121.6644, GNorm = 0.2102
Meta loss on this task batch = 3.1438e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 121.6725, GNorm = 0.2111
Meta loss on this task batch = 2.8718e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 121.6803, GNorm = 0.2291
Meta loss on this task batch = 3.2774e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 121.6877, GNorm = 0.2149
Meta loss on this task batch = 3.4617e-01, Meta loss averaged over last 500 steps = 3.0560e-01, PNorm = 121.6956, GNorm = 0.2978
Took 114.4243266582489 seconds to complete one epoch of meta training
Took 121.32421827316284 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489956
Epoch 375
Meta loss on this task batch = 2.8955e-01, Meta loss averaged over last 500 steps = 3.0549e-01, PNorm = 121.7032, GNorm = 0.2071
Meta loss on this task batch = 3.0952e-01, Meta loss averaged over last 500 steps = 3.0552e-01, PNorm = 121.7106, GNorm = 0.2287
Meta loss on this task batch = 3.4870e-01, Meta loss averaged over last 500 steps = 3.0547e-01, PNorm = 121.7173, GNorm = 0.2330
Meta loss on this task batch = 3.1022e-01, Meta loss averaged over last 500 steps = 3.0548e-01, PNorm = 121.7235, GNorm = 0.2449
Meta loss on this task batch = 3.3529e-01, Meta loss averaged over last 500 steps = 3.0555e-01, PNorm = 121.7297, GNorm = 0.2128
Meta loss on this task batch = 3.2469e-01, Meta loss averaged over last 500 steps = 3.0557e-01, PNorm = 121.7356, GNorm = 0.2101
Meta loss on this task batch = 2.7704e-01, Meta loss averaged over last 500 steps = 3.0554e-01, PNorm = 121.7414, GNorm = 0.1973
Meta loss on this task batch = 2.4492e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 121.7477, GNorm = 0.2181
Meta loss on this task batch = 2.8681e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 121.7541, GNorm = 0.2332
Meta loss on this task batch = 3.1272e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 121.7603, GNorm = 0.2240
Meta loss on this task batch = 2.6449e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 121.7681, GNorm = 0.2533
Meta loss on this task batch = 2.9361e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 121.7781, GNorm = 0.2035
Meta loss on this task batch = 3.5774e-01, Meta loss averaged over last 500 steps = 3.0543e-01, PNorm = 121.7885, GNorm = 0.2271
Meta loss on this task batch = 3.1449e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 121.7983, GNorm = 0.2057
Meta loss on this task batch = 3.0999e-01, Meta loss averaged over last 500 steps = 3.0546e-01, PNorm = 121.8076, GNorm = 0.2479
Meta loss on this task batch = 2.3602e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 121.8177, GNorm = 0.2057
Meta loss on this task batch = 3.0267e-01, Meta loss averaged over last 500 steps = 3.0537e-01, PNorm = 121.8275, GNorm = 0.2314
Meta loss on this task batch = 3.1410e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 121.8371, GNorm = 0.2244
Meta loss on this task batch = 3.3187e-01, Meta loss averaged over last 500 steps = 3.0541e-01, PNorm = 121.8461, GNorm = 0.2819
Took 116.04687738418579 seconds to complete one epoch of meta training
Took 124.18060111999512 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493407
Epoch 376
Meta loss on this task batch = 3.5518e-01, Meta loss averaged over last 500 steps = 3.0550e-01, PNorm = 121.8548, GNorm = 0.2371
Meta loss on this task batch = 3.2095e-01, Meta loss averaged over last 500 steps = 3.0547e-01, PNorm = 121.8615, GNorm = 0.2591
Meta loss on this task batch = 2.1302e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 121.8682, GNorm = 0.1892
Meta loss on this task batch = 2.6234e-01, Meta loss averaged over last 500 steps = 3.0505e-01, PNorm = 121.8744, GNorm = 0.1989
Meta loss on this task batch = 2.9822e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 121.8812, GNorm = 0.2186
Meta loss on this task batch = 3.0856e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 121.8887, GNorm = 0.2691
Meta loss on this task batch = 3.1170e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 121.8968, GNorm = 0.2434
Meta loss on this task batch = 3.3530e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 121.9044, GNorm = 0.2384
Meta loss on this task batch = 3.5589e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 121.9113, GNorm = 0.2329
Meta loss on this task batch = 3.2638e-01, Meta loss averaged over last 500 steps = 3.0522e-01, PNorm = 121.9175, GNorm = 0.2220
Meta loss on this task batch = 2.9808e-01, Meta loss averaged over last 500 steps = 3.0522e-01, PNorm = 121.9229, GNorm = 0.2262
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 121.9278, GNorm = 0.2596
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 3.0529e-01, PNorm = 121.9341, GNorm = 0.1939
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 121.9413, GNorm = 0.2077
Meta loss on this task batch = 3.4819e-01, Meta loss averaged over last 500 steps = 3.0538e-01, PNorm = 121.9480, GNorm = 0.2402
Meta loss on this task batch = 3.3876e-01, Meta loss averaged over last 500 steps = 3.0547e-01, PNorm = 121.9556, GNorm = 0.2337
Meta loss on this task batch = 2.6377e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 121.9641, GNorm = 0.1908
Meta loss on this task batch = 3.0369e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 121.9727, GNorm = 0.2220
Meta loss on this task batch = 3.7822e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 121.9801, GNorm = 0.3117
Took 116.28808546066284 seconds to complete one epoch of meta training
Took 122.82252550125122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515523
Epoch 377
Meta loss on this task batch = 3.2654e-01, Meta loss averaged over last 500 steps = 3.0552e-01, PNorm = 121.9884, GNorm = 0.2237
Meta loss on this task batch = 3.0133e-01, Meta loss averaged over last 500 steps = 3.0556e-01, PNorm = 121.9967, GNorm = 0.2021
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 122.0054, GNorm = 0.1721
Meta loss on this task batch = 3.0852e-01, Meta loss averaged over last 500 steps = 3.0544e-01, PNorm = 122.0144, GNorm = 0.2249
Meta loss on this task batch = 2.8208e-01, Meta loss averaged over last 500 steps = 3.0534e-01, PNorm = 122.0230, GNorm = 0.2175
Meta loss on this task batch = 2.4400e-01, Meta loss averaged over last 500 steps = 3.0517e-01, PNorm = 122.0305, GNorm = 0.2126
Meta loss on this task batch = 2.8884e-01, Meta loss averaged over last 500 steps = 3.0510e-01, PNorm = 122.0374, GNorm = 0.2366
Meta loss on this task batch = 3.1143e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 122.0436, GNorm = 0.2220
Meta loss on this task batch = 3.2953e-01, Meta loss averaged over last 500 steps = 3.0520e-01, PNorm = 122.0495, GNorm = 0.2286
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 122.0556, GNorm = 0.2379
Meta loss on this task batch = 2.9208e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 122.0618, GNorm = 0.2424
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 122.0682, GNorm = 0.2137
Meta loss on this task batch = 3.0720e-01, Meta loss averaged over last 500 steps = 3.0545e-01, PNorm = 122.0733, GNorm = 0.2522
Meta loss on this task batch = 2.9973e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 122.0797, GNorm = 0.2332
Meta loss on this task batch = 3.3325e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 122.0861, GNorm = 0.2569
Meta loss on this task batch = 3.3931e-01, Meta loss averaged over last 500 steps = 3.0529e-01, PNorm = 122.0924, GNorm = 0.2291
Meta loss on this task batch = 3.3912e-01, Meta loss averaged over last 500 steps = 3.0530e-01, PNorm = 122.0989, GNorm = 0.2340
Meta loss on this task batch = 3.4113e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 122.1049, GNorm = 0.2411
Meta loss on this task batch = 2.3957e-01, Meta loss averaged over last 500 steps = 3.0522e-01, PNorm = 122.1116, GNorm = 0.2666
Took 114.15878033638 seconds to complete one epoch of meta training
Took 121.85436224937439 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488884
Epoch 378
Meta loss on this task batch = 2.6501e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 122.1195, GNorm = 0.1920
Meta loss on this task batch = 3.0759e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 122.1273, GNorm = 0.2358
Meta loss on this task batch = 2.9021e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 122.1356, GNorm = 0.2168
Meta loss on this task batch = 3.2548e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 122.1440, GNorm = 0.2275
Meta loss on this task batch = 3.9483e-01, Meta loss averaged over last 500 steps = 3.0509e-01, PNorm = 122.1517, GNorm = 0.2440
Meta loss on this task batch = 2.7560e-01, Meta loss averaged over last 500 steps = 3.0505e-01, PNorm = 122.1596, GNorm = 0.1861
Meta loss on this task batch = 3.2898e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 122.1667, GNorm = 0.2353
Meta loss on this task batch = 2.9357e-01, Meta loss averaged over last 500 steps = 3.0509e-01, PNorm = 122.1755, GNorm = 0.2195
Meta loss on this task batch = 3.1920e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 122.1842, GNorm = 0.2232
Meta loss on this task batch = 3.0420e-01, Meta loss averaged over last 500 steps = 3.0522e-01, PNorm = 122.1922, GNorm = 0.2501
Meta loss on this task batch = 2.7324e-01, Meta loss averaged over last 500 steps = 3.0516e-01, PNorm = 122.2007, GNorm = 0.1943
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 3.0501e-01, PNorm = 122.2087, GNorm = 0.1908
Meta loss on this task batch = 3.0467e-01, Meta loss averaged over last 500 steps = 3.0507e-01, PNorm = 122.2164, GNorm = 0.2266
Meta loss on this task batch = 3.0036e-01, Meta loss averaged over last 500 steps = 3.0522e-01, PNorm = 122.2239, GNorm = 0.2174
Meta loss on this task batch = 2.7639e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 122.2311, GNorm = 0.2022
Meta loss on this task batch = 3.0055e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 122.2385, GNorm = 0.2061
Meta loss on this task batch = 3.2212e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 122.2456, GNorm = 0.2278
Meta loss on this task batch = 3.1641e-01, Meta loss averaged over last 500 steps = 3.0511e-01, PNorm = 122.2541, GNorm = 0.2187
Meta loss on this task batch = 3.4093e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 122.2619, GNorm = 0.2844
Took 115.94448113441467 seconds to complete one epoch of meta training
Took 123.76456713676453 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499143
Epoch 379
Meta loss on this task batch = 2.8849e-01, Meta loss averaged over last 500 steps = 3.0515e-01, PNorm = 122.2690, GNorm = 0.2316
Meta loss on this task batch = 3.1087e-01, Meta loss averaged over last 500 steps = 3.0517e-01, PNorm = 122.2738, GNorm = 0.2626
Meta loss on this task batch = 3.0789e-01, Meta loss averaged over last 500 steps = 3.0510e-01, PNorm = 122.2792, GNorm = 0.2040
Meta loss on this task batch = 2.8855e-01, Meta loss averaged over last 500 steps = 3.0510e-01, PNorm = 122.2846, GNorm = 0.2326
Meta loss on this task batch = 2.7376e-01, Meta loss averaged over last 500 steps = 3.0499e-01, PNorm = 122.2900, GNorm = 0.2274
Meta loss on this task batch = 3.1946e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 122.2948, GNorm = 0.2069
Meta loss on this task batch = 2.7358e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 122.3003, GNorm = 0.2075
Meta loss on this task batch = 2.6787e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 122.3074, GNorm = 0.2370
Meta loss on this task batch = 3.3397e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 122.3155, GNorm = 0.2757
Meta loss on this task batch = 2.8212e-01, Meta loss averaged over last 500 steps = 3.0492e-01, PNorm = 122.3251, GNorm = 0.2194
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 3.0483e-01, PNorm = 122.3352, GNorm = 0.2253
Meta loss on this task batch = 2.8012e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 122.3468, GNorm = 0.2371
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 122.3578, GNorm = 0.2550
Meta loss on this task batch = 2.9422e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 122.3684, GNorm = 0.2307
Meta loss on this task batch = 3.7673e-01, Meta loss averaged over last 500 steps = 3.0493e-01, PNorm = 122.3760, GNorm = 0.3088
Meta loss on this task batch = 3.6096e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 122.3827, GNorm = 0.2407
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 122.3897, GNorm = 0.2305
Meta loss on this task batch = 3.5953e-01, Meta loss averaged over last 500 steps = 3.0519e-01, PNorm = 122.3960, GNorm = 0.2380
Meta loss on this task batch = 2.9472e-01, Meta loss averaged over last 500 steps = 3.0517e-01, PNorm = 122.4019, GNorm = 0.2833
Took 114.99169445037842 seconds to complete one epoch of meta training
Took 122.95810270309448 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477075
Epoch 380
Meta loss on this task batch = 3.2214e-01, Meta loss averaged over last 500 steps = 3.0528e-01, PNorm = 122.4080, GNorm = 0.2205
Meta loss on this task batch = 3.0643e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 122.4143, GNorm = 0.2064
Meta loss on this task batch = 2.9749e-01, Meta loss averaged over last 500 steps = 3.0515e-01, PNorm = 122.4216, GNorm = 0.2168
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 3.0527e-01, PNorm = 122.4287, GNorm = 0.2118
Meta loss on this task batch = 2.7826e-01, Meta loss averaged over last 500 steps = 3.0531e-01, PNorm = 122.4357, GNorm = 0.2239
Meta loss on this task batch = 3.2240e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 122.4428, GNorm = 0.2138
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 122.4504, GNorm = 0.2297
Meta loss on this task batch = 3.6762e-01, Meta loss averaged over last 500 steps = 3.0519e-01, PNorm = 122.4585, GNorm = 0.2796
Meta loss on this task batch = 2.9116e-01, Meta loss averaged over last 500 steps = 3.0535e-01, PNorm = 122.4668, GNorm = 0.1944
Meta loss on this task batch = 3.0324e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 122.4736, GNorm = 0.2176
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 122.4815, GNorm = 0.2127
Meta loss on this task batch = 2.8823e-01, Meta loss averaged over last 500 steps = 3.0512e-01, PNorm = 122.4894, GNorm = 0.2123
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 122.4982, GNorm = 0.2473
Meta loss on this task batch = 2.7680e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 122.5071, GNorm = 0.2232
Meta loss on this task batch = 3.1995e-01, Meta loss averaged over last 500 steps = 3.0518e-01, PNorm = 122.5162, GNorm = 0.2374
Meta loss on this task batch = 3.1470e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 122.5250, GNorm = 0.2296
Meta loss on this task batch = 2.9269e-01, Meta loss averaged over last 500 steps = 3.0524e-01, PNorm = 122.5335, GNorm = 0.2245
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 3.0503e-01, PNorm = 122.5434, GNorm = 0.2308
Meta loss on this task batch = 2.7141e-01, Meta loss averaged over last 500 steps = 3.0501e-01, PNorm = 122.5536, GNorm = 0.2610
Took 110.81030941009521 seconds to complete one epoch of meta training
Took 118.31180453300476 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491633
Epoch 381
Meta loss on this task batch = 3.4206e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 122.5630, GNorm = 0.2755
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 3.0505e-01, PNorm = 122.5708, GNorm = 0.2560
Meta loss on this task batch = 3.3681e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 122.5778, GNorm = 0.2340
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 122.5835, GNorm = 0.2354
Meta loss on this task batch = 3.6323e-01, Meta loss averaged over last 500 steps = 3.0521e-01, PNorm = 122.5899, GNorm = 0.2659
Meta loss on this task batch = 3.4952e-01, Meta loss averaged over last 500 steps = 3.0523e-01, PNorm = 122.5959, GNorm = 0.2257
Meta loss on this task batch = 3.3071e-01, Meta loss averaged over last 500 steps = 3.0532e-01, PNorm = 122.6009, GNorm = 0.2556
Meta loss on this task batch = 2.5802e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 122.6059, GNorm = 0.1823
Meta loss on this task batch = 2.4687e-01, Meta loss averaged over last 500 steps = 3.0501e-01, PNorm = 122.6126, GNorm = 0.2014
Meta loss on this task batch = 3.0480e-01, Meta loss averaged over last 500 steps = 3.0497e-01, PNorm = 122.6202, GNorm = 0.2200
Meta loss on this task batch = 2.7121e-01, Meta loss averaged over last 500 steps = 3.0487e-01, PNorm = 122.6288, GNorm = 0.1929
Meta loss on this task batch = 2.9051e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 122.6373, GNorm = 0.2216
Meta loss on this task batch = 3.5766e-01, Meta loss averaged over last 500 steps = 3.0515e-01, PNorm = 122.6455, GNorm = 0.2247
Meta loss on this task batch = 3.1675e-01, Meta loss averaged over last 500 steps = 3.0533e-01, PNorm = 122.6537, GNorm = 0.2083
Meta loss on this task batch = 2.9738e-01, Meta loss averaged over last 500 steps = 3.0540e-01, PNorm = 122.6613, GNorm = 0.1966
Meta loss on this task batch = 3.2156e-01, Meta loss averaged over last 500 steps = 3.0539e-01, PNorm = 122.6689, GNorm = 0.2146
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 3.0536e-01, PNorm = 122.6771, GNorm = 0.2047
Meta loss on this task batch = 2.7921e-01, Meta loss averaged over last 500 steps = 3.0531e-01, PNorm = 122.6860, GNorm = 0.2381
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 3.0526e-01, PNorm = 122.6959, GNorm = 0.2680
Took 114.9431164264679 seconds to complete one epoch of meta training
Took 121.42480182647705 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501058
Epoch 382
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 122.7053, GNorm = 0.2287
Meta loss on this task batch = 2.6414e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 122.7149, GNorm = 0.2154
Meta loss on this task batch = 3.5010e-01, Meta loss averaged over last 500 steps = 3.0516e-01, PNorm = 122.7225, GNorm = 0.2625
Meta loss on this task batch = 2.6100e-01, Meta loss averaged over last 500 steps = 3.0509e-01, PNorm = 122.7305, GNorm = 0.2399
Meta loss on this task batch = 3.2777e-01, Meta loss averaged over last 500 steps = 3.0508e-01, PNorm = 122.7383, GNorm = 0.2414
Meta loss on this task batch = 3.1588e-01, Meta loss averaged over last 500 steps = 3.0506e-01, PNorm = 122.7443, GNorm = 0.2646
Meta loss on this task batch = 2.6611e-01, Meta loss averaged over last 500 steps = 3.0499e-01, PNorm = 122.7507, GNorm = 0.2064
Meta loss on this task batch = 3.7242e-01, Meta loss averaged over last 500 steps = 3.0513e-01, PNorm = 122.7566, GNorm = 0.2484
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 122.7634, GNorm = 0.2233
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 3.0519e-01, PNorm = 122.7701, GNorm = 0.2186
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 3.0514e-01, PNorm = 122.7777, GNorm = 0.2141
Meta loss on this task batch = 2.7836e-01, Meta loss averaged over last 500 steps = 3.0511e-01, PNorm = 122.7857, GNorm = 0.1993
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 122.7934, GNorm = 0.1962
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 3.0492e-01, PNorm = 122.8007, GNorm = 0.2332
Meta loss on this task batch = 3.4596e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 122.8078, GNorm = 0.2846
Meta loss on this task batch = 3.3080e-01, Meta loss averaged over last 500 steps = 3.0495e-01, PNorm = 122.8154, GNorm = 0.2141
Meta loss on this task batch = 3.1826e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 122.8228, GNorm = 0.2221
Meta loss on this task batch = 3.3800e-01, Meta loss averaged over last 500 steps = 3.0505e-01, PNorm = 122.8287, GNorm = 0.2655
Meta loss on this task batch = 2.7869e-01, Meta loss averaged over last 500 steps = 3.0490e-01, PNorm = 122.8352, GNorm = 0.2747
Took 112.60463571548462 seconds to complete one epoch of meta training
Took 120.63223552703857 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488829
Epoch 383
Meta loss on this task batch = 2.6625e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 122.8416, GNorm = 0.1963
Meta loss on this task batch = 3.1358e-01, Meta loss averaged over last 500 steps = 3.0495e-01, PNorm = 122.8481, GNorm = 0.2251
Meta loss on this task batch = 3.2341e-01, Meta loss averaged over last 500 steps = 3.0494e-01, PNorm = 122.8536, GNorm = 0.2553
Meta loss on this task batch = 3.2878e-01, Meta loss averaged over last 500 steps = 3.0504e-01, PNorm = 122.8585, GNorm = 0.2367
Meta loss on this task batch = 2.4794e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 122.8636, GNorm = 0.2084
Meta loss on this task batch = 2.4550e-01, Meta loss averaged over last 500 steps = 3.0475e-01, PNorm = 122.8680, GNorm = 0.2223
Meta loss on this task batch = 3.5160e-01, Meta loss averaged over last 500 steps = 3.0489e-01, PNorm = 122.8737, GNorm = 0.2315
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 122.8800, GNorm = 0.2229
Meta loss on this task batch = 2.9548e-01, Meta loss averaged over last 500 steps = 3.0481e-01, PNorm = 122.8872, GNorm = 0.2204
Meta loss on this task batch = 2.4804e-01, Meta loss averaged over last 500 steps = 3.0478e-01, PNorm = 122.8948, GNorm = 0.1854
Meta loss on this task batch = 3.6236e-01, Meta loss averaged over last 500 steps = 3.0485e-01, PNorm = 122.9029, GNorm = 0.2484
Meta loss on this task batch = 3.0704e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 122.9109, GNorm = 0.1994
Meta loss on this task batch = 3.0679e-01, Meta loss averaged over last 500 steps = 3.0486e-01, PNorm = 122.9188, GNorm = 0.2177
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 3.0487e-01, PNorm = 122.9273, GNorm = 0.2219
Meta loss on this task batch = 2.9784e-01, Meta loss averaged over last 500 steps = 3.0490e-01, PNorm = 122.9355, GNorm = 0.2030
Meta loss on this task batch = 3.1209e-01, Meta loss averaged over last 500 steps = 3.0485e-01, PNorm = 122.9440, GNorm = 0.2402
Meta loss on this task batch = 3.4847e-01, Meta loss averaged over last 500 steps = 3.0495e-01, PNorm = 122.9514, GNorm = 0.2684
Meta loss on this task batch = 2.9296e-01, Meta loss averaged over last 500 steps = 3.0497e-01, PNorm = 122.9585, GNorm = 0.2298
Meta loss on this task batch = 3.3214e-01, Meta loss averaged over last 500 steps = 3.0497e-01, PNorm = 122.9661, GNorm = 0.2822
Took 115.09380912780762 seconds to complete one epoch of meta training
Took 122.77384924888611 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502677
Epoch 384
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 122.9736, GNorm = 0.2296
Meta loss on this task batch = 2.8674e-01, Meta loss averaged over last 500 steps = 3.0491e-01, PNorm = 122.9812, GNorm = 0.2145
Meta loss on this task batch = 2.9515e-01, Meta loss averaged over last 500 steps = 3.0482e-01, PNorm = 122.9889, GNorm = 0.1958
Meta loss on this task batch = 3.0197e-01, Meta loss averaged over last 500 steps = 3.0475e-01, PNorm = 122.9970, GNorm = 0.2137
Meta loss on this task batch = 2.9054e-01, Meta loss averaged over last 500 steps = 3.0464e-01, PNorm = 123.0059, GNorm = 0.1954
Meta loss on this task batch = 3.1583e-01, Meta loss averaged over last 500 steps = 3.0471e-01, PNorm = 123.0143, GNorm = 0.2040
Meta loss on this task batch = 3.7529e-01, Meta loss averaged over last 500 steps = 3.0471e-01, PNorm = 123.0221, GNorm = 0.2333
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 3.0463e-01, PNorm = 123.0303, GNorm = 0.2023
Meta loss on this task batch = 2.5580e-01, Meta loss averaged over last 500 steps = 3.0448e-01, PNorm = 123.0399, GNorm = 0.2135
Meta loss on this task batch = 3.3943e-01, Meta loss averaged over last 500 steps = 3.0462e-01, PNorm = 123.0488, GNorm = 0.2736
Meta loss on this task batch = 2.8342e-01, Meta loss averaged over last 500 steps = 3.0456e-01, PNorm = 123.0571, GNorm = 0.2561
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 3.0449e-01, PNorm = 123.0645, GNorm = 0.2061
Meta loss on this task batch = 2.9682e-01, Meta loss averaged over last 500 steps = 3.0462e-01, PNorm = 123.0714, GNorm = 0.2369
Meta loss on this task batch = 3.1472e-01, Meta loss averaged over last 500 steps = 3.0469e-01, PNorm = 123.0765, GNorm = 0.2448
Meta loss on this task batch = 3.6108e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 123.0800, GNorm = 0.2508
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 3.0482e-01, PNorm = 123.0831, GNorm = 0.2539
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 123.0870, GNorm = 0.1952
Meta loss on this task batch = 2.7771e-01, Meta loss averaged over last 500 steps = 3.0470e-01, PNorm = 123.0922, GNorm = 0.2291
Meta loss on this task batch = 2.7510e-01, Meta loss averaged over last 500 steps = 3.0465e-01, PNorm = 123.0981, GNorm = 0.2375
Took 116.92133855819702 seconds to complete one epoch of meta training
Took 124.94575214385986 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497786
Epoch 385
Meta loss on this task batch = 3.5162e-01, Meta loss averaged over last 500 steps = 3.0466e-01, PNorm = 123.1040, GNorm = 0.2253
Meta loss on this task batch = 3.3550e-01, Meta loss averaged over last 500 steps = 3.0477e-01, PNorm = 123.1093, GNorm = 0.2522
Meta loss on this task batch = 2.5695e-01, Meta loss averaged over last 500 steps = 3.0457e-01, PNorm = 123.1169, GNorm = 0.2139
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 3.0448e-01, PNorm = 123.1243, GNorm = 0.2073
Meta loss on this task batch = 2.4921e-01, Meta loss averaged over last 500 steps = 3.0431e-01, PNorm = 123.1326, GNorm = 0.1967
Meta loss on this task batch = 2.7754e-01, Meta loss averaged over last 500 steps = 3.0441e-01, PNorm = 123.1409, GNorm = 0.2095
Meta loss on this task batch = 2.6421e-01, Meta loss averaged over last 500 steps = 3.0433e-01, PNorm = 123.1493, GNorm = 0.2287
Meta loss on this task batch = 2.8500e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 123.1578, GNorm = 0.1888
Meta loss on this task batch = 3.1927e-01, Meta loss averaged over last 500 steps = 3.0424e-01, PNorm = 123.1645, GNorm = 0.2580
Meta loss on this task batch = 3.6957e-01, Meta loss averaged over last 500 steps = 3.0440e-01, PNorm = 123.1719, GNorm = 0.2957
Meta loss on this task batch = 3.0134e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 123.1786, GNorm = 0.2617
Meta loss on this task batch = 3.4863e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 123.1841, GNorm = 0.2711
Meta loss on this task batch = 3.3707e-01, Meta loss averaged over last 500 steps = 3.0447e-01, PNorm = 123.1887, GNorm = 0.2480
Meta loss on this task batch = 2.5966e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 123.1946, GNorm = 0.2137
Meta loss on this task batch = 3.0104e-01, Meta loss averaged over last 500 steps = 3.0438e-01, PNorm = 123.2011, GNorm = 0.2413
Meta loss on this task batch = 3.0451e-01, Meta loss averaged over last 500 steps = 3.0442e-01, PNorm = 123.2073, GNorm = 0.2233
Meta loss on this task batch = 3.6833e-01, Meta loss averaged over last 500 steps = 3.0457e-01, PNorm = 123.2115, GNorm = 0.3244
Meta loss on this task batch = 2.6836e-01, Meta loss averaged over last 500 steps = 3.0450e-01, PNorm = 123.2159, GNorm = 0.2250
Meta loss on this task batch = 3.2232e-01, Meta loss averaged over last 500 steps = 3.0456e-01, PNorm = 123.2204, GNorm = 0.2657
Took 112.11008405685425 seconds to complete one epoch of meta training
Took 119.6124861240387 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502702
Epoch 386
Meta loss on this task batch = 3.0342e-01, Meta loss averaged over last 500 steps = 3.0455e-01, PNorm = 123.2258, GNorm = 0.2250
Meta loss on this task batch = 3.6952e-01, Meta loss averaged over last 500 steps = 3.0470e-01, PNorm = 123.2310, GNorm = 0.2409
Meta loss on this task batch = 3.1822e-01, Meta loss averaged over last 500 steps = 3.0478e-01, PNorm = 123.2372, GNorm = 0.2399
Meta loss on this task batch = 2.9263e-01, Meta loss averaged over last 500 steps = 3.0473e-01, PNorm = 123.2450, GNorm = 0.1999
Meta loss on this task batch = 2.4521e-01, Meta loss averaged over last 500 steps = 3.0459e-01, PNorm = 123.2534, GNorm = 0.1959
Meta loss on this task batch = 3.3678e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 123.2599, GNorm = 0.2659
Meta loss on this task batch = 3.3021e-01, Meta loss averaged over last 500 steps = 3.0488e-01, PNorm = 123.2665, GNorm = 0.2059
Meta loss on this task batch = 3.2781e-01, Meta loss averaged over last 500 steps = 3.0498e-01, PNorm = 123.2730, GNorm = 0.2702
Meta loss on this task batch = 3.1098e-01, Meta loss averaged over last 500 steps = 3.0496e-01, PNorm = 123.2786, GNorm = 0.2458
Meta loss on this task batch = 3.2837e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 123.2833, GNorm = 0.2339
Meta loss on this task batch = 3.0285e-01, Meta loss averaged over last 500 steps = 3.0500e-01, PNorm = 123.2886, GNorm = 0.2420
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 3.0476e-01, PNorm = 123.2951, GNorm = 0.1905
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 3.0484e-01, PNorm = 123.3022, GNorm = 0.2372
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 3.0470e-01, PNorm = 123.3098, GNorm = 0.2042
Meta loss on this task batch = 3.9280e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 123.3181, GNorm = 0.3033
Meta loss on this task batch = 2.7938e-01, Meta loss averaged over last 500 steps = 3.0480e-01, PNorm = 123.3262, GNorm = 0.2131
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 3.0479e-01, PNorm = 123.3348, GNorm = 0.1967
Meta loss on this task batch = 2.6855e-01, Meta loss averaged over last 500 steps = 3.0461e-01, PNorm = 123.3433, GNorm = 0.2045
Meta loss on this task batch = 3.1184e-01, Meta loss averaged over last 500 steps = 3.0461e-01, PNorm = 123.3515, GNorm = 0.2582
Took 115.4565908908844 seconds to complete one epoch of meta training
Took 123.07925868034363 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503828
Epoch 387
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 3.0462e-01, PNorm = 123.3595, GNorm = 0.2125
Meta loss on this task batch = 2.8315e-01, Meta loss averaged over last 500 steps = 3.0447e-01, PNorm = 123.3669, GNorm = 0.2072
Meta loss on this task batch = 3.2065e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 123.3738, GNorm = 0.2352
Meta loss on this task batch = 3.3738e-01, Meta loss averaged over last 500 steps = 3.0455e-01, PNorm = 123.3806, GNorm = 0.2153
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 123.3870, GNorm = 0.2208
Meta loss on this task batch = 2.9380e-01, Meta loss averaged over last 500 steps = 3.0442e-01, PNorm = 123.3926, GNorm = 0.2316
Meta loss on this task batch = 3.0614e-01, Meta loss averaged over last 500 steps = 3.0444e-01, PNorm = 123.3979, GNorm = 0.2088
Meta loss on this task batch = 3.1750e-01, Meta loss averaged over last 500 steps = 3.0447e-01, PNorm = 123.4041, GNorm = 0.2131
Meta loss on this task batch = 3.3336e-01, Meta loss averaged over last 500 steps = 3.0458e-01, PNorm = 123.4112, GNorm = 0.2052
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 3.0448e-01, PNorm = 123.4192, GNorm = 0.2086
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 123.4273, GNorm = 0.2266
Meta loss on this task batch = 3.1675e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 123.4360, GNorm = 0.2242
Meta loss on this task batch = 2.6989e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 123.4447, GNorm = 0.1945
Meta loss on this task batch = 3.4278e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 123.4521, GNorm = 0.2494
Meta loss on this task batch = 2.5155e-01, Meta loss averaged over last 500 steps = 3.0452e-01, PNorm = 123.4595, GNorm = 0.2053
Meta loss on this task batch = 3.2112e-01, Meta loss averaged over last 500 steps = 3.0460e-01, PNorm = 123.4657, GNorm = 0.2332
Meta loss on this task batch = 2.6886e-01, Meta loss averaged over last 500 steps = 3.0454e-01, PNorm = 123.4724, GNorm = 0.1984
Meta loss on this task batch = 3.0202e-01, Meta loss averaged over last 500 steps = 3.0451e-01, PNorm = 123.4804, GNorm = 0.2532
Meta loss on this task batch = 3.2409e-01, Meta loss averaged over last 500 steps = 3.0457e-01, PNorm = 123.4878, GNorm = 0.2872
Took 115.29958605766296 seconds to complete one epoch of meta training
Took 122.67168712615967 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473153
Epoch 388
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 3.0455e-01, PNorm = 123.4953, GNorm = 0.2429
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 3.0442e-01, PNorm = 123.5029, GNorm = 0.2076
Meta loss on this task batch = 2.6853e-01, Meta loss averaged over last 500 steps = 3.0438e-01, PNorm = 123.5105, GNorm = 0.2369
Meta loss on this task batch = 2.8384e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 123.5183, GNorm = 0.2148
Meta loss on this task batch = 3.2342e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 123.5259, GNorm = 0.2748
Meta loss on this task batch = 3.5760e-01, Meta loss averaged over last 500 steps = 3.0426e-01, PNorm = 123.5314, GNorm = 0.3037
Meta loss on this task batch = 2.7502e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 123.5366, GNorm = 0.2137
Meta loss on this task batch = 3.2066e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 123.5412, GNorm = 0.2185
Meta loss on this task batch = 3.1129e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 123.5464, GNorm = 0.2083
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 3.0415e-01, PNorm = 123.5519, GNorm = 0.2341
Meta loss on this task batch = 3.1376e-01, Meta loss averaged over last 500 steps = 3.0416e-01, PNorm = 123.5576, GNorm = 0.2090
Meta loss on this task batch = 2.6745e-01, Meta loss averaged over last 500 steps = 3.0414e-01, PNorm = 123.5647, GNorm = 0.2251
Meta loss on this task batch = 3.6259e-01, Meta loss averaged over last 500 steps = 3.0424e-01, PNorm = 123.5716, GNorm = 0.2163
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 3.0431e-01, PNorm = 123.5775, GNorm = 0.2180
Meta loss on this task batch = 3.2246e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 123.5835, GNorm = 0.2103
Meta loss on this task batch = 2.8035e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 123.5900, GNorm = 0.1939
Meta loss on this task batch = 2.6794e-01, Meta loss averaged over last 500 steps = 3.0416e-01, PNorm = 123.5976, GNorm = 0.2076
Meta loss on this task batch = 2.5748e-01, Meta loss averaged over last 500 steps = 3.0395e-01, PNorm = 123.6049, GNorm = 0.1864
Meta loss on this task batch = 3.4222e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 123.6119, GNorm = 0.2283
Took 114.05689668655396 seconds to complete one epoch of meta training
Took 121.57826685905457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503473
Epoch 389
Meta loss on this task batch = 3.4534e-01, Meta loss averaged over last 500 steps = 3.0404e-01, PNorm = 123.6191, GNorm = 0.2168
Meta loss on this task batch = 3.2596e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 123.6262, GNorm = 0.1944
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 123.6325, GNorm = 0.2001
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 123.6393, GNorm = 0.2004
Meta loss on this task batch = 2.5442e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 123.6473, GNorm = 0.2119
Meta loss on this task batch = 2.9702e-01, Meta loss averaged over last 500 steps = 3.0386e-01, PNorm = 123.6542, GNorm = 0.2209
Meta loss on this task batch = 2.7553e-01, Meta loss averaged over last 500 steps = 3.0380e-01, PNorm = 123.6611, GNorm = 0.2013
Meta loss on this task batch = 3.3695e-01, Meta loss averaged over last 500 steps = 3.0387e-01, PNorm = 123.6677, GNorm = 0.2460
Meta loss on this task batch = 2.7742e-01, Meta loss averaged over last 500 steps = 3.0386e-01, PNorm = 123.6743, GNorm = 0.2308
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 3.0386e-01, PNorm = 123.6804, GNorm = 0.2461
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 123.6861, GNorm = 0.2196
Meta loss on this task batch = 3.6441e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 123.6894, GNorm = 0.3040
Meta loss on this task batch = 3.1810e-01, Meta loss averaged over last 500 steps = 3.0386e-01, PNorm = 123.6928, GNorm = 0.2542
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 3.0377e-01, PNorm = 123.6971, GNorm = 0.2360
Meta loss on this task batch = 3.0160e-01, Meta loss averaged over last 500 steps = 3.0385e-01, PNorm = 123.7019, GNorm = 0.2330
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 3.0382e-01, PNorm = 123.7068, GNorm = 0.2315
Meta loss on this task batch = 3.0784e-01, Meta loss averaged over last 500 steps = 3.0378e-01, PNorm = 123.7120, GNorm = 0.2254
Meta loss on this task batch = 3.5989e-01, Meta loss averaged over last 500 steps = 3.0388e-01, PNorm = 123.7167, GNorm = 0.2461
Meta loss on this task batch = 3.4150e-01, Meta loss averaged over last 500 steps = 3.0408e-01, PNorm = 123.7211, GNorm = 0.2823
Took 113.11153817176819 seconds to complete one epoch of meta training
Took 121.30284571647644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505856
Epoch 390
Meta loss on this task batch = 3.0320e-01, Meta loss averaged over last 500 steps = 3.0408e-01, PNorm = 123.7266, GNorm = 0.2151
Meta loss on this task batch = 3.1746e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 123.7327, GNorm = 0.2198
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 3.0415e-01, PNorm = 123.7398, GNorm = 0.2504
Meta loss on this task batch = 2.8095e-01, Meta loss averaged over last 500 steps = 3.0409e-01, PNorm = 123.7479, GNorm = 0.2079
Meta loss on this task batch = 2.8801e-01, Meta loss averaged over last 500 steps = 3.0402e-01, PNorm = 123.7565, GNorm = 0.2321
Meta loss on this task batch = 3.0876e-01, Meta loss averaged over last 500 steps = 3.0413e-01, PNorm = 123.7663, GNorm = 0.2251
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 123.7762, GNorm = 0.2110
Meta loss on this task batch = 2.9395e-01, Meta loss averaged over last 500 steps = 3.0412e-01, PNorm = 123.7859, GNorm = 0.2049
Meta loss on this task batch = 2.9632e-01, Meta loss averaged over last 500 steps = 3.0398e-01, PNorm = 123.7942, GNorm = 0.2421
Meta loss on this task batch = 2.4286e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 123.8009, GNorm = 0.1934
Meta loss on this task batch = 3.5111e-01, Meta loss averaged over last 500 steps = 3.0407e-01, PNorm = 123.8068, GNorm = 0.2583
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 3.0405e-01, PNorm = 123.8123, GNorm = 0.2556
Meta loss on this task batch = 3.7185e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 123.8150, GNorm = 0.3026
Meta loss on this task batch = 2.8485e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 123.8181, GNorm = 0.1894
Meta loss on this task batch = 2.9662e-01, Meta loss averaged over last 500 steps = 3.0419e-01, PNorm = 123.8205, GNorm = 0.2478
Meta loss on this task batch = 2.8365e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 123.8233, GNorm = 0.2101
Meta loss on this task batch = 3.1995e-01, Meta loss averaged over last 500 steps = 3.0418e-01, PNorm = 123.8268, GNorm = 0.2174
Meta loss on this task batch = 3.6714e-01, Meta loss averaged over last 500 steps = 3.0433e-01, PNorm = 123.8317, GNorm = 0.2469
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 3.0424e-01, PNorm = 123.8385, GNorm = 0.2327
Took 111.8800117969513 seconds to complete one epoch of meta training
Took 119.96783542633057 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473899
Epoch 391
Meta loss on this task batch = 3.2233e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 123.8457, GNorm = 0.2236
Meta loss on this task batch = 2.7228e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 123.8536, GNorm = 0.2151
Meta loss on this task batch = 3.1390e-01, Meta loss averaged over last 500 steps = 3.0423e-01, PNorm = 123.8621, GNorm = 0.2153
Meta loss on this task batch = 3.6237e-01, Meta loss averaged over last 500 steps = 3.0441e-01, PNorm = 123.8714, GNorm = 0.2412
Meta loss on this task batch = 2.7365e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 123.8819, GNorm = 0.2214
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 3.0419e-01, PNorm = 123.8927, GNorm = 0.1920
Meta loss on this task batch = 2.6946e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 123.9045, GNorm = 0.1973
Meta loss on this task batch = 2.6861e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 123.9151, GNorm = 0.2309
Meta loss on this task batch = 3.3363e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 123.9228, GNorm = 0.2756
Meta loss on this task batch = 2.5313e-01, Meta loss averaged over last 500 steps = 3.0402e-01, PNorm = 123.9302, GNorm = 0.2466
Meta loss on this task batch = 3.3320e-01, Meta loss averaged over last 500 steps = 3.0406e-01, PNorm = 123.9368, GNorm = 0.2969
Meta loss on this task batch = 3.4084e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 123.9415, GNorm = 0.2824
Meta loss on this task batch = 2.9249e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 123.9453, GNorm = 0.2160
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 3.0421e-01, PNorm = 123.9482, GNorm = 0.2476
Meta loss on this task batch = 2.3263e-01, Meta loss averaged over last 500 steps = 3.0392e-01, PNorm = 123.9524, GNorm = 0.1970
Meta loss on this task batch = 3.2458e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 123.9570, GNorm = 0.2236
Meta loss on this task batch = 3.5032e-01, Meta loss averaged over last 500 steps = 3.0403e-01, PNorm = 123.9619, GNorm = 0.2419
Meta loss on this task batch = 3.0099e-01, Meta loss averaged over last 500 steps = 3.0406e-01, PNorm = 123.9673, GNorm = 0.2398
Meta loss on this task batch = 3.1230e-01, Meta loss averaged over last 500 steps = 3.0393e-01, PNorm = 123.9734, GNorm = 0.2748
Took 113.4799964427948 seconds to complete one epoch of meta training
Took 121.16492462158203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486666
Epoch 392
Meta loss on this task batch = 3.4340e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 123.9795, GNorm = 0.2598
Meta loss on this task batch = 3.0576e-01, Meta loss averaged over last 500 steps = 3.0406e-01, PNorm = 123.9861, GNorm = 0.2233
Meta loss on this task batch = 3.0608e-01, Meta loss averaged over last 500 steps = 3.0405e-01, PNorm = 123.9930, GNorm = 0.2180
Meta loss on this task batch = 2.9666e-01, Meta loss averaged over last 500 steps = 3.0405e-01, PNorm = 123.9989, GNorm = 0.2505
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 3.0403e-01, PNorm = 124.0043, GNorm = 0.2123
Meta loss on this task batch = 2.4133e-01, Meta loss averaged over last 500 steps = 3.0399e-01, PNorm = 124.0109, GNorm = 0.2018
Meta loss on this task batch = 3.7704e-01, Meta loss averaged over last 500 steps = 3.0416e-01, PNorm = 124.0183, GNorm = 0.2726
Meta loss on this task batch = 3.6156e-01, Meta loss averaged over last 500 steps = 3.0430e-01, PNorm = 124.0255, GNorm = 0.2352
Meta loss on this task batch = 3.3964e-01, Meta loss averaged over last 500 steps = 3.0433e-01, PNorm = 124.0318, GNorm = 0.2667
Meta loss on this task batch = 3.3139e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 124.0379, GNorm = 0.2469
Meta loss on this task batch = 3.1597e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 124.0445, GNorm = 0.2079
Meta loss on this task batch = 2.9927e-01, Meta loss averaged over last 500 steps = 3.0455e-01, PNorm = 124.0516, GNorm = 0.2172
Meta loss on this task batch = 2.8795e-01, Meta loss averaged over last 500 steps = 3.0441e-01, PNorm = 124.0584, GNorm = 0.2180
Meta loss on this task batch = 2.7720e-01, Meta loss averaged over last 500 steps = 3.0447e-01, PNorm = 124.0650, GNorm = 0.2195
Meta loss on this task batch = 3.0395e-01, Meta loss averaged over last 500 steps = 3.0446e-01, PNorm = 124.0713, GNorm = 0.2337
Meta loss on this task batch = 3.4267e-01, Meta loss averaged over last 500 steps = 3.0452e-01, PNorm = 124.0778, GNorm = 0.2194
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 3.0458e-01, PNorm = 124.0843, GNorm = 0.2429
Meta loss on this task batch = 2.8359e-01, Meta loss averaged over last 500 steps = 3.0441e-01, PNorm = 124.0922, GNorm = 0.1871
Meta loss on this task batch = 2.9306e-01, Meta loss averaged over last 500 steps = 3.0437e-01, PNorm = 124.1017, GNorm = 0.2546
Took 116.98643636703491 seconds to complete one epoch of meta training
Took 124.61171579360962 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504638
Epoch 393
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 3.0425e-01, PNorm = 124.1117, GNorm = 0.2075
Meta loss on this task batch = 3.1179e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 124.1217, GNorm = 0.2206
Meta loss on this task batch = 3.1610e-01, Meta loss averaged over last 500 steps = 3.0432e-01, PNorm = 124.1311, GNorm = 0.1905
Meta loss on this task batch = 3.1336e-01, Meta loss averaged over last 500 steps = 3.0433e-01, PNorm = 124.1405, GNorm = 0.2154
Meta loss on this task batch = 3.1796e-01, Meta loss averaged over last 500 steps = 3.0434e-01, PNorm = 124.1487, GNorm = 0.2100
Meta loss on this task batch = 2.7674e-01, Meta loss averaged over last 500 steps = 3.0433e-01, PNorm = 124.1570, GNorm = 0.2152
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 3.0418e-01, PNorm = 124.1654, GNorm = 0.2195
Meta loss on this task batch = 3.1563e-01, Meta loss averaged over last 500 steps = 3.0422e-01, PNorm = 124.1742, GNorm = 0.2630
Meta loss on this task batch = 3.0672e-01, Meta loss averaged over last 500 steps = 3.0434e-01, PNorm = 124.1829, GNorm = 0.2244
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 3.0429e-01, PNorm = 124.1904, GNorm = 0.2292
Meta loss on this task batch = 3.0554e-01, Meta loss averaged over last 500 steps = 3.0419e-01, PNorm = 124.1978, GNorm = 0.2685
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 3.0420e-01, PNorm = 124.2044, GNorm = 0.2482
Meta loss on this task batch = 2.8424e-01, Meta loss averaged over last 500 steps = 3.0417e-01, PNorm = 124.2106, GNorm = 0.2304
Meta loss on this task batch = 2.1775e-01, Meta loss averaged over last 500 steps = 3.0404e-01, PNorm = 124.2185, GNorm = 0.2198
Meta loss on this task batch = 3.0897e-01, Meta loss averaged over last 500 steps = 3.0410e-01, PNorm = 124.2260, GNorm = 0.2235
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 3.0397e-01, PNorm = 124.2332, GNorm = 0.2588
Meta loss on this task batch = 3.5986e-01, Meta loss averaged over last 500 steps = 3.0400e-01, PNorm = 124.2408, GNorm = 0.2749
Meta loss on this task batch = 2.8836e-01, Meta loss averaged over last 500 steps = 3.0389e-01, PNorm = 124.2491, GNorm = 0.2247
Meta loss on this task batch = 2.3214e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 124.2569, GNorm = 0.2605
Took 116.69697952270508 seconds to complete one epoch of meta training
Took 124.11371421813965 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486233
Epoch 394
Meta loss on this task batch = 3.3689e-01, Meta loss averaged over last 500 steps = 3.0383e-01, PNorm = 124.2639, GNorm = 0.2411
Meta loss on this task batch = 2.5220e-01, Meta loss averaged over last 500 steps = 3.0369e-01, PNorm = 124.2705, GNorm = 0.2058
Meta loss on this task batch = 2.8580e-01, Meta loss averaged over last 500 steps = 3.0364e-01, PNorm = 124.2770, GNorm = 0.2335
Meta loss on this task batch = 2.7506e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 124.2840, GNorm = 0.2238
Meta loss on this task batch = 2.8849e-01, Meta loss averaged over last 500 steps = 3.0353e-01, PNorm = 124.2917, GNorm = 0.2236
Meta loss on this task batch = 3.8663e-01, Meta loss averaged over last 500 steps = 3.0374e-01, PNorm = 124.2981, GNorm = 0.2587
Meta loss on this task batch = 3.0946e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 124.3062, GNorm = 0.2728
Meta loss on this task batch = 2.3714e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 124.3147, GNorm = 0.1886
Meta loss on this task batch = 2.9359e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 124.3237, GNorm = 0.1969
Meta loss on this task batch = 3.0620e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 124.3321, GNorm = 0.2739
Meta loss on this task batch = 3.5077e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 124.3398, GNorm = 0.2795
Meta loss on this task batch = 2.1870e-01, Meta loss averaged over last 500 steps = 3.0334e-01, PNorm = 124.3471, GNorm = 0.1801
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 124.3541, GNorm = 0.2228
Meta loss on this task batch = 3.3440e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 124.3608, GNorm = 0.2687
Meta loss on this task batch = 2.5227e-01, Meta loss averaged over last 500 steps = 3.0329e-01, PNorm = 124.3680, GNorm = 0.2059
Meta loss on this task batch = 2.8401e-01, Meta loss averaged over last 500 steps = 3.0321e-01, PNorm = 124.3758, GNorm = 0.2658
Meta loss on this task batch = 3.0443e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 124.3828, GNorm = 0.2378
Meta loss on this task batch = 3.4525e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 124.3886, GNorm = 0.2846
Meta loss on this task batch = 2.9932e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 124.3934, GNorm = 0.2810
Took 113.51357984542847 seconds to complete one epoch of meta training
Took 120.40880227088928 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494135
Epoch 395
Meta loss on this task batch = 2.3897e-01, Meta loss averaged over last 500 steps = 3.0334e-01, PNorm = 124.3978, GNorm = 0.2382
Meta loss on this task batch = 2.5955e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 124.4027, GNorm = 0.1962
Meta loss on this task batch = 3.2049e-01, Meta loss averaged over last 500 steps = 3.0334e-01, PNorm = 124.4072, GNorm = 0.2287
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 124.4133, GNorm = 0.1837
Meta loss on this task batch = 3.0585e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 124.4181, GNorm = 0.2421
Meta loss on this task batch = 3.1944e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 124.4226, GNorm = 0.2179
Meta loss on this task batch = 2.8418e-01, Meta loss averaged over last 500 steps = 3.0332e-01, PNorm = 124.4281, GNorm = 0.2189
Meta loss on this task batch = 3.4971e-01, Meta loss averaged over last 500 steps = 3.0335e-01, PNorm = 124.4324, GNorm = 0.2361
Meta loss on this task batch = 3.2201e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 124.4382, GNorm = 0.2285
Meta loss on this task batch = 2.8990e-01, Meta loss averaged over last 500 steps = 3.0333e-01, PNorm = 124.4445, GNorm = 0.2123
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 124.4506, GNorm = 0.2217
Meta loss on this task batch = 3.3651e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 124.4563, GNorm = 0.2399
Meta loss on this task batch = 3.2584e-01, Meta loss averaged over last 500 steps = 3.0357e-01, PNorm = 124.4625, GNorm = 0.2078
Meta loss on this task batch = 3.0763e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 124.4698, GNorm = 0.2319
Meta loss on this task batch = 2.8471e-01, Meta loss averaged over last 500 steps = 3.0358e-01, PNorm = 124.4777, GNorm = 0.2257
Meta loss on this task batch = 3.5020e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 124.4860, GNorm = 0.2216
Meta loss on this task batch = 2.9598e-01, Meta loss averaged over last 500 steps = 3.0373e-01, PNorm = 124.4931, GNorm = 0.2410
Meta loss on this task batch = 2.9261e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 124.5007, GNorm = 0.2145
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 3.0362e-01, PNorm = 124.5079, GNorm = 0.2518
Took 116.32628154754639 seconds to complete one epoch of meta training
Took 124.2985429763794 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490847
Epoch 396
Meta loss on this task batch = 3.3069e-01, Meta loss averaged over last 500 steps = 3.0365e-01, PNorm = 124.5150, GNorm = 0.2193
Meta loss on this task batch = 3.0151e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 124.5204, GNorm = 0.2510
Meta loss on this task batch = 3.2170e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 124.5244, GNorm = 0.2479
Meta loss on this task batch = 3.1848e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 124.5288, GNorm = 0.2257
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 3.0347e-01, PNorm = 124.5350, GNorm = 0.2498
Meta loss on this task batch = 3.1396e-01, Meta loss averaged over last 500 steps = 3.0355e-01, PNorm = 124.5412, GNorm = 0.2391
Meta loss on this task batch = 2.9525e-01, Meta loss averaged over last 500 steps = 3.0350e-01, PNorm = 124.5476, GNorm = 0.2089
Meta loss on this task batch = 3.6133e-01, Meta loss averaged over last 500 steps = 3.0370e-01, PNorm = 124.5544, GNorm = 0.2613
Meta loss on this task batch = 3.1851e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 124.5608, GNorm = 0.2576
Meta loss on this task batch = 2.6590e-01, Meta loss averaged over last 500 steps = 3.0359e-01, PNorm = 124.5685, GNorm = 0.2058
Meta loss on this task batch = 2.4935e-01, Meta loss averaged over last 500 steps = 3.0336e-01, PNorm = 124.5761, GNorm = 0.2127
Meta loss on this task batch = 2.6519e-01, Meta loss averaged over last 500 steps = 3.0332e-01, PNorm = 124.5840, GNorm = 0.1978
Meta loss on this task batch = 2.9660e-01, Meta loss averaged over last 500 steps = 3.0330e-01, PNorm = 124.5914, GNorm = 0.2032
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 124.5985, GNorm = 0.2241
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 124.6048, GNorm = 0.2474
Meta loss on this task batch = 2.9905e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 124.6114, GNorm = 0.2092
Meta loss on this task batch = 3.9392e-01, Meta loss averaged over last 500 steps = 3.0356e-01, PNorm = 124.6173, GNorm = 0.2872
Meta loss on this task batch = 2.6682e-01, Meta loss averaged over last 500 steps = 3.0348e-01, PNorm = 124.6239, GNorm = 0.2549
Meta loss on this task batch = 2.8060e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 124.6295, GNorm = 0.2637
Took 115.63872385025024 seconds to complete one epoch of meta training
Took 123.3363938331604 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486593
Epoch 397
Meta loss on this task batch = 3.3219e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 124.6348, GNorm = 0.2529
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 124.6404, GNorm = 0.1919
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 124.6473, GNorm = 0.2188
Meta loss on this task batch = 2.6306e-01, Meta loss averaged over last 500 steps = 3.0342e-01, PNorm = 124.6540, GNorm = 0.1801
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 3.0331e-01, PNorm = 124.6608, GNorm = 0.2227
Meta loss on this task batch = 3.1025e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 124.6678, GNorm = 0.2234
Meta loss on this task batch = 2.5589e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 124.6759, GNorm = 0.2283
Meta loss on this task batch = 3.5921e-01, Meta loss averaged over last 500 steps = 3.0339e-01, PNorm = 124.6842, GNorm = 0.2552
Meta loss on this task batch = 3.1379e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 124.6921, GNorm = 0.2208
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 3.0352e-01, PNorm = 124.6989, GNorm = 0.2063
Meta loss on this task batch = 3.7834e-01, Meta loss averaged over last 500 steps = 3.0368e-01, PNorm = 124.7052, GNorm = 0.2338
Meta loss on this task batch = 2.5658e-01, Meta loss averaged over last 500 steps = 3.0341e-01, PNorm = 124.7116, GNorm = 0.1999
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 124.7181, GNorm = 0.2422
Meta loss on this task batch = 3.5043e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 124.7234, GNorm = 0.2516
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 3.0345e-01, PNorm = 124.7292, GNorm = 0.2033
Meta loss on this task batch = 3.2837e-01, Meta loss averaged over last 500 steps = 3.0354e-01, PNorm = 124.7354, GNorm = 0.2265
Meta loss on this task batch = 2.8431e-01, Meta loss averaged over last 500 steps = 3.0346e-01, PNorm = 124.7422, GNorm = 0.2103
Meta loss on this task batch = 3.1786e-01, Meta loss averaged over last 500 steps = 3.0340e-01, PNorm = 124.7488, GNorm = 0.2162
Meta loss on this task batch = 2.3683e-01, Meta loss averaged over last 500 steps = 3.0326e-01, PNorm = 124.7551, GNorm = 0.2479
Took 112.13950228691101 seconds to complete one epoch of meta training
Took 120.08520555496216 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491905
Epoch 398
Meta loss on this task batch = 2.7773e-01, Meta loss averaged over last 500 steps = 3.0322e-01, PNorm = 124.7619, GNorm = 0.2106
Meta loss on this task batch = 3.3869e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 124.7691, GNorm = 0.2222
Meta loss on this task batch = 3.5592e-01, Meta loss averaged over last 500 steps = 3.0337e-01, PNorm = 124.7757, GNorm = 0.2520
Meta loss on this task batch = 2.5764e-01, Meta loss averaged over last 500 steps = 3.0328e-01, PNorm = 124.7824, GNorm = 0.2121
Meta loss on this task batch = 2.4773e-01, Meta loss averaged over last 500 steps = 3.0319e-01, PNorm = 124.7904, GNorm = 0.1914
Meta loss on this task batch = 3.0422e-01, Meta loss averaged over last 500 steps = 3.0305e-01, PNorm = 124.7990, GNorm = 0.2648
Meta loss on this task batch = 2.4415e-01, Meta loss averaged over last 500 steps = 3.0282e-01, PNorm = 124.8079, GNorm = 0.2207
Meta loss on this task batch = 3.6529e-01, Meta loss averaged over last 500 steps = 3.0285e-01, PNorm = 124.8164, GNorm = 0.3105
Meta loss on this task batch = 2.7675e-01, Meta loss averaged over last 500 steps = 3.0277e-01, PNorm = 124.8251, GNorm = 0.2118
Meta loss on this task batch = 2.5322e-01, Meta loss averaged over last 500 steps = 3.0274e-01, PNorm = 124.8336, GNorm = 0.1902
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 3.0272e-01, PNorm = 124.8414, GNorm = 0.2239
Meta loss on this task batch = 2.4270e-01, Meta loss averaged over last 500 steps = 3.0257e-01, PNorm = 124.8487, GNorm = 0.2180
Meta loss on this task batch = 3.6009e-01, Meta loss averaged over last 500 steps = 3.0257e-01, PNorm = 124.8551, GNorm = 0.2534
Meta loss on this task batch = 3.5037e-01, Meta loss averaged over last 500 steps = 3.0265e-01, PNorm = 124.8609, GNorm = 0.2512
Meta loss on this task batch = 3.2053e-01, Meta loss averaged over last 500 steps = 3.0265e-01, PNorm = 124.8670, GNorm = 0.2528
Meta loss on this task batch = 3.1007e-01, Meta loss averaged over last 500 steps = 3.0267e-01, PNorm = 124.8723, GNorm = 0.2433
Meta loss on this task batch = 2.8628e-01, Meta loss averaged over last 500 steps = 3.0268e-01, PNorm = 124.8774, GNorm = 0.2177
Meta loss on this task batch = 2.3183e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 124.8840, GNorm = 0.1710
Meta loss on this task batch = 3.0248e-01, Meta loss averaged over last 500 steps = 3.0249e-01, PNorm = 124.8903, GNorm = 0.2468
Took 113.75501108169556 seconds to complete one epoch of meta training
Took 121.34092402458191 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502584
Epoch 399
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 3.0237e-01, PNorm = 124.8971, GNorm = 0.1842
Meta loss on this task batch = 3.2257e-01, Meta loss averaged over last 500 steps = 3.0252e-01, PNorm = 124.9030, GNorm = 0.2346
Meta loss on this task batch = 2.8151e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 124.9093, GNorm = 0.2164
Meta loss on this task batch = 2.9539e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 124.9159, GNorm = 0.2006
Meta loss on this task batch = 2.6418e-01, Meta loss averaged over last 500 steps = 3.0243e-01, PNorm = 124.9233, GNorm = 0.1917
Meta loss on this task batch = 3.7220e-01, Meta loss averaged over last 500 steps = 3.0268e-01, PNorm = 124.9315, GNorm = 0.2452
Meta loss on this task batch = 2.8974e-01, Meta loss averaged over last 500 steps = 3.0273e-01, PNorm = 124.9405, GNorm = 0.2012
Meta loss on this task batch = 2.8681e-01, Meta loss averaged over last 500 steps = 3.0274e-01, PNorm = 124.9495, GNorm = 0.2168
Meta loss on this task batch = 2.7131e-01, Meta loss averaged over last 500 steps = 3.0275e-01, PNorm = 124.9584, GNorm = 0.2403
Meta loss on this task batch = 3.1723e-01, Meta loss averaged over last 500 steps = 3.0286e-01, PNorm = 124.9662, GNorm = 0.2665
Meta loss on this task batch = 3.3925e-01, Meta loss averaged over last 500 steps = 3.0298e-01, PNorm = 124.9720, GNorm = 0.2824
Meta loss on this task batch = 3.0744e-01, Meta loss averaged over last 500 steps = 3.0296e-01, PNorm = 124.9768, GNorm = 0.2080
Meta loss on this task batch = 3.0053e-01, Meta loss averaged over last 500 steps = 3.0298e-01, PNorm = 124.9808, GNorm = 0.2208
Meta loss on this task batch = 3.4402e-01, Meta loss averaged over last 500 steps = 3.0313e-01, PNorm = 124.9840, GNorm = 0.2636
Meta loss on this task batch = 2.4359e-01, Meta loss averaged over last 500 steps = 3.0300e-01, PNorm = 124.9884, GNorm = 0.2031
Meta loss on this task batch = 3.4972e-01, Meta loss averaged over last 500 steps = 3.0311e-01, PNorm = 124.9918, GNorm = 0.2481
Meta loss on this task batch = 2.8060e-01, Meta loss averaged over last 500 steps = 3.0312e-01, PNorm = 124.9965, GNorm = 0.2058
Meta loss on this task batch = 3.0556e-01, Meta loss averaged over last 500 steps = 3.0301e-01, PNorm = 125.0026, GNorm = 0.2050
Meta loss on this task batch = 3.0530e-01, Meta loss averaged over last 500 steps = 3.0311e-01, PNorm = 125.0101, GNorm = 0.2749
Took 113.93946146965027 seconds to complete one epoch of meta training
Took 121.44992232322693 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478285
Epoch 400
Meta loss on this task batch = 3.0602e-01, Meta loss averaged over last 500 steps = 3.0308e-01, PNorm = 125.0189, GNorm = 0.2260
Meta loss on this task batch = 3.0352e-01, Meta loss averaged over last 500 steps = 3.0304e-01, PNorm = 125.0275, GNorm = 0.2217
Meta loss on this task batch = 2.8624e-01, Meta loss averaged over last 500 steps = 3.0304e-01, PNorm = 125.0362, GNorm = 0.2211
Meta loss on this task batch = 3.4172e-01, Meta loss averaged over last 500 steps = 3.0306e-01, PNorm = 125.0445, GNorm = 0.2125
Meta loss on this task batch = 3.0453e-01, Meta loss averaged over last 500 steps = 3.0301e-01, PNorm = 125.0523, GNorm = 0.2323
Meta loss on this task batch = 3.1044e-01, Meta loss averaged over last 500 steps = 3.0295e-01, PNorm = 125.0595, GNorm = 0.2217
Meta loss on this task batch = 2.9866e-01, Meta loss averaged over last 500 steps = 3.0301e-01, PNorm = 125.0670, GNorm = 0.2219
Meta loss on this task batch = 3.5516e-01, Meta loss averaged over last 500 steps = 3.0319e-01, PNorm = 125.0734, GNorm = 0.2245
Meta loss on this task batch = 2.5628e-01, Meta loss averaged over last 500 steps = 3.0297e-01, PNorm = 125.0795, GNorm = 0.2079
Meta loss on this task batch = 3.1486e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 125.0849, GNorm = 0.2184
Meta loss on this task batch = 3.1969e-01, Meta loss averaged over last 500 steps = 3.0316e-01, PNorm = 125.0906, GNorm = 0.2374
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 3.0315e-01, PNorm = 125.0960, GNorm = 0.2199
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 3.0310e-01, PNorm = 125.1007, GNorm = 0.2786
Meta loss on this task batch = 3.0847e-01, Meta loss averaged over last 500 steps = 3.0305e-01, PNorm = 125.1055, GNorm = 0.2784
Meta loss on this task batch = 2.6660e-01, Meta loss averaged over last 500 steps = 3.0299e-01, PNorm = 125.1118, GNorm = 0.2016
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 3.0300e-01, PNorm = 125.1183, GNorm = 0.2078
Meta loss on this task batch = 2.7544e-01, Meta loss averaged over last 500 steps = 3.0298e-01, PNorm = 125.1262, GNorm = 0.1978
Meta loss on this task batch = 3.0522e-01, Meta loss averaged over last 500 steps = 3.0293e-01, PNorm = 125.1355, GNorm = 0.2309
Meta loss on this task batch = 3.4377e-01, Meta loss averaged over last 500 steps = 3.0299e-01, PNorm = 125.1456, GNorm = 0.2519
Took 112.65255165100098 seconds to complete one epoch of meta training
Took 120.89181232452393 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486023
Epoch 401
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 3.0286e-01, PNorm = 125.1559, GNorm = 0.1984
Meta loss on this task batch = 2.7702e-01, Meta loss averaged over last 500 steps = 3.0287e-01, PNorm = 125.1657, GNorm = 0.2246
Meta loss on this task batch = 2.8252e-01, Meta loss averaged over last 500 steps = 3.0280e-01, PNorm = 125.1752, GNorm = 0.2195
Meta loss on this task batch = 2.2822e-01, Meta loss averaged over last 500 steps = 3.0268e-01, PNorm = 125.1840, GNorm = 0.1987
Meta loss on this task batch = 3.2283e-01, Meta loss averaged over last 500 steps = 3.0267e-01, PNorm = 125.1925, GNorm = 0.2193
Meta loss on this task batch = 2.8564e-01, Meta loss averaged over last 500 steps = 3.0255e-01, PNorm = 125.2010, GNorm = 0.2167
Meta loss on this task batch = 3.4176e-01, Meta loss averaged over last 500 steps = 3.0266e-01, PNorm = 125.2079, GNorm = 0.2348
Meta loss on this task batch = 2.9733e-01, Meta loss averaged over last 500 steps = 3.0263e-01, PNorm = 125.2144, GNorm = 0.2102
Meta loss on this task batch = 2.9963e-01, Meta loss averaged over last 500 steps = 3.0253e-01, PNorm = 125.2200, GNorm = 0.2325
Meta loss on this task batch = 3.1968e-01, Meta loss averaged over last 500 steps = 3.0255e-01, PNorm = 125.2252, GNorm = 0.2172
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 3.0243e-01, PNorm = 125.2314, GNorm = 0.2106
Meta loss on this task batch = 3.0936e-01, Meta loss averaged over last 500 steps = 3.0240e-01, PNorm = 125.2373, GNorm = 0.2780
Meta loss on this task batch = 2.4844e-01, Meta loss averaged over last 500 steps = 3.0234e-01, PNorm = 125.2437, GNorm = 0.2013
Meta loss on this task batch = 2.4091e-01, Meta loss averaged over last 500 steps = 3.0233e-01, PNorm = 125.2511, GNorm = 0.2325
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 3.0233e-01, PNorm = 125.2589, GNorm = 0.2442
Meta loss on this task batch = 3.3178e-01, Meta loss averaged over last 500 steps = 3.0237e-01, PNorm = 125.2671, GNorm = 0.2387
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 125.2741, GNorm = 0.2404
Meta loss on this task batch = 3.1275e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 125.2807, GNorm = 0.2665
Meta loss on this task batch = 2.7324e-01, Meta loss averaged over last 500 steps = 3.0234e-01, PNorm = 125.2873, GNorm = 0.2426
Took 114.61135244369507 seconds to complete one epoch of meta training
Took 122.51577997207642 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485087
Epoch 402
Meta loss on this task batch = 2.8903e-01, Meta loss averaged over last 500 steps = 3.0229e-01, PNorm = 125.2936, GNorm = 0.2062
Meta loss on this task batch = 3.3016e-01, Meta loss averaged over last 500 steps = 3.0233e-01, PNorm = 125.2996, GNorm = 0.2555
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 125.3049, GNorm = 0.2468
Meta loss on this task batch = 2.8858e-01, Meta loss averaged over last 500 steps = 3.0245e-01, PNorm = 125.3110, GNorm = 0.2192
Meta loss on this task batch = 3.1855e-01, Meta loss averaged over last 500 steps = 3.0246e-01, PNorm = 125.3164, GNorm = 0.2515
Meta loss on this task batch = 2.7895e-01, Meta loss averaged over last 500 steps = 3.0235e-01, PNorm = 125.3225, GNorm = 0.2135
Meta loss on this task batch = 3.1844e-01, Meta loss averaged over last 500 steps = 3.0228e-01, PNorm = 125.3285, GNorm = 0.2164
Meta loss on this task batch = 3.1549e-01, Meta loss averaged over last 500 steps = 3.0227e-01, PNorm = 125.3344, GNorm = 0.2402
Meta loss on this task batch = 3.3475e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 125.3399, GNorm = 0.2141
Meta loss on this task batch = 3.0248e-01, Meta loss averaged over last 500 steps = 3.0259e-01, PNorm = 125.3461, GNorm = 0.2323
Meta loss on this task batch = 3.0220e-01, Meta loss averaged over last 500 steps = 3.0260e-01, PNorm = 125.3520, GNorm = 0.2168
Meta loss on this task batch = 3.1359e-01, Meta loss averaged over last 500 steps = 3.0261e-01, PNorm = 125.3586, GNorm = 0.2416
Meta loss on this task batch = 2.6335e-01, Meta loss averaged over last 500 steps = 3.0251e-01, PNorm = 125.3659, GNorm = 0.1909
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 3.0247e-01, PNorm = 125.3711, GNorm = 0.2384
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 3.0231e-01, PNorm = 125.3771, GNorm = 0.2104
Meta loss on this task batch = 3.0345e-01, Meta loss averaged over last 500 steps = 3.0226e-01, PNorm = 125.3830, GNorm = 0.2233
Meta loss on this task batch = 2.7435e-01, Meta loss averaged over last 500 steps = 3.0222e-01, PNorm = 125.3896, GNorm = 0.2226
Meta loss on this task batch = 2.2810e-01, Meta loss averaged over last 500 steps = 3.0204e-01, PNorm = 125.3965, GNorm = 0.1981
Meta loss on this task batch = 2.8559e-01, Meta loss averaged over last 500 steps = 3.0207e-01, PNorm = 125.4041, GNorm = 0.2421
Took 114.66832065582275 seconds to complete one epoch of meta training
Took 122.42907118797302 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499885
Epoch 403
Meta loss on this task batch = 2.8520e-01, Meta loss averaged over last 500 steps = 3.0209e-01, PNorm = 125.4110, GNorm = 0.2250
Meta loss on this task batch = 3.3695e-01, Meta loss averaged over last 500 steps = 3.0207e-01, PNorm = 125.4169, GNorm = 0.2697
Meta loss on this task batch = 3.1633e-01, Meta loss averaged over last 500 steps = 3.0202e-01, PNorm = 125.4233, GNorm = 0.1987
Meta loss on this task batch = 2.8529e-01, Meta loss averaged over last 500 steps = 3.0207e-01, PNorm = 125.4295, GNorm = 0.2185
Meta loss on this task batch = 3.0333e-01, Meta loss averaged over last 500 steps = 3.0206e-01, PNorm = 125.4353, GNorm = 0.2415
Meta loss on this task batch = 3.3329e-01, Meta loss averaged over last 500 steps = 3.0197e-01, PNorm = 125.4419, GNorm = 0.2365
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 3.0188e-01, PNorm = 125.4485, GNorm = 0.2298
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 3.0181e-01, PNorm = 125.4553, GNorm = 0.2044
Meta loss on this task batch = 2.7418e-01, Meta loss averaged over last 500 steps = 3.0178e-01, PNorm = 125.4619, GNorm = 0.1968
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 3.0171e-01, PNorm = 125.4692, GNorm = 0.2454
Meta loss on this task batch = 3.1938e-01, Meta loss averaged over last 500 steps = 3.0178e-01, PNorm = 125.4775, GNorm = 0.2248
Meta loss on this task batch = 3.1450e-01, Meta loss averaged over last 500 steps = 3.0193e-01, PNorm = 125.4858, GNorm = 0.2048
Meta loss on this task batch = 3.0122e-01, Meta loss averaged over last 500 steps = 3.0195e-01, PNorm = 125.4940, GNorm = 0.2074
Meta loss on this task batch = 3.5177e-01, Meta loss averaged over last 500 steps = 3.0203e-01, PNorm = 125.5018, GNorm = 0.2388
Meta loss on this task batch = 3.1106e-01, Meta loss averaged over last 500 steps = 3.0199e-01, PNorm = 125.5088, GNorm = 0.2467
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 3.0194e-01, PNorm = 125.5156, GNorm = 0.2091
Meta loss on this task batch = 3.2689e-01, Meta loss averaged over last 500 steps = 3.0201e-01, PNorm = 125.5226, GNorm = 0.2184
Meta loss on this task batch = 2.6745e-01, Meta loss averaged over last 500 steps = 3.0196e-01, PNorm = 125.5300, GNorm = 0.1957
Meta loss on this task batch = 3.5105e-01, Meta loss averaged over last 500 steps = 3.0205e-01, PNorm = 125.5359, GNorm = 0.2762
Took 114.3979721069336 seconds to complete one epoch of meta training
Took 122.27210998535156 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495599
Epoch 404
Meta loss on this task batch = 2.7159e-01, Meta loss averaged over last 500 steps = 3.0199e-01, PNorm = 125.5423, GNorm = 0.1993
Meta loss on this task batch = 3.4779e-01, Meta loss averaged over last 500 steps = 3.0202e-01, PNorm = 125.5491, GNorm = 0.2376
Meta loss on this task batch = 2.9235e-01, Meta loss averaged over last 500 steps = 3.0193e-01, PNorm = 125.5561, GNorm = 0.2045
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 3.0172e-01, PNorm = 125.5635, GNorm = 0.2202
Meta loss on this task batch = 3.1245e-01, Meta loss averaged over last 500 steps = 3.0166e-01, PNorm = 125.5704, GNorm = 0.2608
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 3.0170e-01, PNorm = 125.5775, GNorm = 0.2160
Meta loss on this task batch = 3.2497e-01, Meta loss averaged over last 500 steps = 3.0182e-01, PNorm = 125.5851, GNorm = 0.2363
Meta loss on this task batch = 3.3318e-01, Meta loss averaged over last 500 steps = 3.0187e-01, PNorm = 125.5919, GNorm = 0.2637
Meta loss on this task batch = 2.6042e-01, Meta loss averaged over last 500 steps = 3.0181e-01, PNorm = 125.5995, GNorm = 0.2285
Meta loss on this task batch = 2.8525e-01, Meta loss averaged over last 500 steps = 3.0173e-01, PNorm = 125.6074, GNorm = 0.2296
Meta loss on this task batch = 3.3794e-01, Meta loss averaged over last 500 steps = 3.0161e-01, PNorm = 125.6154, GNorm = 0.2753
Meta loss on this task batch = 2.1925e-01, Meta loss averaged over last 500 steps = 3.0150e-01, PNorm = 125.6233, GNorm = 0.2205
Meta loss on this task batch = 2.6257e-01, Meta loss averaged over last 500 steps = 3.0137e-01, PNorm = 125.6316, GNorm = 0.2117
Meta loss on this task batch = 3.0997e-01, Meta loss averaged over last 500 steps = 3.0140e-01, PNorm = 125.6392, GNorm = 0.2622
Meta loss on this task batch = 3.5629e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 125.6452, GNorm = 0.2382
Meta loss on this task batch = 3.0575e-01, Meta loss averaged over last 500 steps = 3.0148e-01, PNorm = 125.6483, GNorm = 0.2813
Meta loss on this task batch = 3.3543e-01, Meta loss averaged over last 500 steps = 3.0160e-01, PNorm = 125.6510, GNorm = 0.2497
Meta loss on this task batch = 3.1394e-01, Meta loss averaged over last 500 steps = 3.0168e-01, PNorm = 125.6539, GNorm = 0.2506
Meta loss on this task batch = 3.2831e-01, Meta loss averaged over last 500 steps = 3.0173e-01, PNorm = 125.6587, GNorm = 0.2799
Took 114.56865096092224 seconds to complete one epoch of meta training
Took 122.53821182250977 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478397
Epoch 405
Meta loss on this task batch = 3.1581e-01, Meta loss averaged over last 500 steps = 3.0176e-01, PNorm = 125.6643, GNorm = 0.2605
Meta loss on this task batch = 2.6339e-01, Meta loss averaged over last 500 steps = 3.0173e-01, PNorm = 125.6711, GNorm = 0.2142
Meta loss on this task batch = 3.4484e-01, Meta loss averaged over last 500 steps = 3.0182e-01, PNorm = 125.6787, GNorm = 0.2117
Meta loss on this task batch = 2.8568e-01, Meta loss averaged over last 500 steps = 3.0175e-01, PNorm = 125.6867, GNorm = 0.2236
Meta loss on this task batch = 3.3337e-01, Meta loss averaged over last 500 steps = 3.0178e-01, PNorm = 125.6949, GNorm = 0.2025
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 3.0168e-01, PNorm = 125.7028, GNorm = 0.2170
Meta loss on this task batch = 3.5088e-01, Meta loss averaged over last 500 steps = 3.0181e-01, PNorm = 125.7098, GNorm = 0.2188
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 3.0176e-01, PNorm = 125.7165, GNorm = 0.1914
Meta loss on this task batch = 2.7592e-01, Meta loss averaged over last 500 steps = 3.0170e-01, PNorm = 125.7229, GNorm = 0.2089
Meta loss on this task batch = 3.4015e-01, Meta loss averaged over last 500 steps = 3.0180e-01, PNorm = 125.7292, GNorm = 0.2697
Meta loss on this task batch = 2.9862e-01, Meta loss averaged over last 500 steps = 3.0185e-01, PNorm = 125.7358, GNorm = 0.2548
Meta loss on this task batch = 2.7433e-01, Meta loss averaged over last 500 steps = 3.0176e-01, PNorm = 125.7425, GNorm = 0.2124
Meta loss on this task batch = 3.4054e-01, Meta loss averaged over last 500 steps = 3.0190e-01, PNorm = 125.7495, GNorm = 0.2438
Meta loss on this task batch = 2.4004e-01, Meta loss averaged over last 500 steps = 3.0184e-01, PNorm = 125.7570, GNorm = 0.1816
Meta loss on this task batch = 3.1698e-01, Meta loss averaged over last 500 steps = 3.0181e-01, PNorm = 125.7640, GNorm = 0.2388
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 3.0184e-01, PNorm = 125.7708, GNorm = 0.2147
Meta loss on this task batch = 2.9904e-01, Meta loss averaged over last 500 steps = 3.0190e-01, PNorm = 125.7792, GNorm = 0.2397
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 3.0190e-01, PNorm = 125.7872, GNorm = 0.2295
Meta loss on this task batch = 2.9598e-01, Meta loss averaged over last 500 steps = 3.0183e-01, PNorm = 125.7949, GNorm = 0.2273
Took 112.54450106620789 seconds to complete one epoch of meta training
Took 119.90146708488464 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474714
Epoch 406
Meta loss on this task batch = 2.4628e-01, Meta loss averaged over last 500 steps = 3.0173e-01, PNorm = 125.8031, GNorm = 0.2042
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 3.0157e-01, PNorm = 125.8107, GNorm = 0.2392
Meta loss on this task batch = 3.0181e-01, Meta loss averaged over last 500 steps = 3.0145e-01, PNorm = 125.8181, GNorm = 0.2147
Meta loss on this task batch = 3.2939e-01, Meta loss averaged over last 500 steps = 3.0153e-01, PNorm = 125.8253, GNorm = 0.2236
Meta loss on this task batch = 3.7232e-01, Meta loss averaged over last 500 steps = 3.0156e-01, PNorm = 125.8320, GNorm = 0.2717
Meta loss on this task batch = 3.2508e-01, Meta loss averaged over last 500 steps = 3.0162e-01, PNorm = 125.8386, GNorm = 0.2257
Meta loss on this task batch = 2.8104e-01, Meta loss averaged over last 500 steps = 3.0154e-01, PNorm = 125.8456, GNorm = 0.2508
Meta loss on this task batch = 2.7471e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 125.8528, GNorm = 0.2259
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 3.0145e-01, PNorm = 125.8601, GNorm = 0.2335
Meta loss on this task batch = 2.3931e-01, Meta loss averaged over last 500 steps = 3.0127e-01, PNorm = 125.8680, GNorm = 0.2289
Meta loss on this task batch = 3.4005e-01, Meta loss averaged over last 500 steps = 3.0140e-01, PNorm = 125.8752, GNorm = 0.2584
Meta loss on this task batch = 2.8866e-01, Meta loss averaged over last 500 steps = 3.0133e-01, PNorm = 125.8822, GNorm = 0.2721
Meta loss on this task batch = 2.8058e-01, Meta loss averaged over last 500 steps = 3.0129e-01, PNorm = 125.8900, GNorm = 0.2078
Meta loss on this task batch = 2.8971e-01, Meta loss averaged over last 500 steps = 3.0114e-01, PNorm = 125.8984, GNorm = 0.2000
Meta loss on this task batch = 3.2066e-01, Meta loss averaged over last 500 steps = 3.0120e-01, PNorm = 125.9055, GNorm = 0.2431
Meta loss on this task batch = 2.9663e-01, Meta loss averaged over last 500 steps = 3.0118e-01, PNorm = 125.9117, GNorm = 0.2240
Meta loss on this task batch = 3.2761e-01, Meta loss averaged over last 500 steps = 3.0126e-01, PNorm = 125.9179, GNorm = 0.2088
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 125.9241, GNorm = 0.2257
Meta loss on this task batch = 3.1290e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 125.9302, GNorm = 0.2622
Took 112.66922521591187 seconds to complete one epoch of meta training
Took 120.76919269561768 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502677
Epoch 407
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 125.9367, GNorm = 0.1931
Meta loss on this task batch = 2.6516e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 125.9443, GNorm = 0.1948
Meta loss on this task batch = 3.1129e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 125.9513, GNorm = 0.2144
Meta loss on this task batch = 2.8679e-01, Meta loss averaged over last 500 steps = 3.0117e-01, PNorm = 125.9582, GNorm = 0.2630
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 125.9655, GNorm = 0.1828
Meta loss on this task batch = 3.4621e-01, Meta loss averaged over last 500 steps = 3.0134e-01, PNorm = 125.9726, GNorm = 0.2589
Meta loss on this task batch = 3.1347e-01, Meta loss averaged over last 500 steps = 3.0128e-01, PNorm = 125.9795, GNorm = 0.2042
Meta loss on this task batch = 2.8740e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 125.9862, GNorm = 0.2184
Meta loss on this task batch = 3.1355e-01, Meta loss averaged over last 500 steps = 3.0125e-01, PNorm = 125.9934, GNorm = 0.2304
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 3.0129e-01, PNorm = 125.9999, GNorm = 0.2427
Meta loss on this task batch = 2.6173e-01, Meta loss averaged over last 500 steps = 3.0109e-01, PNorm = 126.0074, GNorm = 0.2352
Meta loss on this task batch = 3.3455e-01, Meta loss averaged over last 500 steps = 3.0106e-01, PNorm = 126.0146, GNorm = 0.2469
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 3.0096e-01, PNorm = 126.0214, GNorm = 0.2435
Meta loss on this task batch = 3.4448e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 126.0290, GNorm = 0.2540
Meta loss on this task batch = 3.6162e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 126.0362, GNorm = 0.3219
Meta loss on this task batch = 3.4261e-01, Meta loss averaged over last 500 steps = 3.0144e-01, PNorm = 126.0423, GNorm = 0.2773
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 3.0152e-01, PNorm = 126.0478, GNorm = 0.2164
Meta loss on this task batch = 2.5490e-01, Meta loss averaged over last 500 steps = 3.0145e-01, PNorm = 126.0543, GNorm = 0.2305
Meta loss on this task batch = 2.8186e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 126.0614, GNorm = 0.2512
Took 116.91680431365967 seconds to complete one epoch of meta training
Took 125.02610969543457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512930
Epoch 408
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 3.0124e-01, PNorm = 126.0678, GNorm = 0.2113
Meta loss on this task batch = 3.5949e-01, Meta loss averaged over last 500 steps = 3.0136e-01, PNorm = 126.0733, GNorm = 0.2424
Meta loss on this task batch = 3.1453e-01, Meta loss averaged over last 500 steps = 3.0135e-01, PNorm = 126.0782, GNorm = 0.2608
Meta loss on this task batch = 2.6323e-01, Meta loss averaged over last 500 steps = 3.0128e-01, PNorm = 126.0844, GNorm = 0.1875
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 3.0134e-01, PNorm = 126.0915, GNorm = 0.2053
Meta loss on this task batch = 3.4704e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 126.0983, GNorm = 0.2492
Meta loss on this task batch = 3.1668e-01, Meta loss averaged over last 500 steps = 3.0153e-01, PNorm = 126.1057, GNorm = 0.2030
Meta loss on this task batch = 3.1548e-01, Meta loss averaged over last 500 steps = 3.0163e-01, PNorm = 126.1136, GNorm = 0.2943
Meta loss on this task batch = 2.6167e-01, Meta loss averaged over last 500 steps = 3.0145e-01, PNorm = 126.1217, GNorm = 0.2208
Meta loss on this task batch = 2.7117e-01, Meta loss averaged over last 500 steps = 3.0147e-01, PNorm = 126.1303, GNorm = 0.2025
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 3.0135e-01, PNorm = 126.1386, GNorm = 0.2304
Meta loss on this task batch = 2.3757e-01, Meta loss averaged over last 500 steps = 3.0120e-01, PNorm = 126.1468, GNorm = 0.1860
Meta loss on this task batch = 2.9695e-01, Meta loss averaged over last 500 steps = 3.0126e-01, PNorm = 126.1546, GNorm = 0.2208
Meta loss on this task batch = 3.1684e-01, Meta loss averaged over last 500 steps = 3.0115e-01, PNorm = 126.1618, GNorm = 0.2316
Meta loss on this task batch = 3.1004e-01, Meta loss averaged over last 500 steps = 3.0116e-01, PNorm = 126.1678, GNorm = 0.2258
Meta loss on this task batch = 2.9838e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 126.1733, GNorm = 0.2222
Meta loss on this task batch = 3.2367e-01, Meta loss averaged over last 500 steps = 3.0121e-01, PNorm = 126.1791, GNorm = 0.2204
Meta loss on this task batch = 3.2296e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 126.1850, GNorm = 0.2105
Meta loss on this task batch = 2.7192e-01, Meta loss averaged over last 500 steps = 3.0129e-01, PNorm = 126.1912, GNorm = 0.2405
Took 116.2497091293335 seconds to complete one epoch of meta training
Took 123.80676245689392 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.451885
Epoch 409
Meta loss on this task batch = 2.8459e-01, Meta loss averaged over last 500 steps = 3.0135e-01, PNorm = 126.1979, GNorm = 0.2098
Meta loss on this task batch = 3.1989e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 126.2054, GNorm = 0.2402
Meta loss on this task batch = 3.2991e-01, Meta loss averaged over last 500 steps = 3.0129e-01, PNorm = 126.2121, GNorm = 0.2722
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 3.0121e-01, PNorm = 126.2199, GNorm = 0.2275
Meta loss on this task batch = 2.6199e-01, Meta loss averaged over last 500 steps = 3.0106e-01, PNorm = 126.2283, GNorm = 0.1967
Meta loss on this task batch = 3.0379e-01, Meta loss averaged over last 500 steps = 3.0111e-01, PNorm = 126.2364, GNorm = 0.2474
Meta loss on this task batch = 3.1218e-01, Meta loss averaged over last 500 steps = 3.0120e-01, PNorm = 126.2448, GNorm = 0.2287
Meta loss on this task batch = 3.2318e-01, Meta loss averaged over last 500 steps = 3.0122e-01, PNorm = 126.2529, GNorm = 0.2417
Meta loss on this task batch = 3.5943e-01, Meta loss averaged over last 500 steps = 3.0130e-01, PNorm = 126.2613, GNorm = 0.2530
Meta loss on this task batch = 2.7147e-01, Meta loss averaged over last 500 steps = 3.0118e-01, PNorm = 126.2696, GNorm = 0.2444
Meta loss on this task batch = 1.7443e-01, Meta loss averaged over last 500 steps = 3.0103e-01, PNorm = 126.2781, GNorm = 0.1589
Meta loss on this task batch = 3.1837e-01, Meta loss averaged over last 500 steps = 3.0118e-01, PNorm = 126.2860, GNorm = 0.2510
Meta loss on this task batch = 3.1953e-01, Meta loss averaged over last 500 steps = 3.0111e-01, PNorm = 126.2926, GNorm = 0.2514
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 3.0109e-01, PNorm = 126.2972, GNorm = 0.3286
Meta loss on this task batch = 3.2932e-01, Meta loss averaged over last 500 steps = 3.0116e-01, PNorm = 126.3023, GNorm = 0.3307
Meta loss on this task batch = 2.7315e-01, Meta loss averaged over last 500 steps = 3.0121e-01, PNorm = 126.3080, GNorm = 0.2056
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 3.0106e-01, PNorm = 126.3138, GNorm = 0.2200
Meta loss on this task batch = 3.0767e-01, Meta loss averaged over last 500 steps = 3.0106e-01, PNorm = 126.3201, GNorm = 0.2278
Meta loss on this task batch = 2.8472e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 126.3272, GNorm = 0.2580
Took 115.62134885787964 seconds to complete one epoch of meta training
Took 123.04766416549683 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487275
Epoch 410
Meta loss on this task batch = 3.0678e-01, Meta loss averaged over last 500 steps = 3.0104e-01, PNorm = 126.3352, GNorm = 0.2183
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 126.3434, GNorm = 0.1932
Meta loss on this task batch = 3.8726e-01, Meta loss averaged over last 500 steps = 3.0116e-01, PNorm = 126.3517, GNorm = 0.2452
Meta loss on this task batch = 3.2974e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 126.3596, GNorm = 0.2958
Meta loss on this task batch = 3.0058e-01, Meta loss averaged over last 500 steps = 3.0114e-01, PNorm = 126.3676, GNorm = 0.2268
Meta loss on this task batch = 3.2397e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 126.3754, GNorm = 0.1878
Meta loss on this task batch = 3.1356e-01, Meta loss averaged over last 500 steps = 3.0119e-01, PNorm = 126.3831, GNorm = 0.2297
Meta loss on this task batch = 2.9705e-01, Meta loss averaged over last 500 steps = 3.0121e-01, PNorm = 126.3905, GNorm = 0.2057
Meta loss on this task batch = 2.3785e-01, Meta loss averaged over last 500 steps = 3.0109e-01, PNorm = 126.3975, GNorm = 0.1940
Meta loss on this task batch = 3.0870e-01, Meta loss averaged over last 500 steps = 3.0111e-01, PNorm = 126.4036, GNorm = 0.2328
Meta loss on this task batch = 3.0317e-01, Meta loss averaged over last 500 steps = 3.0113e-01, PNorm = 126.4091, GNorm = 0.2478
Meta loss on this task batch = 2.6804e-01, Meta loss averaged over last 500 steps = 3.0104e-01, PNorm = 126.4151, GNorm = 0.2549
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 3.0086e-01, PNorm = 126.4191, GNorm = 0.2128
Meta loss on this task batch = 3.0209e-01, Meta loss averaged over last 500 steps = 3.0093e-01, PNorm = 126.4228, GNorm = 0.2421
Meta loss on this task batch = 3.0657e-01, Meta loss averaged over last 500 steps = 3.0103e-01, PNorm = 126.4279, GNorm = 0.2805
Meta loss on this task batch = 3.0411e-01, Meta loss averaged over last 500 steps = 3.0096e-01, PNorm = 126.4323, GNorm = 0.2313
Meta loss on this task batch = 3.0875e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 126.4380, GNorm = 0.2411
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 3.0100e-01, PNorm = 126.4435, GNorm = 0.2273
Meta loss on this task batch = 3.2864e-01, Meta loss averaged over last 500 steps = 3.0107e-01, PNorm = 126.4499, GNorm = 0.3067
Took 115.24374222755432 seconds to complete one epoch of meta training
Took 123.30131816864014 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.525817
Epoch 411
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 3.0103e-01, PNorm = 126.4574, GNorm = 0.2346
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 3.0092e-01, PNorm = 126.4650, GNorm = 0.2246
Meta loss on this task batch = 2.6285e-01, Meta loss averaged over last 500 steps = 3.0083e-01, PNorm = 126.4733, GNorm = 0.2220
Meta loss on this task batch = 3.1964e-01, Meta loss averaged over last 500 steps = 3.0093e-01, PNorm = 126.4814, GNorm = 0.2517
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 3.0094e-01, PNorm = 126.4890, GNorm = 0.2091
Meta loss on this task batch = 2.2600e-01, Meta loss averaged over last 500 steps = 3.0084e-01, PNorm = 126.4970, GNorm = 0.1895
Meta loss on this task batch = 3.8035e-01, Meta loss averaged over last 500 steps = 3.0090e-01, PNorm = 126.5047, GNorm = 0.2539
Meta loss on this task batch = 2.9354e-01, Meta loss averaged over last 500 steps = 3.0081e-01, PNorm = 126.5115, GNorm = 0.2129
Meta loss on this task batch = 2.9125e-01, Meta loss averaged over last 500 steps = 3.0088e-01, PNorm = 126.5184, GNorm = 0.2194
Meta loss on this task batch = 3.5064e-01, Meta loss averaged over last 500 steps = 3.0106e-01, PNorm = 126.5250, GNorm = 0.2312
Meta loss on this task batch = 2.4549e-01, Meta loss averaged over last 500 steps = 3.0105e-01, PNorm = 126.5318, GNorm = 0.2011
Meta loss on this task batch = 3.1231e-01, Meta loss averaged over last 500 steps = 3.0112e-01, PNorm = 126.5384, GNorm = 0.2253
Meta loss on this task batch = 3.4031e-01, Meta loss averaged over last 500 steps = 3.0127e-01, PNorm = 126.5450, GNorm = 0.2042
Meta loss on this task batch = 2.6185e-01, Meta loss averaged over last 500 steps = 3.0122e-01, PNorm = 126.5527, GNorm = 0.2223
Meta loss on this task batch = 3.2441e-01, Meta loss averaged over last 500 steps = 3.0123e-01, PNorm = 126.5604, GNorm = 0.2391
Meta loss on this task batch = 3.1338e-01, Meta loss averaged over last 500 steps = 3.0112e-01, PNorm = 126.5684, GNorm = 0.2633
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 3.0109e-01, PNorm = 126.5769, GNorm = 0.2470
Meta loss on this task batch = 3.1799e-01, Meta loss averaged over last 500 steps = 3.0103e-01, PNorm = 126.5845, GNorm = 0.2764
Meta loss on this task batch = 2.9054e-01, Meta loss averaged over last 500 steps = 3.0094e-01, PNorm = 126.5914, GNorm = 0.2736
Took 114.96581649780273 seconds to complete one epoch of meta training
Took 122.68235945701599 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482718
Epoch 412
Meta loss on this task batch = 3.0384e-01, Meta loss averaged over last 500 steps = 3.0103e-01, PNorm = 126.5978, GNorm = 0.2234
Meta loss on this task batch = 2.9282e-01, Meta loss averaged over last 500 steps = 3.0101e-01, PNorm = 126.6034, GNorm = 0.2159
Meta loss on this task batch = 2.8817e-01, Meta loss averaged over last 500 steps = 3.0098e-01, PNorm = 126.6091, GNorm = 0.1967
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 3.0082e-01, PNorm = 126.6157, GNorm = 0.2283
Meta loss on this task batch = 2.9573e-01, Meta loss averaged over last 500 steps = 3.0087e-01, PNorm = 126.6223, GNorm = 0.2392
Meta loss on this task batch = 3.0924e-01, Meta loss averaged over last 500 steps = 3.0085e-01, PNorm = 126.6286, GNorm = 0.2330
Meta loss on this task batch = 3.3515e-01, Meta loss averaged over last 500 steps = 3.0091e-01, PNorm = 126.6345, GNorm = 0.2607
Meta loss on this task batch = 3.3541e-01, Meta loss averaged over last 500 steps = 3.0084e-01, PNorm = 126.6408, GNorm = 0.2414
Meta loss on this task batch = 3.0566e-01, Meta loss averaged over last 500 steps = 3.0082e-01, PNorm = 126.6469, GNorm = 0.2088
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 3.0078e-01, PNorm = 126.6530, GNorm = 0.1936
Meta loss on this task batch = 2.8815e-01, Meta loss averaged over last 500 steps = 3.0086e-01, PNorm = 126.6592, GNorm = 0.2216
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 3.0073e-01, PNorm = 126.6655, GNorm = 0.1835
Meta loss on this task batch = 2.9172e-01, Meta loss averaged over last 500 steps = 3.0065e-01, PNorm = 126.6726, GNorm = 0.1907
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 3.0055e-01, PNorm = 126.6799, GNorm = 0.2053
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 3.0056e-01, PNorm = 126.6857, GNorm = 0.2453
Meta loss on this task batch = 2.8877e-01, Meta loss averaged over last 500 steps = 3.0048e-01, PNorm = 126.6916, GNorm = 0.2352
Meta loss on this task batch = 2.6149e-01, Meta loss averaged over last 500 steps = 3.0040e-01, PNorm = 126.6978, GNorm = 0.2106
Meta loss on this task batch = 3.0041e-01, Meta loss averaged over last 500 steps = 3.0050e-01, PNorm = 126.7041, GNorm = 0.2328
Meta loss on this task batch = 3.3874e-01, Meta loss averaged over last 500 steps = 3.0056e-01, PNorm = 126.7108, GNorm = 0.2720
Took 114.78811645507812 seconds to complete one epoch of meta training
Took 122.70279479026794 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465638
Epoch 413
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 3.0064e-01, PNorm = 126.7176, GNorm = 0.2594
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 3.0041e-01, PNorm = 126.7237, GNorm = 0.2376
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 3.0048e-01, PNorm = 126.7303, GNorm = 0.2333
Meta loss on this task batch = 3.2941e-01, Meta loss averaged over last 500 steps = 3.0054e-01, PNorm = 126.7367, GNorm = 0.2249
Meta loss on this task batch = 2.2686e-01, Meta loss averaged over last 500 steps = 3.0046e-01, PNorm = 126.7445, GNorm = 0.1982
Meta loss on this task batch = 3.1140e-01, Meta loss averaged over last 500 steps = 3.0046e-01, PNorm = 126.7526, GNorm = 0.2126
Meta loss on this task batch = 2.7468e-01, Meta loss averaged over last 500 steps = 3.0038e-01, PNorm = 126.7607, GNorm = 0.2154
Meta loss on this task batch = 3.0142e-01, Meta loss averaged over last 500 steps = 3.0042e-01, PNorm = 126.7683, GNorm = 0.2321
Meta loss on this task batch = 3.0275e-01, Meta loss averaged over last 500 steps = 3.0038e-01, PNorm = 126.7753, GNorm = 0.2583
Meta loss on this task batch = 2.4155e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 126.7813, GNorm = 0.2547
Meta loss on this task batch = 2.7164e-01, Meta loss averaged over last 500 steps = 3.0021e-01, PNorm = 126.7868, GNorm = 0.2068
Meta loss on this task batch = 3.3304e-01, Meta loss averaged over last 500 steps = 3.0028e-01, PNorm = 126.7917, GNorm = 0.2551
Meta loss on this task batch = 2.3758e-01, Meta loss averaged over last 500 steps = 3.0015e-01, PNorm = 126.7969, GNorm = 0.2016
Meta loss on this task batch = 3.1834e-01, Meta loss averaged over last 500 steps = 3.0015e-01, PNorm = 126.8028, GNorm = 0.2303
Meta loss on this task batch = 3.1728e-01, Meta loss averaged over last 500 steps = 3.0012e-01, PNorm = 126.8084, GNorm = 0.2254
Meta loss on this task batch = 3.1972e-01, Meta loss averaged over last 500 steps = 3.0019e-01, PNorm = 126.8143, GNorm = 0.2245
Meta loss on this task batch = 3.0778e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 126.8202, GNorm = 0.2214
Meta loss on this task batch = 3.1722e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 126.8252, GNorm = 0.2375
Meta loss on this task batch = 3.0420e-01, Meta loss averaged over last 500 steps = 3.0031e-01, PNorm = 126.8303, GNorm = 0.2499
Took 112.66652941703796 seconds to complete one epoch of meta training
Took 120.19917154312134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488844
Epoch 414
Meta loss on this task batch = 3.1316e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 126.8362, GNorm = 0.2081
Meta loss on this task batch = 2.9500e-01, Meta loss averaged over last 500 steps = 3.0034e-01, PNorm = 126.8433, GNorm = 0.2358
Meta loss on this task batch = 2.4578e-01, Meta loss averaged over last 500 steps = 3.0018e-01, PNorm = 126.8510, GNorm = 0.1826
Meta loss on this task batch = 2.9618e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 126.8593, GNorm = 0.2081
Meta loss on this task batch = 2.5086e-01, Meta loss averaged over last 500 steps = 3.0014e-01, PNorm = 126.8687, GNorm = 0.2035
Meta loss on this task batch = 2.5079e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 126.8788, GNorm = 0.1988
Meta loss on this task batch = 3.7637e-01, Meta loss averaged over last 500 steps = 3.0017e-01, PNorm = 126.8878, GNorm = 0.2525
Meta loss on this task batch = 2.4238e-01, Meta loss averaged over last 500 steps = 3.0013e-01, PNorm = 126.8974, GNorm = 0.2071
Meta loss on this task batch = 3.2297e-01, Meta loss averaged over last 500 steps = 3.0024e-01, PNorm = 126.9049, GNorm = 0.2364
Meta loss on this task batch = 2.9233e-01, Meta loss averaged over last 500 steps = 3.0025e-01, PNorm = 126.9113, GNorm = 0.2093
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 3.0023e-01, PNorm = 126.9173, GNorm = 0.2377
Meta loss on this task batch = 2.6676e-01, Meta loss averaged over last 500 steps = 3.0004e-01, PNorm = 126.9239, GNorm = 0.2133
Meta loss on this task batch = 3.2055e-01, Meta loss averaged over last 500 steps = 3.0014e-01, PNorm = 126.9305, GNorm = 0.2136
Meta loss on this task batch = 2.9500e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 126.9364, GNorm = 0.2224
Meta loss on this task batch = 3.0995e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 126.9415, GNorm = 0.2240
Meta loss on this task batch = 2.5331e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 126.9458, GNorm = 0.2085
Meta loss on this task batch = 2.9371e-01, Meta loss averaged over last 500 steps = 2.9995e-01, PNorm = 126.9503, GNorm = 0.2239
Meta loss on this task batch = 3.1763e-01, Meta loss averaged over last 500 steps = 3.0005e-01, PNorm = 126.9544, GNorm = 0.2496
Meta loss on this task batch = 3.1869e-01, Meta loss averaged over last 500 steps = 2.9996e-01, PNorm = 126.9581, GNorm = 0.2566
Took 114.50908041000366 seconds to complete one epoch of meta training
Took 122.20841860771179 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502284
Epoch 415
Meta loss on this task batch = 3.1330e-01, Meta loss averaged over last 500 steps = 2.9998e-01, PNorm = 126.9629, GNorm = 0.2309
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.9992e-01, PNorm = 126.9698, GNorm = 0.2571
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 2.9989e-01, PNorm = 126.9773, GNorm = 0.2088
Meta loss on this task batch = 3.3939e-01, Meta loss averaged over last 500 steps = 3.0003e-01, PNorm = 126.9854, GNorm = 0.2840
Meta loss on this task batch = 2.6034e-01, Meta loss averaged over last 500 steps = 3.0003e-01, PNorm = 126.9931, GNorm = 0.2010
Meta loss on this task batch = 3.2768e-01, Meta loss averaged over last 500 steps = 3.0001e-01, PNorm = 126.9988, GNorm = 0.2516
Meta loss on this task batch = 2.7432e-01, Meta loss averaged over last 500 steps = 2.9986e-01, PNorm = 127.0030, GNorm = 0.2292
Meta loss on this task batch = 3.4180e-01, Meta loss averaged over last 500 steps = 2.9989e-01, PNorm = 127.0087, GNorm = 0.2507
Meta loss on this task batch = 3.1041e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 127.0146, GNorm = 0.2104
Meta loss on this task batch = 3.0293e-01, Meta loss averaged over last 500 steps = 3.0004e-01, PNorm = 127.0194, GNorm = 0.2380
Meta loss on this task batch = 2.7337e-01, Meta loss averaged over last 500 steps = 3.0008e-01, PNorm = 127.0251, GNorm = 0.2255
Meta loss on this task batch = 2.5213e-01, Meta loss averaged over last 500 steps = 2.9999e-01, PNorm = 127.0312, GNorm = 0.1963
Meta loss on this task batch = 2.7784e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 127.0379, GNorm = 0.2276
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 2.9993e-01, PNorm = 127.0451, GNorm = 0.2534
Meta loss on this task batch = 3.1146e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 127.0522, GNorm = 0.2425
Meta loss on this task batch = 2.8217e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 127.0584, GNorm = 0.2459
Meta loss on this task batch = 2.8876e-01, Meta loss averaged over last 500 steps = 3.0000e-01, PNorm = 127.0642, GNorm = 0.2235
Meta loss on this task batch = 3.0017e-01, Meta loss averaged over last 500 steps = 2.9988e-01, PNorm = 127.0700, GNorm = 0.2234
Meta loss on this task batch = 3.3471e-01, Meta loss averaged over last 500 steps = 2.9991e-01, PNorm = 127.0748, GNorm = 0.2702
Took 114.43742752075195 seconds to complete one epoch of meta training
Took 122.13345098495483 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488711
Epoch 416
Meta loss on this task batch = 2.8339e-01, Meta loss averaged over last 500 steps = 2.9991e-01, PNorm = 127.0799, GNorm = 0.2118
Meta loss on this task batch = 2.7317e-01, Meta loss averaged over last 500 steps = 2.9985e-01, PNorm = 127.0865, GNorm = 0.2499
Meta loss on this task batch = 3.2509e-01, Meta loss averaged over last 500 steps = 2.9994e-01, PNorm = 127.0925, GNorm = 0.2204
Meta loss on this task batch = 3.2112e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 127.0989, GNorm = 0.2472
Meta loss on this task batch = 3.2136e-01, Meta loss averaged over last 500 steps = 2.9989e-01, PNorm = 127.1058, GNorm = 0.2016
Meta loss on this task batch = 3.1970e-01, Meta loss averaged over last 500 steps = 2.9985e-01, PNorm = 127.1134, GNorm = 0.2139
Meta loss on this task batch = 2.3201e-01, Meta loss averaged over last 500 steps = 2.9971e-01, PNorm = 127.1202, GNorm = 0.2631
Meta loss on this task batch = 2.8317e-01, Meta loss averaged over last 500 steps = 2.9964e-01, PNorm = 127.1269, GNorm = 0.2548
Meta loss on this task batch = 2.8997e-01, Meta loss averaged over last 500 steps = 2.9956e-01, PNorm = 127.1340, GNorm = 0.2882
Meta loss on this task batch = 3.0320e-01, Meta loss averaged over last 500 steps = 2.9960e-01, PNorm = 127.1408, GNorm = 0.2132
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 2.9964e-01, PNorm = 127.1479, GNorm = 0.2338
Meta loss on this task batch = 2.5489e-01, Meta loss averaged over last 500 steps = 2.9953e-01, PNorm = 127.1559, GNorm = 0.2342
Meta loss on this task batch = 2.6984e-01, Meta loss averaged over last 500 steps = 2.9954e-01, PNorm = 127.1642, GNorm = 0.2574
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 2.9955e-01, PNorm = 127.1713, GNorm = 0.2307
Meta loss on this task batch = 2.8085e-01, Meta loss averaged over last 500 steps = 2.9952e-01, PNorm = 127.1780, GNorm = 0.2332
Meta loss on this task batch = 2.9549e-01, Meta loss averaged over last 500 steps = 2.9963e-01, PNorm = 127.1845, GNorm = 0.2235
Meta loss on this task batch = 2.9515e-01, Meta loss averaged over last 500 steps = 2.9952e-01, PNorm = 127.1912, GNorm = 0.2370
Meta loss on this task batch = 3.5958e-01, Meta loss averaged over last 500 steps = 2.9966e-01, PNorm = 127.1971, GNorm = 0.2458
Meta loss on this task batch = 3.3775e-01, Meta loss averaged over last 500 steps = 2.9959e-01, PNorm = 127.2042, GNorm = 0.2661
Took 116.12150883674622 seconds to complete one epoch of meta training
Took 123.95104026794434 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504056
Epoch 417
Meta loss on this task batch = 3.5627e-01, Meta loss averaged over last 500 steps = 2.9974e-01, PNorm = 127.2103, GNorm = 0.2706
Meta loss on this task batch = 3.8032e-01, Meta loss averaged over last 500 steps = 2.9990e-01, PNorm = 127.2165, GNorm = 0.2941
Meta loss on this task batch = 3.2093e-01, Meta loss averaged over last 500 steps = 2.9998e-01, PNorm = 127.2224, GNorm = 0.2446
Meta loss on this task batch = 3.1654e-01, Meta loss averaged over last 500 steps = 2.9997e-01, PNorm = 127.2280, GNorm = 0.2307
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 2.9985e-01, PNorm = 127.2333, GNorm = 0.2213
Meta loss on this task batch = 2.4793e-01, Meta loss averaged over last 500 steps = 2.9977e-01, PNorm = 127.2403, GNorm = 0.2012
Meta loss on this task batch = 3.3069e-01, Meta loss averaged over last 500 steps = 2.9979e-01, PNorm = 127.2488, GNorm = 0.2267
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.9979e-01, PNorm = 127.2585, GNorm = 0.2024
Meta loss on this task batch = 2.9459e-01, Meta loss averaged over last 500 steps = 2.9975e-01, PNorm = 127.2672, GNorm = 0.2207
Meta loss on this task batch = 2.7854e-01, Meta loss averaged over last 500 steps = 2.9958e-01, PNorm = 127.2764, GNorm = 0.1892
Meta loss on this task batch = 2.8814e-01, Meta loss averaged over last 500 steps = 2.9961e-01, PNorm = 127.2849, GNorm = 0.2253
Meta loss on this task batch = 2.7333e-01, Meta loss averaged over last 500 steps = 2.9964e-01, PNorm = 127.2925, GNorm = 0.1941
Meta loss on this task batch = 2.5955e-01, Meta loss averaged over last 500 steps = 2.9962e-01, PNorm = 127.3004, GNorm = 0.2174
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 2.9963e-01, PNorm = 127.3087, GNorm = 0.2112
Meta loss on this task batch = 2.8136e-01, Meta loss averaged over last 500 steps = 2.9953e-01, PNorm = 127.3165, GNorm = 0.2455
Meta loss on this task batch = 2.9284e-01, Meta loss averaged over last 500 steps = 2.9961e-01, PNorm = 127.3238, GNorm = 0.2064
Meta loss on this task batch = 3.1159e-01, Meta loss averaged over last 500 steps = 2.9956e-01, PNorm = 127.3304, GNorm = 0.2430
Meta loss on this task batch = 2.8369e-01, Meta loss averaged over last 500 steps = 2.9945e-01, PNorm = 127.3374, GNorm = 0.2485
Meta loss on this task batch = 2.9561e-01, Meta loss averaged over last 500 steps = 2.9945e-01, PNorm = 127.3440, GNorm = 0.2355
Took 113.52924847602844 seconds to complete one epoch of meta training
Took 121.59914588928223 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503684
Epoch 418
Meta loss on this task batch = 3.3527e-01, Meta loss averaged over last 500 steps = 2.9956e-01, PNorm = 127.3500, GNorm = 0.2523
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 2.9972e-01, PNorm = 127.3554, GNorm = 0.2508
Meta loss on this task batch = 2.6074e-01, Meta loss averaged over last 500 steps = 2.9959e-01, PNorm = 127.3608, GNorm = 0.2011
Meta loss on this task batch = 3.1416e-01, Meta loss averaged over last 500 steps = 2.9952e-01, PNorm = 127.3669, GNorm = 0.2178
Meta loss on this task batch = 2.9016e-01, Meta loss averaged over last 500 steps = 2.9950e-01, PNorm = 127.3736, GNorm = 0.2082
Meta loss on this task batch = 2.6895e-01, Meta loss averaged over last 500 steps = 2.9941e-01, PNorm = 127.3809, GNorm = 0.2024
Meta loss on this task batch = 3.1848e-01, Meta loss averaged over last 500 steps = 2.9936e-01, PNorm = 127.3875, GNorm = 0.2340
Meta loss on this task batch = 2.8487e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 127.3949, GNorm = 0.2575
Meta loss on this task batch = 3.0422e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 127.4019, GNorm = 0.2015
Meta loss on this task batch = 2.8272e-01, Meta loss averaged over last 500 steps = 2.9929e-01, PNorm = 127.4081, GNorm = 0.2009
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 2.9926e-01, PNorm = 127.4152, GNorm = 0.1981
Meta loss on this task batch = 2.8971e-01, Meta loss averaged over last 500 steps = 2.9936e-01, PNorm = 127.4224, GNorm = 0.2202
Meta loss on this task batch = 3.1999e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 127.4292, GNorm = 0.2419
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.9903e-01, PNorm = 127.4369, GNorm = 0.2266
Meta loss on this task batch = 2.6251e-01, Meta loss averaged over last 500 steps = 2.9888e-01, PNorm = 127.4443, GNorm = 0.2092
Meta loss on this task batch = 3.4498e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 127.4522, GNorm = 0.2459
Meta loss on this task batch = 3.6176e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 127.4601, GNorm = 0.2332
Meta loss on this task batch = 2.9864e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 127.4678, GNorm = 0.2190
Meta loss on this task batch = 2.9161e-01, Meta loss averaged over last 500 steps = 2.9901e-01, PNorm = 127.4757, GNorm = 0.2576
Took 113.42550301551819 seconds to complete one epoch of meta training
Took 121.49258828163147 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481017
Epoch 419
Meta loss on this task batch = 3.1291e-01, Meta loss averaged over last 500 steps = 2.9908e-01, PNorm = 127.4824, GNorm = 0.2546
Meta loss on this task batch = 2.7807e-01, Meta loss averaged over last 500 steps = 2.9902e-01, PNorm = 127.4903, GNorm = 0.2325
Meta loss on this task batch = 3.2314e-01, Meta loss averaged over last 500 steps = 2.9899e-01, PNorm = 127.4981, GNorm = 0.2463
Meta loss on this task batch = 2.8102e-01, Meta loss averaged over last 500 steps = 2.9899e-01, PNorm = 127.5061, GNorm = 0.2367
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 127.5133, GNorm = 0.2285
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.9895e-01, PNorm = 127.5201, GNorm = 0.2007
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 2.9886e-01, PNorm = 127.5264, GNorm = 0.1968
Meta loss on this task batch = 3.4500e-01, Meta loss averaged over last 500 steps = 2.9892e-01, PNorm = 127.5324, GNorm = 0.2166
Meta loss on this task batch = 3.1221e-01, Meta loss averaged over last 500 steps = 2.9891e-01, PNorm = 127.5383, GNorm = 0.2162
Meta loss on this task batch = 3.2789e-01, Meta loss averaged over last 500 steps = 2.9894e-01, PNorm = 127.5433, GNorm = 0.2051
Meta loss on this task batch = 2.8900e-01, Meta loss averaged over last 500 steps = 2.9888e-01, PNorm = 127.5479, GNorm = 0.2332
Meta loss on this task batch = 3.3484e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 127.5528, GNorm = 0.2147
Meta loss on this task batch = 2.9412e-01, Meta loss averaged over last 500 steps = 2.9904e-01, PNorm = 127.5589, GNorm = 0.2220
Meta loss on this task batch = 3.5662e-01, Meta loss averaged over last 500 steps = 2.9912e-01, PNorm = 127.5649, GNorm = 0.2290
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.9904e-01, PNorm = 127.5722, GNorm = 0.2072
Meta loss on this task batch = 3.3627e-01, Meta loss averaged over last 500 steps = 2.9909e-01, PNorm = 127.5800, GNorm = 0.2262
Meta loss on this task batch = 3.4172e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 127.5876, GNorm = 0.2390
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.9912e-01, PNorm = 127.5951, GNorm = 0.2003
Meta loss on this task batch = 3.2608e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 127.6027, GNorm = 0.2593
Took 111.68560028076172 seconds to complete one epoch of meta training
Took 119.41699409484863 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495527
Epoch 420
Meta loss on this task batch = 3.4093e-01, Meta loss averaged over last 500 steps = 2.9945e-01, PNorm = 127.6101, GNorm = 0.2584
Meta loss on this task batch = 2.6178e-01, Meta loss averaged over last 500 steps = 2.9935e-01, PNorm = 127.6181, GNorm = 0.1970
Meta loss on this task batch = 2.9155e-01, Meta loss averaged over last 500 steps = 2.9930e-01, PNorm = 127.6255, GNorm = 0.2073
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 127.6323, GNorm = 0.2100
Meta loss on this task batch = 3.1201e-01, Meta loss averaged over last 500 steps = 2.9921e-01, PNorm = 127.6384, GNorm = 0.2777
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 127.6451, GNorm = 0.2351
Meta loss on this task batch = 2.6973e-01, Meta loss averaged over last 500 steps = 2.9918e-01, PNorm = 127.6508, GNorm = 0.2378
Meta loss on this task batch = 3.5932e-01, Meta loss averaged over last 500 steps = 2.9940e-01, PNorm = 127.6553, GNorm = 0.2705
Meta loss on this task batch = 2.7383e-01, Meta loss averaged over last 500 steps = 2.9937e-01, PNorm = 127.6586, GNorm = 0.2416
Meta loss on this task batch = 3.2062e-01, Meta loss averaged over last 500 steps = 2.9946e-01, PNorm = 127.6626, GNorm = 0.2436
Meta loss on this task batch = 2.6163e-01, Meta loss averaged over last 500 steps = 2.9941e-01, PNorm = 127.6665, GNorm = 0.2064
Meta loss on this task batch = 2.7010e-01, Meta loss averaged over last 500 steps = 2.9918e-01, PNorm = 127.6711, GNorm = 0.1969
Meta loss on this task batch = 2.9986e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 127.6773, GNorm = 0.2310
Meta loss on this task batch = 3.5243e-01, Meta loss averaged over last 500 steps = 2.9939e-01, PNorm = 127.6831, GNorm = 0.2191
Meta loss on this task batch = 3.6838e-01, Meta loss averaged over last 500 steps = 2.9954e-01, PNorm = 127.6887, GNorm = 0.2385
Meta loss on this task batch = 2.8364e-01, Meta loss averaged over last 500 steps = 2.9949e-01, PNorm = 127.6952, GNorm = 0.2221
Meta loss on this task batch = 2.5132e-01, Meta loss averaged over last 500 steps = 2.9929e-01, PNorm = 127.7024, GNorm = 0.1987
Meta loss on this task batch = 2.7461e-01, Meta loss averaged over last 500 steps = 2.9941e-01, PNorm = 127.7099, GNorm = 0.2035
Meta loss on this task batch = 2.7575e-01, Meta loss averaged over last 500 steps = 2.9936e-01, PNorm = 127.7180, GNorm = 0.2338
Took 112.27811098098755 seconds to complete one epoch of meta training
Took 119.7622594833374 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484054
Epoch 421
Meta loss on this task batch = 2.1021e-01, Meta loss averaged over last 500 steps = 2.9911e-01, PNorm = 127.7273, GNorm = 0.1807
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 127.7363, GNorm = 0.2242
Meta loss on this task batch = 3.1533e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 127.7441, GNorm = 0.2497
Meta loss on this task batch = 3.5896e-01, Meta loss averaged over last 500 steps = 2.9933e-01, PNorm = 127.7518, GNorm = 0.2235
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 127.7594, GNorm = 0.2116
Meta loss on this task batch = 3.1604e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 127.7648, GNorm = 0.2714
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.9927e-01, PNorm = 127.7715, GNorm = 0.2342
Meta loss on this task batch = 2.8585e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 127.7781, GNorm = 0.2420
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 127.7835, GNorm = 0.2213
Meta loss on this task batch = 3.3749e-01, Meta loss averaged over last 500 steps = 2.9935e-01, PNorm = 127.7893, GNorm = 0.2789
Meta loss on this task batch = 2.8772e-01, Meta loss averaged over last 500 steps = 2.9931e-01, PNorm = 127.7954, GNorm = 0.2160
Meta loss on this task batch = 3.0466e-01, Meta loss averaged over last 500 steps = 2.9928e-01, PNorm = 127.8005, GNorm = 0.2286
Meta loss on this task batch = 3.0462e-01, Meta loss averaged over last 500 steps = 2.9932e-01, PNorm = 127.8053, GNorm = 0.2276
Meta loss on this task batch = 3.0754e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 127.8110, GNorm = 0.2091
Meta loss on this task batch = 3.1678e-01, Meta loss averaged over last 500 steps = 2.9923e-01, PNorm = 127.8170, GNorm = 0.2002
Meta loss on this task batch = 2.5565e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 127.8227, GNorm = 0.2061
Meta loss on this task batch = 3.2742e-01, Meta loss averaged over last 500 steps = 2.9922e-01, PNorm = 127.8273, GNorm = 0.2441
Meta loss on this task batch = 3.2060e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 127.8324, GNorm = 0.2248
Meta loss on this task batch = 3.3212e-01, Meta loss averaged over last 500 steps = 2.9920e-01, PNorm = 127.8383, GNorm = 0.2793
Took 114.56339573860168 seconds to complete one epoch of meta training
Took 122.8215606212616 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494595
Epoch 422
Meta loss on this task batch = 2.9150e-01, Meta loss averaged over last 500 steps = 2.9917e-01, PNorm = 127.8445, GNorm = 0.1953
Meta loss on this task batch = 2.3122e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 127.8509, GNorm = 0.1802
Meta loss on this task batch = 3.6885e-01, Meta loss averaged over last 500 steps = 2.9910e-01, PNorm = 127.8570, GNorm = 0.2502
Meta loss on this task batch = 2.7449e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 127.8644, GNorm = 0.2013
Meta loss on this task batch = 2.6845e-01, Meta loss averaged over last 500 steps = 2.9901e-01, PNorm = 127.8726, GNorm = 0.2087
Meta loss on this task batch = 3.0316e-01, Meta loss averaged over last 500 steps = 2.9905e-01, PNorm = 127.8805, GNorm = 0.2159
Meta loss on this task batch = 3.5880e-01, Meta loss averaged over last 500 steps = 2.9911e-01, PNorm = 127.8876, GNorm = 0.2328
Meta loss on this task batch = 3.1793e-01, Meta loss averaged over last 500 steps = 2.9914e-01, PNorm = 127.8944, GNorm = 0.2078
Meta loss on this task batch = 3.3302e-01, Meta loss averaged over last 500 steps = 2.9916e-01, PNorm = 127.9014, GNorm = 0.2288
Meta loss on this task batch = 3.3377e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 127.9052, GNorm = 0.2841
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.9918e-01, PNorm = 127.9094, GNorm = 0.2245
Meta loss on this task batch = 2.4849e-01, Meta loss averaged over last 500 steps = 2.9905e-01, PNorm = 127.9139, GNorm = 0.1877
Meta loss on this task batch = 2.7179e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 127.9202, GNorm = 0.2212
Meta loss on this task batch = 3.0571e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 127.9268, GNorm = 0.2024
Meta loss on this task batch = 3.2274e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 127.9337, GNorm = 0.2099
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 127.9402, GNorm = 0.2163
Meta loss on this task batch = 2.2991e-01, Meta loss averaged over last 500 steps = 2.9894e-01, PNorm = 127.9471, GNorm = 0.1824
Meta loss on this task batch = 2.8084e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 127.9544, GNorm = 0.2151
Meta loss on this task batch = 3.1427e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 127.9614, GNorm = 0.2794
Took 115.1706440448761 seconds to complete one epoch of meta training
Took 122.49487042427063 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481859
Epoch 423
Meta loss on this task batch = 3.3755e-01, Meta loss averaged over last 500 steps = 2.9908e-01, PNorm = 127.9681, GNorm = 0.2156
Meta loss on this task batch = 3.0724e-01, Meta loss averaged over last 500 steps = 2.9912e-01, PNorm = 127.9745, GNorm = 0.2583
Meta loss on this task batch = 2.3709e-01, Meta loss averaged over last 500 steps = 2.9899e-01, PNorm = 127.9824, GNorm = 0.2212
Meta loss on this task batch = 2.9319e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 127.9897, GNorm = 0.2363
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.9884e-01, PNorm = 127.9962, GNorm = 0.2858
Meta loss on this task batch = 3.2560e-01, Meta loss averaged over last 500 steps = 2.9893e-01, PNorm = 128.0038, GNorm = 0.2411
Meta loss on this task batch = 2.8199e-01, Meta loss averaged over last 500 steps = 2.9883e-01, PNorm = 128.0116, GNorm = 0.2029
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 128.0192, GNorm = 0.2016
Meta loss on this task batch = 2.7388e-01, Meta loss averaged over last 500 steps = 2.9878e-01, PNorm = 128.0272, GNorm = 0.2339
Meta loss on this task batch = 2.9455e-01, Meta loss averaged over last 500 steps = 2.9885e-01, PNorm = 128.0352, GNorm = 0.2331
Meta loss on this task batch = 2.7814e-01, Meta loss averaged over last 500 steps = 2.9877e-01, PNorm = 128.0426, GNorm = 0.2131
Meta loss on this task batch = 3.1972e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 128.0484, GNorm = 0.2170
Meta loss on this task batch = 3.0627e-01, Meta loss averaged over last 500 steps = 2.9889e-01, PNorm = 128.0544, GNorm = 0.2267
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.9880e-01, PNorm = 128.0608, GNorm = 0.2047
Meta loss on this task batch = 3.4043e-01, Meta loss averaged over last 500 steps = 2.9886e-01, PNorm = 128.0675, GNorm = 0.2390
Meta loss on this task batch = 3.2140e-01, Meta loss averaged over last 500 steps = 2.9892e-01, PNorm = 128.0743, GNorm = 0.2559
Meta loss on this task batch = 2.7364e-01, Meta loss averaged over last 500 steps = 2.9871e-01, PNorm = 128.0810, GNorm = 0.2209
Meta loss on this task batch = 3.1536e-01, Meta loss averaged over last 500 steps = 2.9883e-01, PNorm = 128.0877, GNorm = 0.2248
Meta loss on this task batch = 3.1432e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 128.0942, GNorm = 0.2427
Took 115.74766802787781 seconds to complete one epoch of meta training
Took 123.4408164024353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478894
Epoch 424
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.9877e-01, PNorm = 128.1008, GNorm = 0.2116
Meta loss on this task batch = 2.3209e-01, Meta loss averaged over last 500 steps = 2.9870e-01, PNorm = 128.1079, GNorm = 0.1755
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 2.9858e-01, PNorm = 128.1154, GNorm = 0.1771
Meta loss on this task batch = 2.9755e-01, Meta loss averaged over last 500 steps = 2.9861e-01, PNorm = 128.1220, GNorm = 0.2322
Meta loss on this task batch = 2.8262e-01, Meta loss averaged over last 500 steps = 2.9854e-01, PNorm = 128.1290, GNorm = 0.1960
Meta loss on this task batch = 3.3661e-01, Meta loss averaged over last 500 steps = 2.9874e-01, PNorm = 128.1353, GNorm = 0.2350
Meta loss on this task batch = 2.8783e-01, Meta loss averaged over last 500 steps = 2.9876e-01, PNorm = 128.1411, GNorm = 0.2218
Meta loss on this task batch = 2.8184e-01, Meta loss averaged over last 500 steps = 2.9864e-01, PNorm = 128.1462, GNorm = 0.2351
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 2.9854e-01, PNorm = 128.1511, GNorm = 0.2041
Meta loss on this task batch = 3.4032e-01, Meta loss averaged over last 500 steps = 2.9871e-01, PNorm = 128.1550, GNorm = 0.2403
Meta loss on this task batch = 3.0245e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 128.1594, GNorm = 0.2448
Meta loss on this task batch = 3.2395e-01, Meta loss averaged over last 500 steps = 2.9886e-01, PNorm = 128.1639, GNorm = 0.2229
Meta loss on this task batch = 2.2751e-01, Meta loss averaged over last 500 steps = 2.9882e-01, PNorm = 128.1699, GNorm = 0.1975
Meta loss on this task batch = 3.8490e-01, Meta loss averaged over last 500 steps = 2.9886e-01, PNorm = 128.1761, GNorm = 0.3198
Meta loss on this task batch = 3.2410e-01, Meta loss averaged over last 500 steps = 2.9896e-01, PNorm = 128.1828, GNorm = 0.2723
Meta loss on this task batch = 3.3094e-01, Meta loss averaged over last 500 steps = 2.9911e-01, PNorm = 128.1904, GNorm = 0.2530
Meta loss on this task batch = 2.5787e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 128.1984, GNorm = 0.2195
Meta loss on this task batch = 2.8332e-01, Meta loss averaged over last 500 steps = 2.9915e-01, PNorm = 128.2065, GNorm = 0.2151
Meta loss on this task batch = 3.4023e-01, Meta loss averaged over last 500 steps = 2.9911e-01, PNorm = 128.2142, GNorm = 0.2554
Took 149.99873208999634 seconds to complete one epoch of meta training
Took 157.19226503372192 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479726
Epoch 425
Meta loss on this task batch = 2.8097e-01, Meta loss averaged over last 500 steps = 2.9897e-01, PNorm = 128.2231, GNorm = 0.2273
Meta loss on this task batch = 2.3185e-01, Meta loss averaged over last 500 steps = 2.9879e-01, PNorm = 128.2323, GNorm = 0.2003
Meta loss on this task batch = 3.6760e-01, Meta loss averaged over last 500 steps = 2.9890e-01, PNorm = 128.2407, GNorm = 0.2713
Meta loss on this task batch = 3.3512e-01, Meta loss averaged over last 500 steps = 2.9900e-01, PNorm = 128.2489, GNorm = 0.2375
Meta loss on this task batch = 3.2117e-01, Meta loss averaged over last 500 steps = 2.9918e-01, PNorm = 128.2561, GNorm = 0.2313
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.9919e-01, PNorm = 128.2620, GNorm = 0.2525
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.9925e-01, PNorm = 128.2673, GNorm = 0.2616
Meta loss on this task batch = 3.0998e-01, Meta loss averaged over last 500 steps = 2.9923e-01, PNorm = 128.2723, GNorm = 0.2442
Meta loss on this task batch = 3.1729e-01, Meta loss averaged over last 500 steps = 2.9930e-01, PNorm = 128.2778, GNorm = 0.2180
Meta loss on this task batch = 2.6757e-01, Meta loss averaged over last 500 steps = 2.9924e-01, PNorm = 128.2847, GNorm = 0.2308
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.9927e-01, PNorm = 128.2920, GNorm = 0.1788
Meta loss on this task batch = 2.3091e-01, Meta loss averaged over last 500 steps = 2.9899e-01, PNorm = 128.2991, GNorm = 0.1983
Meta loss on this task batch = 3.4706e-01, Meta loss averaged over last 500 steps = 2.9910e-01, PNorm = 128.3060, GNorm = 0.2095
Meta loss on this task batch = 2.6814e-01, Meta loss averaged over last 500 steps = 2.9906e-01, PNorm = 128.3130, GNorm = 0.2397
Meta loss on this task batch = 3.0342e-01, Meta loss averaged over last 500 steps = 2.9913e-01, PNorm = 128.3188, GNorm = 0.2404
Meta loss on this task batch = 2.3480e-01, Meta loss averaged over last 500 steps = 2.9896e-01, PNorm = 128.3257, GNorm = 0.2216
Meta loss on this task batch = 2.7318e-01, Meta loss averaged over last 500 steps = 2.9883e-01, PNorm = 128.3326, GNorm = 0.2218
Meta loss on this task batch = 2.9585e-01, Meta loss averaged over last 500 steps = 2.9881e-01, PNorm = 128.3404, GNorm = 0.2065
Meta loss on this task batch = 3.2033e-01, Meta loss averaged over last 500 steps = 2.9885e-01, PNorm = 128.3476, GNorm = 0.2943
Took 115.41436433792114 seconds to complete one epoch of meta training
Took 123.63557767868042 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475903
Epoch 426
Meta loss on this task batch = 3.1084e-01, Meta loss averaged over last 500 steps = 2.9878e-01, PNorm = 128.3546, GNorm = 0.2764
Meta loss on this task batch = 3.2109e-01, Meta loss averaged over last 500 steps = 2.9894e-01, PNorm = 128.3610, GNorm = 0.2513
Meta loss on this task batch = 2.8141e-01, Meta loss averaged over last 500 steps = 2.9880e-01, PNorm = 128.3675, GNorm = 0.2318
Meta loss on this task batch = 3.1614e-01, Meta loss averaged over last 500 steps = 2.9887e-01, PNorm = 128.3738, GNorm = 0.2355
Meta loss on this task batch = 2.3092e-01, Meta loss averaged over last 500 steps = 2.9872e-01, PNorm = 128.3807, GNorm = 0.2090
Meta loss on this task batch = 2.3657e-01, Meta loss averaged over last 500 steps = 2.9858e-01, PNorm = 128.3874, GNorm = 0.2022
Meta loss on this task batch = 2.8280e-01, Meta loss averaged over last 500 steps = 2.9854e-01, PNorm = 128.3949, GNorm = 0.2255
Meta loss on this task batch = 2.6262e-01, Meta loss averaged over last 500 steps = 2.9846e-01, PNorm = 128.4021, GNorm = 0.2326
Meta loss on this task batch = 3.3403e-01, Meta loss averaged over last 500 steps = 2.9855e-01, PNorm = 128.4085, GNorm = 0.2493
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.9839e-01, PNorm = 128.4156, GNorm = 0.2056
Meta loss on this task batch = 2.9000e-01, Meta loss averaged over last 500 steps = 2.9836e-01, PNorm = 128.4227, GNorm = 0.2522
Meta loss on this task batch = 2.7903e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 128.4307, GNorm = 0.2357
Meta loss on this task batch = 3.1088e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 128.4377, GNorm = 0.2618
Meta loss on this task batch = 3.3617e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 128.4442, GNorm = 0.2378
Meta loss on this task batch = 3.0262e-01, Meta loss averaged over last 500 steps = 2.9837e-01, PNorm = 128.4513, GNorm = 0.2226
Meta loss on this task batch = 3.1336e-01, Meta loss averaged over last 500 steps = 2.9837e-01, PNorm = 128.4580, GNorm = 0.2322
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.9832e-01, PNorm = 128.4634, GNorm = 0.2136
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 2.9831e-01, PNorm = 128.4691, GNorm = 0.2530
Meta loss on this task batch = 2.6488e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 128.4759, GNorm = 0.2451
Took 112.46830248832703 seconds to complete one epoch of meta training
Took 120.10740733146667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482609
Epoch 427
Meta loss on this task batch = 2.3971e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 128.4832, GNorm = 0.1835
Meta loss on this task batch = 2.8806e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 128.4898, GNorm = 0.2415
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 128.4964, GNorm = 0.2514
Meta loss on this task batch = 3.3896e-01, Meta loss averaged over last 500 steps = 2.9830e-01, PNorm = 128.5023, GNorm = 0.2424
Meta loss on this task batch = 3.0677e-01, Meta loss averaged over last 500 steps = 2.9831e-01, PNorm = 128.5080, GNorm = 0.2220
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 128.5142, GNorm = 0.2159
Meta loss on this task batch = 2.8539e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 128.5204, GNorm = 0.2103
Meta loss on this task batch = 2.8806e-01, Meta loss averaged over last 500 steps = 2.9823e-01, PNorm = 128.5272, GNorm = 0.2381
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 128.5345, GNorm = 0.2577
Meta loss on this task batch = 3.3586e-01, Meta loss averaged over last 500 steps = 2.9844e-01, PNorm = 128.5413, GNorm = 0.2264
Meta loss on this task batch = 2.3541e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 128.5471, GNorm = 0.2304
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.9823e-01, PNorm = 128.5535, GNorm = 0.1988
Meta loss on this task batch = 3.1392e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 128.5599, GNorm = 0.1956
Meta loss on this task batch = 2.5488e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 128.5657, GNorm = 0.2079
Meta loss on this task batch = 3.3858e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 128.5721, GNorm = 0.2454
Meta loss on this task batch = 3.4347e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 128.5768, GNorm = 0.2483
Meta loss on this task batch = 2.8141e-01, Meta loss averaged over last 500 steps = 2.9824e-01, PNorm = 128.5813, GNorm = 0.2314
Meta loss on this task batch = 2.4138e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 128.5868, GNorm = 0.1887
Meta loss on this task batch = 2.9975e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 128.5914, GNorm = 0.2817
Took 114.45928859710693 seconds to complete one epoch of meta training
Took 122.53855681419373 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472827
Epoch 428
Meta loss on this task batch = 3.0931e-01, Meta loss averaged over last 500 steps = 2.9834e-01, PNorm = 128.5972, GNorm = 0.2301
Meta loss on this task batch = 3.0404e-01, Meta loss averaged over last 500 steps = 2.9837e-01, PNorm = 128.6037, GNorm = 0.2072
Meta loss on this task batch = 2.6321e-01, Meta loss averaged over last 500 steps = 2.9823e-01, PNorm = 128.6099, GNorm = 0.2001
Meta loss on this task batch = 3.0328e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 128.6161, GNorm = 0.2275
Meta loss on this task batch = 3.0699e-01, Meta loss averaged over last 500 steps = 2.9820e-01, PNorm = 128.6217, GNorm = 0.2604
Meta loss on this task batch = 2.8168e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 128.6267, GNorm = 0.2249
Meta loss on this task batch = 3.2756e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 128.6313, GNorm = 0.2648
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 128.6366, GNorm = 0.2284
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 128.6424, GNorm = 0.1979
Meta loss on this task batch = 3.3929e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 128.6490, GNorm = 0.2251
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.9818e-01, PNorm = 128.6556, GNorm = 0.2069
Meta loss on this task batch = 2.8115e-01, Meta loss averaged over last 500 steps = 2.9819e-01, PNorm = 128.6621, GNorm = 0.2059
Meta loss on this task batch = 3.2714e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 128.6682, GNorm = 0.2915
Meta loss on this task batch = 2.8780e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 128.6754, GNorm = 0.2205
Meta loss on this task batch = 3.1370e-01, Meta loss averaged over last 500 steps = 2.9811e-01, PNorm = 128.6829, GNorm = 0.2166
Meta loss on this task batch = 2.8003e-01, Meta loss averaged over last 500 steps = 2.9806e-01, PNorm = 128.6908, GNorm = 0.2348
Meta loss on this task batch = 3.5565e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 128.6984, GNorm = 0.2293
Meta loss on this task batch = 2.8943e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 128.7065, GNorm = 0.2067
Meta loss on this task batch = 2.7506e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 128.7149, GNorm = 0.2462
Took 113.44565773010254 seconds to complete one epoch of meta training
Took 121.48361992835999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486246
Epoch 429
Meta loss on this task batch = 2.7570e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 128.7237, GNorm = 0.2063
Meta loss on this task batch = 2.8594e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 128.7323, GNorm = 0.2137
Meta loss on this task batch = 2.4158e-01, Meta loss averaged over last 500 steps = 2.9796e-01, PNorm = 128.7408, GNorm = 0.2288
Meta loss on this task batch = 2.7734e-01, Meta loss averaged over last 500 steps = 2.9797e-01, PNorm = 128.7487, GNorm = 0.2328
Meta loss on this task batch = 3.2127e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 128.7551, GNorm = 0.2700
Meta loss on this task batch = 2.5635e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 128.7622, GNorm = 0.2276
Meta loss on this task batch = 3.3921e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 128.7687, GNorm = 0.2644
Meta loss on this task batch = 2.7607e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 128.7756, GNorm = 0.2311
Meta loss on this task batch = 3.1559e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 128.7820, GNorm = 0.2545
Meta loss on this task batch = 2.7899e-01, Meta loss averaged over last 500 steps = 2.9807e-01, PNorm = 128.7884, GNorm = 0.2062
Meta loss on this task batch = 3.3076e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 128.7949, GNorm = 0.2403
Meta loss on this task batch = 2.8995e-01, Meta loss averaged over last 500 steps = 2.9804e-01, PNorm = 128.8020, GNorm = 0.2397
Meta loss on this task batch = 3.4228e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 128.8080, GNorm = 0.2420
Meta loss on this task batch = 3.2250e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 128.8134, GNorm = 0.2342
Meta loss on this task batch = 2.7883e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 128.8196, GNorm = 0.2262
Meta loss on this task batch = 2.6680e-01, Meta loss averaged over last 500 steps = 2.9828e-01, PNorm = 128.8252, GNorm = 0.2118
Meta loss on this task batch = 3.7070e-01, Meta loss averaged over last 500 steps = 2.9838e-01, PNorm = 128.8307, GNorm = 0.2365
Meta loss on this task batch = 3.4414e-01, Meta loss averaged over last 500 steps = 2.9844e-01, PNorm = 128.8363, GNorm = 0.2232
Meta loss on this task batch = 2.5842e-01, Meta loss averaged over last 500 steps = 2.9835e-01, PNorm = 128.8424, GNorm = 0.2315
Took 114.60089445114136 seconds to complete one epoch of meta training
Took 122.07868909835815 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480911
Epoch 430
Meta loss on this task batch = 3.1987e-01, Meta loss averaged over last 500 steps = 2.9829e-01, PNorm = 128.8484, GNorm = 0.1983
Meta loss on this task batch = 2.7417e-01, Meta loss averaged over last 500 steps = 2.9822e-01, PNorm = 128.8545, GNorm = 0.2062
Meta loss on this task batch = 2.7765e-01, Meta loss averaged over last 500 steps = 2.9826e-01, PNorm = 128.8612, GNorm = 0.2085
Meta loss on this task batch = 3.0454e-01, Meta loss averaged over last 500 steps = 2.9821e-01, PNorm = 128.8680, GNorm = 0.1983
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 2.9825e-01, PNorm = 128.8757, GNorm = 0.1871
Meta loss on this task batch = 3.0504e-01, Meta loss averaged over last 500 steps = 2.9815e-01, PNorm = 128.8840, GNorm = 0.2064
Meta loss on this task batch = 2.7296e-01, Meta loss averaged over last 500 steps = 2.9816e-01, PNorm = 128.8928, GNorm = 0.2375
Meta loss on this task batch = 2.8396e-01, Meta loss averaged over last 500 steps = 2.9803e-01, PNorm = 128.9006, GNorm = 0.2029
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.9800e-01, PNorm = 128.9093, GNorm = 0.2204
Meta loss on this task batch = 2.9858e-01, Meta loss averaged over last 500 steps = 2.9812e-01, PNorm = 128.9179, GNorm = 0.2365
Meta loss on this task batch = 2.9202e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 128.9259, GNorm = 0.2482
Meta loss on this task batch = 2.8140e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 128.9330, GNorm = 0.2198
Meta loss on this task batch = 3.0660e-01, Meta loss averaged over last 500 steps = 2.9810e-01, PNorm = 128.9386, GNorm = 0.2303
Meta loss on this task batch = 3.2491e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 128.9434, GNorm = 0.2644
Meta loss on this task batch = 2.6612e-01, Meta loss averaged over last 500 steps = 2.9809e-01, PNorm = 128.9485, GNorm = 0.2092
Meta loss on this task batch = 3.1011e-01, Meta loss averaged over last 500 steps = 2.9814e-01, PNorm = 128.9536, GNorm = 0.2372
Meta loss on this task batch = 2.7023e-01, Meta loss averaged over last 500 steps = 2.9801e-01, PNorm = 128.9592, GNorm = 0.2016
Meta loss on this task batch = 2.5715e-01, Meta loss averaged over last 500 steps = 2.9808e-01, PNorm = 128.9659, GNorm = 0.2396
Meta loss on this task batch = 3.0763e-01, Meta loss averaged over last 500 steps = 2.9817e-01, PNorm = 128.9725, GNorm = 0.2342
Took 115.61134195327759 seconds to complete one epoch of meta training
Took 123.96323323249817 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488181
Epoch 431
Meta loss on this task batch = 2.7944e-01, Meta loss averaged over last 500 steps = 2.9811e-01, PNorm = 128.9787, GNorm = 0.2275
Meta loss on this task batch = 2.3967e-01, Meta loss averaged over last 500 steps = 2.9788e-01, PNorm = 128.9854, GNorm = 0.2320
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.9787e-01, PNorm = 128.9915, GNorm = 0.2521
Meta loss on this task batch = 2.8292e-01, Meta loss averaged over last 500 steps = 2.9776e-01, PNorm = 128.9984, GNorm = 0.2209
Meta loss on this task batch = 2.6373e-01, Meta loss averaged over last 500 steps = 2.9766e-01, PNorm = 129.0050, GNorm = 0.2294
Meta loss on this task batch = 2.7640e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 129.0108, GNorm = 0.2366
Meta loss on this task batch = 3.1494e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 129.0181, GNorm = 0.2789
Meta loss on this task batch = 2.9847e-01, Meta loss averaged over last 500 steps = 2.9763e-01, PNorm = 129.0254, GNorm = 0.2167
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 2.9751e-01, PNorm = 129.0326, GNorm = 0.2171
Meta loss on this task batch = 3.3666e-01, Meta loss averaged over last 500 steps = 2.9761e-01, PNorm = 129.0390, GNorm = 0.2508
Meta loss on this task batch = 3.5570e-01, Meta loss averaged over last 500 steps = 2.9765e-01, PNorm = 129.0453, GNorm = 0.2282
Meta loss on this task batch = 3.1491e-01, Meta loss averaged over last 500 steps = 2.9770e-01, PNorm = 129.0517, GNorm = 0.2492
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 2.9756e-01, PNorm = 129.0588, GNorm = 0.2287
Meta loss on this task batch = 2.4626e-01, Meta loss averaged over last 500 steps = 2.9747e-01, PNorm = 129.0659, GNorm = 0.1803
Meta loss on this task batch = 3.0615e-01, Meta loss averaged over last 500 steps = 2.9753e-01, PNorm = 129.0736, GNorm = 0.2479
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 2.9748e-01, PNorm = 129.0787, GNorm = 0.2836
Meta loss on this task batch = 2.7181e-01, Meta loss averaged over last 500 steps = 2.9742e-01, PNorm = 129.0839, GNorm = 0.2252
Meta loss on this task batch = 2.9061e-01, Meta loss averaged over last 500 steps = 2.9746e-01, PNorm = 129.0892, GNorm = 0.2287
Meta loss on this task batch = 2.7805e-01, Meta loss averaged over last 500 steps = 2.9733e-01, PNorm = 129.0954, GNorm = 0.2818
Took 113.05452680587769 seconds to complete one epoch of meta training
Took 121.02599549293518 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494278
Epoch 432
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.9743e-01, PNorm = 129.1028, GNorm = 0.2104
Meta loss on this task batch = 2.9923e-01, Meta loss averaged over last 500 steps = 2.9739e-01, PNorm = 129.1098, GNorm = 0.2034
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 2.9737e-01, PNorm = 129.1172, GNorm = 0.2201
Meta loss on this task batch = 3.4422e-01, Meta loss averaged over last 500 steps = 2.9746e-01, PNorm = 129.1250, GNorm = 0.2370
Meta loss on this task batch = 3.3541e-01, Meta loss averaged over last 500 steps = 2.9757e-01, PNorm = 129.1322, GNorm = 0.2256
Meta loss on this task batch = 2.3868e-01, Meta loss averaged over last 500 steps = 2.9745e-01, PNorm = 129.1397, GNorm = 0.1997
Meta loss on this task batch = 2.4395e-01, Meta loss averaged over last 500 steps = 2.9745e-01, PNorm = 129.1480, GNorm = 0.2026
Meta loss on this task batch = 2.8381e-01, Meta loss averaged over last 500 steps = 2.9743e-01, PNorm = 129.1564, GNorm = 0.2100
Meta loss on this task batch = 2.9091e-01, Meta loss averaged over last 500 steps = 2.9741e-01, PNorm = 129.1644, GNorm = 0.2213
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 2.9731e-01, PNorm = 129.1718, GNorm = 0.2155
Meta loss on this task batch = 2.9188e-01, Meta loss averaged over last 500 steps = 2.9715e-01, PNorm = 129.1791, GNorm = 0.2470
Meta loss on this task batch = 2.9091e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 129.1857, GNorm = 0.2112
Meta loss on this task batch = 2.4929e-01, Meta loss averaged over last 500 steps = 2.9702e-01, PNorm = 129.1932, GNorm = 0.2171
Meta loss on this task batch = 2.5790e-01, Meta loss averaged over last 500 steps = 2.9699e-01, PNorm = 129.2015, GNorm = 0.2401
Meta loss on this task batch = 3.4315e-01, Meta loss averaged over last 500 steps = 2.9710e-01, PNorm = 129.2079, GNorm = 0.2840
Meta loss on this task batch = 3.2428e-01, Meta loss averaged over last 500 steps = 2.9727e-01, PNorm = 129.2148, GNorm = 0.2419
Meta loss on this task batch = 2.7490e-01, Meta loss averaged over last 500 steps = 2.9714e-01, PNorm = 129.2218, GNorm = 0.2232
Meta loss on this task batch = 2.8365e-01, Meta loss averaged over last 500 steps = 2.9713e-01, PNorm = 129.2293, GNorm = 0.2226
Meta loss on this task batch = 3.0579e-01, Meta loss averaged over last 500 steps = 2.9718e-01, PNorm = 129.2357, GNorm = 0.2941
Took 141.04611802101135 seconds to complete one epoch of meta training
Took 148.41404795646667 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485941
Epoch 433
Meta loss on this task batch = 2.5206e-01, Meta loss averaged over last 500 steps = 2.9711e-01, PNorm = 129.2426, GNorm = 0.2040
Meta loss on this task batch = 2.9509e-01, Meta loss averaged over last 500 steps = 2.9706e-01, PNorm = 129.2501, GNorm = 0.2408
Meta loss on this task batch = 2.7207e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 129.2565, GNorm = 0.2265
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.9688e-01, PNorm = 129.2631, GNorm = 0.2285
Meta loss on this task batch = 3.1953e-01, Meta loss averaged over last 500 steps = 2.9690e-01, PNorm = 129.2693, GNorm = 0.2309
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.9685e-01, PNorm = 129.2756, GNorm = 0.2097
Meta loss on this task batch = 2.8307e-01, Meta loss averaged over last 500 steps = 2.9686e-01, PNorm = 129.2809, GNorm = 0.2013
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 129.2844, GNorm = 0.2636
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 2.9701e-01, PNorm = 129.2881, GNorm = 0.2274
Meta loss on this task batch = 3.8391e-01, Meta loss averaged over last 500 steps = 2.9721e-01, PNorm = 129.2908, GNorm = 0.2309
Meta loss on this task batch = 2.8877e-01, Meta loss averaged over last 500 steps = 2.9723e-01, PNorm = 129.2935, GNorm = 0.2137
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 2.9708e-01, PNorm = 129.2965, GNorm = 0.2201
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.9702e-01, PNorm = 129.3006, GNorm = 0.1986
Meta loss on this task batch = 3.2922e-01, Meta loss averaged over last 500 steps = 2.9710e-01, PNorm = 129.3060, GNorm = 0.2831
Meta loss on this task batch = 3.4527e-01, Meta loss averaged over last 500 steps = 2.9716e-01, PNorm = 129.3120, GNorm = 0.2091
Meta loss on this task batch = 3.3158e-01, Meta loss averaged over last 500 steps = 2.9721e-01, PNorm = 129.3187, GNorm = 0.2204
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.9724e-01, PNorm = 129.3266, GNorm = 0.2191
Meta loss on this task batch = 2.4955e-01, Meta loss averaged over last 500 steps = 2.9707e-01, PNorm = 129.3350, GNorm = 0.2039
Meta loss on this task batch = 2.7002e-01, Meta loss averaged over last 500 steps = 2.9704e-01, PNorm = 129.3428, GNorm = 0.2357
Took 115.59732985496521 seconds to complete one epoch of meta training
Took 123.95933699607849 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501606
Epoch 434
Meta loss on this task batch = 2.8365e-01, Meta loss averaged over last 500 steps = 2.9692e-01, PNorm = 129.3507, GNorm = 0.2521
Meta loss on this task batch = 2.8028e-01, Meta loss averaged over last 500 steps = 2.9676e-01, PNorm = 129.3592, GNorm = 0.1841
Meta loss on this task batch = 3.2123e-01, Meta loss averaged over last 500 steps = 2.9672e-01, PNorm = 129.3681, GNorm = 0.2400
Meta loss on this task batch = 2.5474e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 129.3777, GNorm = 0.2004
Meta loss on this task batch = 3.2218e-01, Meta loss averaged over last 500 steps = 2.9674e-01, PNorm = 129.3862, GNorm = 0.2750
Meta loss on this task batch = 3.2586e-01, Meta loss averaged over last 500 steps = 2.9682e-01, PNorm = 129.3931, GNorm = 0.2570
Meta loss on this task batch = 2.3708e-01, Meta loss averaged over last 500 steps = 2.9672e-01, PNorm = 129.4001, GNorm = 0.2266
Meta loss on this task batch = 2.9892e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 129.4068, GNorm = 0.2357
Meta loss on this task batch = 3.2115e-01, Meta loss averaged over last 500 steps = 2.9662e-01, PNorm = 129.4132, GNorm = 0.2287
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 129.4190, GNorm = 0.2146
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 2.9659e-01, PNorm = 129.4242, GNorm = 0.2186
Meta loss on this task batch = 3.0991e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 129.4297, GNorm = 0.2298
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 129.4356, GNorm = 0.2100
Meta loss on this task batch = 3.4606e-01, Meta loss averaged over last 500 steps = 2.9654e-01, PNorm = 129.4404, GNorm = 0.2815
Meta loss on this task batch = 3.0681e-01, Meta loss averaged over last 500 steps = 2.9663e-01, PNorm = 129.4455, GNorm = 0.2368
Meta loss on this task batch = 2.4661e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 129.4516, GNorm = 0.2088
Meta loss on this task batch = 3.0170e-01, Meta loss averaged over last 500 steps = 2.9665e-01, PNorm = 129.4584, GNorm = 0.2380
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.9675e-01, PNorm = 129.4661, GNorm = 0.2284
Meta loss on this task batch = 2.9690e-01, Meta loss averaged over last 500 steps = 2.9675e-01, PNorm = 129.4744, GNorm = 0.2799
Took 120.7480320930481 seconds to complete one epoch of meta training
Took 128.59424901008606 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499246
Epoch 435
Meta loss on this task batch = 2.4495e-01, Meta loss averaged over last 500 steps = 2.9660e-01, PNorm = 129.4832, GNorm = 0.1983
Meta loss on this task batch = 3.0092e-01, Meta loss averaged over last 500 steps = 2.9659e-01, PNorm = 129.4914, GNorm = 0.2230
Meta loss on this task batch = 3.2765e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 129.4995, GNorm = 0.2913
Meta loss on this task batch = 2.4896e-01, Meta loss averaged over last 500 steps = 2.9649e-01, PNorm = 129.5080, GNorm = 0.2137
Meta loss on this task batch = 3.2272e-01, Meta loss averaged over last 500 steps = 2.9649e-01, PNorm = 129.5163, GNorm = 0.2322
Meta loss on this task batch = 3.1517e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 129.5236, GNorm = 0.2432
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 2.9657e-01, PNorm = 129.5304, GNorm = 0.2129
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 129.5372, GNorm = 0.2143
Meta loss on this task batch = 2.7310e-01, Meta loss averaged over last 500 steps = 2.9640e-01, PNorm = 129.5438, GNorm = 0.2628
Meta loss on this task batch = 3.0542e-01, Meta loss averaged over last 500 steps = 2.9646e-01, PNorm = 129.5515, GNorm = 0.2673
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 2.9658e-01, PNorm = 129.5585, GNorm = 0.2399
Meta loss on this task batch = 2.7423e-01, Meta loss averaged over last 500 steps = 2.9652e-01, PNorm = 129.5656, GNorm = 0.2100
Meta loss on this task batch = 3.0667e-01, Meta loss averaged over last 500 steps = 2.9651e-01, PNorm = 129.5726, GNorm = 0.2112
Meta loss on this task batch = 3.0744e-01, Meta loss averaged over last 500 steps = 2.9648e-01, PNorm = 129.5789, GNorm = 0.2613
Meta loss on this task batch = 3.0039e-01, Meta loss averaged over last 500 steps = 2.9636e-01, PNorm = 129.5848, GNorm = 0.2311
Meta loss on this task batch = 3.1379e-01, Meta loss averaged over last 500 steps = 2.9644e-01, PNorm = 129.5905, GNorm = 0.2343
Meta loss on this task batch = 3.0624e-01, Meta loss averaged over last 500 steps = 2.9671e-01, PNorm = 129.5965, GNorm = 0.2353
Meta loss on this task batch = 2.8509e-01, Meta loss averaged over last 500 steps = 2.9664e-01, PNorm = 129.6028, GNorm = 0.2155
Meta loss on this task batch = 3.0762e-01, Meta loss averaged over last 500 steps = 2.9662e-01, PNorm = 129.6099, GNorm = 0.2738
Took 113.67297530174255 seconds to complete one epoch of meta training
Took 121.73647570610046 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512121
Epoch 436
Meta loss on this task batch = 2.7218e-01, Meta loss averaged over last 500 steps = 2.9655e-01, PNorm = 129.6164, GNorm = 0.2187
Meta loss on this task batch = 2.6666e-01, Meta loss averaged over last 500 steps = 2.9643e-01, PNorm = 129.6240, GNorm = 0.2297
Meta loss on this task batch = 2.2666e-01, Meta loss averaged over last 500 steps = 2.9634e-01, PNorm = 129.6319, GNorm = 0.1970
Meta loss on this task batch = 2.9637e-01, Meta loss averaged over last 500 steps = 2.9635e-01, PNorm = 129.6395, GNorm = 0.2111
Meta loss on this task batch = 2.9105e-01, Meta loss averaged over last 500 steps = 2.9632e-01, PNorm = 129.6461, GNorm = 0.2592
Meta loss on this task batch = 2.6547e-01, Meta loss averaged over last 500 steps = 2.9628e-01, PNorm = 129.6533, GNorm = 0.2333
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 2.9625e-01, PNorm = 129.6596, GNorm = 0.2371
Meta loss on this task batch = 3.1185e-01, Meta loss averaged over last 500 steps = 2.9630e-01, PNorm = 129.6643, GNorm = 0.2638
Meta loss on this task batch = 3.3718e-01, Meta loss averaged over last 500 steps = 2.9620e-01, PNorm = 129.6684, GNorm = 0.2350
Meta loss on this task batch = 3.0194e-01, Meta loss averaged over last 500 steps = 2.9614e-01, PNorm = 129.6728, GNorm = 0.2293
Meta loss on this task batch = 3.6982e-01, Meta loss averaged over last 500 steps = 2.9628e-01, PNorm = 129.6766, GNorm = 0.2353
Meta loss on this task batch = 3.2571e-01, Meta loss averaged over last 500 steps = 2.9629e-01, PNorm = 129.6788, GNorm = 0.2734
Meta loss on this task batch = 3.1558e-01, Meta loss averaged over last 500 steps = 2.9629e-01, PNorm = 129.6817, GNorm = 0.2266
Meta loss on this task batch = 2.6545e-01, Meta loss averaged over last 500 steps = 2.9623e-01, PNorm = 129.6858, GNorm = 0.2172
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 2.9631e-01, PNorm = 129.6914, GNorm = 0.2161
Meta loss on this task batch = 2.8717e-01, Meta loss averaged over last 500 steps = 2.9626e-01, PNorm = 129.6979, GNorm = 0.2458
Meta loss on this task batch = 2.7774e-01, Meta loss averaged over last 500 steps = 2.9621e-01, PNorm = 129.7050, GNorm = 0.2308
Meta loss on this task batch = 2.5553e-01, Meta loss averaged over last 500 steps = 2.9619e-01, PNorm = 129.7121, GNorm = 0.2019
Meta loss on this task batch = 2.4480e-01, Meta loss averaged over last 500 steps = 2.9610e-01, PNorm = 129.7204, GNorm = 0.2524
Took 124.35748410224915 seconds to complete one epoch of meta training
Took 131.7265169620514 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486507
Epoch 437
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.9602e-01, PNorm = 129.7292, GNorm = 0.2179
Meta loss on this task batch = 2.9646e-01, Meta loss averaged over last 500 steps = 2.9600e-01, PNorm = 129.7398, GNorm = 0.2336
Meta loss on this task batch = 3.4843e-01, Meta loss averaged over last 500 steps = 2.9608e-01, PNorm = 129.7495, GNorm = 0.2663
Meta loss on this task batch = 2.1934e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 129.7602, GNorm = 0.1890
Meta loss on this task batch = 2.9782e-01, Meta loss averaged over last 500 steps = 2.9595e-01, PNorm = 129.7689, GNorm = 0.2621
Meta loss on this task batch = 3.5271e-01, Meta loss averaged over last 500 steps = 2.9599e-01, PNorm = 129.7762, GNorm = 0.2384
Meta loss on this task batch = 3.0948e-01, Meta loss averaged over last 500 steps = 2.9603e-01, PNorm = 129.7834, GNorm = 0.2673
Meta loss on this task batch = 2.5984e-01, Meta loss averaged over last 500 steps = 2.9593e-01, PNorm = 129.7913, GNorm = 0.2007
Meta loss on this task batch = 2.5117e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 129.7991, GNorm = 0.2103
Meta loss on this task batch = 3.2615e-01, Meta loss averaged over last 500 steps = 2.9592e-01, PNorm = 129.8069, GNorm = 0.2710
Meta loss on this task batch = 3.2503e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 129.8148, GNorm = 0.2211
Meta loss on this task batch = 2.8403e-01, Meta loss averaged over last 500 steps = 2.9612e-01, PNorm = 129.8220, GNorm = 0.2272
Meta loss on this task batch = 2.6782e-01, Meta loss averaged over last 500 steps = 2.9590e-01, PNorm = 129.8267, GNorm = 0.2888
Meta loss on this task batch = 2.6818e-01, Meta loss averaged over last 500 steps = 2.9585e-01, PNorm = 129.8313, GNorm = 0.2155
Meta loss on this task batch = 3.3559e-01, Meta loss averaged over last 500 steps = 2.9594e-01, PNorm = 129.8364, GNorm = 0.2412
Meta loss on this task batch = 2.9512e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 129.8425, GNorm = 0.2073
Meta loss on this task batch = 2.8596e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 129.8496, GNorm = 0.2435
Meta loss on this task batch = 2.7739e-01, Meta loss averaged over last 500 steps = 2.9584e-01, PNorm = 129.8573, GNorm = 0.2068
Meta loss on this task batch = 3.5660e-01, Meta loss averaged over last 500 steps = 2.9587e-01, PNorm = 129.8650, GNorm = 0.2957
Took 161.03157138824463 seconds to complete one epoch of meta training
Took 168.52249383926392 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488877
Epoch 438
Meta loss on this task batch = 2.9696e-01, Meta loss averaged over last 500 steps = 2.9594e-01, PNorm = 129.8721, GNorm = 0.1984
Meta loss on this task batch = 2.7799e-01, Meta loss averaged over last 500 steps = 2.9585e-01, PNorm = 129.8791, GNorm = 0.2336
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 2.9575e-01, PNorm = 129.8862, GNorm = 0.2213
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.9577e-01, PNorm = 129.8939, GNorm = 0.2300
Meta loss on this task batch = 2.6846e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 129.9022, GNorm = 0.2616
Meta loss on this task batch = 2.9012e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 129.9101, GNorm = 0.2267
Meta loss on this task batch = 3.2245e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 129.9175, GNorm = 0.2204
Meta loss on this task batch = 2.8496e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 129.9236, GNorm = 0.2599
Meta loss on this task batch = 2.9191e-01, Meta loss averaged over last 500 steps = 2.9570e-01, PNorm = 129.9300, GNorm = 0.2274
Meta loss on this task batch = 2.3927e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 129.9367, GNorm = 0.2077
Meta loss on this task batch = 2.9196e-01, Meta loss averaged over last 500 steps = 2.9559e-01, PNorm = 129.9438, GNorm = 0.2590
Meta loss on this task batch = 2.9583e-01, Meta loss averaged over last 500 steps = 2.9557e-01, PNorm = 129.9504, GNorm = 0.1944
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 129.9576, GNorm = 0.2454
Meta loss on this task batch = 3.1177e-01, Meta loss averaged over last 500 steps = 2.9548e-01, PNorm = 129.9649, GNorm = 0.2572
Meta loss on this task batch = 2.8491e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 129.9719, GNorm = 0.2111
Meta loss on this task batch = 3.0407e-01, Meta loss averaged over last 500 steps = 2.9550e-01, PNorm = 129.9795, GNorm = 0.2276
Meta loss on this task batch = 3.0867e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 129.9862, GNorm = 0.2772
Meta loss on this task batch = 3.1637e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 129.9927, GNorm = 0.2517
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 129.9987, GNorm = 0.2866
Took 112.62409734725952 seconds to complete one epoch of meta training
Took 120.3897807598114 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475754
Epoch 439
Meta loss on this task batch = 3.2298e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.0046, GNorm = 0.2428
Meta loss on this task batch = 3.2900e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 130.0095, GNorm = 0.2587
Meta loss on this task batch = 3.2698e-01, Meta loss averaged over last 500 steps = 2.9586e-01, PNorm = 130.0136, GNorm = 0.2486
Meta loss on this task batch = 3.1851e-01, Meta loss averaged over last 500 steps = 2.9598e-01, PNorm = 130.0180, GNorm = 0.2546
Meta loss on this task batch = 3.1596e-01, Meta loss averaged over last 500 steps = 2.9601e-01, PNorm = 130.0226, GNorm = 0.2338
Meta loss on this task batch = 2.8155e-01, Meta loss averaged over last 500 steps = 2.9589e-01, PNorm = 130.0271, GNorm = 0.2275
Meta loss on this task batch = 2.3604e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 130.0330, GNorm = 0.1845
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 130.0390, GNorm = 0.2233
Meta loss on this task batch = 2.4891e-01, Meta loss averaged over last 500 steps = 2.9555e-01, PNorm = 130.0448, GNorm = 0.2154
Meta loss on this task batch = 2.8710e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 130.0505, GNorm = 0.2172
Meta loss on this task batch = 2.6643e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 130.0549, GNorm = 0.2561
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 130.0606, GNorm = 0.2451
Meta loss on this task batch = 3.2129e-01, Meta loss averaged over last 500 steps = 2.9563e-01, PNorm = 130.0668, GNorm = 0.2502
Meta loss on this task batch = 2.5179e-01, Meta loss averaged over last 500 steps = 2.9553e-01, PNorm = 130.0729, GNorm = 0.1995
Meta loss on this task batch = 3.0796e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 130.0784, GNorm = 0.2309
Meta loss on this task batch = 2.6200e-01, Meta loss averaged over last 500 steps = 2.9558e-01, PNorm = 130.0845, GNorm = 0.2264
Meta loss on this task batch = 3.2814e-01, Meta loss averaged over last 500 steps = 2.9570e-01, PNorm = 130.0907, GNorm = 0.2308
Meta loss on this task batch = 3.5285e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 130.0972, GNorm = 0.2429
Meta loss on this task batch = 2.5271e-01, Meta loss averaged over last 500 steps = 2.9577e-01, PNorm = 130.1041, GNorm = 0.2369
Took 113.88293528556824 seconds to complete one epoch of meta training
Took 121.38510942459106 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486479
Epoch 440
Meta loss on this task batch = 3.1445e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.1108, GNorm = 0.2309
Meta loss on this task batch = 2.4254e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 130.1174, GNorm = 0.2189
Meta loss on this task batch = 3.1551e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 130.1240, GNorm = 0.2590
Meta loss on this task batch = 3.4293e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 130.1306, GNorm = 0.2498
Meta loss on this task batch = 3.2718e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 130.1367, GNorm = 0.2207
Meta loss on this task batch = 2.6338e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 130.1433, GNorm = 0.2353
Meta loss on this task batch = 3.4522e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 130.1487, GNorm = 0.2884
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 130.1546, GNorm = 0.2217
Meta loss on this task batch = 2.8996e-01, Meta loss averaged over last 500 steps = 2.9575e-01, PNorm = 130.1610, GNorm = 0.2052
Meta loss on this task batch = 3.0178e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.1669, GNorm = 0.2395
Meta loss on this task batch = 2.4953e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.1737, GNorm = 0.1973
Meta loss on this task batch = 3.2675e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 130.1802, GNorm = 0.2052
Meta loss on this task batch = 2.6673e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 130.1872, GNorm = 0.2129
Meta loss on this task batch = 3.5688e-01, Meta loss averaged over last 500 steps = 2.9592e-01, PNorm = 130.1928, GNorm = 0.2654
Meta loss on this task batch = 2.7465e-01, Meta loss averaged over last 500 steps = 2.9582e-01, PNorm = 130.1991, GNorm = 0.2159
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 2.9582e-01, PNorm = 130.2054, GNorm = 0.2072
Meta loss on this task batch = 3.5813e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 130.2110, GNorm = 0.2308
Meta loss on this task batch = 2.6432e-01, Meta loss averaged over last 500 steps = 2.9591e-01, PNorm = 130.2181, GNorm = 0.2118
Meta loss on this task batch = 2.8802e-01, Meta loss averaged over last 500 steps = 2.9584e-01, PNorm = 130.2259, GNorm = 0.2421
Took 112.99282026290894 seconds to complete one epoch of meta training
Took 120.3040771484375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491327
Epoch 441
Meta loss on this task batch = 3.1118e-01, Meta loss averaged over last 500 steps = 2.9588e-01, PNorm = 130.2342, GNorm = 0.2250
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 2.9577e-01, PNorm = 130.2425, GNorm = 0.2354
Meta loss on this task batch = 2.9230e-01, Meta loss averaged over last 500 steps = 2.9585e-01, PNorm = 130.2506, GNorm = 0.2089
Meta loss on this task batch = 2.4828e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.2589, GNorm = 0.1779
Meta loss on this task batch = 3.3046e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 130.2675, GNorm = 0.2242
Meta loss on this task batch = 3.0747e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.2759, GNorm = 0.2527
Meta loss on this task batch = 2.8456e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 130.2832, GNorm = 0.2458
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.9562e-01, PNorm = 130.2899, GNorm = 0.2106
Meta loss on this task batch = 3.6693e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 130.2956, GNorm = 0.2440
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 130.3010, GNorm = 0.2228
Meta loss on this task batch = 3.0794e-01, Meta loss averaged over last 500 steps = 2.9583e-01, PNorm = 130.3050, GNorm = 0.2721
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 2.9578e-01, PNorm = 130.3092, GNorm = 0.2289
Meta loss on this task batch = 2.6472e-01, Meta loss averaged over last 500 steps = 2.9577e-01, PNorm = 130.3142, GNorm = 0.2376
Meta loss on this task batch = 2.7804e-01, Meta loss averaged over last 500 steps = 2.9564e-01, PNorm = 130.3196, GNorm = 0.1929
Meta loss on this task batch = 2.5236e-01, Meta loss averaged over last 500 steps = 2.9552e-01, PNorm = 130.3259, GNorm = 0.1962
Meta loss on this task batch = 2.7265e-01, Meta loss averaged over last 500 steps = 2.9546e-01, PNorm = 130.3336, GNorm = 0.2286
Meta loss on this task batch = 3.3795e-01, Meta loss averaged over last 500 steps = 2.9559e-01, PNorm = 130.3417, GNorm = 0.2209
Meta loss on this task batch = 2.9093e-01, Meta loss averaged over last 500 steps = 2.9567e-01, PNorm = 130.3501, GNorm = 0.2353
Meta loss on this task batch = 3.1495e-01, Meta loss averaged over last 500 steps = 2.9574e-01, PNorm = 130.3589, GNorm = 0.2684
Took 118.29601049423218 seconds to complete one epoch of meta training
Took 126.57165837287903 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484464
Epoch 442
Meta loss on this task batch = 3.1461e-01, Meta loss averaged over last 500 steps = 2.9576e-01, PNorm = 130.3673, GNorm = 0.2368
Meta loss on this task batch = 2.5995e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 130.3762, GNorm = 0.2020
Meta loss on this task batch = 3.4556e-01, Meta loss averaged over last 500 steps = 2.9578e-01, PNorm = 130.3844, GNorm = 0.2480
Meta loss on this task batch = 2.8902e-01, Meta loss averaged over last 500 steps = 2.9579e-01, PNorm = 130.3910, GNorm = 0.2090
Meta loss on this task batch = 2.7122e-01, Meta loss averaged over last 500 steps = 2.9573e-01, PNorm = 130.3978, GNorm = 0.2089
Meta loss on this task batch = 3.2694e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 130.4042, GNorm = 0.2658
Meta loss on this task batch = 2.8153e-01, Meta loss averaged over last 500 steps = 2.9571e-01, PNorm = 130.4113, GNorm = 0.2343
Meta loss on this task batch = 2.6260e-01, Meta loss averaged over last 500 steps = 2.9569e-01, PNorm = 130.4185, GNorm = 0.2228
Meta loss on this task batch = 2.1471e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 130.4267, GNorm = 0.1701
Meta loss on this task batch = 2.4902e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 130.4351, GNorm = 0.2106
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 130.4432, GNorm = 0.2348
Meta loss on this task batch = 3.3302e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 130.4501, GNorm = 0.2627
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 2.9540e-01, PNorm = 130.4568, GNorm = 0.2409
Meta loss on this task batch = 3.0149e-01, Meta loss averaged over last 500 steps = 2.9543e-01, PNorm = 130.4623, GNorm = 0.2311
Meta loss on this task batch = 3.3235e-01, Meta loss averaged over last 500 steps = 2.9552e-01, PNorm = 130.4672, GNorm = 0.2377
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.9549e-01, PNorm = 130.4720, GNorm = 0.2415
Meta loss on this task batch = 3.2757e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 130.4779, GNorm = 0.2287
Meta loss on this task batch = 2.9022e-01, Meta loss averaged over last 500 steps = 2.9561e-01, PNorm = 130.4846, GNorm = 0.2682
Meta loss on this task batch = 2.3587e-01, Meta loss averaged over last 500 steps = 2.9554e-01, PNorm = 130.4910, GNorm = 0.2457
Took 114.76569771766663 seconds to complete one epoch of meta training
Took 122.45146417617798 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505533
Epoch 443
Meta loss on this task batch = 2.6873e-01, Meta loss averaged over last 500 steps = 2.9547e-01, PNorm = 130.4982, GNorm = 0.1983
Meta loss on this task batch = 3.1619e-01, Meta loss averaged over last 500 steps = 2.9555e-01, PNorm = 130.5066, GNorm = 0.2248
Meta loss on this task batch = 3.5463e-01, Meta loss averaged over last 500 steps = 2.9566e-01, PNorm = 130.5137, GNorm = 0.2470
Meta loss on this task batch = 2.6387e-01, Meta loss averaged over last 500 steps = 2.9560e-01, PNorm = 130.5207, GNorm = 0.1893
Meta loss on this task batch = 3.1440e-01, Meta loss averaged over last 500 steps = 2.9551e-01, PNorm = 130.5272, GNorm = 0.2174
Meta loss on this task batch = 2.5988e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 130.5342, GNorm = 0.1969
Meta loss on this task batch = 2.4534e-01, Meta loss averaged over last 500 steps = 2.9513e-01, PNorm = 130.5428, GNorm = 0.1952
Meta loss on this task batch = 3.2971e-01, Meta loss averaged over last 500 steps = 2.9503e-01, PNorm = 130.5506, GNorm = 0.2266
Meta loss on this task batch = 2.7160e-01, Meta loss averaged over last 500 steps = 2.9493e-01, PNorm = 130.5593, GNorm = 0.1986
Meta loss on this task batch = 2.3867e-01, Meta loss averaged over last 500 steps = 2.9478e-01, PNorm = 130.5679, GNorm = 0.2047
Meta loss on this task batch = 3.2501e-01, Meta loss averaged over last 500 steps = 2.9481e-01, PNorm = 130.5758, GNorm = 0.2154
Meta loss on this task batch = 3.2049e-01, Meta loss averaged over last 500 steps = 2.9496e-01, PNorm = 130.5833, GNorm = 0.2220
Meta loss on this task batch = 3.0789e-01, Meta loss averaged over last 500 steps = 2.9491e-01, PNorm = 130.5891, GNorm = 0.2692
Meta loss on this task batch = 3.2197e-01, Meta loss averaged over last 500 steps = 2.9502e-01, PNorm = 130.5941, GNorm = 0.2424
Meta loss on this task batch = 3.4294e-01, Meta loss averaged over last 500 steps = 2.9511e-01, PNorm = 130.5993, GNorm = 0.2357
Meta loss on this task batch = 2.4334e-01, Meta loss averaged over last 500 steps = 2.9504e-01, PNorm = 130.6054, GNorm = 0.1953
Meta loss on this task batch = 2.8368e-01, Meta loss averaged over last 500 steps = 2.9503e-01, PNorm = 130.6116, GNorm = 0.2193
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 2.9508e-01, PNorm = 130.6167, GNorm = 0.2108
Meta loss on this task batch = 3.4152e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 130.6213, GNorm = 0.2651
Took 114.15944004058838 seconds to complete one epoch of meta training
Took 121.87856864929199 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491815
Epoch 444
Meta loss on this task batch = 2.9330e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 130.6252, GNorm = 0.2421
Meta loss on this task batch = 2.8705e-01, Meta loss averaged over last 500 steps = 2.9529e-01, PNorm = 130.6305, GNorm = 0.2228
Meta loss on this task batch = 3.4907e-01, Meta loss averaged over last 500 steps = 2.9540e-01, PNorm = 130.6359, GNorm = 0.2491
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 2.9535e-01, PNorm = 130.6424, GNorm = 0.2064
Meta loss on this task batch = 3.0535e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 130.6484, GNorm = 0.2150
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 130.6542, GNorm = 0.2011
Meta loss on this task batch = 2.9890e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 130.6599, GNorm = 0.2173
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 2.9521e-01, PNorm = 130.6656, GNorm = 0.2017
Meta loss on this task batch = 3.2245e-01, Meta loss averaged over last 500 steps = 2.9533e-01, PNorm = 130.6730, GNorm = 0.2811
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.9527e-01, PNorm = 130.6817, GNorm = 0.2161
Meta loss on this task batch = 2.9634e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 130.6912, GNorm = 0.2421
Meta loss on this task batch = 2.8517e-01, Meta loss averaged over last 500 steps = 2.9532e-01, PNorm = 130.7006, GNorm = 0.1902
Meta loss on this task batch = 2.5591e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 130.7091, GNorm = 0.2092
Meta loss on this task batch = 2.7807e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 130.7175, GNorm = 0.2090
Meta loss on this task batch = 3.0751e-01, Meta loss averaged over last 500 steps = 2.9518e-01, PNorm = 130.7241, GNorm = 0.2407
Meta loss on this task batch = 2.3629e-01, Meta loss averaged over last 500 steps = 2.9509e-01, PNorm = 130.7288, GNorm = 0.2597
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 2.9515e-01, PNorm = 130.7333, GNorm = 0.2145
Meta loss on this task batch = 3.3607e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 130.7382, GNorm = 0.2268
Meta loss on this task batch = 3.2096e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 130.7416, GNorm = 0.3010
Took 117.32790637016296 seconds to complete one epoch of meta training
Took 124.49752449989319 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512733
Epoch 445
Meta loss on this task batch = 3.0315e-01, Meta loss averaged over last 500 steps = 2.9534e-01, PNorm = 130.7450, GNorm = 0.2256
Meta loss on this task batch = 3.6807e-01, Meta loss averaged over last 500 steps = 2.9555e-01, PNorm = 130.7492, GNorm = 0.2300
Meta loss on this task batch = 3.2484e-01, Meta loss averaged over last 500 steps = 2.9551e-01, PNorm = 130.7543, GNorm = 0.2095
Meta loss on this task batch = 3.2582e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 130.7606, GNorm = 0.2225
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 2.9538e-01, PNorm = 130.7673, GNorm = 0.1993
Meta loss on this task batch = 2.9951e-01, Meta loss averaged over last 500 steps = 2.9540e-01, PNorm = 130.7747, GNorm = 0.2122
Meta loss on this task batch = 2.9825e-01, Meta loss averaged over last 500 steps = 2.9537e-01, PNorm = 130.7823, GNorm = 0.2180
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 130.7907, GNorm = 0.1984
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.9530e-01, PNorm = 130.7999, GNorm = 0.2153
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 2.9539e-01, PNorm = 130.8093, GNorm = 0.2228
Meta loss on this task batch = 2.4492e-01, Meta loss averaged over last 500 steps = 2.9531e-01, PNorm = 130.8194, GNorm = 0.1983
Meta loss on this task batch = 3.2546e-01, Meta loss averaged over last 500 steps = 2.9541e-01, PNorm = 130.8294, GNorm = 0.2887
Meta loss on this task batch = 2.6411e-01, Meta loss averaged over last 500 steps = 2.9542e-01, PNorm = 130.8388, GNorm = 0.2451
Meta loss on this task batch = 3.5259e-01, Meta loss averaged over last 500 steps = 2.9544e-01, PNorm = 130.8475, GNorm = 0.2584
Meta loss on this task batch = 2.1811e-01, Meta loss averaged over last 500 steps = 2.9525e-01, PNorm = 130.8556, GNorm = 0.2244
Meta loss on this task batch = 3.4587e-01, Meta loss averaged over last 500 steps = 2.9528e-01, PNorm = 130.8623, GNorm = 0.2634
Meta loss on this task batch = 2.4203e-01, Meta loss averaged over last 500 steps = 2.9519e-01, PNorm = 130.8676, GNorm = 0.2332
Meta loss on this task batch = 2.6450e-01, Meta loss averaged over last 500 steps = 2.9505e-01, PNorm = 130.8704, GNorm = 0.2432
Meta loss on this task batch = 2.7683e-01, Meta loss averaged over last 500 steps = 2.9502e-01, PNorm = 130.8712, GNorm = 0.3609
Took 121.74693489074707 seconds to complete one epoch of meta training
Took 129.81977701187134 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494653
Epoch 446
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 2.9492e-01, PNorm = 130.8728, GNorm = 0.2328
Meta loss on this task batch = 2.7539e-01, Meta loss averaged over last 500 steps = 2.9494e-01, PNorm = 130.8749, GNorm = 0.2211
Meta loss on this task batch = 2.7985e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 130.8780, GNorm = 0.2324
Meta loss on this task batch = 2.8005e-01, Meta loss averaged over last 500 steps = 2.9470e-01, PNorm = 130.8831, GNorm = 0.2323
Meta loss on this task batch = 3.1595e-01, Meta loss averaged over last 500 steps = 2.9483e-01, PNorm = 130.8891, GNorm = 0.2661
Meta loss on this task batch = 3.3205e-01, Meta loss averaged over last 500 steps = 2.9484e-01, PNorm = 130.8961, GNorm = 0.2450
Meta loss on this task batch = 3.6236e-01, Meta loss averaged over last 500 steps = 2.9489e-01, PNorm = 130.9036, GNorm = 0.2736
Meta loss on this task batch = 3.1293e-01, Meta loss averaged over last 500 steps = 2.9499e-01, PNorm = 130.9125, GNorm = 0.2295
Meta loss on this task batch = 3.0384e-01, Meta loss averaged over last 500 steps = 2.9501e-01, PNorm = 130.9215, GNorm = 0.2126
Meta loss on this task batch = 3.0757e-01, Meta loss averaged over last 500 steps = 2.9504e-01, PNorm = 130.9305, GNorm = 0.2525
Meta loss on this task batch = 2.5419e-01, Meta loss averaged over last 500 steps = 2.9493e-01, PNorm = 130.9399, GNorm = 0.2397
Meta loss on this task batch = 2.6421e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 130.9503, GNorm = 0.2217
Meta loss on this task batch = 3.0687e-01, Meta loss averaged over last 500 steps = 2.9496e-01, PNorm = 130.9592, GNorm = 0.2324
Meta loss on this task batch = 2.6320e-01, Meta loss averaged over last 500 steps = 2.9477e-01, PNorm = 130.9683, GNorm = 0.2139
Meta loss on this task batch = 3.1976e-01, Meta loss averaged over last 500 steps = 2.9486e-01, PNorm = 130.9764, GNorm = 0.2753
Meta loss on this task batch = 2.6191e-01, Meta loss averaged over last 500 steps = 2.9474e-01, PNorm = 130.9827, GNorm = 0.2491
Meta loss on this task batch = 2.8717e-01, Meta loss averaged over last 500 steps = 2.9479e-01, PNorm = 130.9901, GNorm = 0.2204
Meta loss on this task batch = 3.0065e-01, Meta loss averaged over last 500 steps = 2.9485e-01, PNorm = 130.9972, GNorm = 0.2211
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 131.0037, GNorm = 0.3001
Took 140.79323863983154 seconds to complete one epoch of meta training
Took 148.3984293937683 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.523610
Epoch 447
Meta loss on this task batch = 2.7877e-01, Meta loss averaged over last 500 steps = 2.9473e-01, PNorm = 131.0098, GNorm = 0.2342
Meta loss on this task batch = 3.0662e-01, Meta loss averaged over last 500 steps = 2.9461e-01, PNorm = 131.0150, GNorm = 0.2746
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 2.9458e-01, PNorm = 131.0210, GNorm = 0.2136
Meta loss on this task batch = 3.1832e-01, Meta loss averaged over last 500 steps = 2.9472e-01, PNorm = 131.0275, GNorm = 0.2054
Meta loss on this task batch = 3.1799e-01, Meta loss averaged over last 500 steps = 2.9481e-01, PNorm = 131.0341, GNorm = 0.2241
Meta loss on this task batch = 2.6674e-01, Meta loss averaged over last 500 steps = 2.9479e-01, PNorm = 131.0410, GNorm = 0.2018
Meta loss on this task batch = 3.3017e-01, Meta loss averaged over last 500 steps = 2.9503e-01, PNorm = 131.0475, GNorm = 0.2146
Meta loss on this task batch = 2.9201e-01, Meta loss averaged over last 500 steps = 2.9506e-01, PNorm = 131.0546, GNorm = 0.2119
Meta loss on this task batch = 2.6799e-01, Meta loss averaged over last 500 steps = 2.9497e-01, PNorm = 131.0615, GNorm = 0.2464
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 2.9484e-01, PNorm = 131.0693, GNorm = 0.2246
Meta loss on this task batch = 3.0069e-01, Meta loss averaged over last 500 steps = 2.9489e-01, PNorm = 131.0773, GNorm = 0.2283
Meta loss on this task batch = 2.9717e-01, Meta loss averaged over last 500 steps = 2.9486e-01, PNorm = 131.0848, GNorm = 0.2372
Meta loss on this task batch = 2.8689e-01, Meta loss averaged over last 500 steps = 2.9491e-01, PNorm = 131.0936, GNorm = 0.2298
Meta loss on this task batch = 2.7129e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 131.1030, GNorm = 0.2338
Meta loss on this task batch = 2.7786e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 131.1127, GNorm = 0.2285
Meta loss on this task batch = 2.9174e-01, Meta loss averaged over last 500 steps = 2.9479e-01, PNorm = 131.1221, GNorm = 0.1958
Meta loss on this task batch = 3.2009e-01, Meta loss averaged over last 500 steps = 2.9485e-01, PNorm = 131.1311, GNorm = 0.2245
Meta loss on this task batch = 3.2072e-01, Meta loss averaged over last 500 steps = 2.9488e-01, PNorm = 131.1379, GNorm = 0.2428
Meta loss on this task batch = 2.6957e-01, Meta loss averaged over last 500 steps = 2.9481e-01, PNorm = 131.1446, GNorm = 0.2310
Took 129.67578125 seconds to complete one epoch of meta training
Took 137.2899775505066 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492773
Epoch 448
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.9474e-01, PNorm = 131.1518, GNorm = 0.2018
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 2.9465e-01, PNorm = 131.1590, GNorm = 0.2072
Meta loss on this task batch = 2.8314e-01, Meta loss averaged over last 500 steps = 2.9471e-01, PNorm = 131.1649, GNorm = 0.2551
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.9465e-01, PNorm = 131.1702, GNorm = 0.2235
Meta loss on this task batch = 2.9681e-01, Meta loss averaged over last 500 steps = 2.9460e-01, PNorm = 131.1741, GNorm = 0.2663
Meta loss on this task batch = 3.1404e-01, Meta loss averaged over last 500 steps = 2.9456e-01, PNorm = 131.1785, GNorm = 0.2441
Meta loss on this task batch = 2.5028e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 131.1829, GNorm = 0.2053
Meta loss on this task batch = 3.2782e-01, Meta loss averaged over last 500 steps = 2.9467e-01, PNorm = 131.1879, GNorm = 0.2407
Meta loss on this task batch = 2.0572e-01, Meta loss averaged over last 500 steps = 2.9435e-01, PNorm = 131.1929, GNorm = 0.2038
Meta loss on this task batch = 3.4578e-01, Meta loss averaged over last 500 steps = 2.9449e-01, PNorm = 131.1976, GNorm = 0.2359
Meta loss on this task batch = 2.7853e-01, Meta loss averaged over last 500 steps = 2.9451e-01, PNorm = 131.2026, GNorm = 0.2297
Meta loss on this task batch = 3.1395e-01, Meta loss averaged over last 500 steps = 2.9453e-01, PNorm = 131.2080, GNorm = 0.2722
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 2.9435e-01, PNorm = 131.2136, GNorm = 0.2236
Meta loss on this task batch = 3.1324e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 131.2193, GNorm = 0.2354
Meta loss on this task batch = 3.1248e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 131.2258, GNorm = 0.2260
Meta loss on this task batch = 3.1200e-01, Meta loss averaged over last 500 steps = 2.9426e-01, PNorm = 131.2323, GNorm = 0.2359
Meta loss on this task batch = 3.4425e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 131.2393, GNorm = 0.2314
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.9448e-01, PNorm = 131.2472, GNorm = 0.2333
Meta loss on this task batch = 2.8421e-01, Meta loss averaged over last 500 steps = 2.9450e-01, PNorm = 131.2549, GNorm = 0.2676
Took 136.82516765594482 seconds to complete one epoch of meta training
Took 144.0812382698059 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515765
Epoch 449
Meta loss on this task batch = 2.3870e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 131.2632, GNorm = 0.1943
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.9429e-01, PNorm = 131.2716, GNorm = 0.2354
Meta loss on this task batch = 2.3687e-01, Meta loss averaged over last 500 steps = 2.9416e-01, PNorm = 131.2799, GNorm = 0.1710
Meta loss on this task batch = 2.7680e-01, Meta loss averaged over last 500 steps = 2.9425e-01, PNorm = 131.2880, GNorm = 0.1881
Meta loss on this task batch = 3.2921e-01, Meta loss averaged over last 500 steps = 2.9435e-01, PNorm = 131.2957, GNorm = 0.2485
Meta loss on this task batch = 2.8804e-01, Meta loss averaged over last 500 steps = 2.9430e-01, PNorm = 131.3028, GNorm = 0.2179
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.9417e-01, PNorm = 131.3108, GNorm = 0.2058
Meta loss on this task batch = 3.0545e-01, Meta loss averaged over last 500 steps = 2.9417e-01, PNorm = 131.3184, GNorm = 0.2482
Meta loss on this task batch = 3.3036e-01, Meta loss averaged over last 500 steps = 2.9436e-01, PNorm = 131.3255, GNorm = 0.2278
Meta loss on this task batch = 2.8024e-01, Meta loss averaged over last 500 steps = 2.9433e-01, PNorm = 131.3325, GNorm = 0.2134
Meta loss on this task batch = 3.2379e-01, Meta loss averaged over last 500 steps = 2.9440e-01, PNorm = 131.3394, GNorm = 0.2687
Meta loss on this task batch = 3.2094e-01, Meta loss averaged over last 500 steps = 2.9439e-01, PNorm = 131.3460, GNorm = 0.2396
Meta loss on this task batch = 2.5997e-01, Meta loss averaged over last 500 steps = 2.9434e-01, PNorm = 131.3529, GNorm = 0.2112
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 131.3597, GNorm = 0.2276
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.9440e-01, PNorm = 131.3674, GNorm = 0.2351
Meta loss on this task batch = 2.9010e-01, Meta loss averaged over last 500 steps = 2.9439e-01, PNorm = 131.3750, GNorm = 0.2448
Meta loss on this task batch = 2.9816e-01, Meta loss averaged over last 500 steps = 2.9443e-01, PNorm = 131.3824, GNorm = 0.2539
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 131.3899, GNorm = 0.2462
Meta loss on this task batch = 2.3678e-01, Meta loss averaged over last 500 steps = 2.9424e-01, PNorm = 131.3975, GNorm = 0.2556
Took 124.11798071861267 seconds to complete one epoch of meta training
Took 132.00790143013 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486810
Epoch 450
Meta loss on this task batch = 3.0587e-01, Meta loss averaged over last 500 steps = 2.9423e-01, PNorm = 131.4053, GNorm = 0.2009
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.9410e-01, PNorm = 131.4124, GNorm = 0.2357
Meta loss on this task batch = 2.8171e-01, Meta loss averaged over last 500 steps = 2.9402e-01, PNorm = 131.4184, GNorm = 0.2134
Meta loss on this task batch = 2.7952e-01, Meta loss averaged over last 500 steps = 2.9403e-01, PNorm = 131.4243, GNorm = 0.2147
Meta loss on this task batch = 3.4244e-01, Meta loss averaged over last 500 steps = 2.9409e-01, PNorm = 131.4303, GNorm = 0.2476
Meta loss on this task batch = 2.9188e-01, Meta loss averaged over last 500 steps = 2.9404e-01, PNorm = 131.4364, GNorm = 0.2219
Meta loss on this task batch = 3.2048e-01, Meta loss averaged over last 500 steps = 2.9411e-01, PNorm = 131.4414, GNorm = 0.2123
Meta loss on this task batch = 3.6799e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 131.4463, GNorm = 0.2562
Meta loss on this task batch = 2.6819e-01, Meta loss averaged over last 500 steps = 2.9438e-01, PNorm = 131.4505, GNorm = 0.2281
Meta loss on this task batch = 2.9396e-01, Meta loss averaged over last 500 steps = 2.9437e-01, PNorm = 131.4548, GNorm = 0.2319
Meta loss on this task batch = 2.7216e-01, Meta loss averaged over last 500 steps = 2.9435e-01, PNorm = 131.4589, GNorm = 0.2254
Meta loss on this task batch = 2.6796e-01, Meta loss averaged over last 500 steps = 2.9421e-01, PNorm = 131.4636, GNorm = 0.2003
Meta loss on this task batch = 2.7952e-01, Meta loss averaged over last 500 steps = 2.9419e-01, PNorm = 131.4694, GNorm = 0.2066
Meta loss on this task batch = 2.6014e-01, Meta loss averaged over last 500 steps = 2.9415e-01, PNorm = 131.4765, GNorm = 0.2146
Meta loss on this task batch = 2.6278e-01, Meta loss averaged over last 500 steps = 2.9406e-01, PNorm = 131.4848, GNorm = 0.2359
Meta loss on this task batch = 2.7373e-01, Meta loss averaged over last 500 steps = 2.9393e-01, PNorm = 131.4937, GNorm = 0.2170
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 2.9393e-01, PNorm = 131.5015, GNorm = 0.2545
Meta loss on this task batch = 2.9489e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 131.5101, GNorm = 0.2594
Meta loss on this task batch = 2.7039e-01, Meta loss averaged over last 500 steps = 2.9395e-01, PNorm = 131.5197, GNorm = 0.2530
Took 129.166401386261 seconds to complete one epoch of meta training
Took 135.79814314842224 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487162
Epoch 451
Meta loss on this task batch = 2.3840e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 131.5293, GNorm = 0.2237
Meta loss on this task batch = 3.0642e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 131.5380, GNorm = 0.2117
Meta loss on this task batch = 3.1933e-01, Meta loss averaged over last 500 steps = 2.9360e-01, PNorm = 131.5463, GNorm = 0.2304
Meta loss on this task batch = 2.6941e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 131.5539, GNorm = 0.2537
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 2.9364e-01, PNorm = 131.5614, GNorm = 0.2270
Meta loss on this task batch = 2.5632e-01, Meta loss averaged over last 500 steps = 2.9347e-01, PNorm = 131.5695, GNorm = 0.1928
Meta loss on this task batch = 2.9539e-01, Meta loss averaged over last 500 steps = 2.9350e-01, PNorm = 131.5777, GNorm = 0.2238
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 131.5855, GNorm = 0.2091
Meta loss on this task batch = 2.8454e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 131.5931, GNorm = 0.2165
Meta loss on this task batch = 3.1570e-01, Meta loss averaged over last 500 steps = 2.9342e-01, PNorm = 131.6003, GNorm = 0.2242
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 131.6077, GNorm = 0.2313
Meta loss on this task batch = 2.7965e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 131.6151, GNorm = 0.2243
Meta loss on this task batch = 2.9238e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 131.6222, GNorm = 0.2341
Meta loss on this task batch = 2.6193e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 131.6292, GNorm = 0.2125
Meta loss on this task batch = 2.9718e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 131.6354, GNorm = 0.2295
Meta loss on this task batch = 2.7677e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 131.6402, GNorm = 0.2350
Meta loss on this task batch = 3.3917e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 131.6451, GNorm = 0.2496
Meta loss on this task batch = 2.7867e-01, Meta loss averaged over last 500 steps = 2.9343e-01, PNorm = 131.6501, GNorm = 0.2287
Meta loss on this task batch = 3.2212e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 131.6530, GNorm = 0.2947
Took 120.55626654624939 seconds to complete one epoch of meta training
Took 127.99409890174866 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486753
Epoch 452
Meta loss on this task batch = 2.9807e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 131.6567, GNorm = 0.2412
Meta loss on this task batch = 2.8788e-01, Meta loss averaged over last 500 steps = 2.9340e-01, PNorm = 131.6616, GNorm = 0.1961
Meta loss on this task batch = 2.7926e-01, Meta loss averaged over last 500 steps = 2.9349e-01, PNorm = 131.6672, GNorm = 0.2459
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 131.6750, GNorm = 0.2411
Meta loss on this task batch = 2.5645e-01, Meta loss averaged over last 500 steps = 2.9347e-01, PNorm = 131.6847, GNorm = 0.2374
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 131.6948, GNorm = 0.1966
Meta loss on this task batch = 3.5650e-01, Meta loss averaged over last 500 steps = 2.9347e-01, PNorm = 131.7048, GNorm = 0.2369
Meta loss on this task batch = 2.9204e-01, Meta loss averaged over last 500 steps = 2.9341e-01, PNorm = 131.7143, GNorm = 0.2078
Meta loss on this task batch = 2.7284e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 131.7228, GNorm = 0.2069
Meta loss on this task batch = 2.9535e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 131.7309, GNorm = 0.2328
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 2.9348e-01, PNorm = 131.7380, GNorm = 0.2427
Meta loss on this task batch = 3.0399e-01, Meta loss averaged over last 500 steps = 2.9361e-01, PNorm = 131.7442, GNorm = 0.2189
Meta loss on this task batch = 2.9328e-01, Meta loss averaged over last 500 steps = 2.9363e-01, PNorm = 131.7502, GNorm = 0.2191
Meta loss on this task batch = 3.2552e-01, Meta loss averaged over last 500 steps = 2.9376e-01, PNorm = 131.7565, GNorm = 0.2304
Meta loss on this task batch = 2.5511e-01, Meta loss averaged over last 500 steps = 2.9360e-01, PNorm = 131.7623, GNorm = 0.2358
Meta loss on this task batch = 2.3286e-01, Meta loss averaged over last 500 steps = 2.9355e-01, PNorm = 131.7682, GNorm = 0.1987
Meta loss on this task batch = 3.4301e-01, Meta loss averaged over last 500 steps = 2.9365e-01, PNorm = 131.7739, GNorm = 0.2739
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 2.9363e-01, PNorm = 131.7795, GNorm = 0.2274
Meta loss on this task batch = 3.0482e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 131.7860, GNorm = 0.2852
Took 126.98418641090393 seconds to complete one epoch of meta training
Took 135.15517783164978 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483328
Epoch 453
Meta loss on this task batch = 3.1220e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 131.7928, GNorm = 0.2402
Meta loss on this task batch = 3.7234e-01, Meta loss averaged over last 500 steps = 2.9371e-01, PNorm = 131.7995, GNorm = 0.2626
Meta loss on this task batch = 3.0426e-01, Meta loss averaged over last 500 steps = 2.9369e-01, PNorm = 131.8068, GNorm = 0.2299
Meta loss on this task batch = 2.7020e-01, Meta loss averaged over last 500 steps = 2.9364e-01, PNorm = 131.8150, GNorm = 0.2330
Meta loss on this task batch = 2.5905e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 131.8243, GNorm = 0.2115
Meta loss on this task batch = 3.0533e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 131.8325, GNorm = 0.2771
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 2.9374e-01, PNorm = 131.8392, GNorm = 0.2240
Meta loss on this task batch = 2.9443e-01, Meta loss averaged over last 500 steps = 2.9375e-01, PNorm = 131.8464, GNorm = 0.2417
Meta loss on this task batch = 3.1093e-01, Meta loss averaged over last 500 steps = 2.9381e-01, PNorm = 131.8527, GNorm = 0.2515
Meta loss on this task batch = 3.3099e-01, Meta loss averaged over last 500 steps = 2.9379e-01, PNorm = 131.8563, GNorm = 0.2943
Meta loss on this task batch = 2.6434e-01, Meta loss averaged over last 500 steps = 2.9371e-01, PNorm = 131.8601, GNorm = 0.2131
Meta loss on this task batch = 3.0761e-01, Meta loss averaged over last 500 steps = 2.9374e-01, PNorm = 131.8629, GNorm = 0.2378
Meta loss on this task batch = 3.3680e-01, Meta loss averaged over last 500 steps = 2.9384e-01, PNorm = 131.8663, GNorm = 0.2303
Meta loss on this task batch = 2.5987e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 131.8706, GNorm = 0.2145
Meta loss on this task batch = 2.3025e-01, Meta loss averaged over last 500 steps = 2.9369e-01, PNorm = 131.8758, GNorm = 0.2105
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.9356e-01, PNorm = 131.8818, GNorm = 0.2156
Meta loss on this task batch = 2.6054e-01, Meta loss averaged over last 500 steps = 2.9361e-01, PNorm = 131.8890, GNorm = 0.2049
Meta loss on this task batch = 2.8826e-01, Meta loss averaged over last 500 steps = 2.9365e-01, PNorm = 131.8961, GNorm = 0.2227
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 2.9357e-01, PNorm = 131.9041, GNorm = 0.2714
Took 113.49817848205566 seconds to complete one epoch of meta training
Took 121.01776266098022 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495668
Epoch 454
Meta loss on this task batch = 2.8134e-01, Meta loss averaged over last 500 steps = 2.9362e-01, PNorm = 131.9125, GNorm = 0.2050
Meta loss on this task batch = 3.3260e-01, Meta loss averaged over last 500 steps = 2.9361e-01, PNorm = 131.9209, GNorm = 0.2567
Meta loss on this task batch = 2.9347e-01, Meta loss averaged over last 500 steps = 2.9351e-01, PNorm = 131.9302, GNorm = 0.2615
Meta loss on this task batch = 3.6377e-01, Meta loss averaged over last 500 steps = 2.9368e-01, PNorm = 131.9379, GNorm = 0.2868
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 131.9457, GNorm = 0.2613
Meta loss on this task batch = 3.7675e-01, Meta loss averaged over last 500 steps = 2.9398e-01, PNorm = 131.9534, GNorm = 0.2641
Meta loss on this task batch = 2.5362e-01, Meta loss averaged over last 500 steps = 2.9387e-01, PNorm = 131.9609, GNorm = 0.1962
Meta loss on this task batch = 2.4535e-01, Meta loss averaged over last 500 steps = 2.9375e-01, PNorm = 131.9696, GNorm = 0.2137
Meta loss on this task batch = 2.9903e-01, Meta loss averaged over last 500 steps = 2.9382e-01, PNorm = 131.9760, GNorm = 0.2791
Meta loss on this task batch = 2.8303e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 131.9808, GNorm = 0.2561
Meta loss on this task batch = 3.3834e-01, Meta loss averaged over last 500 steps = 2.9384e-01, PNorm = 131.9855, GNorm = 0.2524
Meta loss on this task batch = 2.5228e-01, Meta loss averaged over last 500 steps = 2.9378e-01, PNorm = 131.9903, GNorm = 0.2021
Meta loss on this task batch = 2.6686e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 131.9957, GNorm = 0.2180
Meta loss on this task batch = 2.7114e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 132.0015, GNorm = 0.2245
Meta loss on this task batch = 2.2947e-01, Meta loss averaged over last 500 steps = 2.9354e-01, PNorm = 132.0082, GNorm = 0.2080
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.9338e-01, PNorm = 132.0140, GNorm = 0.1875
Meta loss on this task batch = 2.7730e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 132.0197, GNorm = 0.2161
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 132.0257, GNorm = 0.2251
Meta loss on this task batch = 3.2594e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 132.0317, GNorm = 0.2847
Took 113.65109705924988 seconds to complete one epoch of meta training
Took 121.270512342453 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500733
Epoch 455
Meta loss on this task batch = 2.9842e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 132.0378, GNorm = 0.2400
Meta loss on this task batch = 3.1076e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 132.0433, GNorm = 0.2718
Meta loss on this task batch = 2.2412e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 132.0496, GNorm = 0.1767
Meta loss on this task batch = 2.8393e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 132.0557, GNorm = 0.2334
Meta loss on this task batch = 2.7871e-01, Meta loss averaged over last 500 steps = 2.9307e-01, PNorm = 132.0617, GNorm = 0.2143
Meta loss on this task batch = 2.8947e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 132.0679, GNorm = 0.2426
Meta loss on this task batch = 2.8430e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 132.0741, GNorm = 0.2670
Meta loss on this task batch = 3.0971e-01, Meta loss averaged over last 500 steps = 2.9317e-01, PNorm = 132.0801, GNorm = 0.2227
Meta loss on this task batch = 3.0167e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 132.0866, GNorm = 0.2313
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 132.0932, GNorm = 0.2317
Meta loss on this task batch = 2.6346e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.1000, GNorm = 0.2322
Meta loss on this task batch = 3.1102e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 132.1080, GNorm = 0.2490
Meta loss on this task batch = 3.1697e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 132.1155, GNorm = 0.1977
Meta loss on this task batch = 3.1871e-01, Meta loss averaged over last 500 steps = 2.9337e-01, PNorm = 132.1228, GNorm = 0.2283
Meta loss on this task batch = 2.8769e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.1299, GNorm = 0.2265
Meta loss on this task batch = 3.4705e-01, Meta loss averaged over last 500 steps = 2.9345e-01, PNorm = 132.1359, GNorm = 0.2721
Meta loss on this task batch = 2.6100e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.1421, GNorm = 0.2147
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 132.1484, GNorm = 0.2129
Meta loss on this task batch = 2.6241e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 132.1549, GNorm = 0.2739
Took 116.17380261421204 seconds to complete one epoch of meta training
Took 123.1073899269104 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484601
Epoch 456
Meta loss on this task batch = 2.9959e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 132.1614, GNorm = 0.2130
Meta loss on this task batch = 3.0588e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 132.1673, GNorm = 0.2268
Meta loss on this task batch = 3.3604e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 132.1740, GNorm = 0.2375
Meta loss on this task batch = 3.0245e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 132.1803, GNorm = 0.2369
Meta loss on this task batch = 3.1363e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 132.1863, GNorm = 0.2712
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 132.1926, GNorm = 0.2051
Meta loss on this task batch = 2.4239e-01, Meta loss averaged over last 500 steps = 2.9303e-01, PNorm = 132.1991, GNorm = 0.2480
Meta loss on this task batch = 3.1189e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 132.2054, GNorm = 0.2193
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 132.2115, GNorm = 0.2578
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 2.9307e-01, PNorm = 132.2176, GNorm = 0.2929
Meta loss on this task batch = 2.9977e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 132.2234, GNorm = 0.2362
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 2.9307e-01, PNorm = 132.2295, GNorm = 0.2324
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 132.2359, GNorm = 0.2191
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.9308e-01, PNorm = 132.2421, GNorm = 0.2324
Meta loss on this task batch = 2.9042e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 132.2491, GNorm = 0.2055
Meta loss on this task batch = 3.0420e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 132.2556, GNorm = 0.2222
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 132.2627, GNorm = 0.2489
Meta loss on this task batch = 2.4031e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 132.2700, GNorm = 0.1999
Meta loss on this task batch = 3.1586e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 132.2781, GNorm = 0.2689
Took 114.73370695114136 seconds to complete one epoch of meta training
Took 122.95323371887207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492583
Epoch 457
Meta loss on this task batch = 2.8626e-01, Meta loss averaged over last 500 steps = 2.9303e-01, PNorm = 132.2845, GNorm = 0.2267
Meta loss on this task batch = 3.1353e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 132.2901, GNorm = 0.2617
Meta loss on this task batch = 3.2481e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 132.2946, GNorm = 0.2584
Meta loss on this task batch = 2.8602e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 132.2996, GNorm = 0.1989
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 132.3052, GNorm = 0.2359
Meta loss on this task batch = 3.1671e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 132.3103, GNorm = 0.2172
Meta loss on this task batch = 3.1664e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.3143, GNorm = 0.2628
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 132.3180, GNorm = 0.2165
Meta loss on this task batch = 3.0802e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 132.3226, GNorm = 0.2240
Meta loss on this task batch = 2.7172e-01, Meta loss averaged over last 500 steps = 2.9343e-01, PNorm = 132.3284, GNorm = 0.1873
Meta loss on this task batch = 2.5156e-01, Meta loss averaged over last 500 steps = 2.9341e-01, PNorm = 132.3347, GNorm = 0.1878
Meta loss on this task batch = 2.6813e-01, Meta loss averaged over last 500 steps = 2.9339e-01, PNorm = 132.3404, GNorm = 0.2624
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.3458, GNorm = 0.2117
Meta loss on this task batch = 3.1029e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 132.3492, GNorm = 0.2877
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 132.3529, GNorm = 0.2207
Meta loss on this task batch = 2.8504e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 132.3576, GNorm = 0.2337
Meta loss on this task batch = 2.4608e-01, Meta loss averaged over last 500 steps = 2.9304e-01, PNorm = 132.3632, GNorm = 0.2089
Meta loss on this task batch = 3.2663e-01, Meta loss averaged over last 500 steps = 2.9306e-01, PNorm = 132.3681, GNorm = 0.2610
Meta loss on this task batch = 3.5196e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.3707, GNorm = 0.3752
Took 114.86791181564331 seconds to complete one epoch of meta training
Took 122.79269075393677 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476505
Epoch 458
Meta loss on this task batch = 2.8088e-01, Meta loss averaged over last 500 steps = 2.9328e-01, PNorm = 132.3743, GNorm = 0.2031
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 132.3783, GNorm = 0.2370
Meta loss on this task batch = 2.9551e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 132.3832, GNorm = 0.2207
Meta loss on this task batch = 2.4659e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.3897, GNorm = 0.2218
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 132.3971, GNorm = 0.2289
Meta loss on this task batch = 3.6397e-01, Meta loss averaged over last 500 steps = 2.9336e-01, PNorm = 132.4049, GNorm = 0.2244
Meta loss on this task batch = 2.7158e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.4135, GNorm = 0.1834
Meta loss on this task batch = 2.9004e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 132.4227, GNorm = 0.2210
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 132.4311, GNorm = 0.2499
Meta loss on this task batch = 2.3822e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 132.4393, GNorm = 0.1844
Meta loss on this task batch = 3.2536e-01, Meta loss averaged over last 500 steps = 2.9307e-01, PNorm = 132.4473, GNorm = 0.2239
Meta loss on this task batch = 2.6665e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 132.4557, GNorm = 0.2243
Meta loss on this task batch = 3.0874e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 132.4619, GNorm = 0.2534
Meta loss on this task batch = 2.4458e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 132.4678, GNorm = 0.2229
Meta loss on this task batch = 2.9436e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 132.4732, GNorm = 0.2325
Meta loss on this task batch = 2.6333e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 132.4789, GNorm = 0.2118
Meta loss on this task batch = 3.2875e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.4848, GNorm = 0.2651
Meta loss on this task batch = 2.9652e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 132.4916, GNorm = 0.2437
Meta loss on this task batch = 3.0129e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 132.4988, GNorm = 0.2465
Took 114.87351894378662 seconds to complete one epoch of meta training
Took 122.80413269996643 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510012
Epoch 459
Meta loss on this task batch = 2.7463e-01, Meta loss averaged over last 500 steps = 2.9337e-01, PNorm = 132.5051, GNorm = 0.2544
Meta loss on this task batch = 2.7439e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 132.5130, GNorm = 0.2213
Meta loss on this task batch = 3.2127e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 132.5205, GNorm = 0.2160
Meta loss on this task batch = 3.2418e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 132.5281, GNorm = 0.2658
Meta loss on this task batch = 3.0456e-01, Meta loss averaged over last 500 steps = 2.9337e-01, PNorm = 132.5350, GNorm = 0.1960
Meta loss on this task batch = 3.5267e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 132.5413, GNorm = 0.2368
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 2.9348e-01, PNorm = 132.5490, GNorm = 0.2260
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 132.5569, GNorm = 0.1876
Meta loss on this task batch = 3.0584e-01, Meta loss averaged over last 500 steps = 2.9353e-01, PNorm = 132.5645, GNorm = 0.2209
Meta loss on this task batch = 3.4687e-01, Meta loss averaged over last 500 steps = 2.9369e-01, PNorm = 132.5716, GNorm = 0.2203
Meta loss on this task batch = 3.0545e-01, Meta loss averaged over last 500 steps = 2.9366e-01, PNorm = 132.5772, GNorm = 0.2364
Meta loss on this task batch = 2.4653e-01, Meta loss averaged over last 500 steps = 2.9358e-01, PNorm = 132.5827, GNorm = 0.2330
Meta loss on this task batch = 2.6132e-01, Meta loss averaged over last 500 steps = 2.9354e-01, PNorm = 132.5884, GNorm = 0.1939
Meta loss on this task batch = 2.9056e-01, Meta loss averaged over last 500 steps = 2.9341e-01, PNorm = 132.5946, GNorm = 0.2241
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.9341e-01, PNorm = 132.5997, GNorm = 0.2194
Meta loss on this task batch = 2.6846e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 132.6044, GNorm = 0.2114
Meta loss on this task batch = 2.6312e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 132.6094, GNorm = 0.2299
Meta loss on this task batch = 3.1342e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 132.6145, GNorm = 0.2445
Meta loss on this task batch = 3.5142e-01, Meta loss averaged over last 500 steps = 2.9335e-01, PNorm = 132.6202, GNorm = 0.3113
Took 115.41498875617981 seconds to complete one epoch of meta training
Took 123.05313730239868 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489231
Epoch 460
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 132.6266, GNorm = 0.2134
Meta loss on this task batch = 3.2736e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 132.6330, GNorm = 0.2323
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 132.6391, GNorm = 0.2340
Meta loss on this task batch = 2.5437e-01, Meta loss averaged over last 500 steps = 2.9306e-01, PNorm = 132.6458, GNorm = 0.2228
Meta loss on this task batch = 2.9301e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 132.6527, GNorm = 0.2254
Meta loss on this task batch = 3.1270e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 132.6593, GNorm = 0.2309
Meta loss on this task batch = 3.2825e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.6652, GNorm = 0.2262
Meta loss on this task batch = 2.5609e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 132.6713, GNorm = 0.2416
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.6773, GNorm = 0.2248
Meta loss on this task batch = 3.0271e-01, Meta loss averaged over last 500 steps = 2.9332e-01, PNorm = 132.6823, GNorm = 0.2918
Meta loss on this task batch = 3.1609e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 132.6854, GNorm = 0.2699
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.6893, GNorm = 0.2245
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 132.6933, GNorm = 0.2236
Meta loss on this task batch = 2.5728e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.6979, GNorm = 0.2185
Meta loss on this task batch = 3.1767e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.7031, GNorm = 0.2466
Meta loss on this task batch = 2.5474e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 132.7096, GNorm = 0.2039
Meta loss on this task batch = 3.0480e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.7162, GNorm = 0.2422
Meta loss on this task batch = 3.3111e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 132.7230, GNorm = 0.2460
Meta loss on this task batch = 3.1716e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 132.7298, GNorm = 0.2586
Took 116.02609372138977 seconds to complete one epoch of meta training
Took 123.87450361251831 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503085
Epoch 461
Meta loss on this task batch = 3.2980e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 132.7372, GNorm = 0.2281
Meta loss on this task batch = 2.6748e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 132.7452, GNorm = 0.2140
Meta loss on this task batch = 2.6717e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.7530, GNorm = 0.2240
Meta loss on this task batch = 3.1728e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 132.7608, GNorm = 0.1979
Meta loss on this task batch = 2.9368e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 132.7680, GNorm = 0.1995
Meta loss on this task batch = 2.5781e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 132.7751, GNorm = 0.2242
Meta loss on this task batch = 2.7183e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 132.7832, GNorm = 0.1933
Meta loss on this task batch = 3.2241e-01, Meta loss averaged over last 500 steps = 2.9328e-01, PNorm = 132.7908, GNorm = 0.2360
Meta loss on this task batch = 3.0893e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 132.7968, GNorm = 0.2414
Meta loss on this task batch = 2.9454e-01, Meta loss averaged over last 500 steps = 2.9334e-01, PNorm = 132.8031, GNorm = 0.2672
Meta loss on this task batch = 3.0897e-01, Meta loss averaged over last 500 steps = 2.9331e-01, PNorm = 132.8086, GNorm = 0.2239
Meta loss on this task batch = 3.0702e-01, Meta loss averaged over last 500 steps = 2.9329e-01, PNorm = 132.8133, GNorm = 0.2182
Meta loss on this task batch = 2.3924e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.8184, GNorm = 0.2125
Meta loss on this task batch = 3.5050e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 132.8223, GNorm = 0.2713
Meta loss on this task batch = 2.3430e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 132.8266, GNorm = 0.1988
Meta loss on this task batch = 3.0986e-01, Meta loss averaged over last 500 steps = 2.9326e-01, PNorm = 132.8305, GNorm = 0.2282
Meta loss on this task batch = 3.1704e-01, Meta loss averaged over last 500 steps = 2.9325e-01, PNorm = 132.8345, GNorm = 0.2731
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.8387, GNorm = 0.2371
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 132.8424, GNorm = 0.2785
Took 113.16483306884766 seconds to complete one epoch of meta training
Took 121.15816187858582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498237
Epoch 462
Meta loss on this task batch = 3.1660e-01, Meta loss averaged over last 500 steps = 2.9321e-01, PNorm = 132.8462, GNorm = 0.2287
Meta loss on this task batch = 3.0498e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.8503, GNorm = 0.2159
Meta loss on this task batch = 2.7142e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 132.8553, GNorm = 0.2057
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 132.8616, GNorm = 0.2174
Meta loss on this task batch = 2.9026e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 132.8680, GNorm = 0.2156
Meta loss on this task batch = 2.8286e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 132.8739, GNorm = 0.2317
Meta loss on this task batch = 2.7653e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 132.8808, GNorm = 0.2119
Meta loss on this task batch = 2.7230e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 132.8883, GNorm = 0.1941
Meta loss on this task batch = 2.4448e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 132.8959, GNorm = 0.2382
Meta loss on this task batch = 2.9715e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 132.9047, GNorm = 0.2628
Meta loss on this task batch = 2.7681e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 132.9130, GNorm = 0.2369
Meta loss on this task batch = 3.1607e-01, Meta loss averaged over last 500 steps = 2.9322e-01, PNorm = 132.9208, GNorm = 0.2223
Meta loss on this task batch = 2.7973e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 132.9282, GNorm = 0.2153
Meta loss on this task batch = 2.3972e-01, Meta loss averaged over last 500 steps = 2.9305e-01, PNorm = 132.9352, GNorm = 0.2178
Meta loss on this task batch = 3.5717e-01, Meta loss averaged over last 500 steps = 2.9309e-01, PNorm = 132.9393, GNorm = 0.3295
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.9301e-01, PNorm = 132.9430, GNorm = 0.2176
Meta loss on this task batch = 3.3739e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 132.9465, GNorm = 0.2386
Meta loss on this task batch = 2.7872e-01, Meta loss averaged over last 500 steps = 2.9286e-01, PNorm = 132.9499, GNorm = 0.2224
Meta loss on this task batch = 3.2444e-01, Meta loss averaged over last 500 steps = 2.9287e-01, PNorm = 132.9526, GNorm = 0.3331
Took 130.19734621047974 seconds to complete one epoch of meta training
Took 137.88681173324585 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496507
Epoch 463
Meta loss on this task batch = 3.0098e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 132.9563, GNorm = 0.2155
Meta loss on this task batch = 2.8880e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 132.9616, GNorm = 0.2279
Meta loss on this task batch = 3.0245e-01, Meta loss averaged over last 500 steps = 2.9300e-01, PNorm = 132.9678, GNorm = 0.2167
Meta loss on this task batch = 3.4421e-01, Meta loss averaged over last 500 steps = 2.9313e-01, PNorm = 132.9728, GNorm = 0.2720
Meta loss on this task batch = 3.3933e-01, Meta loss averaged over last 500 steps = 2.9330e-01, PNorm = 132.9774, GNorm = 0.2211
Meta loss on this task batch = 3.1535e-01, Meta loss averaged over last 500 steps = 2.9344e-01, PNorm = 132.9827, GNorm = 0.2330
Meta loss on this task batch = 2.9936e-01, Meta loss averaged over last 500 steps = 2.9352e-01, PNorm = 132.9889, GNorm = 0.1947
Meta loss on this task batch = 3.5115e-01, Meta loss averaged over last 500 steps = 2.9363e-01, PNorm = 132.9950, GNorm = 0.2376
Meta loss on this task batch = 2.9124e-01, Meta loss averaged over last 500 steps = 2.9352e-01, PNorm = 133.0015, GNorm = 0.2357
Meta loss on this task batch = 3.2036e-01, Meta loss averaged over last 500 steps = 2.9372e-01, PNorm = 133.0077, GNorm = 0.2328
Meta loss on this task batch = 2.6342e-01, Meta loss averaged over last 500 steps = 2.9365e-01, PNorm = 133.0144, GNorm = 0.1872
Meta loss on this task batch = 2.5225e-01, Meta loss averaged over last 500 steps = 2.9345e-01, PNorm = 133.0214, GNorm = 0.2433
Meta loss on this task batch = 3.0212e-01, Meta loss averaged over last 500 steps = 2.9343e-01, PNorm = 133.0283, GNorm = 0.2344
Meta loss on this task batch = 2.4937e-01, Meta loss averaged over last 500 steps = 2.9341e-01, PNorm = 133.0350, GNorm = 0.1956
Meta loss on this task batch = 2.7469e-01, Meta loss averaged over last 500 steps = 2.9346e-01, PNorm = 133.0417, GNorm = 0.2264
Meta loss on this task batch = 2.6302e-01, Meta loss averaged over last 500 steps = 2.9333e-01, PNorm = 133.0491, GNorm = 0.2290
Meta loss on this task batch = 2.9434e-01, Meta loss averaged over last 500 steps = 2.9327e-01, PNorm = 133.0566, GNorm = 0.2346
Meta loss on this task batch = 2.6022e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 133.0646, GNorm = 0.2267
Meta loss on this task batch = 2.2448e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 133.0730, GNorm = 0.2366
Took 171.76321625709534 seconds to complete one epoch of meta training
Took 179.33979439735413 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.532503
Found better MAML checkpoint after meta validation, saving now
Epoch 464
Meta loss on this task batch = 2.9833e-01, Meta loss averaged over last 500 steps = 2.9320e-01, PNorm = 133.0805, GNorm = 0.2430
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 2.9311e-01, PNorm = 133.0869, GNorm = 0.2406
Meta loss on this task batch = 2.9974e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 133.0936, GNorm = 0.2869
Meta loss on this task batch = 2.8585e-01, Meta loss averaged over last 500 steps = 2.9312e-01, PNorm = 133.1001, GNorm = 0.2226
Meta loss on this task batch = 2.9228e-01, Meta loss averaged over last 500 steps = 2.9315e-01, PNorm = 133.1061, GNorm = 0.2506
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 2.9296e-01, PNorm = 133.1124, GNorm = 0.2082
Meta loss on this task batch = 2.8551e-01, Meta loss averaged over last 500 steps = 2.9293e-01, PNorm = 133.1190, GNorm = 0.2147
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 133.1245, GNorm = 0.2243
Meta loss on this task batch = 3.8056e-01, Meta loss averaged over last 500 steps = 2.9319e-01, PNorm = 133.1289, GNorm = 0.3379
Meta loss on this task batch = 3.1086e-01, Meta loss averaged over last 500 steps = 2.9323e-01, PNorm = 133.1335, GNorm = 0.2432
Meta loss on this task batch = 2.7762e-01, Meta loss averaged over last 500 steps = 2.9324e-01, PNorm = 133.1392, GNorm = 0.2510
Meta loss on this task batch = 2.5699e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 133.1457, GNorm = 0.2180
Meta loss on this task batch = 2.4269e-01, Meta loss averaged over last 500 steps = 2.9302e-01, PNorm = 133.1527, GNorm = 0.2081
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 2.9310e-01, PNorm = 133.1596, GNorm = 0.2961
Meta loss on this task batch = 3.0843e-01, Meta loss averaged over last 500 steps = 2.9314e-01, PNorm = 133.1665, GNorm = 0.2429
Meta loss on this task batch = 2.5191e-01, Meta loss averaged over last 500 steps = 2.9316e-01, PNorm = 133.1728, GNorm = 0.1990
Meta loss on this task batch = 3.0233e-01, Meta loss averaged over last 500 steps = 2.9318e-01, PNorm = 133.1778, GNorm = 0.2657
Meta loss on this task batch = 2.1977e-01, Meta loss averaged over last 500 steps = 2.9303e-01, PNorm = 133.1833, GNorm = 0.2007
Meta loss on this task batch = 3.0069e-01, Meta loss averaged over last 500 steps = 2.9300e-01, PNorm = 133.1890, GNorm = 0.2645
Took 164.9500823020935 seconds to complete one epoch of meta training
Took 172.7389886379242 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493854
Epoch 465
Meta loss on this task batch = 2.9747e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 133.1956, GNorm = 0.2342
Meta loss on this task batch = 2.8596e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 133.2030, GNorm = 0.2124
Meta loss on this task batch = 2.8849e-01, Meta loss averaged over last 500 steps = 2.9294e-01, PNorm = 133.2109, GNorm = 0.2057
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 2.9295e-01, PNorm = 133.2184, GNorm = 0.2638
Meta loss on this task batch = 3.4670e-01, Meta loss averaged over last 500 steps = 2.9301e-01, PNorm = 133.2258, GNorm = 0.2229
Meta loss on this task batch = 2.3275e-01, Meta loss averaged over last 500 steps = 2.9286e-01, PNorm = 133.2335, GNorm = 0.2161
Meta loss on this task batch = 2.9874e-01, Meta loss averaged over last 500 steps = 2.9281e-01, PNorm = 133.2414, GNorm = 0.2287
Meta loss on this task batch = 2.7030e-01, Meta loss averaged over last 500 steps = 2.9269e-01, PNorm = 133.2498, GNorm = 0.2512
Meta loss on this task batch = 3.0602e-01, Meta loss averaged over last 500 steps = 2.9265e-01, PNorm = 133.2577, GNorm = 0.2272
Meta loss on this task batch = 3.0164e-01, Meta loss averaged over last 500 steps = 2.9261e-01, PNorm = 133.2656, GNorm = 0.2502
Meta loss on this task batch = 2.4593e-01, Meta loss averaged over last 500 steps = 2.9247e-01, PNorm = 133.2739, GNorm = 0.2079
Meta loss on this task batch = 2.4742e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 133.2826, GNorm = 0.2026
Meta loss on this task batch = 2.6523e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 133.2909, GNorm = 0.1968
Meta loss on this task batch = 3.0435e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 133.2979, GNorm = 0.2848
Meta loss on this task batch = 3.3034e-01, Meta loss averaged over last 500 steps = 2.9270e-01, PNorm = 133.3033, GNorm = 0.2537
Meta loss on this task batch = 3.2092e-01, Meta loss averaged over last 500 steps = 2.9277e-01, PNorm = 133.3078, GNorm = 0.2583
Meta loss on this task batch = 2.7085e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 133.3119, GNorm = 0.2199
Meta loss on this task batch = 2.7385e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 133.3164, GNorm = 0.1946
Meta loss on this task batch = 3.1734e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 133.3213, GNorm = 0.2624
Took 139.54113674163818 seconds to complete one epoch of meta training
Took 147.72396755218506 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493041
Epoch 466
Meta loss on this task batch = 3.4373e-01, Meta loss averaged over last 500 steps = 2.9289e-01, PNorm = 133.3270, GNorm = 0.2160
Meta loss on this task batch = 3.4251e-01, Meta loss averaged over last 500 steps = 2.9296e-01, PNorm = 133.3325, GNorm = 0.2217
Meta loss on this task batch = 2.9864e-01, Meta loss averaged over last 500 steps = 2.9303e-01, PNorm = 133.3385, GNorm = 0.2058
Meta loss on this task batch = 2.9842e-01, Meta loss averaged over last 500 steps = 2.9297e-01, PNorm = 133.3457, GNorm = 0.2250
Meta loss on this task batch = 2.7416e-01, Meta loss averaged over last 500 steps = 2.9282e-01, PNorm = 133.3528, GNorm = 0.1934
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.9288e-01, PNorm = 133.3601, GNorm = 0.2075
Meta loss on this task batch = 2.6658e-01, Meta loss averaged over last 500 steps = 2.9278e-01, PNorm = 133.3671, GNorm = 0.1872
Meta loss on this task batch = 3.1491e-01, Meta loss averaged over last 500 steps = 2.9293e-01, PNorm = 133.3736, GNorm = 0.2423
Meta loss on this task batch = 2.8345e-01, Meta loss averaged over last 500 steps = 2.9286e-01, PNorm = 133.3796, GNorm = 0.2287
Meta loss on this task batch = 2.6549e-01, Meta loss averaged over last 500 steps = 2.9271e-01, PNorm = 133.3859, GNorm = 0.2044
Meta loss on this task batch = 2.8325e-01, Meta loss averaged over last 500 steps = 2.9262e-01, PNorm = 133.3929, GNorm = 0.2162
Meta loss on this task batch = 2.2423e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 133.4008, GNorm = 0.2049
Meta loss on this task batch = 2.7230e-01, Meta loss averaged over last 500 steps = 2.9240e-01, PNorm = 133.4080, GNorm = 0.2060
Meta loss on this task batch = 2.7286e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 133.4146, GNorm = 0.2482
Meta loss on this task batch = 3.1190e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 133.4208, GNorm = 0.2443
Meta loss on this task batch = 3.1510e-01, Meta loss averaged over last 500 steps = 2.9243e-01, PNorm = 133.4262, GNorm = 0.2849
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 133.4318, GNorm = 0.2738
Meta loss on this task batch = 3.1334e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 133.4365, GNorm = 0.2322
Meta loss on this task batch = 3.2715e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 133.4413, GNorm = 0.3037
Took 144.07046151161194 seconds to complete one epoch of meta training
Took 152.4744188785553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502098
Epoch 467
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 133.4463, GNorm = 0.2247
Meta loss on this task batch = 3.4281e-01, Meta loss averaged over last 500 steps = 2.9259e-01, PNorm = 133.4507, GNorm = 0.2834
Meta loss on this task batch = 3.0943e-01, Meta loss averaged over last 500 steps = 2.9264e-01, PNorm = 133.4563, GNorm = 0.2760
Meta loss on this task batch = 3.1013e-01, Meta loss averaged over last 500 steps = 2.9254e-01, PNorm = 133.4628, GNorm = 0.2413
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.9258e-01, PNorm = 133.4688, GNorm = 0.2306
Meta loss on this task batch = 2.2933e-01, Meta loss averaged over last 500 steps = 2.9246e-01, PNorm = 133.4747, GNorm = 0.2523
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 2.9241e-01, PNorm = 133.4809, GNorm = 0.2300
Meta loss on this task batch = 2.3885e-01, Meta loss averaged over last 500 steps = 2.9237e-01, PNorm = 133.4879, GNorm = 0.1844
Meta loss on this task batch = 2.7964e-01, Meta loss averaged over last 500 steps = 2.9234e-01, PNorm = 133.4951, GNorm = 0.2358
Meta loss on this task batch = 3.2054e-01, Meta loss averaged over last 500 steps = 2.9249e-01, PNorm = 133.5019, GNorm = 0.2428
Meta loss on this task batch = 2.9384e-01, Meta loss averaged over last 500 steps = 2.9242e-01, PNorm = 133.5095, GNorm = 0.2321
Meta loss on this task batch = 3.2187e-01, Meta loss averaged over last 500 steps = 2.9244e-01, PNorm = 133.5172, GNorm = 0.2287
Meta loss on this task batch = 2.4066e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 133.5255, GNorm = 0.1971
Meta loss on this task batch = 2.5041e-01, Meta loss averaged over last 500 steps = 2.9236e-01, PNorm = 133.5346, GNorm = 0.2106
Meta loss on this task batch = 3.0507e-01, Meta loss averaged over last 500 steps = 2.9224e-01, PNorm = 133.5428, GNorm = 0.2366
Meta loss on this task batch = 2.8586e-01, Meta loss averaged over last 500 steps = 2.9223e-01, PNorm = 133.5505, GNorm = 0.2170
Meta loss on this task batch = 2.8785e-01, Meta loss averaged over last 500 steps = 2.9219e-01, PNorm = 133.5575, GNorm = 0.2331
Meta loss on this task batch = 3.1083e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 133.5655, GNorm = 0.2520
Meta loss on this task batch = 2.6789e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 133.5735, GNorm = 0.2461
Took 175.16621160507202 seconds to complete one epoch of meta training
Took 183.71717643737793 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494336
Epoch 468
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.9220e-01, PNorm = 133.5811, GNorm = 0.2085
Meta loss on this task batch = 3.0056e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 133.5894, GNorm = 0.2349
Meta loss on this task batch = 2.7283e-01, Meta loss averaged over last 500 steps = 2.9230e-01, PNorm = 133.5975, GNorm = 0.2116
Meta loss on this task batch = 2.6269e-01, Meta loss averaged over last 500 steps = 2.9215e-01, PNorm = 133.6059, GNorm = 0.1928
Meta loss on this task batch = 2.5348e-01, Meta loss averaged over last 500 steps = 2.9207e-01, PNorm = 133.6134, GNorm = 0.2439
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 133.6207, GNorm = 0.2435
Meta loss on this task batch = 2.9319e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 133.6276, GNorm = 0.2293
Meta loss on this task batch = 3.1294e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 133.6346, GNorm = 0.2637
Meta loss on this task batch = 2.9545e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 133.6414, GNorm = 0.2608
Meta loss on this task batch = 3.0752e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 133.6482, GNorm = 0.2295
Meta loss on this task batch = 3.1557e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 133.6547, GNorm = 0.2475
Meta loss on this task batch = 2.8045e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 133.6610, GNorm = 0.2194
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 133.6672, GNorm = 0.2371
Meta loss on this task batch = 2.4496e-01, Meta loss averaged over last 500 steps = 2.9190e-01, PNorm = 133.6735, GNorm = 0.2165
Meta loss on this task batch = 2.8526e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 133.6796, GNorm = 0.2288
Meta loss on this task batch = 2.9963e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 133.6849, GNorm = 0.2348
Meta loss on this task batch = 2.9992e-01, Meta loss averaged over last 500 steps = 2.9214e-01, PNorm = 133.6908, GNorm = 0.2279
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 133.6978, GNorm = 0.2640
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 133.7044, GNorm = 0.2585
Took 137.92169880867004 seconds to complete one epoch of meta training
Took 145.14448356628418 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491538
Epoch 469
Meta loss on this task batch = 2.8569e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 133.7108, GNorm = 0.2082
Meta loss on this task batch = 2.5930e-01, Meta loss averaged over last 500 steps = 2.9191e-01, PNorm = 133.7177, GNorm = 0.2048
Meta loss on this task batch = 2.8153e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 133.7243, GNorm = 0.2541
Meta loss on this task batch = 2.5132e-01, Meta loss averaged over last 500 steps = 2.9174e-01, PNorm = 133.7307, GNorm = 0.2274
Meta loss on this task batch = 2.9622e-01, Meta loss averaged over last 500 steps = 2.9175e-01, PNorm = 133.7362, GNorm = 0.1966
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 133.7424, GNorm = 0.2588
Meta loss on this task batch = 3.2279e-01, Meta loss averaged over last 500 steps = 2.9194e-01, PNorm = 133.7481, GNorm = 0.2518
Meta loss on this task batch = 3.3460e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 133.7539, GNorm = 0.2399
Meta loss on this task batch = 2.4796e-01, Meta loss averaged over last 500 steps = 2.9176e-01, PNorm = 133.7603, GNorm = 0.1903
Meta loss on this task batch = 3.2686e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 133.7673, GNorm = 0.2598
Meta loss on this task batch = 2.6230e-01, Meta loss averaged over last 500 steps = 2.9179e-01, PNorm = 133.7742, GNorm = 0.2049
Meta loss on this task batch = 2.6158e-01, Meta loss averaged over last 500 steps = 2.9179e-01, PNorm = 133.7815, GNorm = 0.2297
Meta loss on this task batch = 2.9180e-01, Meta loss averaged over last 500 steps = 2.9188e-01, PNorm = 133.7893, GNorm = 0.2027
Meta loss on this task batch = 3.0374e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 133.7965, GNorm = 0.2774
Meta loss on this task batch = 3.4940e-01, Meta loss averaged over last 500 steps = 2.9199e-01, PNorm = 133.8036, GNorm = 0.2538
Meta loss on this task batch = 3.1147e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 133.8102, GNorm = 0.2299
Meta loss on this task batch = 3.0201e-01, Meta loss averaged over last 500 steps = 2.9209e-01, PNorm = 133.8164, GNorm = 0.2149
Meta loss on this task batch = 2.6788e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 133.8213, GNorm = 0.2240
Meta loss on this task batch = 3.7130e-01, Meta loss averaged over last 500 steps = 2.9211e-01, PNorm = 133.8239, GNorm = 0.3090
Took 147.9434621334076 seconds to complete one epoch of meta training
Took 156.4276933670044 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485750
Epoch 470
Meta loss on this task batch = 2.9982e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 133.8254, GNorm = 0.2909
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 2.9195e-01, PNorm = 133.8276, GNorm = 0.2087
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 2.9206e-01, PNorm = 133.8305, GNorm = 0.2333
Meta loss on this task batch = 3.1605e-01, Meta loss averaged over last 500 steps = 2.9213e-01, PNorm = 133.8340, GNorm = 0.2259
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 2.9217e-01, PNorm = 133.8385, GNorm = 0.2294
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.9201e-01, PNorm = 133.8444, GNorm = 0.2298
Meta loss on this task batch = 2.6419e-01, Meta loss averaged over last 500 steps = 2.9195e-01, PNorm = 133.8505, GNorm = 0.1974
Meta loss on this task batch = 2.4246e-01, Meta loss averaged over last 500 steps = 2.9186e-01, PNorm = 133.8577, GNorm = 0.2206
Meta loss on this task batch = 3.3305e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 133.8649, GNorm = 0.2458
Meta loss on this task batch = 2.8190e-01, Meta loss averaged over last 500 steps = 2.9183e-01, PNorm = 133.8717, GNorm = 0.2155
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.9176e-01, PNorm = 133.8784, GNorm = 0.2095
Meta loss on this task batch = 3.0730e-01, Meta loss averaged over last 500 steps = 2.9180e-01, PNorm = 133.8852, GNorm = 0.2574
Meta loss on this task batch = 3.3493e-01, Meta loss averaged over last 500 steps = 2.9188e-01, PNorm = 133.8911, GNorm = 0.2602
Meta loss on this task batch = 3.2731e-01, Meta loss averaged over last 500 steps = 2.9200e-01, PNorm = 133.8977, GNorm = 0.2935
Meta loss on this task batch = 3.0907e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 133.9048, GNorm = 0.2557
Meta loss on this task batch = 2.7814e-01, Meta loss averaged over last 500 steps = 2.9196e-01, PNorm = 133.9127, GNorm = 0.2191
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 133.9210, GNorm = 0.1989
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 2.9192e-01, PNorm = 133.9292, GNorm = 0.2330
Meta loss on this task batch = 3.1403e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 133.9375, GNorm = 0.2591
Took 132.6576371192932 seconds to complete one epoch of meta training
Took 141.30584120750427 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475414
Epoch 471
Meta loss on this task batch = 2.5079e-01, Meta loss averaged over last 500 steps = 2.9198e-01, PNorm = 133.9463, GNorm = 0.1904
Meta loss on this task batch = 2.6472e-01, Meta loss averaged over last 500 steps = 2.9189e-01, PNorm = 133.9550, GNorm = 0.2619
Meta loss on this task batch = 3.0627e-01, Meta loss averaged over last 500 steps = 2.9203e-01, PNorm = 133.9634, GNorm = 0.2503
Meta loss on this task batch = 2.8850e-01, Meta loss averaged over last 500 steps = 2.9204e-01, PNorm = 133.9714, GNorm = 0.2112
Meta loss on this task batch = 3.0232e-01, Meta loss averaged over last 500 steps = 2.9197e-01, PNorm = 133.9789, GNorm = 0.2324
Meta loss on this task batch = 3.0012e-01, Meta loss averaged over last 500 steps = 2.9193e-01, PNorm = 133.9863, GNorm = 0.2468
Meta loss on this task batch = 2.6446e-01, Meta loss averaged over last 500 steps = 2.9185e-01, PNorm = 133.9941, GNorm = 0.2262
Meta loss on this task batch = 2.8595e-01, Meta loss averaged over last 500 steps = 2.9169e-01, PNorm = 134.0016, GNorm = 0.1960
Meta loss on this task batch = 2.6111e-01, Meta loss averaged over last 500 steps = 2.9156e-01, PNorm = 134.0082, GNorm = 0.2167
Meta loss on this task batch = 3.0894e-01, Meta loss averaged over last 500 steps = 2.9153e-01, PNorm = 134.0142, GNorm = 0.2518
Meta loss on this task batch = 3.2155e-01, Meta loss averaged over last 500 steps = 2.9163e-01, PNorm = 134.0189, GNorm = 0.2488
Meta loss on this task batch = 3.6871e-01, Meta loss averaged over last 500 steps = 2.9177e-01, PNorm = 134.0215, GNorm = 0.3057
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 2.9178e-01, PNorm = 134.0246, GNorm = 0.2283
Meta loss on this task batch = 2.5045e-01, Meta loss averaged over last 500 steps = 2.9171e-01, PNorm = 134.0284, GNorm = 0.2210
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 2.9175e-01, PNorm = 134.0328, GNorm = 0.2382
Meta loss on this task batch = 3.0607e-01, Meta loss averaged over last 500 steps = 2.9172e-01, PNorm = 134.0383, GNorm = 0.2592
Meta loss on this task batch = 2.9438e-01, Meta loss averaged over last 500 steps = 2.9182e-01, PNorm = 134.0448, GNorm = 0.2595
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.9171e-01, PNorm = 134.0515, GNorm = 0.2043
Meta loss on this task batch = 2.6275e-01, Meta loss averaged over last 500 steps = 2.9171e-01, PNorm = 134.0591, GNorm = 0.2362
Took 143.39641308784485 seconds to complete one epoch of meta training
Took 151.29739236831665 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501019
Epoch 472
Meta loss on this task batch = 2.8068e-01, Meta loss averaged over last 500 steps = 2.9157e-01, PNorm = 134.0670, GNorm = 0.2081
Meta loss on this task batch = 2.7330e-01, Meta loss averaged over last 500 steps = 2.9168e-01, PNorm = 134.0748, GNorm = 0.2186
Meta loss on this task batch = 2.2137e-01, Meta loss averaged over last 500 steps = 2.9143e-01, PNorm = 134.0823, GNorm = 0.2060
Meta loss on this task batch = 2.8182e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 134.0895, GNorm = 0.1999
Meta loss on this task batch = 2.9787e-01, Meta loss averaged over last 500 steps = 2.9157e-01, PNorm = 134.0958, GNorm = 0.2729
Meta loss on this task batch = 2.4521e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 134.1019, GNorm = 0.2049
Meta loss on this task batch = 2.6198e-01, Meta loss averaged over last 500 steps = 2.9141e-01, PNorm = 134.1066, GNorm = 0.2353
Meta loss on this task batch = 3.5411e-01, Meta loss averaged over last 500 steps = 2.9157e-01, PNorm = 134.1111, GNorm = 0.2585
Meta loss on this task batch = 3.6469e-01, Meta loss averaged over last 500 steps = 2.9174e-01, PNorm = 134.1164, GNorm = 0.2345
Meta loss on this task batch = 2.7702e-01, Meta loss averaged over last 500 steps = 2.9174e-01, PNorm = 134.1208, GNorm = 0.3263
Meta loss on this task batch = 2.2031e-01, Meta loss averaged over last 500 steps = 2.9154e-01, PNorm = 134.1257, GNorm = 0.1810
Meta loss on this task batch = 3.0755e-01, Meta loss averaged over last 500 steps = 2.9150e-01, PNorm = 134.1305, GNorm = 0.2399
Meta loss on this task batch = 3.0501e-01, Meta loss averaged over last 500 steps = 2.9138e-01, PNorm = 134.1353, GNorm = 0.2902
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.9133e-01, PNorm = 134.1415, GNorm = 0.2395
Meta loss on this task batch = 3.4393e-01, Meta loss averaged over last 500 steps = 2.9141e-01, PNorm = 134.1479, GNorm = 0.2318
Meta loss on this task batch = 2.9730e-01, Meta loss averaged over last 500 steps = 2.9139e-01, PNorm = 134.1545, GNorm = 0.2339
Meta loss on this task batch = 2.3591e-01, Meta loss averaged over last 500 steps = 2.9135e-01, PNorm = 134.1621, GNorm = 0.1750
Meta loss on this task batch = 3.6703e-01, Meta loss averaged over last 500 steps = 2.9156e-01, PNorm = 134.1685, GNorm = 0.2725
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.9146e-01, PNorm = 134.1752, GNorm = 0.2433
Took 121.08561825752258 seconds to complete one epoch of meta training
Took 129.07080459594727 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487740
Epoch 473
Meta loss on this task batch = 3.0382e-01, Meta loss averaged over last 500 steps = 2.9154e-01, PNorm = 134.1814, GNorm = 0.2347
Meta loss on this task batch = 2.6368e-01, Meta loss averaged over last 500 steps = 2.9143e-01, PNorm = 134.1885, GNorm = 0.2357
Meta loss on this task batch = 3.0882e-01, Meta loss averaged over last 500 steps = 2.9152e-01, PNorm = 134.1948, GNorm = 0.2632
Meta loss on this task batch = 2.8249e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 134.2014, GNorm = 0.1909
Meta loss on this task batch = 3.0851e-01, Meta loss averaged over last 500 steps = 2.9153e-01, PNorm = 134.2064, GNorm = 0.2764
Meta loss on this task batch = 2.6113e-01, Meta loss averaged over last 500 steps = 2.9143e-01, PNorm = 134.2108, GNorm = 0.2244
Meta loss on this task batch = 2.9468e-01, Meta loss averaged over last 500 steps = 2.9146e-01, PNorm = 134.2151, GNorm = 0.2230
Meta loss on this task batch = 3.8445e-01, Meta loss averaged over last 500 steps = 2.9161e-01, PNorm = 134.2194, GNorm = 0.2650
Meta loss on this task batch = 2.8218e-01, Meta loss averaged over last 500 steps = 2.9163e-01, PNorm = 134.2251, GNorm = 0.2335
Meta loss on this task batch = 2.9458e-01, Meta loss averaged over last 500 steps = 2.9159e-01, PNorm = 134.2316, GNorm = 0.2379
Meta loss on this task batch = 2.8770e-01, Meta loss averaged over last 500 steps = 2.9153e-01, PNorm = 134.2383, GNorm = 0.2157
Meta loss on this task batch = 2.4079e-01, Meta loss averaged over last 500 steps = 2.9147e-01, PNorm = 134.2458, GNorm = 0.2231
Meta loss on this task batch = 2.5597e-01, Meta loss averaged over last 500 steps = 2.9133e-01, PNorm = 134.2538, GNorm = 0.1945
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 2.9134e-01, PNorm = 134.2617, GNorm = 0.2449
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 2.9137e-01, PNorm = 134.2698, GNorm = 0.2187
Meta loss on this task batch = 2.9875e-01, Meta loss averaged over last 500 steps = 2.9137e-01, PNorm = 134.2778, GNorm = 0.2400
Meta loss on this task batch = 2.7469e-01, Meta loss averaged over last 500 steps = 2.9132e-01, PNorm = 134.2852, GNorm = 0.2213
Meta loss on this task batch = 2.4611e-01, Meta loss averaged over last 500 steps = 2.9122e-01, PNorm = 134.2934, GNorm = 0.2010
Meta loss on this task batch = 3.1613e-01, Meta loss averaged over last 500 steps = 2.9128e-01, PNorm = 134.3006, GNorm = 0.3082
Took 116.71878576278687 seconds to complete one epoch of meta training
Took 124.7386429309845 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479476
Epoch 474
Meta loss on this task batch = 2.8719e-01, Meta loss averaged over last 500 steps = 2.9131e-01, PNorm = 134.3085, GNorm = 0.2104
Meta loss on this task batch = 2.4844e-01, Meta loss averaged over last 500 steps = 2.9125e-01, PNorm = 134.3166, GNorm = 0.2491
Meta loss on this task batch = 3.0745e-01, Meta loss averaged over last 500 steps = 2.9128e-01, PNorm = 134.3239, GNorm = 0.2285
Meta loss on this task batch = 3.5842e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 134.3306, GNorm = 0.2757
Meta loss on this task batch = 2.8433e-01, Meta loss averaged over last 500 steps = 2.9129e-01, PNorm = 134.3370, GNorm = 0.2292
Meta loss on this task batch = 2.9728e-01, Meta loss averaged over last 500 steps = 2.9134e-01, PNorm = 134.3442, GNorm = 0.2357
Meta loss on this task batch = 2.6736e-01, Meta loss averaged over last 500 steps = 2.9134e-01, PNorm = 134.3511, GNorm = 0.2425
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 134.3594, GNorm = 0.2684
Meta loss on this task batch = 2.9701e-01, Meta loss averaged over last 500 steps = 2.9139e-01, PNorm = 134.3680, GNorm = 0.2265
Meta loss on this task batch = 3.4392e-01, Meta loss averaged over last 500 steps = 2.9148e-01, PNorm = 134.3768, GNorm = 0.2210
Meta loss on this task batch = 3.1660e-01, Meta loss averaged over last 500 steps = 2.9152e-01, PNorm = 134.3852, GNorm = 0.2185
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 2.9148e-01, PNorm = 134.3940, GNorm = 0.2335
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.9157e-01, PNorm = 134.4019, GNorm = 0.2084
Meta loss on this task batch = 2.2107e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 134.4095, GNorm = 0.2155
Meta loss on this task batch = 2.8163e-01, Meta loss averaged over last 500 steps = 2.9151e-01, PNorm = 134.4173, GNorm = 0.2076
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.9135e-01, PNorm = 134.4244, GNorm = 0.1971
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 2.9140e-01, PNorm = 134.4306, GNorm = 0.2475
Meta loss on this task batch = 3.1871e-01, Meta loss averaged over last 500 steps = 2.9141e-01, PNorm = 134.4362, GNorm = 0.2845
Meta loss on this task batch = 2.2848e-01, Meta loss averaged over last 500 steps = 2.9133e-01, PNorm = 134.4428, GNorm = 0.2741
Took 112.53122234344482 seconds to complete one epoch of meta training
Took 120.03350329399109 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483521
Epoch 475
Meta loss on this task batch = 3.3615e-01, Meta loss averaged over last 500 steps = 2.9137e-01, PNorm = 134.4481, GNorm = 0.2723
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.9133e-01, PNorm = 134.4540, GNorm = 0.2201
Meta loss on this task batch = 2.7478e-01, Meta loss averaged over last 500 steps = 2.9126e-01, PNorm = 134.4596, GNorm = 0.2289
Meta loss on this task batch = 3.0886e-01, Meta loss averaged over last 500 steps = 2.9119e-01, PNorm = 134.4654, GNorm = 0.2493
Meta loss on this task batch = 3.1080e-01, Meta loss averaged over last 500 steps = 2.9122e-01, PNorm = 134.4703, GNorm = 0.2201
Meta loss on this task batch = 2.9146e-01, Meta loss averaged over last 500 steps = 2.9123e-01, PNorm = 134.4758, GNorm = 0.2429
Meta loss on this task batch = 2.8742e-01, Meta loss averaged over last 500 steps = 2.9133e-01, PNorm = 134.4818, GNorm = 0.2104
Meta loss on this task batch = 2.9876e-01, Meta loss averaged over last 500 steps = 2.9136e-01, PNorm = 134.4866, GNorm = 0.2355
Meta loss on this task batch = 2.8667e-01, Meta loss averaged over last 500 steps = 2.9146e-01, PNorm = 134.4921, GNorm = 0.2314
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.9148e-01, PNorm = 134.4986, GNorm = 0.2130
Meta loss on this task batch = 3.1089e-01, Meta loss averaged over last 500 steps = 2.9144e-01, PNorm = 134.5063, GNorm = 0.2505
Meta loss on this task batch = 2.9101e-01, Meta loss averaged over last 500 steps = 2.9145e-01, PNorm = 134.5148, GNorm = 0.2269
Meta loss on this task batch = 3.0198e-01, Meta loss averaged over last 500 steps = 2.9150e-01, PNorm = 134.5238, GNorm = 0.2376
Meta loss on this task batch = 2.8114e-01, Meta loss averaged over last 500 steps = 2.9145e-01, PNorm = 134.5329, GNorm = 0.2070
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.9135e-01, PNorm = 134.5412, GNorm = 0.1978
Meta loss on this task batch = 2.9557e-01, Meta loss averaged over last 500 steps = 2.9138e-01, PNorm = 134.5490, GNorm = 0.2315
Meta loss on this task batch = 2.3524e-01, Meta loss averaged over last 500 steps = 2.9120e-01, PNorm = 134.5565, GNorm = 0.1972
Meta loss on this task batch = 2.3467e-01, Meta loss averaged over last 500 steps = 2.9103e-01, PNorm = 134.5635, GNorm = 0.2259
Meta loss on this task batch = 2.7943e-01, Meta loss averaged over last 500 steps = 2.9107e-01, PNorm = 134.5708, GNorm = 0.2471
Took 115.68122482299805 seconds to complete one epoch of meta training
Took 122.52482390403748 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493725
Epoch 476
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 2.9105e-01, PNorm = 134.5784, GNorm = 0.2380
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.9111e-01, PNorm = 134.5861, GNorm = 0.2288
Meta loss on this task batch = 3.1076e-01, Meta loss averaged over last 500 steps = 2.9115e-01, PNorm = 134.5929, GNorm = 0.2777
Meta loss on this task batch = 2.8443e-01, Meta loss averaged over last 500 steps = 2.9112e-01, PNorm = 134.5990, GNorm = 0.2381
Meta loss on this task batch = 2.5724e-01, Meta loss averaged over last 500 steps = 2.9105e-01, PNorm = 134.6051, GNorm = 0.1882
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.9113e-01, PNorm = 134.6095, GNorm = 0.2988
Meta loss on this task batch = 2.2980e-01, Meta loss averaged over last 500 steps = 2.9098e-01, PNorm = 134.6142, GNorm = 0.2093
Meta loss on this task batch = 2.0663e-01, Meta loss averaged over last 500 steps = 2.9084e-01, PNorm = 134.6199, GNorm = 0.2081
Meta loss on this task batch = 2.8810e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 134.6263, GNorm = 0.2028
Meta loss on this task batch = 3.3114e-01, Meta loss averaged over last 500 steps = 2.9096e-01, PNorm = 134.6314, GNorm = 0.2596
Meta loss on this task batch = 3.0249e-01, Meta loss averaged over last 500 steps = 2.9088e-01, PNorm = 134.6366, GNorm = 0.2363
Meta loss on this task batch = 3.0053e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 134.6421, GNorm = 0.2510
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 2.9083e-01, PNorm = 134.6479, GNorm = 0.2237
Meta loss on this task batch = 2.9736e-01, Meta loss averaged over last 500 steps = 2.9069e-01, PNorm = 134.6543, GNorm = 0.2393
Meta loss on this task batch = 3.3730e-01, Meta loss averaged over last 500 steps = 2.9083e-01, PNorm = 134.6606, GNorm = 0.2390
Meta loss on this task batch = 3.7457e-01, Meta loss averaged over last 500 steps = 2.9099e-01, PNorm = 134.6661, GNorm = 0.2825
Meta loss on this task batch = 2.2511e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 134.6717, GNorm = 0.2074
Meta loss on this task batch = 2.7904e-01, Meta loss averaged over last 500 steps = 2.9092e-01, PNorm = 134.6773, GNorm = 0.2362
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.9091e-01, PNorm = 134.6830, GNorm = 0.2346
Took 118.44653344154358 seconds to complete one epoch of meta training
Took 125.94146752357483 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494884
Epoch 477
Meta loss on this task batch = 2.8159e-01, Meta loss averaged over last 500 steps = 2.9096e-01, PNorm = 134.6894, GNorm = 0.2067
Meta loss on this task batch = 3.5231e-01, Meta loss averaged over last 500 steps = 2.9114e-01, PNorm = 134.6953, GNorm = 0.2314
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.9110e-01, PNorm = 134.7016, GNorm = 0.2072
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.9102e-01, PNorm = 134.7078, GNorm = 0.1859
Meta loss on this task batch = 2.9069e-01, Meta loss averaged over last 500 steps = 2.9102e-01, PNorm = 134.7145, GNorm = 0.2295
Meta loss on this task batch = 2.8069e-01, Meta loss averaged over last 500 steps = 2.9104e-01, PNorm = 134.7214, GNorm = 0.2110
Meta loss on this task batch = 3.0486e-01, Meta loss averaged over last 500 steps = 2.9117e-01, PNorm = 134.7279, GNorm = 0.2398
Meta loss on this task batch = 2.8997e-01, Meta loss averaged over last 500 steps = 2.9114e-01, PNorm = 134.7351, GNorm = 0.2334
Meta loss on this task batch = 2.9576e-01, Meta loss averaged over last 500 steps = 2.9109e-01, PNorm = 134.7422, GNorm = 0.2106
Meta loss on this task batch = 2.6088e-01, Meta loss averaged over last 500 steps = 2.9107e-01, PNorm = 134.7495, GNorm = 0.2338
Meta loss on this task batch = 2.8428e-01, Meta loss averaged over last 500 steps = 2.9106e-01, PNorm = 134.7573, GNorm = 0.2438
Meta loss on this task batch = 3.4373e-01, Meta loss averaged over last 500 steps = 2.9124e-01, PNorm = 134.7650, GNorm = 0.2411
Meta loss on this task batch = 3.2519e-01, Meta loss averaged over last 500 steps = 2.9130e-01, PNorm = 134.7726, GNorm = 0.2488
Meta loss on this task batch = 2.3200e-01, Meta loss averaged over last 500 steps = 2.9117e-01, PNorm = 134.7802, GNorm = 0.2446
Meta loss on this task batch = 2.6406e-01, Meta loss averaged over last 500 steps = 2.9113e-01, PNorm = 134.7884, GNorm = 0.2334
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 2.9110e-01, PNorm = 134.7969, GNorm = 0.2345
Meta loss on this task batch = 2.6758e-01, Meta loss averaged over last 500 steps = 2.9104e-01, PNorm = 134.8056, GNorm = 0.2254
Meta loss on this task batch = 2.4998e-01, Meta loss averaged over last 500 steps = 2.9099e-01, PNorm = 134.8128, GNorm = 0.2529
Meta loss on this task batch = 2.9865e-01, Meta loss averaged over last 500 steps = 2.9100e-01, PNorm = 134.8193, GNorm = 0.2444
Took 112.63556599617004 seconds to complete one epoch of meta training
Took 120.4624502658844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504117
Epoch 478
Meta loss on this task batch = 2.3499e-01, Meta loss averaged over last 500 steps = 2.9094e-01, PNorm = 134.8257, GNorm = 0.2177
Meta loss on this task batch = 2.6485e-01, Meta loss averaged over last 500 steps = 2.9088e-01, PNorm = 134.8324, GNorm = 0.2382
Meta loss on this task batch = 2.8269e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 134.8372, GNorm = 0.2791
Meta loss on this task batch = 3.1974e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 134.8413, GNorm = 0.2609
Meta loss on this task batch = 2.6028e-01, Meta loss averaged over last 500 steps = 2.9082e-01, PNorm = 134.8454, GNorm = 0.2052
Meta loss on this task batch = 3.0514e-01, Meta loss averaged over last 500 steps = 2.9078e-01, PNorm = 134.8491, GNorm = 0.2887
Meta loss on this task batch = 2.8178e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 134.8534, GNorm = 0.2548
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.9080e-01, PNorm = 134.8584, GNorm = 0.2522
Meta loss on this task batch = 2.4341e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 134.8643, GNorm = 0.2041
Meta loss on this task batch = 2.7292e-01, Meta loss averaged over last 500 steps = 2.9067e-01, PNorm = 134.8706, GNorm = 0.2128
Meta loss on this task batch = 2.9779e-01, Meta loss averaged over last 500 steps = 2.9076e-01, PNorm = 134.8777, GNorm = 0.2200
Meta loss on this task batch = 3.2566e-01, Meta loss averaged over last 500 steps = 2.9086e-01, PNorm = 134.8828, GNorm = 0.2794
Meta loss on this task batch = 3.5046e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 134.8867, GNorm = 0.2699
Meta loss on this task batch = 3.5760e-01, Meta loss averaged over last 500 steps = 2.9098e-01, PNorm = 134.8906, GNorm = 0.2503
Meta loss on this task batch = 3.0205e-01, Meta loss averaged over last 500 steps = 2.9104e-01, PNorm = 134.8941, GNorm = 0.2673
Meta loss on this task batch = 3.0589e-01, Meta loss averaged over last 500 steps = 2.9106e-01, PNorm = 134.8974, GNorm = 0.2834
Meta loss on this task batch = 2.7055e-01, Meta loss averaged over last 500 steps = 2.9102e-01, PNorm = 134.9020, GNorm = 0.2427
Meta loss on this task batch = 2.7896e-01, Meta loss averaged over last 500 steps = 2.9097e-01, PNorm = 134.9070, GNorm = 0.2019
Meta loss on this task batch = 2.6064e-01, Meta loss averaged over last 500 steps = 2.9090e-01, PNorm = 134.9124, GNorm = 0.2259
Took 113.79306221008301 seconds to complete one epoch of meta training
Took 121.30756139755249 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468012
Epoch 479
Meta loss on this task batch = 2.7285e-01, Meta loss averaged over last 500 steps = 2.9080e-01, PNorm = 134.9195, GNorm = 0.2343
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 2.9078e-01, PNorm = 134.9273, GNorm = 0.2029
Meta loss on this task batch = 2.6726e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 134.9358, GNorm = 0.2113
Meta loss on this task batch = 2.7120e-01, Meta loss averaged over last 500 steps = 2.9071e-01, PNorm = 134.9441, GNorm = 0.1841
Meta loss on this task batch = 3.0506e-01, Meta loss averaged over last 500 steps = 2.9079e-01, PNorm = 134.9518, GNorm = 0.2601
Meta loss on this task batch = 2.6257e-01, Meta loss averaged over last 500 steps = 2.9070e-01, PNorm = 134.9586, GNorm = 0.2670
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 2.9069e-01, PNorm = 134.9654, GNorm = 0.2267
Meta loss on this task batch = 2.9010e-01, Meta loss averaged over last 500 steps = 2.9053e-01, PNorm = 134.9719, GNorm = 0.2381
Meta loss on this task batch = 2.8957e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 134.9772, GNorm = 0.2478
Meta loss on this task batch = 2.8186e-01, Meta loss averaged over last 500 steps = 2.9052e-01, PNorm = 134.9834, GNorm = 0.2020
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.9062e-01, PNorm = 134.9889, GNorm = 0.2251
Meta loss on this task batch = 3.5062e-01, Meta loss averaged over last 500 steps = 2.9071e-01, PNorm = 134.9924, GNorm = 0.3280
Meta loss on this task batch = 2.4315e-01, Meta loss averaged over last 500 steps = 2.9064e-01, PNorm = 134.9957, GNorm = 0.2139
Meta loss on this task batch = 3.1410e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 135.0000, GNorm = 0.2206
Meta loss on this task batch = 3.0989e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 135.0049, GNorm = 0.2361
Meta loss on this task batch = 3.0473e-01, Meta loss averaged over last 500 steps = 2.9063e-01, PNorm = 135.0118, GNorm = 0.2797
Meta loss on this task batch = 3.1922e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 135.0187, GNorm = 0.2473
Meta loss on this task batch = 2.8545e-01, Meta loss averaged over last 500 steps = 2.9069e-01, PNorm = 135.0260, GNorm = 0.2349
Meta loss on this task batch = 2.3958e-01, Meta loss averaged over last 500 steps = 2.9050e-01, PNorm = 135.0340, GNorm = 0.2691
Took 119.51854252815247 seconds to complete one epoch of meta training
Took 127.40091896057129 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488057
Epoch 480
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.9052e-01, PNorm = 135.0429, GNorm = 0.1880
Meta loss on this task batch = 2.6567e-01, Meta loss averaged over last 500 steps = 2.9059e-01, PNorm = 135.0515, GNorm = 0.2360
Meta loss on this task batch = 2.5809e-01, Meta loss averaged over last 500 steps = 2.9056e-01, PNorm = 135.0597, GNorm = 0.1986
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 2.9066e-01, PNorm = 135.0672, GNorm = 0.2302
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 2.9066e-01, PNorm = 135.0756, GNorm = 0.2326
Meta loss on this task batch = 3.1203e-01, Meta loss averaged over last 500 steps = 2.9073e-01, PNorm = 135.0834, GNorm = 0.2317
Meta loss on this task batch = 2.9187e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 135.0899, GNorm = 0.2344
Meta loss on this task batch = 3.1361e-01, Meta loss averaged over last 500 steps = 2.9072e-01, PNorm = 135.0961, GNorm = 0.2637
Meta loss on this task batch = 2.8006e-01, Meta loss averaged over last 500 steps = 2.9069e-01, PNorm = 135.1024, GNorm = 0.2229
Meta loss on this task batch = 3.1220e-01, Meta loss averaged over last 500 steps = 2.9059e-01, PNorm = 135.1091, GNorm = 0.2521
Meta loss on this task batch = 3.5574e-01, Meta loss averaged over last 500 steps = 2.9067e-01, PNorm = 135.1150, GNorm = 0.2495
Meta loss on this task batch = 2.8148e-01, Meta loss averaged over last 500 steps = 2.9048e-01, PNorm = 135.1206, GNorm = 0.2385
Meta loss on this task batch = 2.3214e-01, Meta loss averaged over last 500 steps = 2.9043e-01, PNorm = 135.1268, GNorm = 0.2044
Meta loss on this task batch = 2.9983e-01, Meta loss averaged over last 500 steps = 2.9054e-01, PNorm = 135.1326, GNorm = 0.2233
Meta loss on this task batch = 2.9264e-01, Meta loss averaged over last 500 steps = 2.9053e-01, PNorm = 135.1379, GNorm = 0.2260
Meta loss on this task batch = 3.0229e-01, Meta loss averaged over last 500 steps = 2.9057e-01, PNorm = 135.1439, GNorm = 0.2602
Meta loss on this task batch = 2.6328e-01, Meta loss averaged over last 500 steps = 2.9042e-01, PNorm = 135.1509, GNorm = 0.2373
Meta loss on this task batch = 2.8054e-01, Meta loss averaged over last 500 steps = 2.9047e-01, PNorm = 135.1576, GNorm = 0.2256
Meta loss on this task batch = 3.3723e-01, Meta loss averaged over last 500 steps = 2.9062e-01, PNorm = 135.1636, GNorm = 0.3708
Took 117.67399859428406 seconds to complete one epoch of meta training
Took 125.50369048118591 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491242
Epoch 481
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 135.1704, GNorm = 0.2631
Meta loss on this task batch = 3.1306e-01, Meta loss averaged over last 500 steps = 2.9085e-01, PNorm = 135.1769, GNorm = 0.2311
Meta loss on this task batch = 2.8363e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 135.1831, GNorm = 0.2182
Meta loss on this task batch = 3.0674e-01, Meta loss averaged over last 500 steps = 2.9095e-01, PNorm = 135.1889, GNorm = 0.2427
Meta loss on this task batch = 2.6824e-01, Meta loss averaged over last 500 steps = 2.9095e-01, PNorm = 135.1943, GNorm = 0.2547
Meta loss on this task batch = 3.1891e-01, Meta loss averaged over last 500 steps = 2.9093e-01, PNorm = 135.1997, GNorm = 0.2429
Meta loss on this task batch = 2.8966e-01, Meta loss averaged over last 500 steps = 2.9092e-01, PNorm = 135.2051, GNorm = 0.2928
Meta loss on this task batch = 2.9220e-01, Meta loss averaged over last 500 steps = 2.9088e-01, PNorm = 135.2093, GNorm = 0.2373
Meta loss on this task batch = 3.0816e-01, Meta loss averaged over last 500 steps = 2.9105e-01, PNorm = 135.2136, GNorm = 0.2169
Meta loss on this task batch = 3.1776e-01, Meta loss averaged over last 500 steps = 2.9112e-01, PNorm = 135.2177, GNorm = 0.1951
Meta loss on this task batch = 2.2033e-01, Meta loss averaged over last 500 steps = 2.9100e-01, PNorm = 135.2234, GNorm = 0.1981
Meta loss on this task batch = 2.9795e-01, Meta loss averaged over last 500 steps = 2.9102e-01, PNorm = 135.2303, GNorm = 0.2414
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.9093e-01, PNorm = 135.2372, GNorm = 0.2174
Meta loss on this task batch = 2.9365e-01, Meta loss averaged over last 500 steps = 2.9089e-01, PNorm = 135.2440, GNorm = 0.2312
Meta loss on this task batch = 2.5747e-01, Meta loss averaged over last 500 steps = 2.9081e-01, PNorm = 135.2508, GNorm = 0.2067
Meta loss on this task batch = 2.6196e-01, Meta loss averaged over last 500 steps = 2.9072e-01, PNorm = 135.2576, GNorm = 0.2094
Meta loss on this task batch = 2.8438e-01, Meta loss averaged over last 500 steps = 2.9076e-01, PNorm = 135.2639, GNorm = 0.2396
Meta loss on this task batch = 3.5720e-01, Meta loss averaged over last 500 steps = 2.9086e-01, PNorm = 135.2695, GNorm = 0.2932
Meta loss on this task batch = 2.9267e-01, Meta loss averaged over last 500 steps = 2.9081e-01, PNorm = 135.2752, GNorm = 0.2505
Took 116.85030269622803 seconds to complete one epoch of meta training
Took 124.61961555480957 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.462627
Epoch 482
Meta loss on this task batch = 2.6259e-01, Meta loss averaged over last 500 steps = 2.9070e-01, PNorm = 135.2812, GNorm = 0.2210
Meta loss on this task batch = 2.7947e-01, Meta loss averaged over last 500 steps = 2.9068e-01, PNorm = 135.2870, GNorm = 0.2326
Meta loss on this task batch = 3.1470e-01, Meta loss averaged over last 500 steps = 2.9061e-01, PNorm = 135.2934, GNorm = 0.2409
Meta loss on this task batch = 2.5660e-01, Meta loss averaged over last 500 steps = 2.9061e-01, PNorm = 135.2994, GNorm = 0.2108
Meta loss on this task batch = 3.5110e-01, Meta loss averaged over last 500 steps = 2.9070e-01, PNorm = 135.3052, GNorm = 0.2552
Meta loss on this task batch = 2.9086e-01, Meta loss averaged over last 500 steps = 2.9075e-01, PNorm = 135.3108, GNorm = 0.2131
Meta loss on this task batch = 2.9106e-01, Meta loss averaged over last 500 steps = 2.9074e-01, PNorm = 135.3167, GNorm = 0.2246
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 2.9070e-01, PNorm = 135.3232, GNorm = 0.2154
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 2.9057e-01, PNorm = 135.3297, GNorm = 0.2138
Meta loss on this task batch = 2.9688e-01, Meta loss averaged over last 500 steps = 2.9055e-01, PNorm = 135.3361, GNorm = 0.2245
Meta loss on this task batch = 2.8923e-01, Meta loss averaged over last 500 steps = 2.9051e-01, PNorm = 135.3425, GNorm = 0.2323
Meta loss on this task batch = 2.8610e-01, Meta loss averaged over last 500 steps = 2.9051e-01, PNorm = 135.3488, GNorm = 0.2303
Meta loss on this task batch = 3.2174e-01, Meta loss averaged over last 500 steps = 2.9067e-01, PNorm = 135.3551, GNorm = 0.2039
Meta loss on this task batch = 2.9018e-01, Meta loss averaged over last 500 steps = 2.9063e-01, PNorm = 135.3614, GNorm = 0.2223
Meta loss on this task batch = 2.5870e-01, Meta loss averaged over last 500 steps = 2.9056e-01, PNorm = 135.3679, GNorm = 0.1974
Meta loss on this task batch = 3.2011e-01, Meta loss averaged over last 500 steps = 2.9065e-01, PNorm = 135.3739, GNorm = 0.2269
Meta loss on this task batch = 2.3512e-01, Meta loss averaged over last 500 steps = 2.9052e-01, PNorm = 135.3806, GNorm = 0.2171
Meta loss on this task batch = 2.5003e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 135.3892, GNorm = 0.2155
Meta loss on this task batch = 2.5430e-01, Meta loss averaged over last 500 steps = 2.9038e-01, PNorm = 135.3982, GNorm = 0.2486
Took 140.25048637390137 seconds to complete one epoch of meta training
Took 147.00354051589966 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480882
Epoch 483
Meta loss on this task batch = 3.2063e-01, Meta loss averaged over last 500 steps = 2.9047e-01, PNorm = 135.4067, GNorm = 0.2463
Meta loss on this task batch = 2.5525e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 135.4144, GNorm = 0.2001
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.9035e-01, PNorm = 135.4213, GNorm = 0.2238
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 135.4278, GNorm = 0.2535
Meta loss on this task batch = 3.1374e-01, Meta loss averaged over last 500 steps = 2.9038e-01, PNorm = 135.4336, GNorm = 0.2331
Meta loss on this task batch = 3.1210e-01, Meta loss averaged over last 500 steps = 2.9037e-01, PNorm = 135.4391, GNorm = 0.2770
Meta loss on this task batch = 3.1306e-01, Meta loss averaged over last 500 steps = 2.9042e-01, PNorm = 135.4426, GNorm = 0.2489
Meta loss on this task batch = 3.1037e-01, Meta loss averaged over last 500 steps = 2.9042e-01, PNorm = 135.4457, GNorm = 0.2573
Meta loss on this task batch = 3.1625e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 135.4487, GNorm = 0.2374
Meta loss on this task batch = 2.6538e-01, Meta loss averaged over last 500 steps = 2.9036e-01, PNorm = 135.4524, GNorm = 0.2147
Meta loss on this task batch = 2.2297e-01, Meta loss averaged over last 500 steps = 2.9025e-01, PNorm = 135.4574, GNorm = 0.2192
Meta loss on this task batch = 2.7163e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 135.4630, GNorm = 0.2192
Meta loss on this task batch = 2.8811e-01, Meta loss averaged over last 500 steps = 2.9011e-01, PNorm = 135.4690, GNorm = 0.2106
Meta loss on this task batch = 3.3116e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 135.4753, GNorm = 0.2455
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 2.9008e-01, PNorm = 135.4819, GNorm = 0.2150
Meta loss on this task batch = 3.0436e-01, Meta loss averaged over last 500 steps = 2.9015e-01, PNorm = 135.4903, GNorm = 0.2855
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.9021e-01, PNorm = 135.4987, GNorm = 0.2196
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 135.5070, GNorm = 0.2291
Meta loss on this task batch = 2.5621e-01, Meta loss averaged over last 500 steps = 2.9027e-01, PNorm = 135.5150, GNorm = 0.2770
Took 118.06053137779236 seconds to complete one epoch of meta training
Took 126.02753853797913 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495677
Epoch 484
Meta loss on this task batch = 3.3563e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 135.5221, GNorm = 0.2395
Meta loss on this task batch = 2.5033e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 135.5296, GNorm = 0.1868
Meta loss on this task batch = 2.5017e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 135.5367, GNorm = 0.2252
Meta loss on this task batch = 3.2253e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 135.5431, GNorm = 0.2118
Meta loss on this task batch = 3.0288e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 135.5492, GNorm = 0.2070
Meta loss on this task batch = 2.7095e-01, Meta loss averaged over last 500 steps = 2.9011e-01, PNorm = 135.5545, GNorm = 0.2513
Meta loss on this task batch = 3.2558e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 135.5589, GNorm = 0.2139
Meta loss on this task batch = 3.1339e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 135.5627, GNorm = 0.2324
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.9018e-01, PNorm = 135.5671, GNorm = 0.1954
Meta loss on this task batch = 2.5498e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 135.5726, GNorm = 0.1965
Meta loss on this task batch = 3.3178e-01, Meta loss averaged over last 500 steps = 2.9030e-01, PNorm = 135.5790, GNorm = 0.2360
Meta loss on this task batch = 3.1321e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 135.5857, GNorm = 0.2597
Meta loss on this task batch = 2.8113e-01, Meta loss averaged over last 500 steps = 2.9022e-01, PNorm = 135.5927, GNorm = 0.2248
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.9019e-01, PNorm = 135.6008, GNorm = 0.2233
Meta loss on this task batch = 2.9302e-01, Meta loss averaged over last 500 steps = 2.9020e-01, PNorm = 135.6080, GNorm = 0.2912
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 135.6156, GNorm = 0.2424
Meta loss on this task batch = 3.0346e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 135.6234, GNorm = 0.2425
Meta loss on this task batch = 3.1867e-01, Meta loss averaged over last 500 steps = 2.9034e-01, PNorm = 135.6311, GNorm = 0.2278
Meta loss on this task batch = 2.1857e-01, Meta loss averaged over last 500 steps = 2.9016e-01, PNorm = 135.6391, GNorm = 0.2134
Took 148.82587480545044 seconds to complete one epoch of meta training
Took 156.70666313171387 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496196
Epoch 485
Meta loss on this task batch = 3.4380e-01, Meta loss averaged over last 500 steps = 2.9036e-01, PNorm = 135.6461, GNorm = 0.2757
Meta loss on this task batch = 2.4945e-01, Meta loss averaged over last 500 steps = 2.9027e-01, PNorm = 135.6538, GNorm = 0.2100
Meta loss on this task batch = 3.5099e-01, Meta loss averaged over last 500 steps = 2.9044e-01, PNorm = 135.6613, GNorm = 0.2820
Meta loss on this task batch = 3.2437e-01, Meta loss averaged over last 500 steps = 2.9044e-01, PNorm = 135.6683, GNorm = 0.2658
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 2.9045e-01, PNorm = 135.6761, GNorm = 0.2581
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 2.9038e-01, PNorm = 135.6838, GNorm = 0.2146
Meta loss on this task batch = 2.7012e-01, Meta loss averaged over last 500 steps = 2.9038e-01, PNorm = 135.6907, GNorm = 0.2223
Meta loss on this task batch = 2.8040e-01, Meta loss averaged over last 500 steps = 2.9039e-01, PNorm = 135.6971, GNorm = 0.2389
Meta loss on this task batch = 3.0033e-01, Meta loss averaged over last 500 steps = 2.9035e-01, PNorm = 135.7036, GNorm = 0.2517
Meta loss on this task batch = 3.0894e-01, Meta loss averaged over last 500 steps = 2.9032e-01, PNorm = 135.7102, GNorm = 0.2260
Meta loss on this task batch = 3.4891e-01, Meta loss averaged over last 500 steps = 2.9040e-01, PNorm = 135.7174, GNorm = 0.2770
Meta loss on this task batch = 2.6780e-01, Meta loss averaged over last 500 steps = 2.9023e-01, PNorm = 135.7252, GNorm = 0.2214
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 135.7338, GNorm = 0.2258
Meta loss on this task batch = 2.6180e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 135.7424, GNorm = 0.2231
Meta loss on this task batch = 3.0612e-01, Meta loss averaged over last 500 steps = 2.9024e-01, PNorm = 135.7500, GNorm = 0.2266
Meta loss on this task batch = 2.5791e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 135.7575, GNorm = 0.2434
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 135.7662, GNorm = 0.2335
Meta loss on this task batch = 2.8770e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 135.7721, GNorm = 0.2487
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.9009e-01, PNorm = 135.7772, GNorm = 0.2440
Took 118.37289595603943 seconds to complete one epoch of meta training
Took 126.40359377861023 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486567
Epoch 486
Meta loss on this task batch = 3.0906e-01, Meta loss averaged over last 500 steps = 2.9013e-01, PNorm = 135.7819, GNorm = 0.2322
Meta loss on this task batch = 3.6070e-01, Meta loss averaged over last 500 steps = 2.9026e-01, PNorm = 135.7857, GNorm = 0.2356
Meta loss on this task batch = 2.7754e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 135.7889, GNorm = 0.2117
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 2.9031e-01, PNorm = 135.7920, GNorm = 0.2044
Meta loss on this task batch = 2.9573e-01, Meta loss averaged over last 500 steps = 2.9028e-01, PNorm = 135.7955, GNorm = 0.2115
Meta loss on this task batch = 3.2134e-01, Meta loss averaged over last 500 steps = 2.9022e-01, PNorm = 135.7986, GNorm = 0.2257
Meta loss on this task batch = 2.5117e-01, Meta loss averaged over last 500 steps = 2.9017e-01, PNorm = 135.8032, GNorm = 0.1937
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.9006e-01, PNorm = 135.8081, GNorm = 0.2254
Meta loss on this task batch = 3.0414e-01, Meta loss averaged over last 500 steps = 2.9010e-01, PNorm = 135.8131, GNorm = 0.2449
Meta loss on this task batch = 2.3798e-01, Meta loss averaged over last 500 steps = 2.9007e-01, PNorm = 135.8186, GNorm = 0.1994
Meta loss on this task batch = 2.3034e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 135.8246, GNorm = 0.1893
Meta loss on this task batch = 3.2320e-01, Meta loss averaged over last 500 steps = 2.8996e-01, PNorm = 135.8309, GNorm = 0.2539
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 2.8990e-01, PNorm = 135.8377, GNorm = 0.2340
Meta loss on this task batch = 2.7862e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 135.8454, GNorm = 0.2869
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 2.8989e-01, PNorm = 135.8532, GNorm = 0.2326
Meta loss on this task batch = 3.2960e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 135.8607, GNorm = 0.2931
Meta loss on this task batch = 2.3576e-01, Meta loss averaged over last 500 steps = 2.8978e-01, PNorm = 135.8688, GNorm = 0.1919
Meta loss on this task batch = 3.0441e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 135.8761, GNorm = 0.2711
Meta loss on this task batch = 2.6907e-01, Meta loss averaged over last 500 steps = 2.8981e-01, PNorm = 135.8838, GNorm = 0.2579
Took 117.9589352607727 seconds to complete one epoch of meta training
Took 125.99086236953735 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482244
Epoch 487
Meta loss on this task batch = 3.0159e-01, Meta loss averaged over last 500 steps = 2.8990e-01, PNorm = 135.8912, GNorm = 0.2309
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 2.8984e-01, PNorm = 135.8969, GNorm = 0.2437
Meta loss on this task batch = 2.4086e-01, Meta loss averaged over last 500 steps = 2.8981e-01, PNorm = 135.9026, GNorm = 0.2548
Meta loss on this task batch = 3.1872e-01, Meta loss averaged over last 500 steps = 2.8984e-01, PNorm = 135.9076, GNorm = 0.2538
Meta loss on this task batch = 2.2610e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 135.9131, GNorm = 0.2099
Meta loss on this task batch = 2.7224e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 135.9193, GNorm = 0.2421
Meta loss on this task batch = 3.1975e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 135.9240, GNorm = 0.2541
Meta loss on this task batch = 2.8262e-01, Meta loss averaged over last 500 steps = 2.8955e-01, PNorm = 135.9291, GNorm = 0.2563
Meta loss on this task batch = 3.0762e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 135.9349, GNorm = 0.2335
Meta loss on this task batch = 2.6016e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 135.9411, GNorm = 0.2084
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 2.8955e-01, PNorm = 135.9471, GNorm = 0.2494
Meta loss on this task batch = 2.9440e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 135.9521, GNorm = 0.2386
Meta loss on this task batch = 2.9350e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 135.9577, GNorm = 0.2102
Meta loss on this task batch = 2.9823e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 135.9632, GNorm = 0.2298
Meta loss on this task batch = 3.2625e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 135.9686, GNorm = 0.2264
Meta loss on this task batch = 2.7385e-01, Meta loss averaged over last 500 steps = 2.8961e-01, PNorm = 135.9743, GNorm = 0.2307
Meta loss on this task batch = 2.7637e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 135.9804, GNorm = 0.2216
Meta loss on this task batch = 3.0520e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 135.9873, GNorm = 0.2337
Meta loss on this task batch = 3.0453e-01, Meta loss averaged over last 500 steps = 2.8967e-01, PNorm = 135.9949, GNorm = 0.2480
Took 116.44367933273315 seconds to complete one epoch of meta training
Took 123.18891978263855 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496968
Epoch 488
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.0023, GNorm = 0.1916
Meta loss on this task batch = 3.0328e-01, Meta loss averaged over last 500 steps = 2.8964e-01, PNorm = 136.0094, GNorm = 0.2428
Meta loss on this task batch = 2.6965e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 136.0169, GNorm = 0.2389
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 136.0248, GNorm = 0.2108
Meta loss on this task batch = 2.6457e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 136.0324, GNorm = 0.2294
Meta loss on this task batch = 3.0904e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 136.0392, GNorm = 0.2574
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 136.0465, GNorm = 0.2272
Meta loss on this task batch = 2.4415e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 136.0537, GNorm = 0.2089
Meta loss on this task batch = 3.2575e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 136.0600, GNorm = 0.2836
Meta loss on this task batch = 2.6631e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 136.0655, GNorm = 0.2546
Meta loss on this task batch = 2.9817e-01, Meta loss averaged over last 500 steps = 2.8939e-01, PNorm = 136.0701, GNorm = 0.2601
Meta loss on this task batch = 3.0449e-01, Meta loss averaged over last 500 steps = 2.8943e-01, PNorm = 136.0742, GNorm = 0.2295
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 136.0784, GNorm = 0.2364
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 136.0833, GNorm = 0.2481
Meta loss on this task batch = 3.7160e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 136.0883, GNorm = 0.4046
Meta loss on this task batch = 2.9514e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 136.0945, GNorm = 0.2421
Meta loss on this task batch = 3.1125e-01, Meta loss averaged over last 500 steps = 2.8981e-01, PNorm = 136.1009, GNorm = 0.2338
Meta loss on this task batch = 3.0199e-01, Meta loss averaged over last 500 steps = 2.8978e-01, PNorm = 136.1078, GNorm = 0.2115
Meta loss on this task batch = 3.4501e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 136.1153, GNorm = 0.2805
Took 118.0497817993164 seconds to complete one epoch of meta training
Took 125.63537454605103 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470675
Epoch 489
Meta loss on this task batch = 2.7779e-01, Meta loss averaged over last 500 steps = 2.8999e-01, PNorm = 136.1231, GNorm = 0.2011
Meta loss on this task batch = 2.8966e-01, Meta loss averaged over last 500 steps = 2.8985e-01, PNorm = 136.1299, GNorm = 0.2372
Meta loss on this task batch = 3.1682e-01, Meta loss averaged over last 500 steps = 2.8996e-01, PNorm = 136.1367, GNorm = 0.2345
Meta loss on this task batch = 3.2652e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 136.1435, GNorm = 0.2207
Meta loss on this task batch = 2.7937e-01, Meta loss averaged over last 500 steps = 2.8994e-01, PNorm = 136.1503, GNorm = 0.2374
Meta loss on this task batch = 2.9471e-01, Meta loss averaged over last 500 steps = 2.8988e-01, PNorm = 136.1582, GNorm = 0.2638
Meta loss on this task batch = 3.0013e-01, Meta loss averaged over last 500 steps = 2.8988e-01, PNorm = 136.1658, GNorm = 0.2339
Meta loss on this task batch = 2.4734e-01, Meta loss averaged over last 500 steps = 2.8979e-01, PNorm = 136.1736, GNorm = 0.2389
Meta loss on this task batch = 2.8423e-01, Meta loss averaged over last 500 steps = 2.8976e-01, PNorm = 136.1813, GNorm = 0.2316
Meta loss on this task batch = 2.6403e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 136.1890, GNorm = 0.2060
Meta loss on this task batch = 2.9999e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 136.1965, GNorm = 0.2374
Meta loss on this task batch = 2.9893e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 136.2028, GNorm = 0.2479
Meta loss on this task batch = 2.2224e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 136.2093, GNorm = 0.2052
Meta loss on this task batch = 2.8917e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 136.2164, GNorm = 0.2188
Meta loss on this task batch = 3.0935e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 136.2234, GNorm = 0.2215
Meta loss on this task batch = 2.9374e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 136.2304, GNorm = 0.2292
Meta loss on this task batch = 3.2876e-01, Meta loss averaged over last 500 steps = 2.8932e-01, PNorm = 136.2366, GNorm = 0.2703
Meta loss on this task batch = 2.9162e-01, Meta loss averaged over last 500 steps = 2.8940e-01, PNorm = 136.2437, GNorm = 0.2366
Meta loss on this task batch = 2.5914e-01, Meta loss averaged over last 500 steps = 2.8931e-01, PNorm = 136.2515, GNorm = 0.2742
Took 115.30445528030396 seconds to complete one epoch of meta training
Took 123.478679895401 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500538
Epoch 490
Meta loss on this task batch = 2.5283e-01, Meta loss averaged over last 500 steps = 2.8932e-01, PNorm = 136.2589, GNorm = 0.2118
Meta loss on this task batch = 2.9222e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 136.2651, GNorm = 0.2372
Meta loss on this task batch = 2.8123e-01, Meta loss averaged over last 500 steps = 2.8939e-01, PNorm = 136.2703, GNorm = 0.2005
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 136.2756, GNorm = 0.2937
Meta loss on this task batch = 2.3574e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 136.2822, GNorm = 0.1874
Meta loss on this task batch = 2.6982e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 136.2883, GNorm = 0.1920
Meta loss on this task batch = 2.5799e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 136.2940, GNorm = 0.1929
Meta loss on this task batch = 3.0282e-01, Meta loss averaged over last 500 steps = 2.8939e-01, PNorm = 136.2986, GNorm = 0.2312
Meta loss on this task batch = 2.4970e-01, Meta loss averaged over last 500 steps = 2.8929e-01, PNorm = 136.3034, GNorm = 0.2351
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.8928e-01, PNorm = 136.3090, GNorm = 0.2010
Meta loss on this task batch = 3.0264e-01, Meta loss averaged over last 500 steps = 2.8930e-01, PNorm = 136.3151, GNorm = 0.2541
Meta loss on this task batch = 3.3405e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 136.3205, GNorm = 0.2614
Meta loss on this task batch = 3.3083e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 136.3253, GNorm = 0.2521
Meta loss on this task batch = 3.0563e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 136.3301, GNorm = 0.2332
Meta loss on this task batch = 3.2468e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 136.3345, GNorm = 0.2619
Meta loss on this task batch = 2.7900e-01, Meta loss averaged over last 500 steps = 2.8938e-01, PNorm = 136.3398, GNorm = 0.2117
Meta loss on this task batch = 2.5445e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 136.3460, GNorm = 0.1958
Meta loss on this task batch = 3.1547e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 136.3525, GNorm = 0.2191
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 136.3590, GNorm = 0.2557
Took 114.10398459434509 seconds to complete one epoch of meta training
Took 121.8999674320221 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500450
Epoch 491
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 2.8939e-01, PNorm = 136.3645, GNorm = 0.2319
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.8932e-01, PNorm = 136.3692, GNorm = 0.2253
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.8938e-01, PNorm = 136.3738, GNorm = 0.1913
Meta loss on this task batch = 3.1884e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 136.3781, GNorm = 0.2147
Meta loss on this task batch = 2.7838e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 136.3832, GNorm = 0.2087
Meta loss on this task batch = 2.8371e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 136.3880, GNorm = 0.2182
Meta loss on this task batch = 3.3243e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 136.3923, GNorm = 0.2437
Meta loss on this task batch = 2.6714e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 136.3968, GNorm = 0.2010
Meta loss on this task batch = 2.8307e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 136.4020, GNorm = 0.2131
Meta loss on this task batch = 2.9664e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 136.4078, GNorm = 0.2145
Meta loss on this task batch = 3.1387e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 136.4139, GNorm = 0.2081
Meta loss on this task batch = 2.6111e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 136.4215, GNorm = 0.2424
Meta loss on this task batch = 2.7552e-01, Meta loss averaged over last 500 steps = 2.8943e-01, PNorm = 136.4297, GNorm = 0.1969
Meta loss on this task batch = 2.9173e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 136.4378, GNorm = 0.2430
Meta loss on this task batch = 2.8921e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 136.4452, GNorm = 0.2229
Meta loss on this task batch = 3.5281e-01, Meta loss averaged over last 500 steps = 2.8955e-01, PNorm = 136.4496, GNorm = 0.3076
Meta loss on this task batch = 2.5893e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 136.4541, GNorm = 0.2155
Meta loss on this task batch = 2.9005e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 136.4593, GNorm = 0.2453
Meta loss on this task batch = 3.0165e-01, Meta loss averaged over last 500 steps = 2.8973e-01, PNorm = 136.4641, GNorm = 0.2595
Took 114.01474261283875 seconds to complete one epoch of meta training
Took 121.781076669693 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502833
Epoch 492
Meta loss on this task batch = 3.3990e-01, Meta loss averaged over last 500 steps = 2.8980e-01, PNorm = 136.4685, GNorm = 0.2242
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 2.8969e-01, PNorm = 136.4739, GNorm = 0.1933
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 2.8964e-01, PNorm = 136.4801, GNorm = 0.1988
Meta loss on this task batch = 3.6798e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 136.4859, GNorm = 0.2325
Meta loss on this task batch = 2.7567e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 136.4924, GNorm = 0.2552
Meta loss on this task batch = 2.4999e-01, Meta loss averaged over last 500 steps = 2.8970e-01, PNorm = 136.4990, GNorm = 0.2053
Meta loss on this task batch = 2.6274e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 136.5068, GNorm = 0.2241
Meta loss on this task batch = 3.2934e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 136.5133, GNorm = 0.2580
Meta loss on this task batch = 2.9430e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.5198, GNorm = 0.2303
Meta loss on this task batch = 3.2849e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 136.5253, GNorm = 0.2524
Meta loss on this task batch = 2.7711e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 136.5307, GNorm = 0.2225
Meta loss on this task batch = 2.4935e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.5349, GNorm = 0.2291
Meta loss on this task batch = 2.6575e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.5394, GNorm = 0.2098
Meta loss on this task batch = 2.6845e-01, Meta loss averaged over last 500 steps = 2.8940e-01, PNorm = 136.5443, GNorm = 0.2295
Meta loss on this task batch = 3.2331e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 136.5491, GNorm = 0.2215
Meta loss on this task batch = 2.3992e-01, Meta loss averaged over last 500 steps = 2.8943e-01, PNorm = 136.5555, GNorm = 0.2186
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 136.5623, GNorm = 0.2108
Meta loss on this task batch = 3.5648e-01, Meta loss averaged over last 500 steps = 2.8968e-01, PNorm = 136.5696, GNorm = 0.2839
Meta loss on this task batch = 2.9205e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 136.5767, GNorm = 0.2719
Took 167.467937707901 seconds to complete one epoch of meta training
Took 175.31177687644958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501660
Epoch 493
Meta loss on this task batch = 2.8026e-01, Meta loss averaged over last 500 steps = 2.8974e-01, PNorm = 136.5836, GNorm = 0.2345
Meta loss on this task batch = 2.7476e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 136.5903, GNorm = 0.2187
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 136.5972, GNorm = 0.2062
Meta loss on this task batch = 3.1187e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 136.6045, GNorm = 0.2163
Meta loss on this task batch = 3.0216e-01, Meta loss averaged over last 500 steps = 2.8963e-01, PNorm = 136.6111, GNorm = 0.2400
Meta loss on this task batch = 3.3802e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 136.6164, GNorm = 0.2465
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 136.6212, GNorm = 0.2288
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 136.6260, GNorm = 0.2137
Meta loss on this task batch = 2.7790e-01, Meta loss averaged over last 500 steps = 2.8935e-01, PNorm = 136.6318, GNorm = 0.2400
Meta loss on this task batch = 3.0478e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 136.6371, GNorm = 0.2422
Meta loss on this task batch = 2.6411e-01, Meta loss averaged over last 500 steps = 2.8930e-01, PNorm = 136.6426, GNorm = 0.2165
Meta loss on this task batch = 3.0715e-01, Meta loss averaged over last 500 steps = 2.8946e-01, PNorm = 136.6479, GNorm = 0.2288
Meta loss on this task batch = 2.6039e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 136.6541, GNorm = 0.2385
Meta loss on this task batch = 3.0763e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 136.6600, GNorm = 0.2284
Meta loss on this task batch = 3.0320e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 136.6650, GNorm = 0.2705
Meta loss on this task batch = 2.7563e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.6707, GNorm = 0.2085
Meta loss on this task batch = 2.7683e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 136.6762, GNorm = 0.2085
Meta loss on this task batch = 2.9275e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 136.6822, GNorm = 0.2214
Meta loss on this task batch = 2.9308e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 136.6885, GNorm = 0.2550
Took 127.25992703437805 seconds to complete one epoch of meta training
Took 135.45703887939453 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497645
Epoch 494
Meta loss on this task batch = 3.0557e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 136.6943, GNorm = 0.2587
Meta loss on this task batch = 2.4807e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 136.7007, GNorm = 0.2018
Meta loss on this task batch = 3.1425e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 136.7058, GNorm = 0.2299
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 2.8955e-01, PNorm = 136.7115, GNorm = 0.1989
Meta loss on this task batch = 2.2464e-01, Meta loss averaged over last 500 steps = 2.8937e-01, PNorm = 136.7172, GNorm = 0.2181
Meta loss on this task batch = 3.1686e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 136.7218, GNorm = 0.2515
Meta loss on this task batch = 2.5304e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 136.7269, GNorm = 0.1995
Meta loss on this task batch = 2.8028e-01, Meta loss averaged over last 500 steps = 2.8938e-01, PNorm = 136.7319, GNorm = 0.2394
Meta loss on this task batch = 3.0675e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 136.7372, GNorm = 0.2377
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.7428, GNorm = 0.2247
Meta loss on this task batch = 2.9739e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 136.7494, GNorm = 0.2657
Meta loss on this task batch = 3.2776e-01, Meta loss averaged over last 500 steps = 2.8971e-01, PNorm = 136.7570, GNorm = 0.2270
Meta loss on this task batch = 2.6666e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 136.7648, GNorm = 0.2899
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.8961e-01, PNorm = 136.7729, GNorm = 0.2371
Meta loss on this task batch = 3.4463e-01, Meta loss averaged over last 500 steps = 2.8971e-01, PNorm = 136.7810, GNorm = 0.2439
Meta loss on this task batch = 3.1317e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 136.7884, GNorm = 0.2336
Meta loss on this task batch = 3.4109e-01, Meta loss averaged over last 500 steps = 2.8977e-01, PNorm = 136.7964, GNorm = 0.2582
Meta loss on this task batch = 2.9713e-01, Meta loss averaged over last 500 steps = 2.8981e-01, PNorm = 136.8025, GNorm = 0.2757
Meta loss on this task batch = 3.1823e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 136.8075, GNorm = 0.2940
Took 130.00348472595215 seconds to complete one epoch of meta training
Took 138.04442286491394 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496138
Epoch 495
Meta loss on this task batch = 2.7988e-01, Meta loss averaged over last 500 steps = 2.8998e-01, PNorm = 136.8123, GNorm = 0.2093
Meta loss on this task batch = 2.5935e-01, Meta loss averaged over last 500 steps = 2.8993e-01, PNorm = 136.8178, GNorm = 0.2505
Meta loss on this task batch = 3.0164e-01, Meta loss averaged over last 500 steps = 2.8993e-01, PNorm = 136.8237, GNorm = 0.1974
Meta loss on this task batch = 2.9979e-01, Meta loss averaged over last 500 steps = 2.8993e-01, PNorm = 136.8297, GNorm = 0.2231
Meta loss on this task batch = 2.7553e-01, Meta loss averaged over last 500 steps = 2.8990e-01, PNorm = 136.8353, GNorm = 0.2091
Meta loss on this task batch = 2.3401e-01, Meta loss averaged over last 500 steps = 2.8978e-01, PNorm = 136.8412, GNorm = 0.1907
Meta loss on this task batch = 2.4406e-01, Meta loss averaged over last 500 steps = 2.8970e-01, PNorm = 136.8477, GNorm = 0.2271
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 2.8975e-01, PNorm = 136.8542, GNorm = 0.2140
Meta loss on this task batch = 3.2098e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 136.8610, GNorm = 0.2170
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 2.8997e-01, PNorm = 136.8681, GNorm = 0.2261
Meta loss on this task batch = 3.2429e-01, Meta loss averaged over last 500 steps = 2.9002e-01, PNorm = 136.8755, GNorm = 0.2202
Meta loss on this task batch = 2.4287e-01, Meta loss averaged over last 500 steps = 2.8996e-01, PNorm = 136.8831, GNorm = 0.2173
Meta loss on this task batch = 2.8829e-01, Meta loss averaged over last 500 steps = 2.8989e-01, PNorm = 136.8900, GNorm = 0.2648
Meta loss on this task batch = 2.5204e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 136.8970, GNorm = 0.2243
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 2.8978e-01, PNorm = 136.9041, GNorm = 0.3049
Meta loss on this task batch = 2.9300e-01, Meta loss averaged over last 500 steps = 2.8971e-01, PNorm = 136.9113, GNorm = 0.2075
Meta loss on this task batch = 3.5881e-01, Meta loss averaged over last 500 steps = 2.8991e-01, PNorm = 136.9184, GNorm = 0.2467
Meta loss on this task batch = 2.7511e-01, Meta loss averaged over last 500 steps = 2.8993e-01, PNorm = 136.9255, GNorm = 0.2010
Meta loss on this task batch = 2.4751e-01, Meta loss averaged over last 500 steps = 2.8985e-01, PNorm = 136.9330, GNorm = 0.2183
Took 117.2483184337616 seconds to complete one epoch of meta training
Took 125.16725730895996 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500366
Epoch 496
Meta loss on this task batch = 2.5843e-01, Meta loss averaged over last 500 steps = 2.8975e-01, PNorm = 136.9396, GNorm = 0.2192
Meta loss on this task batch = 2.5610e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 136.9470, GNorm = 0.2449
Meta loss on this task batch = 3.2556e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 136.9533, GNorm = 0.2692
Meta loss on this task batch = 2.7393e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 136.9595, GNorm = 0.2285
Meta loss on this task batch = 3.0007e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 136.9656, GNorm = 0.2018
Meta loss on this task batch = 3.1891e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.9718, GNorm = 0.2391
Meta loss on this task batch = 3.3328e-01, Meta loss averaged over last 500 steps = 2.8957e-01, PNorm = 136.9783, GNorm = 0.2123
Meta loss on this task batch = 2.7833e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 136.9844, GNorm = 0.2257
Meta loss on this task batch = 3.3423e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 136.9899, GNorm = 0.2435
Meta loss on this task batch = 3.0236e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 136.9950, GNorm = 0.2505
Meta loss on this task batch = 2.7183e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 136.9993, GNorm = 0.2178
Meta loss on this task batch = 2.5693e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 137.0041, GNorm = 0.2052
Meta loss on this task batch = 2.7323e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 137.0082, GNorm = 0.2213
Meta loss on this task batch = 3.1912e-01, Meta loss averaged over last 500 steps = 2.8966e-01, PNorm = 137.0123, GNorm = 0.2383
Meta loss on this task batch = 3.0128e-01, Meta loss averaged over last 500 steps = 2.8959e-01, PNorm = 137.0185, GNorm = 0.2374
Meta loss on this task batch = 3.3012e-01, Meta loss averaged over last 500 steps = 2.8969e-01, PNorm = 137.0245, GNorm = 0.2288
Meta loss on this task batch = 2.4997e-01, Meta loss averaged over last 500 steps = 2.8965e-01, PNorm = 137.0310, GNorm = 0.2069
Meta loss on this task batch = 2.5743e-01, Meta loss averaged over last 500 steps = 2.8955e-01, PNorm = 137.0372, GNorm = 0.1991
Meta loss on this task batch = 2.8466e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 137.0444, GNorm = 0.2982
Took 116.99053120613098 seconds to complete one epoch of meta training
Took 124.3792769908905 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497736
Epoch 497
Meta loss on this task batch = 3.1552e-01, Meta loss averaged over last 500 steps = 2.8943e-01, PNorm = 137.0505, GNorm = 0.2545
Meta loss on this task batch = 3.2182e-01, Meta loss averaged over last 500 steps = 2.8945e-01, PNorm = 137.0566, GNorm = 0.2585
Meta loss on this task batch = 3.1647e-01, Meta loss averaged over last 500 steps = 2.8953e-01, PNorm = 137.0622, GNorm = 0.2580
Meta loss on this task batch = 2.6974e-01, Meta loss averaged over last 500 steps = 2.8951e-01, PNorm = 137.0669, GNorm = 0.2315
Meta loss on this task batch = 2.5882e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 137.0705, GNorm = 0.2349
Meta loss on this task batch = 3.0310e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 137.0729, GNorm = 0.2370
Meta loss on this task batch = 2.9838e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 137.0755, GNorm = 0.2045
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 137.0789, GNorm = 0.2264
Meta loss on this task batch = 2.5421e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 137.0830, GNorm = 0.1880
Meta loss on this task batch = 3.1890e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 137.0880, GNorm = 0.2595
Meta loss on this task batch = 3.0248e-01, Meta loss averaged over last 500 steps = 2.8954e-01, PNorm = 137.0929, GNorm = 0.2458
Meta loss on this task batch = 2.7894e-01, Meta loss averaged over last 500 steps = 2.8950e-01, PNorm = 137.0982, GNorm = 0.2083
Meta loss on this task batch = 3.2717e-01, Meta loss averaged over last 500 steps = 2.8962e-01, PNorm = 137.1041, GNorm = 0.2518
Meta loss on this task batch = 3.3496e-01, Meta loss averaged over last 500 steps = 2.8972e-01, PNorm = 137.1106, GNorm = 0.2415
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.8979e-01, PNorm = 137.1168, GNorm = 0.2463
Meta loss on this task batch = 2.3759e-01, Meta loss averaged over last 500 steps = 2.8964e-01, PNorm = 137.1233, GNorm = 0.2100
Meta loss on this task batch = 2.9856e-01, Meta loss averaged over last 500 steps = 2.8960e-01, PNorm = 137.1298, GNorm = 0.2410
Meta loss on this task batch = 2.6897e-01, Meta loss averaged over last 500 steps = 2.8940e-01, PNorm = 137.1375, GNorm = 0.2417
Meta loss on this task batch = 2.6191e-01, Meta loss averaged over last 500 steps = 2.8931e-01, PNorm = 137.1455, GNorm = 0.2655
Took 115.05794906616211 seconds to complete one epoch of meta training
Took 122.82835507392883 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472204
Epoch 498
Meta loss on this task batch = 2.6362e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 137.1543, GNorm = 0.2311
Meta loss on this task batch = 3.1202e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 137.1616, GNorm = 0.2504
Meta loss on this task batch = 2.9628e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 137.1688, GNorm = 0.2193
Meta loss on this task batch = 3.0582e-01, Meta loss averaged over last 500 steps = 2.8936e-01, PNorm = 137.1756, GNorm = 0.2638
Meta loss on this task batch = 3.2395e-01, Meta loss averaged over last 500 steps = 2.8946e-01, PNorm = 137.1820, GNorm = 0.2253
Meta loss on this task batch = 2.7683e-01, Meta loss averaged over last 500 steps = 2.8949e-01, PNorm = 137.1886, GNorm = 0.1830
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 137.1962, GNorm = 0.2186
Meta loss on this task batch = 2.4677e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 137.2033, GNorm = 0.2353
Meta loss on this task batch = 2.5315e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 137.2103, GNorm = 0.2359
Meta loss on this task batch = 3.2407e-01, Meta loss averaged over last 500 steps = 2.8956e-01, PNorm = 137.2177, GNorm = 0.2103
Meta loss on this task batch = 2.7554e-01, Meta loss averaged over last 500 steps = 2.8952e-01, PNorm = 137.2254, GNorm = 0.2497
Meta loss on this task batch = 3.5995e-01, Meta loss averaged over last 500 steps = 2.8975e-01, PNorm = 137.2322, GNorm = 0.2610
Meta loss on this task batch = 3.0114e-01, Meta loss averaged over last 500 steps = 2.8983e-01, PNorm = 137.2380, GNorm = 0.2575
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.8964e-01, PNorm = 137.2441, GNorm = 0.2095
Meta loss on this task batch = 2.5780e-01, Meta loss averaged over last 500 steps = 2.8942e-01, PNorm = 137.2508, GNorm = 0.2059
Meta loss on this task batch = 3.0503e-01, Meta loss averaged over last 500 steps = 2.8948e-01, PNorm = 137.2564, GNorm = 0.2366
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 2.8958e-01, PNorm = 137.2625, GNorm = 0.2236
Meta loss on this task batch = 2.5516e-01, Meta loss averaged over last 500 steps = 2.8947e-01, PNorm = 137.2688, GNorm = 0.1922
Meta loss on this task batch = 2.3747e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 137.2748, GNorm = 0.2341
Took 124.33985090255737 seconds to complete one epoch of meta training
Took 132.6111614704132 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478191
Epoch 499
Meta loss on this task batch = 3.3483e-01, Meta loss averaged over last 500 steps = 2.8944e-01, PNorm = 137.2805, GNorm = 0.2590
Meta loss on this task batch = 2.8345e-01, Meta loss averaged over last 500 steps = 2.8931e-01, PNorm = 137.2864, GNorm = 0.2105
Meta loss on this task batch = 2.7476e-01, Meta loss averaged over last 500 steps = 2.8927e-01, PNorm = 137.2936, GNorm = 0.2419
Meta loss on this task batch = 2.7335e-01, Meta loss averaged over last 500 steps = 2.8934e-01, PNorm = 137.3015, GNorm = 0.2155
Meta loss on this task batch = 3.1073e-01, Meta loss averaged over last 500 steps = 2.8923e-01, PNorm = 137.3089, GNorm = 0.2537
Meta loss on this task batch = 2.6919e-01, Meta loss averaged over last 500 steps = 2.8925e-01, PNorm = 137.3163, GNorm = 0.2224
Meta loss on this task batch = 2.4719e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 137.3236, GNorm = 0.1875
Meta loss on this task batch = 2.9902e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 137.3307, GNorm = 0.2225
Meta loss on this task batch = 3.1179e-01, Meta loss averaged over last 500 steps = 2.8922e-01, PNorm = 137.3370, GNorm = 0.2408
Meta loss on this task batch = 3.0097e-01, Meta loss averaged over last 500 steps = 2.8925e-01, PNorm = 137.3435, GNorm = 0.2644
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 137.3494, GNorm = 0.2857
Meta loss on this task batch = 3.0266e-01, Meta loss averaged over last 500 steps = 2.8933e-01, PNorm = 137.3548, GNorm = 0.2304
Meta loss on this task batch = 3.3534e-01, Meta loss averaged over last 500 steps = 2.8941e-01, PNorm = 137.3600, GNorm = 0.2399
Meta loss on this task batch = 2.6878e-01, Meta loss averaged over last 500 steps = 2.8918e-01, PNorm = 137.3653, GNorm = 0.2378
Meta loss on this task batch = 2.6212e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 137.3696, GNorm = 0.2280
Meta loss on this task batch = 2.4482e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 137.3748, GNorm = 0.2326
Meta loss on this task batch = 2.9880e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 137.3807, GNorm = 0.2247
Meta loss on this task batch = 2.8023e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 137.3880, GNorm = 0.2350
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 2.8923e-01, PNorm = 137.3955, GNorm = 0.2623
Took 130.57654428482056 seconds to complete one epoch of meta training
Took 138.68071365356445 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510578
Epoch 500
Meta loss on this task batch = 2.4358e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 137.4031, GNorm = 0.2229
Meta loss on this task batch = 2.3879e-01, Meta loss averaged over last 500 steps = 2.8903e-01, PNorm = 137.4110, GNorm = 0.2051
Meta loss on this task batch = 2.7181e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 137.4190, GNorm = 0.2636
Meta loss on this task batch = 2.9054e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 137.4273, GNorm = 0.2214
Meta loss on this task batch = 3.2846e-01, Meta loss averaged over last 500 steps = 2.8917e-01, PNorm = 137.4325, GNorm = 0.2604
Meta loss on this task batch = 2.8975e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 137.4379, GNorm = 0.2404
Meta loss on this task batch = 2.7464e-01, Meta loss averaged over last 500 steps = 2.8910e-01, PNorm = 137.4433, GNorm = 0.2202
Meta loss on this task batch = 2.5632e-01, Meta loss averaged over last 500 steps = 2.8911e-01, PNorm = 137.4491, GNorm = 0.2285
Meta loss on this task batch = 2.5309e-01, Meta loss averaged over last 500 steps = 2.8900e-01, PNorm = 137.4554, GNorm = 0.2135
Meta loss on this task batch = 3.3008e-01, Meta loss averaged over last 500 steps = 2.8895e-01, PNorm = 137.4614, GNorm = 0.2995
Meta loss on this task batch = 2.8372e-01, Meta loss averaged over last 500 steps = 2.8895e-01, PNorm = 137.4676, GNorm = 0.2431
Meta loss on this task batch = 3.0325e-01, Meta loss averaged over last 500 steps = 2.8896e-01, PNorm = 137.4739, GNorm = 0.2490
Meta loss on this task batch = 2.8998e-01, Meta loss averaged over last 500 steps = 2.8900e-01, PNorm = 137.4793, GNorm = 0.2475
Meta loss on this task batch = 3.1306e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 137.4834, GNorm = 0.2551
Meta loss on this task batch = 2.5398e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 137.4876, GNorm = 0.2026
Meta loss on this task batch = 3.2402e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 137.4913, GNorm = 0.2442
Meta loss on this task batch = 3.1321e-01, Meta loss averaged over last 500 steps = 2.8892e-01, PNorm = 137.4952, GNorm = 0.2567
Meta loss on this task batch = 3.3119e-01, Meta loss averaged over last 500 steps = 2.8900e-01, PNorm = 137.4998, GNorm = 0.2424
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 137.5051, GNorm = 0.3094
Took 137.10706877708435 seconds to complete one epoch of meta training
Took 144.29088354110718 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500099
Epoch 501
Meta loss on this task batch = 3.0472e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 137.5116, GNorm = 0.2470
Meta loss on this task batch = 3.0881e-01, Meta loss averaged over last 500 steps = 2.8921e-01, PNorm = 137.5173, GNorm = 0.2294
Meta loss on this task batch = 2.5155e-01, Meta loss averaged over last 500 steps = 2.8918e-01, PNorm = 137.5238, GNorm = 0.2023
Meta loss on this task batch = 3.0859e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 137.5299, GNorm = 0.2464
Meta loss on this task batch = 2.7357e-01, Meta loss averaged over last 500 steps = 2.8910e-01, PNorm = 137.5349, GNorm = 0.2080
Meta loss on this task batch = 2.2127e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 137.5402, GNorm = 0.1857
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 137.5459, GNorm = 0.2078
Meta loss on this task batch = 3.0471e-01, Meta loss averaged over last 500 steps = 2.8895e-01, PNorm = 137.5521, GNorm = 0.2327
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 137.5577, GNorm = 0.2093
Meta loss on this task batch = 3.0169e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 137.5629, GNorm = 0.2335
Meta loss on this task batch = 2.7961e-01, Meta loss averaged over last 500 steps = 2.8891e-01, PNorm = 137.5689, GNorm = 0.2062
Meta loss on this task batch = 3.2897e-01, Meta loss averaged over last 500 steps = 2.8898e-01, PNorm = 137.5751, GNorm = 0.2250
Meta loss on this task batch = 3.0652e-01, Meta loss averaged over last 500 steps = 2.8902e-01, PNorm = 137.5813, GNorm = 0.2335
Meta loss on this task batch = 2.5058e-01, Meta loss averaged over last 500 steps = 2.8892e-01, PNorm = 137.5882, GNorm = 0.2155
Meta loss on this task batch = 2.6936e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 137.5956, GNorm = 0.2175
Meta loss on this task batch = 2.8042e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 137.6038, GNorm = 0.2075
Meta loss on this task batch = 2.6886e-01, Meta loss averaged over last 500 steps = 2.8879e-01, PNorm = 137.6119, GNorm = 0.2070
Meta loss on this task batch = 2.9756e-01, Meta loss averaged over last 500 steps = 2.8880e-01, PNorm = 137.6196, GNorm = 0.2158
Meta loss on this task batch = 2.3359e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 137.6277, GNorm = 0.2607
Took 116.5687108039856 seconds to complete one epoch of meta training
Took 123.20416617393494 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482976
Epoch 502
Meta loss on this task batch = 3.2665e-01, Meta loss averaged over last 500 steps = 2.8876e-01, PNorm = 137.6354, GNorm = 0.2999
Meta loss on this task batch = 3.3451e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 137.6436, GNorm = 0.2785
Meta loss on this task batch = 2.8434e-01, Meta loss averaged over last 500 steps = 2.8885e-01, PNorm = 137.6516, GNorm = 0.2171
Meta loss on this task batch = 2.6709e-01, Meta loss averaged over last 500 steps = 2.8891e-01, PNorm = 137.6600, GNorm = 0.2284
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 2.8901e-01, PNorm = 137.6690, GNorm = 0.2337
Meta loss on this task batch = 2.9063e-01, Meta loss averaged over last 500 steps = 2.8903e-01, PNorm = 137.6783, GNorm = 0.2387
Meta loss on this task batch = 2.9246e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 137.6878, GNorm = 0.2069
Meta loss on this task batch = 2.7591e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 137.6977, GNorm = 0.2235
Meta loss on this task batch = 2.5319e-01, Meta loss averaged over last 500 steps = 2.8887e-01, PNorm = 137.7069, GNorm = 0.2307
Meta loss on this task batch = 2.6894e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 137.7158, GNorm = 0.2260
Meta loss on this task batch = 3.1990e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 137.7240, GNorm = 0.2692
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 2.8896e-01, PNorm = 137.7319, GNorm = 0.2952
Meta loss on this task batch = 2.7001e-01, Meta loss averaged over last 500 steps = 2.8904e-01, PNorm = 137.7391, GNorm = 0.2205
Meta loss on this task batch = 3.0702e-01, Meta loss averaged over last 500 steps = 2.8924e-01, PNorm = 137.7457, GNorm = 0.2150
Meta loss on this task batch = 2.5071e-01, Meta loss averaged over last 500 steps = 2.8916e-01, PNorm = 137.7529, GNorm = 0.2118
Meta loss on this task batch = 3.1526e-01, Meta loss averaged over last 500 steps = 2.8913e-01, PNorm = 137.7598, GNorm = 0.2623
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 137.7655, GNorm = 0.2122
Meta loss on this task batch = 2.6732e-01, Meta loss averaged over last 500 steps = 2.8899e-01, PNorm = 137.7716, GNorm = 0.2130
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 137.7784, GNorm = 0.2563
Took 115.16109108924866 seconds to complete one epoch of meta training
Took 121.79104232788086 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489597
Epoch 503
Meta loss on this task batch = 3.2529e-01, Meta loss averaged over last 500 steps = 2.8902e-01, PNorm = 137.7832, GNorm = 0.2713
Meta loss on this task batch = 2.4553e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 137.7883, GNorm = 0.1996
Meta loss on this task batch = 2.5983e-01, Meta loss averaged over last 500 steps = 2.8861e-01, PNorm = 137.7942, GNorm = 0.2265
Meta loss on this task batch = 3.2645e-01, Meta loss averaged over last 500 steps = 2.8881e-01, PNorm = 137.8006, GNorm = 0.2235
Meta loss on this task batch = 3.1742e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 137.8053, GNorm = 0.3153
Meta loss on this task batch = 2.8899e-01, Meta loss averaged over last 500 steps = 2.8891e-01, PNorm = 137.8093, GNorm = 0.2806
Meta loss on this task batch = 2.8489e-01, Meta loss averaged over last 500 steps = 2.8891e-01, PNorm = 137.8138, GNorm = 0.2053
Meta loss on this task batch = 2.6029e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 137.8188, GNorm = 0.2171
Meta loss on this task batch = 2.6076e-01, Meta loss averaged over last 500 steps = 2.8874e-01, PNorm = 137.8245, GNorm = 0.2095
Meta loss on this task batch = 3.1324e-01, Meta loss averaged over last 500 steps = 2.8884e-01, PNorm = 137.8300, GNorm = 0.2335
Meta loss on this task batch = 2.6894e-01, Meta loss averaged over last 500 steps = 2.8880e-01, PNorm = 137.8356, GNorm = 0.2214
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.8880e-01, PNorm = 137.8419, GNorm = 0.3304
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 2.8881e-01, PNorm = 137.8480, GNorm = 0.2211
Meta loss on this task batch = 3.3489e-01, Meta loss averaged over last 500 steps = 2.8890e-01, PNorm = 137.8542, GNorm = 0.2541
Meta loss on this task batch = 2.7090e-01, Meta loss averaged over last 500 steps = 2.8885e-01, PNorm = 137.8614, GNorm = 0.2255
Meta loss on this task batch = 2.9897e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 137.8689, GNorm = 0.2121
Meta loss on this task batch = 2.6020e-01, Meta loss averaged over last 500 steps = 2.8888e-01, PNorm = 137.8764, GNorm = 0.2041
Meta loss on this task batch = 3.1732e-01, Meta loss averaged over last 500 steps = 2.8883e-01, PNorm = 137.8835, GNorm = 0.2299
Meta loss on this task batch = 2.7746e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 137.8902, GNorm = 0.2881
Took 115.49338459968567 seconds to complete one epoch of meta training
Took 123.33466506004333 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487832
Epoch 504
Meta loss on this task batch = 3.1000e-01, Meta loss averaged over last 500 steps = 2.8889e-01, PNorm = 137.8973, GNorm = 0.2135
Meta loss on this task batch = 2.7436e-01, Meta loss averaged over last 500 steps = 2.8891e-01, PNorm = 137.9042, GNorm = 0.2200
Meta loss on this task batch = 2.9840e-01, Meta loss averaged over last 500 steps = 2.8890e-01, PNorm = 137.9109, GNorm = 0.2135
Meta loss on this task batch = 2.7845e-01, Meta loss averaged over last 500 steps = 2.8893e-01, PNorm = 137.9181, GNorm = 0.2108
Meta loss on this task batch = 2.7286e-01, Meta loss averaged over last 500 steps = 2.8897e-01, PNorm = 137.9255, GNorm = 0.2175
Meta loss on this task batch = 2.8420e-01, Meta loss averaged over last 500 steps = 2.8894e-01, PNorm = 137.9318, GNorm = 0.2487
Meta loss on this task batch = 2.9495e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 137.9373, GNorm = 0.2114
Meta loss on this task batch = 3.3069e-01, Meta loss averaged over last 500 steps = 2.8919e-01, PNorm = 137.9424, GNorm = 0.2159
Meta loss on this task batch = 3.0840e-01, Meta loss averaged over last 500 steps = 2.8925e-01, PNorm = 137.9483, GNorm = 0.2604
Meta loss on this task batch = 2.3632e-01, Meta loss averaged over last 500 steps = 2.8908e-01, PNorm = 137.9538, GNorm = 0.1953
Meta loss on this task batch = 2.8265e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 137.9589, GNorm = 0.2619
Meta loss on this task batch = 2.7453e-01, Meta loss averaged over last 500 steps = 2.8906e-01, PNorm = 137.9646, GNorm = 0.2038
Meta loss on this task batch = 3.0898e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 137.9694, GNorm = 0.2652
Meta loss on this task batch = 3.1487e-01, Meta loss averaged over last 500 steps = 2.8912e-01, PNorm = 137.9734, GNorm = 0.2483
Meta loss on this task batch = 2.5579e-01, Meta loss averaged over last 500 steps = 2.8914e-01, PNorm = 137.9783, GNorm = 0.2322
Meta loss on this task batch = 2.4869e-01, Meta loss averaged over last 500 steps = 2.8909e-01, PNorm = 137.9832, GNorm = 0.1934
Meta loss on this task batch = 2.3224e-01, Meta loss averaged over last 500 steps = 2.8896e-01, PNorm = 137.9893, GNorm = 0.2171
Meta loss on this task batch = 2.5704e-01, Meta loss averaged over last 500 steps = 2.8882e-01, PNorm = 137.9939, GNorm = 0.2702
Meta loss on this task batch = 2.5620e-01, Meta loss averaged over last 500 steps = 2.8864e-01, PNorm = 137.9991, GNorm = 0.2653
Took 134.54374241828918 seconds to complete one epoch of meta training
Took 142.83412337303162 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488654
Epoch 505
Meta loss on this task batch = 2.9748e-01, Meta loss averaged over last 500 steps = 2.8852e-01, PNorm = 138.0045, GNorm = 0.2685
Meta loss on this task batch = 2.4702e-01, Meta loss averaged over last 500 steps = 2.8841e-01, PNorm = 138.0102, GNorm = 0.2287
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 138.0167, GNorm = 0.2341
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 2.8838e-01, PNorm = 138.0233, GNorm = 0.2327
Meta loss on this task batch = 3.1324e-01, Meta loss averaged over last 500 steps = 2.8845e-01, PNorm = 138.0289, GNorm = 0.2791
Meta loss on this task batch = 3.0567e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 138.0343, GNorm = 0.2658
Meta loss on this task batch = 2.9286e-01, Meta loss averaged over last 500 steps = 2.8858e-01, PNorm = 138.0387, GNorm = 0.2532
Meta loss on this task batch = 2.2038e-01, Meta loss averaged over last 500 steps = 2.8853e-01, PNorm = 138.0433, GNorm = 0.1918
Meta loss on this task batch = 3.1202e-01, Meta loss averaged over last 500 steps = 2.8862e-01, PNorm = 138.0490, GNorm = 0.2688
Meta loss on this task batch = 3.1683e-01, Meta loss averaged over last 500 steps = 2.8871e-01, PNorm = 138.0551, GNorm = 0.2636
Meta loss on this task batch = 3.1464e-01, Meta loss averaged over last 500 steps = 2.8873e-01, PNorm = 138.0615, GNorm = 0.2344
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.8878e-01, PNorm = 138.0688, GNorm = 0.2505
Meta loss on this task batch = 2.1195e-01, Meta loss averaged over last 500 steps = 2.8859e-01, PNorm = 138.0773, GNorm = 0.1910
Meta loss on this task batch = 2.7057e-01, Meta loss averaged over last 500 steps = 2.8855e-01, PNorm = 138.0869, GNorm = 0.2592
Meta loss on this task batch = 3.0343e-01, Meta loss averaged over last 500 steps = 2.8858e-01, PNorm = 138.0961, GNorm = 0.2136
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 138.1049, GNorm = 0.1959
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.8851e-01, PNorm = 138.1125, GNorm = 0.2292
Meta loss on this task batch = 3.2478e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 138.1190, GNorm = 0.2209
Meta loss on this task batch = 3.4417e-01, Meta loss averaged over last 500 steps = 2.8866e-01, PNorm = 138.1236, GNorm = 0.3280
Took 111.3298749923706 seconds to complete one epoch of meta training
Took 119.38884282112122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499689
Epoch 506
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.8856e-01, PNorm = 138.1287, GNorm = 0.2055
Meta loss on this task batch = 2.6410e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 138.1340, GNorm = 0.2067
Meta loss on this task batch = 3.0958e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 138.1380, GNorm = 0.2516
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 138.1420, GNorm = 0.2203
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 138.1467, GNorm = 0.2294
Meta loss on this task batch = 3.0810e-01, Meta loss averaged over last 500 steps = 2.8847e-01, PNorm = 138.1498, GNorm = 0.2722
Meta loss on this task batch = 2.7111e-01, Meta loss averaged over last 500 steps = 2.8846e-01, PNorm = 138.1535, GNorm = 0.2515
Meta loss on this task batch = 2.7599e-01, Meta loss averaged over last 500 steps = 2.8849e-01, PNorm = 138.1580, GNorm = 0.2371
Meta loss on this task batch = 2.8435e-01, Meta loss averaged over last 500 steps = 2.8854e-01, PNorm = 138.1634, GNorm = 0.2144
Meta loss on this task batch = 2.1480e-01, Meta loss averaged over last 500 steps = 2.8835e-01, PNorm = 138.1691, GNorm = 0.1969
Meta loss on this task batch = 2.5543e-01, Meta loss averaged over last 500 steps = 2.8829e-01, PNorm = 138.1744, GNorm = 0.2053
Meta loss on this task batch = 2.6134e-01, Meta loss averaged over last 500 steps = 2.8818e-01, PNorm = 138.1804, GNorm = 0.2214
Meta loss on this task batch = 3.0848e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 138.1847, GNorm = 0.2735
Meta loss on this task batch = 3.3492e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 138.1888, GNorm = 0.2355
Meta loss on this task batch = 3.6658e-01, Meta loss averaged over last 500 steps = 2.8843e-01, PNorm = 138.1928, GNorm = 0.2689
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 2.8840e-01, PNorm = 138.1974, GNorm = 0.2532
Meta loss on this task batch = 3.2563e-01, Meta loss averaged over last 500 steps = 2.8834e-01, PNorm = 138.2019, GNorm = 0.2471
Meta loss on this task batch = 2.4251e-01, Meta loss averaged over last 500 steps = 2.8826e-01, PNorm = 138.2070, GNorm = 0.2124
Meta loss on this task batch = 2.9890e-01, Meta loss averaged over last 500 steps = 2.8839e-01, PNorm = 138.2126, GNorm = 0.2906
Took 113.93966460227966 seconds to complete one epoch of meta training
Took 120.7647454738617 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477744
Epoch 507
Meta loss on this task batch = 2.6792e-01, Meta loss averaged over last 500 steps = 2.8833e-01, PNorm = 138.2192, GNorm = 0.2209
Meta loss on this task batch = 2.5282e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 138.2260, GNorm = 0.2266
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.8825e-01, PNorm = 138.2320, GNorm = 0.2257
Meta loss on this task batch = 2.4542e-01, Meta loss averaged over last 500 steps = 2.8821e-01, PNorm = 138.2379, GNorm = 0.2037
Meta loss on this task batch = 3.2861e-01, Meta loss averaged over last 500 steps = 2.8831e-01, PNorm = 138.2435, GNorm = 0.2283
Meta loss on this task batch = 3.2406e-01, Meta loss averaged over last 500 steps = 2.8828e-01, PNorm = 138.2483, GNorm = 0.2454
Meta loss on this task batch = 2.9824e-01, Meta loss averaged over last 500 steps = 2.8827e-01, PNorm = 138.2526, GNorm = 0.2376
Meta loss on this task batch = 2.5564e-01, Meta loss averaged over last 500 steps = 2.8816e-01, PNorm = 138.2569, GNorm = 0.2371
Meta loss on this task batch = 2.7770e-01, Meta loss averaged over last 500 steps = 2.8815e-01, PNorm = 138.2618, GNorm = 0.2041
Meta loss on this task batch = 2.9764e-01, Meta loss averaged over last 500 steps = 2.8813e-01, PNorm = 138.2675, GNorm = 0.1948
Meta loss on this task batch = 2.9953e-01, Meta loss averaged over last 500 steps = 2.8819e-01, PNorm = 138.2724, GNorm = 0.2241
Meta loss on this task batch = 2.9192e-01, Meta loss averaged over last 500 steps = 2.8814e-01, PNorm = 138.2780, GNorm = 0.2344
Meta loss on this task batch = 2.9462e-01, Meta loss averaged over last 500 steps = 2.8815e-01, PNorm = 138.2842, GNorm = 0.2593
Meta loss on this task batch = 2.5408e-01, Meta loss averaged over last 500 steps = 2.8807e-01, PNorm = 138.2907, GNorm = 0.1831
Meta loss on this task batch = 2.6289e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 138.2976, GNorm = 0.2012
Meta loss on this task batch = 3.1010e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 138.3037, GNorm = 0.2439
Meta loss on this task batch = 3.4609e-01, Meta loss averaged over last 500 steps = 2.8822e-01, PNorm = 138.3094, GNorm = 0.2563
Meta loss on this task batch = 2.2097e-01, Meta loss averaged over last 500 steps = 2.8806e-01, PNorm = 138.3167, GNorm = 0.2301
Meta loss on this task batch = 2.5323e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 138.3243, GNorm = 0.2441
Took 114.49818277359009 seconds to complete one epoch of meta training
Took 122.42622184753418 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499823
Epoch 508
Meta loss on this task batch = 2.7864e-01, Meta loss averaged over last 500 steps = 2.8806e-01, PNorm = 138.3331, GNorm = 0.2301
Meta loss on this task batch = 2.7280e-01, Meta loss averaged over last 500 steps = 2.8809e-01, PNorm = 138.3418, GNorm = 0.2296
Meta loss on this task batch = 3.1430e-01, Meta loss averaged over last 500 steps = 2.8819e-01, PNorm = 138.3484, GNorm = 0.2348
Meta loss on this task batch = 2.1522e-01, Meta loss averaged over last 500 steps = 2.8806e-01, PNorm = 138.3557, GNorm = 0.2415
Meta loss on this task batch = 2.9206e-01, Meta loss averaged over last 500 steps = 2.8793e-01, PNorm = 138.3625, GNorm = 0.2431
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.8786e-01, PNorm = 138.3687, GNorm = 0.2031
Meta loss on this task batch = 3.0697e-01, Meta loss averaged over last 500 steps = 2.8795e-01, PNorm = 138.3755, GNorm = 0.2001
Meta loss on this task batch = 2.6785e-01, Meta loss averaged over last 500 steps = 2.8793e-01, PNorm = 138.3823, GNorm = 0.2113
Meta loss on this task batch = 2.4865e-01, Meta loss averaged over last 500 steps = 2.8779e-01, PNorm = 138.3893, GNorm = 0.2273
Meta loss on this task batch = 2.8318e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 138.3954, GNorm = 0.2540
Meta loss on this task batch = 3.4368e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 138.4013, GNorm = 0.2486
Meta loss on this task batch = 2.2718e-01, Meta loss averaged over last 500 steps = 2.8771e-01, PNorm = 138.4076, GNorm = 0.1955
Meta loss on this task batch = 3.0302e-01, Meta loss averaged over last 500 steps = 2.8773e-01, PNorm = 138.4139, GNorm = 0.2251
Meta loss on this task batch = 2.9763e-01, Meta loss averaged over last 500 steps = 2.8775e-01, PNorm = 138.4197, GNorm = 0.2177
Meta loss on this task batch = 3.0389e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 138.4253, GNorm = 0.2490
Meta loss on this task batch = 2.9085e-01, Meta loss averaged over last 500 steps = 2.8781e-01, PNorm = 138.4320, GNorm = 0.2371
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 138.4392, GNorm = 0.2529
Meta loss on this task batch = 3.1537e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 138.4463, GNorm = 0.2677
Meta loss on this task batch = 3.4388e-01, Meta loss averaged over last 500 steps = 2.8793e-01, PNorm = 138.4530, GNorm = 0.2672
Took 113.96506118774414 seconds to complete one epoch of meta training
Took 121.67038345336914 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478898
Epoch 509
Meta loss on this task batch = 2.6078e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 138.4583, GNorm = 0.2699
Meta loss on this task batch = 2.5001e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 138.4638, GNorm = 0.2203
Meta loss on this task batch = 2.3912e-01, Meta loss averaged over last 500 steps = 2.8769e-01, PNorm = 138.4701, GNorm = 0.2142
Meta loss on this task batch = 3.1763e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 138.4762, GNorm = 0.2323
Meta loss on this task batch = 2.6421e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 138.4822, GNorm = 0.2180
Meta loss on this task batch = 2.9683e-01, Meta loss averaged over last 500 steps = 2.8797e-01, PNorm = 138.4884, GNorm = 0.2259
Meta loss on this task batch = 3.0727e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 138.4932, GNorm = 0.2730
Meta loss on this task batch = 2.4309e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 138.4988, GNorm = 0.2097
Meta loss on this task batch = 3.2727e-01, Meta loss averaged over last 500 steps = 2.8801e-01, PNorm = 138.5041, GNorm = 0.2288
Meta loss on this task batch = 2.9382e-01, Meta loss averaged over last 500 steps = 2.8808e-01, PNorm = 138.5097, GNorm = 0.2282
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.8801e-01, PNorm = 138.5154, GNorm = 0.2407
Meta loss on this task batch = 2.9781e-01, Meta loss averaged over last 500 steps = 2.8798e-01, PNorm = 138.5218, GNorm = 0.2236
Meta loss on this task batch = 2.9196e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 138.5292, GNorm = 0.2313
Meta loss on this task batch = 2.8293e-01, Meta loss averaged over last 500 steps = 2.8789e-01, PNorm = 138.5363, GNorm = 0.2096
Meta loss on this task batch = 3.0156e-01, Meta loss averaged over last 500 steps = 2.8786e-01, PNorm = 138.5438, GNorm = 0.1993
Meta loss on this task batch = 2.5453e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 138.5522, GNorm = 0.2174
Meta loss on this task batch = 2.6249e-01, Meta loss averaged over last 500 steps = 2.8792e-01, PNorm = 138.5606, GNorm = 0.2224
Meta loss on this task batch = 3.2212e-01, Meta loss averaged over last 500 steps = 2.8802e-01, PNorm = 138.5678, GNorm = 0.2798
Meta loss on this task batch = 2.9518e-01, Meta loss averaged over last 500 steps = 2.8803e-01, PNorm = 138.5739, GNorm = 0.2879
Took 113.8108024597168 seconds to complete one epoch of meta training
Took 121.53487849235535 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509087
Epoch 510
Meta loss on this task batch = 2.9654e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 138.5795, GNorm = 0.2163
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.8800e-01, PNorm = 138.5852, GNorm = 0.2724
Meta loss on this task batch = 2.5589e-01, Meta loss averaged over last 500 steps = 2.8790e-01, PNorm = 138.5914, GNorm = 0.2040
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 138.5982, GNorm = 0.2108
Meta loss on this task batch = 3.1708e-01, Meta loss averaged over last 500 steps = 2.8782e-01, PNorm = 138.6051, GNorm = 0.2374
Meta loss on this task batch = 3.1434e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 138.6113, GNorm = 0.2160
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.8785e-01, PNorm = 138.6178, GNorm = 0.2353
Meta loss on this task batch = 2.9432e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 138.6242, GNorm = 0.2295
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.8799e-01, PNorm = 138.6304, GNorm = 0.1947
Meta loss on this task batch = 3.0685e-01, Meta loss averaged over last 500 steps = 2.8796e-01, PNorm = 138.6372, GNorm = 0.2420
Meta loss on this task batch = 2.6314e-01, Meta loss averaged over last 500 steps = 2.8788e-01, PNorm = 138.6440, GNorm = 0.2244
Meta loss on this task batch = 3.0023e-01, Meta loss averaged over last 500 steps = 2.8794e-01, PNorm = 138.6509, GNorm = 0.2499
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 138.6570, GNorm = 0.2184
Meta loss on this task batch = 3.5613e-01, Meta loss averaged over last 500 steps = 2.8787e-01, PNorm = 138.6619, GNorm = 0.2485
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 138.6676, GNorm = 0.2264
Meta loss on this task batch = 2.5779e-01, Meta loss averaged over last 500 steps = 2.8783e-01, PNorm = 138.6733, GNorm = 0.1917
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.8770e-01, PNorm = 138.6789, GNorm = 0.2330
Meta loss on this task batch = 3.2569e-01, Meta loss averaged over last 500 steps = 2.8772e-01, PNorm = 138.6855, GNorm = 0.2672
Meta loss on this task batch = 3.1124e-01, Meta loss averaged over last 500 steps = 2.8778e-01, PNorm = 138.6918, GNorm = 0.2304
Took 112.62555885314941 seconds to complete one epoch of meta training
Took 120.0155737400055 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488807
Epoch 511
Meta loss on this task batch = 2.0479e-01, Meta loss averaged over last 500 steps = 2.8763e-01, PNorm = 138.6985, GNorm = 0.1775
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 2.8760e-01, PNorm = 138.7053, GNorm = 0.2202
Meta loss on this task batch = 3.2350e-01, Meta loss averaged over last 500 steps = 2.8769e-01, PNorm = 138.7120, GNorm = 0.2438
Meta loss on this task batch = 3.1301e-01, Meta loss averaged over last 500 steps = 2.8771e-01, PNorm = 138.7174, GNorm = 0.2447
Meta loss on this task batch = 2.8409e-01, Meta loss averaged over last 500 steps = 2.8764e-01, PNorm = 138.7238, GNorm = 0.2458
Meta loss on this task batch = 3.1690e-01, Meta loss averaged over last 500 steps = 2.8784e-01, PNorm = 138.7299, GNorm = 0.2448
Meta loss on this task batch = 2.4082e-01, Meta loss averaged over last 500 steps = 2.8763e-01, PNorm = 138.7366, GNorm = 0.2138
Meta loss on this task batch = 2.6295e-01, Meta loss averaged over last 500 steps = 2.8766e-01, PNorm = 138.7440, GNorm = 0.2184
Meta loss on this task batch = 3.2566e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 138.7520, GNorm = 0.3015
Meta loss on this task batch = 2.8457e-01, Meta loss averaged over last 500 steps = 2.8753e-01, PNorm = 138.7593, GNorm = 0.2446
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 2.8752e-01, PNorm = 138.7675, GNorm = 0.2486
Meta loss on this task batch = 3.2706e-01, Meta loss averaged over last 500 steps = 2.8764e-01, PNorm = 138.7757, GNorm = 0.2194
Meta loss on this task batch = 3.2326e-01, Meta loss averaged over last 500 steps = 2.8775e-01, PNorm = 138.7834, GNorm = 0.2565
Meta loss on this task batch = 2.5365e-01, Meta loss averaged over last 500 steps = 2.8769e-01, PNorm = 138.7899, GNorm = 0.2305
Meta loss on this task batch = 2.4667e-01, Meta loss averaged over last 500 steps = 2.8759e-01, PNorm = 138.7968, GNorm = 0.2036
Meta loss on this task batch = 2.5404e-01, Meta loss averaged over last 500 steps = 2.8748e-01, PNorm = 138.8043, GNorm = 0.1996
Meta loss on this task batch = 2.8259e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 138.8116, GNorm = 0.2153
Meta loss on this task batch = 3.3632e-01, Meta loss averaged over last 500 steps = 2.8748e-01, PNorm = 138.8167, GNorm = 0.2677
Meta loss on this task batch = 2.9826e-01, Meta loss averaged over last 500 steps = 2.8751e-01, PNorm = 138.8215, GNorm = 0.2986
Took 118.40096282958984 seconds to complete one epoch of meta training
Took 125.11716628074646 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484656
Epoch 512
Meta loss on this task batch = 2.9437e-01, Meta loss averaged over last 500 steps = 2.8757e-01, PNorm = 138.8266, GNorm = 0.2262
Meta loss on this task batch = 2.9667e-01, Meta loss averaged over last 500 steps = 2.8755e-01, PNorm = 138.8322, GNorm = 0.2009
Meta loss on this task batch = 2.8512e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 138.8375, GNorm = 0.2294
Meta loss on this task batch = 2.8430e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 138.8431, GNorm = 0.2240
Meta loss on this task batch = 2.9212e-01, Meta loss averaged over last 500 steps = 2.8761e-01, PNorm = 138.8486, GNorm = 0.2131
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.8766e-01, PNorm = 138.8546, GNorm = 0.2209
Meta loss on this task batch = 2.9255e-01, Meta loss averaged over last 500 steps = 2.8763e-01, PNorm = 138.8610, GNorm = 0.2125
Meta loss on this task batch = 1.8704e-01, Meta loss averaged over last 500 steps = 2.8728e-01, PNorm = 138.8677, GNorm = 0.1945
Meta loss on this task batch = 3.3099e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 138.8739, GNorm = 0.2404
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 2.8738e-01, PNorm = 138.8810, GNorm = 0.2131
Meta loss on this task batch = 2.5161e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 138.8882, GNorm = 0.2755
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.8721e-01, PNorm = 138.8959, GNorm = 0.2260
Meta loss on this task batch = 2.6599e-01, Meta loss averaged over last 500 steps = 2.8724e-01, PNorm = 138.9033, GNorm = 0.2332
Meta loss on this task batch = 2.7623e-01, Meta loss averaged over last 500 steps = 2.8726e-01, PNorm = 138.9104, GNorm = 0.2296
Meta loss on this task batch = 2.9009e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 138.9177, GNorm = 0.2543
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 138.9249, GNorm = 0.2591
Meta loss on this task batch = 2.7631e-01, Meta loss averaged over last 500 steps = 2.8744e-01, PNorm = 138.9309, GNorm = 0.2597
Meta loss on this task batch = 3.0053e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 138.9360, GNorm = 0.2503
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.8736e-01, PNorm = 138.9414, GNorm = 0.2739
Took 114.87891173362732 seconds to complete one epoch of meta training
Took 122.26079869270325 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487532
Epoch 513
Meta loss on this task batch = 2.4066e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 138.9477, GNorm = 0.2220
Meta loss on this task batch = 2.8903e-01, Meta loss averaged over last 500 steps = 2.8732e-01, PNorm = 138.9550, GNorm = 0.1973
Meta loss on this task batch = 2.2879e-01, Meta loss averaged over last 500 steps = 2.8712e-01, PNorm = 138.9625, GNorm = 0.2700
Meta loss on this task batch = 2.4076e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 138.9702, GNorm = 0.2240
Meta loss on this task batch = 2.6217e-01, Meta loss averaged over last 500 steps = 2.8705e-01, PNorm = 138.9791, GNorm = 0.2848
Meta loss on this task batch = 3.7779e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 138.9865, GNorm = 0.2763
Meta loss on this task batch = 3.1346e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 138.9939, GNorm = 0.2297
Meta loss on this task batch = 3.0981e-01, Meta loss averaged over last 500 steps = 2.8733e-01, PNorm = 139.0004, GNorm = 0.2713
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 2.8743e-01, PNorm = 139.0066, GNorm = 0.2508
Meta loss on this task batch = 2.1327e-01, Meta loss averaged over last 500 steps = 2.8722e-01, PNorm = 139.0134, GNorm = 0.1770
Meta loss on this task batch = 2.6764e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 139.0201, GNorm = 0.2306
Meta loss on this task batch = 3.0067e-01, Meta loss averaged over last 500 steps = 2.8736e-01, PNorm = 139.0262, GNorm = 0.2391
Meta loss on this task batch = 3.3671e-01, Meta loss averaged over last 500 steps = 2.8740e-01, PNorm = 139.0327, GNorm = 0.2452
Meta loss on this task batch = 2.5622e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 139.0396, GNorm = 0.2184
Meta loss on this task batch = 2.9408e-01, Meta loss averaged over last 500 steps = 2.8732e-01, PNorm = 139.0461, GNorm = 0.2335
Meta loss on this task batch = 3.4247e-01, Meta loss averaged over last 500 steps = 2.8748e-01, PNorm = 139.0530, GNorm = 0.2049
Meta loss on this task batch = 2.7593e-01, Meta loss averaged over last 500 steps = 2.8742e-01, PNorm = 139.0594, GNorm = 0.2121
Meta loss on this task batch = 2.7084e-01, Meta loss averaged over last 500 steps = 2.8737e-01, PNorm = 139.0655, GNorm = 0.1999
Meta loss on this task batch = 2.9861e-01, Meta loss averaged over last 500 steps = 2.8738e-01, PNorm = 139.0719, GNorm = 0.2634
Took 112.58536911010742 seconds to complete one epoch of meta training
Took 120.64506459236145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494584
Epoch 514
Meta loss on this task batch = 2.8650e-01, Meta loss averaged over last 500 steps = 2.8736e-01, PNorm = 139.0784, GNorm = 0.2451
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.8723e-01, PNorm = 139.0855, GNorm = 0.2001
Meta loss on this task batch = 3.2595e-01, Meta loss averaged over last 500 steps = 2.8733e-01, PNorm = 139.0921, GNorm = 0.2252
Meta loss on this task batch = 2.9325e-01, Meta loss averaged over last 500 steps = 2.8737e-01, PNorm = 139.0985, GNorm = 0.2159
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.8729e-01, PNorm = 139.1061, GNorm = 0.2136
Meta loss on this task batch = 2.9178e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 139.1133, GNorm = 0.2435
Meta loss on this task batch = 2.6146e-01, Meta loss averaged over last 500 steps = 2.8725e-01, PNorm = 139.1197, GNorm = 0.2395
Meta loss on this task batch = 3.1126e-01, Meta loss averaged over last 500 steps = 2.8727e-01, PNorm = 139.1258, GNorm = 0.2107
Meta loss on this task batch = 3.2095e-01, Meta loss averaged over last 500 steps = 2.8737e-01, PNorm = 139.1313, GNorm = 0.2335
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 2.8741e-01, PNorm = 139.1367, GNorm = 0.2616
Meta loss on this task batch = 2.5637e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 139.1423, GNorm = 0.2171
Meta loss on this task batch = 3.0574e-01, Meta loss averaged over last 500 steps = 2.8739e-01, PNorm = 139.1472, GNorm = 0.2281
Meta loss on this task batch = 2.5912e-01, Meta loss averaged over last 500 steps = 2.8733e-01, PNorm = 139.1525, GNorm = 0.2027
Meta loss on this task batch = 3.1575e-01, Meta loss averaged over last 500 steps = 2.8747e-01, PNorm = 139.1565, GNorm = 0.2673
Meta loss on this task batch = 2.9738e-01, Meta loss averaged over last 500 steps = 2.8741e-01, PNorm = 139.1611, GNorm = 0.2091
Meta loss on this task batch = 3.2749e-01, Meta loss averaged over last 500 steps = 2.8754e-01, PNorm = 139.1659, GNorm = 0.2512
Meta loss on this task batch = 2.5836e-01, Meta loss averaged over last 500 steps = 2.8746e-01, PNorm = 139.1709, GNorm = 0.1993
Meta loss on this task batch = 2.5210e-01, Meta loss averaged over last 500 steps = 2.8735e-01, PNorm = 139.1762, GNorm = 0.2169
Meta loss on this task batch = 2.7820e-01, Meta loss averaged over last 500 steps = 2.8734e-01, PNorm = 139.1812, GNorm = 0.2357
Took 113.77233815193176 seconds to complete one epoch of meta training
Took 121.42707848548889 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482137
Epoch 515
Meta loss on this task batch = 2.7966e-01, Meta loss averaged over last 500 steps = 2.8731e-01, PNorm = 139.1867, GNorm = 0.2542
Meta loss on this task batch = 2.8304e-01, Meta loss averaged over last 500 steps = 2.8713e-01, PNorm = 139.1918, GNorm = 0.2488
Meta loss on this task batch = 2.4078e-01, Meta loss averaged over last 500 steps = 2.8702e-01, PNorm = 139.1978, GNorm = 0.2005
Meta loss on this task batch = 2.9109e-01, Meta loss averaged over last 500 steps = 2.8698e-01, PNorm = 139.2045, GNorm = 0.2312
Meta loss on this task batch = 2.1081e-01, Meta loss averaged over last 500 steps = 2.8680e-01, PNorm = 139.2127, GNorm = 0.2204
Meta loss on this task batch = 3.2249e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 139.2197, GNorm = 0.2411
Meta loss on this task batch = 2.4030e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 139.2270, GNorm = 0.2057
Meta loss on this task batch = 3.2776e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 139.2331, GNorm = 0.2503
Meta loss on this task batch = 2.7040e-01, Meta loss averaged over last 500 steps = 2.8666e-01, PNorm = 139.2393, GNorm = 0.2174
Meta loss on this task batch = 2.6362e-01, Meta loss averaged over last 500 steps = 2.8654e-01, PNorm = 139.2449, GNorm = 0.2271
Meta loss on this task batch = 3.0524e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 139.2504, GNorm = 0.2624
Meta loss on this task batch = 3.1965e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 139.2546, GNorm = 0.2621
Meta loss on this task batch = 2.7673e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 139.2590, GNorm = 0.2266
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 139.2636, GNorm = 0.2397
Meta loss on this task batch = 3.1081e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 139.2674, GNorm = 0.2534
Meta loss on this task batch = 2.8697e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 139.2716, GNorm = 0.2134
Meta loss on this task batch = 3.1213e-01, Meta loss averaged over last 500 steps = 2.8676e-01, PNorm = 139.2753, GNorm = 0.2219
Meta loss on this task batch = 2.5684e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 139.2803, GNorm = 0.2062
Meta loss on this task batch = 2.9259e-01, Meta loss averaged over last 500 steps = 2.8682e-01, PNorm = 139.2853, GNorm = 0.2356
Took 114.37138676643372 seconds to complete one epoch of meta training
Took 122.76967692375183 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469792
Epoch 516
Meta loss on this task batch = 2.9752e-01, Meta loss averaged over last 500 steps = 2.8684e-01, PNorm = 139.2915, GNorm = 0.2086
Meta loss on this task batch = 2.4772e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 139.2976, GNorm = 0.2423
Meta loss on this task batch = 2.3582e-01, Meta loss averaged over last 500 steps = 2.8660e-01, PNorm = 139.3041, GNorm = 0.2108
Meta loss on this task batch = 2.5609e-01, Meta loss averaged over last 500 steps = 2.8645e-01, PNorm = 139.3112, GNorm = 0.2019
Meta loss on this task batch = 2.9970e-01, Meta loss averaged over last 500 steps = 2.8647e-01, PNorm = 139.3182, GNorm = 0.2127
Meta loss on this task batch = 2.6551e-01, Meta loss averaged over last 500 steps = 2.8648e-01, PNorm = 139.3249, GNorm = 0.2143
Meta loss on this task batch = 2.7903e-01, Meta loss averaged over last 500 steps = 2.8653e-01, PNorm = 139.3321, GNorm = 0.2285
Meta loss on this task batch = 2.7311e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 139.3392, GNorm = 0.2412
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.8651e-01, PNorm = 139.3468, GNorm = 0.2498
Meta loss on this task batch = 2.7794e-01, Meta loss averaged over last 500 steps = 2.8646e-01, PNorm = 139.3539, GNorm = 0.2183
Meta loss on this task batch = 2.5440e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 139.3604, GNorm = 0.2109
Meta loss on this task batch = 3.6080e-01, Meta loss averaged over last 500 steps = 2.8668e-01, PNorm = 139.3664, GNorm = 0.2957
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 2.8674e-01, PNorm = 139.3721, GNorm = 0.2335
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 139.3777, GNorm = 0.2448
Meta loss on this task batch = 3.2549e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 139.3827, GNorm = 0.2241
Meta loss on this task batch = 2.8665e-01, Meta loss averaged over last 500 steps = 2.8688e-01, PNorm = 139.3868, GNorm = 0.2484
Meta loss on this task batch = 2.9451e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 139.3921, GNorm = 0.2792
Meta loss on this task batch = 3.3269e-01, Meta loss averaged over last 500 steps = 2.8686e-01, PNorm = 139.3980, GNorm = 0.2619
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.8673e-01, PNorm = 139.4031, GNorm = 0.2564
Took 114.83026266098022 seconds to complete one epoch of meta training
Took 122.26326060295105 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476386
Epoch 517
Meta loss on this task batch = 2.5069e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 139.4098, GNorm = 0.2588
Meta loss on this task batch = 2.9319e-01, Meta loss averaged over last 500 steps = 2.8656e-01, PNorm = 139.4162, GNorm = 0.1977
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.8655e-01, PNorm = 139.4224, GNorm = 0.2283
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 139.4276, GNorm = 0.2896
Meta loss on this task batch = 3.1811e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 139.4309, GNorm = 0.2707
Meta loss on this task batch = 2.9158e-01, Meta loss averaged over last 500 steps = 2.8669e-01, PNorm = 139.4349, GNorm = 0.2224
Meta loss on this task batch = 2.4934e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 139.4390, GNorm = 0.2116
Meta loss on this task batch = 2.7189e-01, Meta loss averaged over last 500 steps = 2.8663e-01, PNorm = 139.4435, GNorm = 0.2258
Meta loss on this task batch = 2.7362e-01, Meta loss averaged over last 500 steps = 2.8662e-01, PNorm = 139.4478, GNorm = 0.2253
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 139.4517, GNorm = 0.2113
Meta loss on this task batch = 3.2841e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 139.4558, GNorm = 0.2247
Meta loss on this task batch = 2.4929e-01, Meta loss averaged over last 500 steps = 2.8652e-01, PNorm = 139.4603, GNorm = 0.2192
Meta loss on this task batch = 2.7452e-01, Meta loss averaged over last 500 steps = 2.8641e-01, PNorm = 139.4649, GNorm = 0.1989
Meta loss on this task batch = 3.4806e-01, Meta loss averaged over last 500 steps = 2.8657e-01, PNorm = 139.4692, GNorm = 0.2577
Meta loss on this task batch = 3.1934e-01, Meta loss averaged over last 500 steps = 2.8664e-01, PNorm = 139.4737, GNorm = 0.2540
Meta loss on this task batch = 3.2981e-01, Meta loss averaged over last 500 steps = 2.8671e-01, PNorm = 139.4789, GNorm = 0.2565
Meta loss on this task batch = 2.5335e-01, Meta loss averaged over last 500 steps = 2.8659e-01, PNorm = 139.4847, GNorm = 0.2163
Meta loss on this task batch = 2.9073e-01, Meta loss averaged over last 500 steps = 2.8665e-01, PNorm = 139.4910, GNorm = 0.2121
Meta loss on this task batch = 2.4472e-01, Meta loss averaged over last 500 steps = 2.8658e-01, PNorm = 139.4982, GNorm = 0.2503
Took 117.68337297439575 seconds to complete one epoch of meta training
Took 125.59106516838074 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472952
Epoch 518
Meta loss on this task batch = 2.4399e-01, Meta loss averaged over last 500 steps = 2.8649e-01, PNorm = 139.5059, GNorm = 0.2013
Meta loss on this task batch = 2.5332e-01, Meta loss averaged over last 500 steps = 2.8642e-01, PNorm = 139.5136, GNorm = 0.2201
Meta loss on this task batch = 2.5506e-01, Meta loss averaged over last 500 steps = 2.8622e-01, PNorm = 139.5212, GNorm = 0.1945
Meta loss on this task batch = 2.6901e-01, Meta loss averaged over last 500 steps = 2.8624e-01, PNorm = 139.5296, GNorm = 0.2416
Meta loss on this task batch = 2.3077e-01, Meta loss averaged over last 500 steps = 2.8612e-01, PNorm = 139.5377, GNorm = 0.2114
Meta loss on this task batch = 2.7511e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 139.5453, GNorm = 0.2303
Meta loss on this task batch = 2.6803e-01, Meta loss averaged over last 500 steps = 2.8593e-01, PNorm = 139.5524, GNorm = 0.2355
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 139.5586, GNorm = 0.2197
Meta loss on this task batch = 2.7580e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 139.5647, GNorm = 0.2139
Meta loss on this task batch = 2.8147e-01, Meta loss averaged over last 500 steps = 2.8571e-01, PNorm = 139.5710, GNorm = 0.2347
Meta loss on this task batch = 2.8739e-01, Meta loss averaged over last 500 steps = 2.8574e-01, PNorm = 139.5781, GNorm = 0.2816
Meta loss on this task batch = 3.3045e-01, Meta loss averaged over last 500 steps = 2.8590e-01, PNorm = 139.5843, GNorm = 0.2680
Meta loss on this task batch = 3.4684e-01, Meta loss averaged over last 500 steps = 2.8607e-01, PNorm = 139.5904, GNorm = 0.3266
Meta loss on this task batch = 3.2043e-01, Meta loss averaged over last 500 steps = 2.8605e-01, PNorm = 139.5948, GNorm = 0.2821
Meta loss on this task batch = 2.6953e-01, Meta loss averaged over last 500 steps = 2.8600e-01, PNorm = 139.5972, GNorm = 0.3191
Meta loss on this task batch = 2.9008e-01, Meta loss averaged over last 500 steps = 2.8592e-01, PNorm = 139.5999, GNorm = 0.2435
Meta loss on this task batch = 2.8984e-01, Meta loss averaged over last 500 steps = 2.8595e-01, PNorm = 139.6024, GNorm = 0.2270
Meta loss on this task batch = 2.9520e-01, Meta loss averaged over last 500 steps = 2.8604e-01, PNorm = 139.6055, GNorm = 0.2279
Meta loss on this task batch = 2.7780e-01, Meta loss averaged over last 500 steps = 2.8606e-01, PNorm = 139.6092, GNorm = 0.2644
Took 115.69517922401428 seconds to complete one epoch of meta training
Took 123.49378752708435 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470554
Epoch 519
Meta loss on this task batch = 2.1502e-01, Meta loss averaged over last 500 steps = 2.8596e-01, PNorm = 139.6146, GNorm = 0.1848
Meta loss on this task batch = 2.9212e-01, Meta loss averaged over last 500 steps = 2.8589e-01, PNorm = 139.6213, GNorm = 0.2454
Meta loss on this task batch = 3.0384e-01, Meta loss averaged over last 500 steps = 2.8602e-01, PNorm = 139.6277, GNorm = 0.2235
Meta loss on this task batch = 2.5636e-01, Meta loss averaged over last 500 steps = 2.8598e-01, PNorm = 139.6345, GNorm = 0.2350
Meta loss on this task batch = 2.7554e-01, Meta loss averaged over last 500 steps = 2.8582e-01, PNorm = 139.6424, GNorm = 0.2287
Meta loss on this task batch = 2.4230e-01, Meta loss averaged over last 500 steps = 2.8572e-01, PNorm = 139.6509, GNorm = 0.2124
Meta loss on this task batch = 3.0748e-01, Meta loss averaged over last 500 steps = 2.8578e-01, PNorm = 139.6596, GNorm = 0.2503
Meta loss on this task batch = 2.3082e-01, Meta loss averaged over last 500 steps = 2.8569e-01, PNorm = 139.6680, GNorm = 0.2072
Meta loss on this task batch = 3.0453e-01, Meta loss averaged over last 500 steps = 2.8575e-01, PNorm = 139.6761, GNorm = 0.2248
Meta loss on this task batch = 2.7776e-01, Meta loss averaged over last 500 steps = 2.8568e-01, PNorm = 139.6845, GNorm = 0.2340
Meta loss on this task batch = 2.8278e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 139.6917, GNorm = 0.2492
Meta loss on this task batch = 2.5071e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 139.6988, GNorm = 0.2328
Meta loss on this task batch = 2.7200e-01, Meta loss averaged over last 500 steps = 2.8550e-01, PNorm = 139.7055, GNorm = 0.2350
Meta loss on this task batch = 3.3131e-01, Meta loss averaged over last 500 steps = 2.8564e-01, PNorm = 139.7109, GNorm = 0.2950
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.8563e-01, PNorm = 139.7159, GNorm = 0.2694
Meta loss on this task batch = 2.5298e-01, Meta loss averaged over last 500 steps = 2.8552e-01, PNorm = 139.7220, GNorm = 0.2556
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.8554e-01, PNorm = 139.7273, GNorm = 0.2294
Meta loss on this task batch = 2.6355e-01, Meta loss averaged over last 500 steps = 2.8545e-01, PNorm = 139.7315, GNorm = 0.2241
Meta loss on this task batch = 3.3661e-01, Meta loss averaged over last 500 steps = 2.8560e-01, PNorm = 139.7347, GNorm = 0.2917
Took 115.57278370857239 seconds to complete one epoch of meta training
Took 122.78268122673035 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477826
Epoch 520
Meta loss on this task batch = 2.4848e-01, Meta loss averaged over last 500 steps = 2.8548e-01, PNorm = 139.7387, GNorm = 0.2211
Meta loss on this task batch = 2.4146e-01, Meta loss averaged over last 500 steps = 2.8536e-01, PNorm = 139.7443, GNorm = 0.2542
Meta loss on this task batch = 2.4907e-01, Meta loss averaged over last 500 steps = 2.8531e-01, PNorm = 139.7502, GNorm = 0.2250
Meta loss on this task batch = 2.7815e-01, Meta loss averaged over last 500 steps = 2.8531e-01, PNorm = 139.7569, GNorm = 0.2184
Meta loss on this task batch = 2.7391e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 139.7624, GNorm = 0.2295
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.8530e-01, PNorm = 139.7685, GNorm = 0.2561
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 2.8528e-01, PNorm = 139.7762, GNorm = 0.3125
Meta loss on this task batch = 2.9960e-01, Meta loss averaged over last 500 steps = 2.8538e-01, PNorm = 139.7837, GNorm = 0.2442
Meta loss on this task batch = 3.0174e-01, Meta loss averaged over last 500 steps = 2.8536e-01, PNorm = 139.7901, GNorm = 0.2285
Meta loss on this task batch = 2.3575e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 139.7958, GNorm = 0.2274
Meta loss on this task batch = 2.8902e-01, Meta loss averaged over last 500 steps = 2.8540e-01, PNorm = 139.8014, GNorm = 0.2104
Meta loss on this task batch = 2.3327e-01, Meta loss averaged over last 500 steps = 2.8524e-01, PNorm = 139.8077, GNorm = 0.2098
Meta loss on this task batch = 3.1240e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 139.8141, GNorm = 0.2263
Meta loss on this task batch = 2.8704e-01, Meta loss averaged over last 500 steps = 2.8537e-01, PNorm = 139.8199, GNorm = 0.2575
Meta loss on this task batch = 3.8953e-01, Meta loss averaged over last 500 steps = 2.8553e-01, PNorm = 139.8256, GNorm = 0.2947
Meta loss on this task batch = 3.0112e-01, Meta loss averaged over last 500 steps = 2.8556e-01, PNorm = 139.8306, GNorm = 0.2683
Meta loss on this task batch = 2.5931e-01, Meta loss averaged over last 500 steps = 2.8549e-01, PNorm = 139.8357, GNorm = 0.2375
Meta loss on this task batch = 3.0135e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 139.8396, GNorm = 0.2385
Meta loss on this task batch = 2.6651e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 139.8449, GNorm = 0.3061
Took 268.44104051589966 seconds to complete one epoch of meta training
Took 276.19142723083496 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482440
Epoch 521
Meta loss on this task batch = 2.4586e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 139.8511, GNorm = 0.2561
Meta loss on this task batch = 2.3435e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 139.8577, GNorm = 0.2193
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 139.8645, GNorm = 0.2132
Meta loss on this task batch = 3.0941e-01, Meta loss averaged over last 500 steps = 2.8501e-01, PNorm = 139.8691, GNorm = 0.3193
Meta loss on this task batch = 2.6867e-01, Meta loss averaged over last 500 steps = 2.8496e-01, PNorm = 139.8740, GNorm = 0.2392
Meta loss on this task batch = 3.1483e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 139.8781, GNorm = 0.2393
Meta loss on this task batch = 2.3614e-01, Meta loss averaged over last 500 steps = 2.8486e-01, PNorm = 139.8830, GNorm = 0.2322
Meta loss on this task batch = 3.1890e-01, Meta loss averaged over last 500 steps = 2.8498e-01, PNorm = 139.8879, GNorm = 0.2526
Meta loss on this task batch = 3.2597e-01, Meta loss averaged over last 500 steps = 2.8503e-01, PNorm = 139.8929, GNorm = 0.2468
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 2.8502e-01, PNorm = 139.8969, GNorm = 0.2195
Meta loss on this task batch = 3.4281e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 139.9011, GNorm = 0.2828
Meta loss on this task batch = 3.3373e-01, Meta loss averaged over last 500 steps = 2.8535e-01, PNorm = 139.9052, GNorm = 0.2051
Meta loss on this task batch = 2.8007e-01, Meta loss averaged over last 500 steps = 2.8543e-01, PNorm = 139.9103, GNorm = 0.1993
Meta loss on this task batch = 3.0931e-01, Meta loss averaged over last 500 steps = 2.8547e-01, PNorm = 139.9166, GNorm = 0.2804
Meta loss on this task batch = 2.4958e-01, Meta loss averaged over last 500 steps = 2.8533e-01, PNorm = 139.9232, GNorm = 0.2151
Meta loss on this task batch = 2.6162e-01, Meta loss averaged over last 500 steps = 2.8521e-01, PNorm = 139.9301, GNorm = 0.2222
Meta loss on this task batch = 3.1888e-01, Meta loss averaged over last 500 steps = 2.8520e-01, PNorm = 139.9362, GNorm = 0.2358
Meta loss on this task batch = 2.9440e-01, Meta loss averaged over last 500 steps = 2.8530e-01, PNorm = 139.9412, GNorm = 0.2522
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.8528e-01, PNorm = 139.9465, GNorm = 0.2368
Took 250.26231122016907 seconds to complete one epoch of meta training
Took 258.45149660110474 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475358
Epoch 522
Meta loss on this task batch = 2.5126e-01, Meta loss averaged over last 500 steps = 2.8527e-01, PNorm = 139.9522, GNorm = 0.2057
Meta loss on this task batch = 3.0180e-01, Meta loss averaged over last 500 steps = 2.8532e-01, PNorm = 139.9579, GNorm = 0.2417
Meta loss on this task batch = 2.7278e-01, Meta loss averaged over last 500 steps = 2.8528e-01, PNorm = 139.9634, GNorm = 0.2346
Meta loss on this task batch = 2.8293e-01, Meta loss averaged over last 500 steps = 2.8513e-01, PNorm = 139.9689, GNorm = 0.2487
Meta loss on this task batch = 2.1242e-01, Meta loss averaged over last 500 steps = 2.8501e-01, PNorm = 139.9756, GNorm = 0.1978
Meta loss on this task batch = 2.7960e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 139.9834, GNorm = 0.1934
Meta loss on this task batch = 2.9052e-01, Meta loss averaged over last 500 steps = 2.8514e-01, PNorm = 139.9913, GNorm = 0.2126
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 139.9988, GNorm = 0.2409
Meta loss on this task batch = 3.2587e-01, Meta loss averaged over last 500 steps = 2.8518e-01, PNorm = 140.0062, GNorm = 0.2886
Meta loss on this task batch = 3.1049e-01, Meta loss averaged over last 500 steps = 2.8526e-01, PNorm = 140.0133, GNorm = 0.2310
Meta loss on this task batch = 2.5205e-01, Meta loss averaged over last 500 steps = 2.8516e-01, PNorm = 140.0201, GNorm = 0.2136
Meta loss on this task batch = 2.7793e-01, Meta loss averaged over last 500 steps = 2.8508e-01, PNorm = 140.0259, GNorm = 0.2372
Meta loss on this task batch = 2.9308e-01, Meta loss averaged over last 500 steps = 2.8500e-01, PNorm = 140.0317, GNorm = 0.2119
Meta loss on this task batch = 3.2337e-01, Meta loss averaged over last 500 steps = 2.8509e-01, PNorm = 140.0370, GNorm = 0.2224
Meta loss on this task batch = 3.1567e-01, Meta loss averaged over last 500 steps = 2.8505e-01, PNorm = 140.0421, GNorm = 0.2122
Meta loss on this task batch = 2.3585e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 140.0484, GNorm = 0.1951
Meta loss on this task batch = 2.5120e-01, Meta loss averaged over last 500 steps = 2.8488e-01, PNorm = 140.0555, GNorm = 0.2261
Meta loss on this task batch = 3.5147e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 140.0618, GNorm = 0.2356
Meta loss on this task batch = 3.1480e-01, Meta loss averaged over last 500 steps = 2.8515e-01, PNorm = 140.0673, GNorm = 0.3125
Took 116.15938448905945 seconds to complete one epoch of meta training
Took 124.06394076347351 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483271
Epoch 523
Meta loss on this task batch = 3.3044e-01, Meta loss averaged over last 500 steps = 2.8517e-01, PNorm = 140.0727, GNorm = 0.2423
Meta loss on this task batch = 3.2770e-01, Meta loss averaged over last 500 steps = 2.8523e-01, PNorm = 140.0789, GNorm = 0.2728
Meta loss on this task batch = 2.4378e-01, Meta loss averaged over last 500 steps = 2.8505e-01, PNorm = 140.0852, GNorm = 0.2073
Meta loss on this task batch = 3.2038e-01, Meta loss averaged over last 500 steps = 2.8519e-01, PNorm = 140.0905, GNorm = 0.2577
Meta loss on this task batch = 1.9589e-01, Meta loss averaged over last 500 steps = 2.8507e-01, PNorm = 140.0963, GNorm = 0.1779
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 140.1013, GNorm = 0.2886
Meta loss on this task batch = 2.3617e-01, Meta loss averaged over last 500 steps = 2.8495e-01, PNorm = 140.1087, GNorm = 0.2188
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.8490e-01, PNorm = 140.1146, GNorm = 0.2596
Meta loss on this task batch = 3.2729e-01, Meta loss averaged over last 500 steps = 2.8492e-01, PNorm = 140.1201, GNorm = 0.2533
Meta loss on this task batch = 3.6968e-01, Meta loss averaged over last 500 steps = 2.8512e-01, PNorm = 140.1255, GNorm = 0.2712
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.8511e-01, PNorm = 140.1316, GNorm = 0.2259
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.8503e-01, PNorm = 140.1380, GNorm = 0.2503
Meta loss on this task batch = 2.5938e-01, Meta loss averaged over last 500 steps = 2.8496e-01, PNorm = 140.1441, GNorm = 0.2036
Meta loss on this task batch = 2.7942e-01, Meta loss averaged over last 500 steps = 2.8494e-01, PNorm = 140.1505, GNorm = 0.2179
Meta loss on this task batch = 2.5558e-01, Meta loss averaged over last 500 steps = 2.8494e-01, PNorm = 140.1568, GNorm = 0.2172
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.8488e-01, PNorm = 140.1628, GNorm = 0.2317
Meta loss on this task batch = 2.9308e-01, Meta loss averaged over last 500 steps = 2.8486e-01, PNorm = 140.1687, GNorm = 0.2328
Meta loss on this task batch = 2.9140e-01, Meta loss averaged over last 500 steps = 2.8489e-01, PNorm = 140.1747, GNorm = 0.2274
Meta loss on this task batch = 2.5168e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 140.1811, GNorm = 0.2532
Took 113.84782934188843 seconds to complete one epoch of meta training
Took 121.67010116577148 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469436
Epoch 524
Meta loss on this task batch = 3.0065e-01, Meta loss averaged over last 500 steps = 2.8467e-01, PNorm = 140.1869, GNorm = 0.2695
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 140.1924, GNorm = 0.2321
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.8471e-01, PNorm = 140.1978, GNorm = 0.2987
Meta loss on this task batch = 2.8764e-01, Meta loss averaged over last 500 steps = 2.8469e-01, PNorm = 140.2040, GNorm = 0.2326
Meta loss on this task batch = 2.9048e-01, Meta loss averaged over last 500 steps = 2.8473e-01, PNorm = 140.2102, GNorm = 0.2148
Meta loss on this task batch = 2.9796e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 140.2159, GNorm = 0.2659
Meta loss on this task batch = 2.5180e-01, Meta loss averaged over last 500 steps = 2.8478e-01, PNorm = 140.2225, GNorm = 0.2437
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.8475e-01, PNorm = 140.2287, GNorm = 0.2339
Meta loss on this task batch = 3.2625e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 140.2346, GNorm = 0.2507
Meta loss on this task batch = 2.3110e-01, Meta loss averaged over last 500 steps = 2.8466e-01, PNorm = 140.2410, GNorm = 0.2019
Meta loss on this task batch = 3.0009e-01, Meta loss averaged over last 500 steps = 2.8461e-01, PNorm = 140.2470, GNorm = 0.2796
Meta loss on this task batch = 2.2601e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 140.2522, GNorm = 0.2520
Meta loss on this task batch = 3.1065e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 140.2557, GNorm = 0.2564
Meta loss on this task batch = 3.1914e-01, Meta loss averaged over last 500 steps = 2.8473e-01, PNorm = 140.2586, GNorm = 0.2723
Meta loss on this task batch = 3.0477e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 140.2618, GNorm = 0.2461
Meta loss on this task batch = 2.7729e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 140.2646, GNorm = 0.2161
Meta loss on this task batch = 2.9304e-01, Meta loss averaged over last 500 steps = 2.8478e-01, PNorm = 140.2676, GNorm = 0.2648
Meta loss on this task batch = 2.3321e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 140.2719, GNorm = 0.2397
Meta loss on this task batch = 2.5433e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.2768, GNorm = 0.2205
Took 114.96100783348083 seconds to complete one epoch of meta training
Took 122.82643985748291 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473400
Epoch 525
Meta loss on this task batch = 2.3408e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 140.2818, GNorm = 0.1956
Meta loss on this task batch = 2.8998e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 140.2868, GNorm = 0.2553
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 140.2923, GNorm = 0.2305
Meta loss on this task batch = 2.8033e-01, Meta loss averaged over last 500 steps = 2.8448e-01, PNorm = 140.2985, GNorm = 0.2086
Meta loss on this task batch = 2.5348e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 140.3049, GNorm = 0.2286
Meta loss on this task batch = 2.4760e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 140.3117, GNorm = 0.2057
Meta loss on this task batch = 3.0312e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.3187, GNorm = 0.2176
Meta loss on this task batch = 2.8343e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.3253, GNorm = 0.2563
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.3315, GNorm = 0.2697
Meta loss on this task batch = 3.0623e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 140.3388, GNorm = 0.2265
Meta loss on this task batch = 2.4329e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 140.3460, GNorm = 0.2360
Meta loss on this task batch = 3.0139e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 140.3519, GNorm = 0.2508
Meta loss on this task batch = 2.5669e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 140.3578, GNorm = 0.2227
Meta loss on this task batch = 2.8824e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 140.3629, GNorm = 0.2523
Meta loss on this task batch = 2.8670e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 140.3671, GNorm = 0.2336
Meta loss on this task batch = 3.2275e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 140.3711, GNorm = 0.2621
Meta loss on this task batch = 3.1560e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 140.3753, GNorm = 0.2447
Meta loss on this task batch = 2.8628e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 140.3797, GNorm = 0.2281
Meta loss on this task batch = 3.1938e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 140.3848, GNorm = 0.2928
Took 113.91151928901672 seconds to complete one epoch of meta training
Took 121.85656380653381 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467405
Epoch 526
Meta loss on this task batch = 3.0360e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 140.3899, GNorm = 0.2199
Meta loss on this task batch = 2.9570e-01, Meta loss averaged over last 500 steps = 2.8451e-01, PNorm = 140.3945, GNorm = 0.2150
Meta loss on this task batch = 2.6758e-01, Meta loss averaged over last 500 steps = 2.8455e-01, PNorm = 140.3995, GNorm = 0.2168
Meta loss on this task batch = 2.3119e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 140.4054, GNorm = 0.2123
Meta loss on this task batch = 2.8640e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.4122, GNorm = 0.2177
Meta loss on this task batch = 3.1863e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 140.4181, GNorm = 0.2504
Meta loss on this task batch = 2.9902e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 140.4240, GNorm = 0.2325
Meta loss on this task batch = 2.9630e-01, Meta loss averaged over last 500 steps = 2.8469e-01, PNorm = 140.4310, GNorm = 0.2217
Meta loss on this task batch = 3.1837e-01, Meta loss averaged over last 500 steps = 2.8478e-01, PNorm = 140.4378, GNorm = 0.2364
Meta loss on this task batch = 3.3573e-01, Meta loss averaged over last 500 steps = 2.8487e-01, PNorm = 140.4451, GNorm = 0.2646
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.8479e-01, PNorm = 140.4525, GNorm = 0.2569
Meta loss on this task batch = 3.0024e-01, Meta loss averaged over last 500 steps = 2.8481e-01, PNorm = 140.4599, GNorm = 0.2464
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.8483e-01, PNorm = 140.4658, GNorm = 0.2350
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.8483e-01, PNorm = 140.4721, GNorm = 0.1896
Meta loss on this task batch = 2.5580e-01, Meta loss averaged over last 500 steps = 2.8484e-01, PNorm = 140.4784, GNorm = 0.2758
Meta loss on this task batch = 2.7398e-01, Meta loss averaged over last 500 steps = 2.8473e-01, PNorm = 140.4831, GNorm = 0.2753
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.8477e-01, PNorm = 140.4870, GNorm = 0.2798
Meta loss on this task batch = 2.0228e-01, Meta loss averaged over last 500 steps = 2.8457e-01, PNorm = 140.4919, GNorm = 0.1798
Meta loss on this task batch = 3.0542e-01, Meta loss averaged over last 500 steps = 2.8460e-01, PNorm = 140.4951, GNorm = 0.3231
Took 113.92687702178955 seconds to complete one epoch of meta training
Took 121.59990167617798 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.458030
Epoch 527
Meta loss on this task batch = 3.0530e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 140.4988, GNorm = 0.2314
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 2.8465e-01, PNorm = 140.5025, GNorm = 0.2315
Meta loss on this task batch = 3.6674e-01, Meta loss averaged over last 500 steps = 2.8474e-01, PNorm = 140.5068, GNorm = 0.2417
Meta loss on this task batch = 2.3784e-01, Meta loss averaged over last 500 steps = 2.8459e-01, PNorm = 140.5125, GNorm = 0.2204
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 140.5191, GNorm = 0.2300
Meta loss on this task batch = 2.3455e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 140.5268, GNorm = 0.1954
Meta loss on this task batch = 2.6701e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 140.5348, GNorm = 0.2173
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 140.5424, GNorm = 0.2201
Meta loss on this task batch = 2.9057e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 140.5501, GNorm = 0.2230
Meta loss on this task batch = 2.2018e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 140.5573, GNorm = 0.2142
Meta loss on this task batch = 3.5687e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 140.5634, GNorm = 0.2387
Meta loss on this task batch = 3.2917e-01, Meta loss averaged over last 500 steps = 2.8456e-01, PNorm = 140.5685, GNorm = 0.2772
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.8458e-01, PNorm = 140.5737, GNorm = 0.2171
Meta loss on this task batch = 2.7960e-01, Meta loss averaged over last 500 steps = 2.8453e-01, PNorm = 140.5793, GNorm = 0.2696
Meta loss on this task batch = 2.5415e-01, Meta loss averaged over last 500 steps = 2.8446e-01, PNorm = 140.5848, GNorm = 0.1982
Meta loss on this task batch = 2.8073e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 140.5899, GNorm = 0.2324
Meta loss on this task batch = 3.1742e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 140.5943, GNorm = 0.2896
Meta loss on this task batch = 2.4364e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 140.5990, GNorm = 0.2162
Meta loss on this task batch = 2.8556e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 140.6045, GNorm = 0.3137
Took 113.83028745651245 seconds to complete one epoch of meta training
Took 121.97133159637451 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491613
Epoch 528
Meta loss on this task batch = 3.2348e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.6095, GNorm = 0.2631
Meta loss on this task batch = 2.5455e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 140.6141, GNorm = 0.2299
Meta loss on this task batch = 3.1460e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 140.6188, GNorm = 0.2182
Meta loss on this task batch = 2.5795e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 140.6241, GNorm = 0.2260
Meta loss on this task batch = 2.5024e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 140.6296, GNorm = 0.2026
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 140.6340, GNorm = 0.2385
Meta loss on this task batch = 2.3924e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 140.6401, GNorm = 0.2151
Meta loss on this task batch = 2.5287e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 140.6463, GNorm = 0.2188
Meta loss on this task batch = 2.4532e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 140.6530, GNorm = 0.2096
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 140.6597, GNorm = 0.2232
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 140.6662, GNorm = 0.2113
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 140.6741, GNorm = 0.2280
Meta loss on this task batch = 3.2312e-01, Meta loss averaged over last 500 steps = 2.8412e-01, PNorm = 140.6819, GNorm = 0.2236
Meta loss on this task batch = 3.2911e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.6890, GNorm = 0.2475
Meta loss on this task batch = 3.5719e-01, Meta loss averaged over last 500 steps = 2.8443e-01, PNorm = 140.6961, GNorm = 0.2795
Meta loss on this task batch = 2.6112e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 140.7029, GNorm = 0.1829
Meta loss on this task batch = 3.1743e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 140.7094, GNorm = 0.2507
Meta loss on this task batch = 2.1041e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 140.7160, GNorm = 0.2056
Meta loss on this task batch = 3.0200e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 140.7216, GNorm = 0.2688
Took 114.7050507068634 seconds to complete one epoch of meta training
Took 121.81591272354126 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481885
Epoch 529
Meta loss on this task batch = 3.1105e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 140.7269, GNorm = 0.2547
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 140.7322, GNorm = 0.2575
Meta loss on this task batch = 2.4544e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.7379, GNorm = 0.2035
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 140.7438, GNorm = 0.2339
Meta loss on this task batch = 3.1298e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 140.7487, GNorm = 0.2678
Meta loss on this task batch = 2.9170e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 140.7537, GNorm = 0.2276
Meta loss on this task batch = 2.8905e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 140.7588, GNorm = 0.2120
Meta loss on this task batch = 2.2066e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 140.7641, GNorm = 0.2045
Meta loss on this task batch = 3.4942e-01, Meta loss averaged over last 500 steps = 2.8438e-01, PNorm = 140.7687, GNorm = 0.2622
Meta loss on this task batch = 2.9293e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 140.7733, GNorm = 0.2333
Meta loss on this task batch = 2.8207e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 140.7785, GNorm = 0.2133
Meta loss on this task batch = 2.7876e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.7850, GNorm = 0.2303
Meta loss on this task batch = 2.8336e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.7909, GNorm = 0.2112
Meta loss on this task batch = 2.6829e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 140.7968, GNorm = 0.2088
Meta loss on this task batch = 2.7421e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 140.8040, GNorm = 0.2556
Meta loss on this task batch = 3.4645e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 140.8108, GNorm = 0.2443
Meta loss on this task batch = 2.7899e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 140.8173, GNorm = 0.2193
Meta loss on this task batch = 2.6676e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 140.8250, GNorm = 0.2202
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 140.8325, GNorm = 0.2790
Took 117.10382556915283 seconds to complete one epoch of meta training
Took 123.74687933921814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459369
Epoch 530
Meta loss on this task batch = 2.9140e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.8399, GNorm = 0.2886
Meta loss on this task batch = 2.7141e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.8476, GNorm = 0.2107
Meta loss on this task batch = 2.3439e-01, Meta loss averaged over last 500 steps = 2.8409e-01, PNorm = 140.8554, GNorm = 0.1987
Meta loss on this task batch = 3.1680e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 140.8630, GNorm = 0.2321
Meta loss on this task batch = 3.1707e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 140.8694, GNorm = 0.2434
Meta loss on this task batch = 2.6443e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 140.8757, GNorm = 0.2092
Meta loss on this task batch = 3.5068e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 140.8816, GNorm = 0.2791
Meta loss on this task batch = 2.5196e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 140.8883, GNorm = 0.2158
Meta loss on this task batch = 3.0203e-01, Meta loss averaged over last 500 steps = 2.8422e-01, PNorm = 140.8952, GNorm = 0.2406
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 2.8423e-01, PNorm = 140.9012, GNorm = 0.2605
Meta loss on this task batch = 2.5537e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 140.9064, GNorm = 0.2189
Meta loss on this task batch = 2.2260e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 140.9114, GNorm = 0.1989
Meta loss on this task batch = 3.1000e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 140.9159, GNorm = 0.2371
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 140.9211, GNorm = 0.2384
Meta loss on this task batch = 2.3432e-01, Meta loss averaged over last 500 steps = 2.8382e-01, PNorm = 140.9261, GNorm = 0.2570
Meta loss on this task batch = 3.4434e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 140.9316, GNorm = 0.2409
Meta loss on this task batch = 2.9334e-01, Meta loss averaged over last 500 steps = 2.8406e-01, PNorm = 140.9376, GNorm = 0.2294
Meta loss on this task batch = 2.9394e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 140.9439, GNorm = 0.2812
Meta loss on this task batch = 2.5618e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 140.9497, GNorm = 0.3040
Took 116.37545394897461 seconds to complete one epoch of meta training
Took 123.84720802307129 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487904
Epoch 531
Meta loss on this task batch = 2.4698e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 140.9557, GNorm = 0.2006
Meta loss on this task batch = 2.5800e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 140.9619, GNorm = 0.2580
Meta loss on this task batch = 2.5612e-01, Meta loss averaged over last 500 steps = 2.8388e-01, PNorm = 140.9685, GNorm = 0.2057
Meta loss on this task batch = 3.0505e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 140.9752, GNorm = 0.2512
Meta loss on this task batch = 2.3407e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 140.9816, GNorm = 0.2187
Meta loss on this task batch = 2.4958e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 140.9880, GNorm = 0.2343
Meta loss on this task batch = 2.7733e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 140.9955, GNorm = 0.2389
Meta loss on this task batch = 2.7956e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 141.0034, GNorm = 0.2498
Meta loss on this task batch = 2.8989e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 141.0098, GNorm = 0.2385
Meta loss on this task batch = 2.6052e-01, Meta loss averaged over last 500 steps = 2.8396e-01, PNorm = 141.0151, GNorm = 0.2453
Meta loss on this task batch = 2.9915e-01, Meta loss averaged over last 500 steps = 2.8393e-01, PNorm = 141.0201, GNorm = 0.2573
Meta loss on this task batch = 2.9158e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 141.0256, GNorm = 0.2242
Meta loss on this task batch = 2.8821e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 141.0311, GNorm = 0.2218
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 141.0367, GNorm = 0.2293
Meta loss on this task batch = 3.2145e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 141.0420, GNorm = 0.2642
Meta loss on this task batch = 3.0618e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 141.0480, GNorm = 0.2386
Meta loss on this task batch = 3.1283e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 141.0544, GNorm = 0.2368
Meta loss on this task batch = 3.3276e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 141.0600, GNorm = 0.2373
Meta loss on this task batch = 3.5044e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 141.0661, GNorm = 0.2939
Took 112.53360891342163 seconds to complete one epoch of meta training
Took 120.17114448547363 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455466
Epoch 532
Meta loss on this task batch = 2.8381e-01, Meta loss averaged over last 500 steps = 2.8441e-01, PNorm = 141.0722, GNorm = 0.2384
Meta loss on this task batch = 3.0863e-01, Meta loss averaged over last 500 steps = 2.8442e-01, PNorm = 141.0786, GNorm = 0.2068
Meta loss on this task batch = 3.1920e-01, Meta loss averaged over last 500 steps = 2.8452e-01, PNorm = 141.0841, GNorm = 0.2216
Meta loss on this task batch = 2.1856e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 141.0904, GNorm = 0.2172
Meta loss on this task batch = 2.7228e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 141.0967, GNorm = 0.2083
Meta loss on this task batch = 2.8018e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 141.1028, GNorm = 0.2451
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 141.1085, GNorm = 0.2444
Meta loss on this task batch = 2.1753e-01, Meta loss averaged over last 500 steps = 2.8406e-01, PNorm = 141.1141, GNorm = 0.1959
Meta loss on this task batch = 2.5397e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 141.1203, GNorm = 0.2045
Meta loss on this task batch = 2.9686e-01, Meta loss averaged over last 500 steps = 2.8402e-01, PNorm = 141.1265, GNorm = 0.2056
Meta loss on this task batch = 2.9772e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 141.1341, GNorm = 0.2333
Meta loss on this task batch = 2.6018e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 141.1417, GNorm = 0.2131
Meta loss on this task batch = 2.6840e-01, Meta loss averaged over last 500 steps = 2.8397e-01, PNorm = 141.1493, GNorm = 0.2238
Meta loss on this task batch = 3.3107e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 141.1559, GNorm = 0.2415
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 141.1620, GNorm = 0.2369
Meta loss on this task batch = 2.4204e-01, Meta loss averaged over last 500 steps = 2.8413e-01, PNorm = 141.1671, GNorm = 0.2303
Meta loss on this task batch = 3.6264e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 141.1705, GNorm = 0.3378
Meta loss on this task batch = 3.2310e-01, Meta loss averaged over last 500 steps = 2.8447e-01, PNorm = 141.1745, GNorm = 0.2422
Meta loss on this task batch = 2.5439e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 141.1793, GNorm = 0.2885
Took 112.6915876865387 seconds to complete one epoch of meta training
Took 120.14395022392273 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463137
Epoch 533
Meta loss on this task batch = 3.1769e-01, Meta loss averaged over last 500 steps = 2.8433e-01, PNorm = 141.1840, GNorm = 0.2668
Meta loss on this task batch = 3.0986e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 141.1883, GNorm = 0.2581
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 141.1928, GNorm = 0.2007
Meta loss on this task batch = 3.5092e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 141.1966, GNorm = 0.2536
Meta loss on this task batch = 2.8666e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 141.2014, GNorm = 0.2485
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 141.2064, GNorm = 0.1888
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 141.2117, GNorm = 0.2224
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.8435e-01, PNorm = 141.2172, GNorm = 0.1935
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.8430e-01, PNorm = 141.2230, GNorm = 0.2075
Meta loss on this task batch = 2.8794e-01, Meta loss averaged over last 500 steps = 2.8439e-01, PNorm = 141.2296, GNorm = 0.2186
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.8429e-01, PNorm = 141.2369, GNorm = 0.2166
Meta loss on this task batch = 2.6950e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 141.2445, GNorm = 0.2171
Meta loss on this task batch = 2.1231e-01, Meta loss averaged over last 500 steps = 2.8401e-01, PNorm = 141.2532, GNorm = 0.1939
Meta loss on this task batch = 3.2257e-01, Meta loss averaged over last 500 steps = 2.8414e-01, PNorm = 141.2615, GNorm = 0.2490
Meta loss on this task batch = 2.6153e-01, Meta loss averaged over last 500 steps = 2.8411e-01, PNorm = 141.2679, GNorm = 0.2475
Meta loss on this task batch = 2.5037e-01, Meta loss averaged over last 500 steps = 2.8401e-01, PNorm = 141.2739, GNorm = 0.2214
Meta loss on this task batch = 3.2301e-01, Meta loss averaged over last 500 steps = 2.8406e-01, PNorm = 141.2785, GNorm = 0.2466
Meta loss on this task batch = 2.5208e-01, Meta loss averaged over last 500 steps = 2.8398e-01, PNorm = 141.2836, GNorm = 0.2084
Meta loss on this task batch = 2.8117e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 141.2885, GNorm = 0.2685
Took 113.5498399734497 seconds to complete one epoch of meta training
Took 121.49924278259277 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478553
Epoch 534
Meta loss on this task batch = 3.0714e-01, Meta loss averaged over last 500 steps = 2.8406e-01, PNorm = 141.2932, GNorm = 0.2574
Meta loss on this task batch = 3.3191e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 141.2969, GNorm = 0.2588
Meta loss on this task batch = 2.6948e-01, Meta loss averaged over last 500 steps = 2.8412e-01, PNorm = 141.3006, GNorm = 0.2027
Meta loss on this task batch = 2.3389e-01, Meta loss averaged over last 500 steps = 2.8389e-01, PNorm = 141.3049, GNorm = 0.2115
Meta loss on this task batch = 3.1477e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 141.3089, GNorm = 0.2680
Meta loss on this task batch = 3.0918e-01, Meta loss averaged over last 500 steps = 2.8419e-01, PNorm = 141.3123, GNorm = 0.2423
Meta loss on this task batch = 2.8078e-01, Meta loss averaged over last 500 steps = 2.8420e-01, PNorm = 141.3161, GNorm = 0.2329
Meta loss on this task batch = 2.5660e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 141.3198, GNorm = 0.2113
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.8408e-01, PNorm = 141.3235, GNorm = 0.2265
Meta loss on this task batch = 2.3250e-01, Meta loss averaged over last 500 steps = 2.8412e-01, PNorm = 141.3275, GNorm = 0.1843
Meta loss on this task batch = 3.2462e-01, Meta loss averaged over last 500 steps = 2.8418e-01, PNorm = 141.3322, GNorm = 0.2436
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 141.3371, GNorm = 0.2244
Meta loss on this task batch = 3.8680e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 141.3418, GNorm = 0.2895
Meta loss on this task batch = 2.4011e-01, Meta loss averaged over last 500 steps = 2.8432e-01, PNorm = 141.3472, GNorm = 0.1990
Meta loss on this task batch = 3.4300e-01, Meta loss averaged over last 500 steps = 2.8450e-01, PNorm = 141.3521, GNorm = 0.2461
Meta loss on this task batch = 2.7859e-01, Meta loss averaged over last 500 steps = 2.8449e-01, PNorm = 141.3575, GNorm = 0.2362
Meta loss on this task batch = 2.3774e-01, Meta loss averaged over last 500 steps = 2.8428e-01, PNorm = 141.3636, GNorm = 0.1744
Meta loss on this task batch = 2.8374e-01, Meta loss averaged over last 500 steps = 2.8440e-01, PNorm = 141.3700, GNorm = 0.2104
Meta loss on this task batch = 3.2395e-01, Meta loss averaged over last 500 steps = 2.8444e-01, PNorm = 141.3760, GNorm = 0.2648
Took 115.29265093803406 seconds to complete one epoch of meta training
Took 123.38672614097595 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478672
Epoch 535
Meta loss on this task batch = 3.0290e-01, Meta loss averaged over last 500 steps = 2.8445e-01, PNorm = 141.3822, GNorm = 0.2421
Meta loss on this task batch = 2.6479e-01, Meta loss averaged over last 500 steps = 2.8437e-01, PNorm = 141.3882, GNorm = 0.2141
Meta loss on this task batch = 2.8364e-01, Meta loss averaged over last 500 steps = 2.8436e-01, PNorm = 141.3936, GNorm = 0.2177
Meta loss on this task batch = 2.7546e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 141.3982, GNorm = 0.2168
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 2.8425e-01, PNorm = 141.4024, GNorm = 0.2150
Meta loss on this task batch = 2.9157e-01, Meta loss averaged over last 500 steps = 2.8415e-01, PNorm = 141.4062, GNorm = 0.2241
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 141.4106, GNorm = 0.2146
Meta loss on this task batch = 2.5359e-01, Meta loss averaged over last 500 steps = 2.8421e-01, PNorm = 141.4156, GNorm = 0.2012
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.8431e-01, PNorm = 141.4207, GNorm = 0.2250
Meta loss on this task batch = 2.8509e-01, Meta loss averaged over last 500 steps = 2.8424e-01, PNorm = 141.4238, GNorm = 0.3021
Meta loss on this task batch = 2.7440e-01, Meta loss averaged over last 500 steps = 2.8426e-01, PNorm = 141.4272, GNorm = 0.2545
Meta loss on this task batch = 2.4560e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 141.4313, GNorm = 0.2017
Meta loss on this task batch = 3.0644e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 141.4360, GNorm = 0.2625
Meta loss on this task batch = 2.4446e-01, Meta loss averaged over last 500 steps = 2.8416e-01, PNorm = 141.4410, GNorm = 0.2140
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 2.8410e-01, PNorm = 141.4458, GNorm = 0.2465
Meta loss on this task batch = 2.6411e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 141.4519, GNorm = 0.2140
Meta loss on this task batch = 2.7762e-01, Meta loss averaged over last 500 steps = 2.8404e-01, PNorm = 141.4583, GNorm = 0.2536
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 2.8399e-01, PNorm = 141.4638, GNorm = 0.2502
Meta loss on this task batch = 2.5696e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 141.4699, GNorm = 0.2385
Took 113.28839635848999 seconds to complete one epoch of meta training
Took 120.81078290939331 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481058
Epoch 536
Meta loss on this task batch = 2.2434e-01, Meta loss averaged over last 500 steps = 2.8380e-01, PNorm = 141.4750, GNorm = 0.2086
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 141.4800, GNorm = 0.2678
Meta loss on this task batch = 2.8733e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 141.4842, GNorm = 0.2474
Meta loss on this task batch = 2.6357e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 141.4893, GNorm = 0.2311
Meta loss on this task batch = 2.4798e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 141.4945, GNorm = 0.2297
Meta loss on this task batch = 3.5325e-01, Meta loss averaged over last 500 steps = 2.8380e-01, PNorm = 141.4997, GNorm = 0.2537
Meta loss on this task batch = 2.5452e-01, Meta loss averaged over last 500 steps = 2.8372e-01, PNorm = 141.5046, GNorm = 0.2968
Meta loss on this task batch = 3.4104e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 141.5086, GNorm = 0.2672
Meta loss on this task batch = 2.7727e-01, Meta loss averaged over last 500 steps = 2.8387e-01, PNorm = 141.5129, GNorm = 0.2379
Meta loss on this task batch = 2.7442e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 141.5177, GNorm = 0.2243
Meta loss on this task batch = 3.2672e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 141.5228, GNorm = 0.2459
Meta loss on this task batch = 2.9082e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 141.5281, GNorm = 0.2592
Meta loss on this task batch = 3.0940e-01, Meta loss averaged over last 500 steps = 2.8393e-01, PNorm = 141.5334, GNorm = 0.2213
Meta loss on this task batch = 3.0573e-01, Meta loss averaged over last 500 steps = 2.8395e-01, PNorm = 141.5385, GNorm = 0.2592
Meta loss on this task batch = 2.6477e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 141.5435, GNorm = 0.1911
Meta loss on this task batch = 3.0227e-01, Meta loss averaged over last 500 steps = 2.8393e-01, PNorm = 141.5493, GNorm = 0.2206
Meta loss on this task batch = 3.2508e-01, Meta loss averaged over last 500 steps = 2.8405e-01, PNorm = 141.5551, GNorm = 0.2092
Meta loss on this task batch = 3.1070e-01, Meta loss averaged over last 500 steps = 2.8407e-01, PNorm = 141.5597, GNorm = 0.2483
Meta loss on this task batch = 2.2932e-01, Meta loss averaged over last 500 steps = 2.8403e-01, PNorm = 141.5646, GNorm = 0.2094
Took 116.53719973564148 seconds to complete one epoch of meta training
Took 123.55477476119995 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472986
Epoch 537
Meta loss on this task batch = 2.5311e-01, Meta loss averaged over last 500 steps = 2.8383e-01, PNorm = 141.5700, GNorm = 0.1778
Meta loss on this task batch = 2.7954e-01, Meta loss averaged over last 500 steps = 2.8386e-01, PNorm = 141.5744, GNorm = 0.2640
Meta loss on this task batch = 2.9769e-01, Meta loss averaged over last 500 steps = 2.8394e-01, PNorm = 141.5777, GNorm = 0.2404
Meta loss on this task batch = 2.5461e-01, Meta loss averaged over last 500 steps = 2.8392e-01, PNorm = 141.5822, GNorm = 0.2309
Meta loss on this task batch = 2.4962e-01, Meta loss averaged over last 500 steps = 2.8377e-01, PNorm = 141.5876, GNorm = 0.2078
Meta loss on this task batch = 3.2457e-01, Meta loss averaged over last 500 steps = 2.8380e-01, PNorm = 141.5935, GNorm = 0.2746
Meta loss on this task batch = 3.0691e-01, Meta loss averaged over last 500 steps = 2.8400e-01, PNorm = 141.6002, GNorm = 0.2372
Meta loss on this task batch = 2.2766e-01, Meta loss averaged over last 500 steps = 2.8390e-01, PNorm = 141.6068, GNorm = 0.2240
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 2.8378e-01, PNorm = 141.6140, GNorm = 0.2477
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.8374e-01, PNorm = 141.6205, GNorm = 0.2347
Meta loss on this task batch = 3.3609e-01, Meta loss averaged over last 500 steps = 2.8384e-01, PNorm = 141.6261, GNorm = 0.2644
Meta loss on this task batch = 3.0307e-01, Meta loss averaged over last 500 steps = 2.8381e-01, PNorm = 141.6316, GNorm = 0.2267
Meta loss on this task batch = 2.5741e-01, Meta loss averaged over last 500 steps = 2.8385e-01, PNorm = 141.6383, GNorm = 0.2486
Meta loss on this task batch = 2.6548e-01, Meta loss averaged over last 500 steps = 2.8385e-01, PNorm = 141.6455, GNorm = 0.2333
Meta loss on this task batch = 2.5728e-01, Meta loss averaged over last 500 steps = 2.8371e-01, PNorm = 141.6526, GNorm = 0.2322
Meta loss on this task batch = 2.6699e-01, Meta loss averaged over last 500 steps = 2.8368e-01, PNorm = 141.6592, GNorm = 0.2484
Meta loss on this task batch = 2.8352e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 141.6661, GNorm = 0.2379
Meta loss on this task batch = 2.4632e-01, Meta loss averaged over last 500 steps = 2.8348e-01, PNorm = 141.6732, GNorm = 0.2001
Meta loss on this task batch = 2.8105e-01, Meta loss averaged over last 500 steps = 2.8340e-01, PNorm = 141.6805, GNorm = 0.2626
Took 114.1454746723175 seconds to complete one epoch of meta training
Took 121.80313897132874 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459655
Epoch 538
Meta loss on this task batch = 3.4040e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 141.6872, GNorm = 0.2794
Meta loss on this task batch = 2.3077e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.6947, GNorm = 0.1983
Meta loss on this task batch = 2.5441e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.7018, GNorm = 0.2427
Meta loss on this task batch = 3.2580e-01, Meta loss averaged over last 500 steps = 2.8363e-01, PNorm = 141.7087, GNorm = 0.2461
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 141.7152, GNorm = 0.2066
Meta loss on this task batch = 3.1920e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.7215, GNorm = 0.2917
Meta loss on this task batch = 3.2751e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 141.7276, GNorm = 0.2615
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 141.7324, GNorm = 0.2353
Meta loss on this task batch = 2.8740e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 141.7380, GNorm = 0.2129
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.7441, GNorm = 0.2271
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.7501, GNorm = 0.2279
Meta loss on this task batch = 2.6857e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 141.7551, GNorm = 0.2459
Meta loss on this task batch = 2.9806e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.7603, GNorm = 0.2346
Meta loss on this task batch = 2.8509e-01, Meta loss averaged over last 500 steps = 2.8373e-01, PNorm = 141.7655, GNorm = 0.2211
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 141.7698, GNorm = 0.2550
Meta loss on this task batch = 2.7058e-01, Meta loss averaged over last 500 steps = 2.8365e-01, PNorm = 141.7740, GNorm = 0.2161
Meta loss on this task batch = 2.4579e-01, Meta loss averaged over last 500 steps = 2.8364e-01, PNorm = 141.7792, GNorm = 0.2049
Meta loss on this task batch = 2.4574e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 141.7843, GNorm = 0.1889
Meta loss on this task batch = 3.2083e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 141.7891, GNorm = 0.2668
Took 114.30341911315918 seconds to complete one epoch of meta training
Took 122.49492239952087 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482014
Epoch 539
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 141.7945, GNorm = 0.2478
Meta loss on this task batch = 2.5169e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 141.8003, GNorm = 0.2070
Meta loss on this task batch = 2.1640e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 141.8061, GNorm = 0.1898
Meta loss on this task batch = 2.3213e-01, Meta loss averaged over last 500 steps = 2.8333e-01, PNorm = 141.8121, GNorm = 0.2156
Meta loss on this task batch = 2.5304e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 141.8185, GNorm = 0.2358
Meta loss on this task batch = 2.4916e-01, Meta loss averaged over last 500 steps = 2.8318e-01, PNorm = 141.8249, GNorm = 0.1987
Meta loss on this task batch = 2.8535e-01, Meta loss averaged over last 500 steps = 2.8327e-01, PNorm = 141.8317, GNorm = 0.2688
Meta loss on this task batch = 3.2127e-01, Meta loss averaged over last 500 steps = 2.8333e-01, PNorm = 141.8381, GNorm = 0.2359
Meta loss on this task batch = 2.7541e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 141.8448, GNorm = 0.2370
Meta loss on this task batch = 3.1103e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 141.8507, GNorm = 0.2248
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.8360e-01, PNorm = 141.8562, GNorm = 0.2536
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 141.8617, GNorm = 0.2538
Meta loss on this task batch = 3.2332e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 141.8670, GNorm = 0.2379
Meta loss on this task batch = 2.6846e-01, Meta loss averaged over last 500 steps = 2.8336e-01, PNorm = 141.8714, GNorm = 0.2250
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.8337e-01, PNorm = 141.8758, GNorm = 0.2174
Meta loss on this task batch = 2.5349e-01, Meta loss averaged over last 500 steps = 2.8345e-01, PNorm = 141.8803, GNorm = 0.2059
Meta loss on this task batch = 3.1633e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.8854, GNorm = 0.2134
Meta loss on this task batch = 3.0476e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 141.8912, GNorm = 0.2351
Meta loss on this task batch = 2.6577e-01, Meta loss averaged over last 500 steps = 2.8341e-01, PNorm = 141.8973, GNorm = 0.2147
Took 115.04515051841736 seconds to complete one epoch of meta training
Took 122.46844434738159 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501014
Epoch 540
Meta loss on this task batch = 3.0212e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 141.9037, GNorm = 0.2569
Meta loss on this task batch = 2.9035e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 141.9102, GNorm = 0.2208
Meta loss on this task batch = 3.1846e-01, Meta loss averaged over last 500 steps = 2.8345e-01, PNorm = 141.9169, GNorm = 0.2044
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 2.8344e-01, PNorm = 141.9239, GNorm = 0.2238
Meta loss on this task batch = 3.2044e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 141.9308, GNorm = 0.2262
Meta loss on this task batch = 2.7959e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 141.9369, GNorm = 0.2257
Meta loss on this task batch = 3.7274e-01, Meta loss averaged over last 500 steps = 2.8367e-01, PNorm = 141.9421, GNorm = 0.3173
Meta loss on this task batch = 2.5690e-01, Meta loss averaged over last 500 steps = 2.8366e-01, PNorm = 141.9476, GNorm = 0.2174
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 141.9533, GNorm = 0.2231
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 141.9595, GNorm = 0.2243
Meta loss on this task batch = 3.2537e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 141.9652, GNorm = 0.2493
Meta loss on this task batch = 2.9298e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 141.9719, GNorm = 0.2121
Meta loss on this task batch = 2.5925e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 141.9784, GNorm = 0.2681
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 2.8359e-01, PNorm = 141.9842, GNorm = 0.2344
Meta loss on this task batch = 2.7651e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 141.9897, GNorm = 0.2332
Meta loss on this task batch = 3.0809e-01, Meta loss averaged over last 500 steps = 2.8354e-01, PNorm = 141.9954, GNorm = 0.2502
Meta loss on this task batch = 2.5888e-01, Meta loss averaged over last 500 steps = 2.8355e-01, PNorm = 142.0007, GNorm = 0.2227
Meta loss on this task batch = 2.7871e-01, Meta loss averaged over last 500 steps = 2.8349e-01, PNorm = 142.0063, GNorm = 0.2230
Meta loss on this task batch = 2.6171e-01, Meta loss averaged over last 500 steps = 2.8350e-01, PNorm = 142.0117, GNorm = 0.2819
Took 115.2114462852478 seconds to complete one epoch of meta training
Took 123.4442982673645 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505262
Epoch 541
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 2.8348e-01, PNorm = 142.0169, GNorm = 0.2493
Meta loss on this task batch = 3.1607e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 142.0217, GNorm = 0.2331
Meta loss on this task batch = 2.8128e-01, Meta loss averaged over last 500 steps = 2.8343e-01, PNorm = 142.0273, GNorm = 0.2234
Meta loss on this task batch = 3.1267e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 142.0333, GNorm = 0.2160
Meta loss on this task batch = 2.7278e-01, Meta loss averaged over last 500 steps = 2.8358e-01, PNorm = 142.0396, GNorm = 0.2045
Meta loss on this task batch = 2.2928e-01, Meta loss averaged over last 500 steps = 2.8348e-01, PNorm = 142.0462, GNorm = 0.1803
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 2.8346e-01, PNorm = 142.0528, GNorm = 0.2292
Meta loss on this task batch = 2.6731e-01, Meta loss averaged over last 500 steps = 2.8343e-01, PNorm = 142.0596, GNorm = 0.2081
Meta loss on this task batch = 2.6307e-01, Meta loss averaged over last 500 steps = 2.8347e-01, PNorm = 142.0656, GNorm = 0.2602
Meta loss on this task batch = 2.5888e-01, Meta loss averaged over last 500 steps = 2.8341e-01, PNorm = 142.0720, GNorm = 0.2070
Meta loss on this task batch = 3.1225e-01, Meta loss averaged over last 500 steps = 2.8361e-01, PNorm = 142.0788, GNorm = 0.2880
Meta loss on this task batch = 3.6349e-01, Meta loss averaged over last 500 steps = 2.8369e-01, PNorm = 142.0840, GNorm = 0.3035
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.8375e-01, PNorm = 142.0887, GNorm = 0.1993
Meta loss on this task batch = 2.3704e-01, Meta loss averaged over last 500 steps = 2.8357e-01, PNorm = 142.0940, GNorm = 0.2095
Meta loss on this task batch = 2.6229e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 142.1005, GNorm = 0.2460
Meta loss on this task batch = 2.4525e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 142.1074, GNorm = 0.2293
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 2.8342e-01, PNorm = 142.1138, GNorm = 0.2329
Meta loss on this task batch = 3.0611e-01, Meta loss averaged over last 500 steps = 2.8339e-01, PNorm = 142.1202, GNorm = 0.2548
Meta loss on this task batch = 2.4024e-01, Meta loss averaged over last 500 steps = 2.8332e-01, PNorm = 142.1270, GNorm = 0.2689
Took 114.12700891494751 seconds to complete one epoch of meta training
Took 121.82285714149475 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499986
Epoch 542
Meta loss on this task batch = 2.9516e-01, Meta loss averaged over last 500 steps = 2.8337e-01, PNorm = 142.1343, GNorm = 0.2369
Meta loss on this task batch = 2.4872e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 142.1418, GNorm = 0.2420
Meta loss on this task batch = 2.8477e-01, Meta loss averaged over last 500 steps = 2.8324e-01, PNorm = 142.1487, GNorm = 0.2653
Meta loss on this task batch = 2.6586e-01, Meta loss averaged over last 500 steps = 2.8315e-01, PNorm = 142.1544, GNorm = 0.2320
Meta loss on this task batch = 2.4455e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 142.1609, GNorm = 0.2297
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.8309e-01, PNorm = 142.1662, GNorm = 0.2389
Meta loss on this task batch = 2.6148e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 142.1701, GNorm = 0.2663
Meta loss on this task batch = 2.2366e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 142.1743, GNorm = 0.1910
Meta loss on this task batch = 2.6493e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 142.1778, GNorm = 0.2414
Meta loss on this task batch = 3.3387e-01, Meta loss averaged over last 500 steps = 2.8318e-01, PNorm = 142.1806, GNorm = 0.2439
Meta loss on this task batch = 3.1959e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 142.1836, GNorm = 0.2382
Meta loss on this task batch = 3.1178e-01, Meta loss averaged over last 500 steps = 2.8332e-01, PNorm = 142.1865, GNorm = 0.2264
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.8325e-01, PNorm = 142.1896, GNorm = 0.2207
Meta loss on this task batch = 3.5283e-01, Meta loss averaged over last 500 steps = 2.8341e-01, PNorm = 142.1936, GNorm = 0.2563
Meta loss on this task batch = 3.3905e-01, Meta loss averaged over last 500 steps = 2.8352e-01, PNorm = 142.1988, GNorm = 0.2977
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.8353e-01, PNorm = 142.2045, GNorm = 0.2411
Meta loss on this task batch = 2.7112e-01, Meta loss averaged over last 500 steps = 2.8356e-01, PNorm = 142.2117, GNorm = 0.2322
Meta loss on this task batch = 2.5657e-01, Meta loss averaged over last 500 steps = 2.8335e-01, PNorm = 142.2190, GNorm = 0.2308
Meta loss on this task batch = 2.5666e-01, Meta loss averaged over last 500 steps = 2.8328e-01, PNorm = 142.2269, GNorm = 0.2413
Took 111.5833957195282 seconds to complete one epoch of meta training
Took 119.71123218536377 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494266
Epoch 543
Meta loss on this task batch = 2.5974e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 142.2351, GNorm = 0.1991
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.8316e-01, PNorm = 142.2435, GNorm = 0.2108
Meta loss on this task batch = 2.7299e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 142.2515, GNorm = 0.2173
Meta loss on this task batch = 2.1500e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 142.2592, GNorm = 0.2194
Meta loss on this task batch = 2.5435e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 142.2665, GNorm = 0.2216
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 142.2734, GNorm = 0.2621
Meta loss on this task batch = 3.1446e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 142.2791, GNorm = 0.2595
Meta loss on this task batch = 2.5100e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 142.2863, GNorm = 0.2303
Meta loss on this task batch = 2.7520e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 142.2925, GNorm = 0.2301
Meta loss on this task batch = 2.6772e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 142.2981, GNorm = 0.1997
Meta loss on this task batch = 2.8419e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 142.3043, GNorm = 0.2071
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 142.3109, GNorm = 0.2553
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 2.8286e-01, PNorm = 142.3174, GNorm = 0.2290
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 142.3242, GNorm = 0.2535
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 2.8297e-01, PNorm = 142.3299, GNorm = 0.2429
Meta loss on this task batch = 2.9913e-01, Meta loss averaged over last 500 steps = 2.8306e-01, PNorm = 142.3343, GNorm = 0.2590
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 142.3394, GNorm = 0.2147
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 142.3443, GNorm = 0.2197
Meta loss on this task batch = 3.3292e-01, Meta loss averaged over last 500 steps = 2.8311e-01, PNorm = 142.3489, GNorm = 0.2871
Took 114.86717224121094 seconds to complete one epoch of meta training
Took 122.514821767807 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483084
Epoch 544
Meta loss on this task batch = 2.4237e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 142.3545, GNorm = 0.2052
Meta loss on this task batch = 2.6168e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 142.3613, GNorm = 0.4436
Meta loss on this task batch = 2.4126e-01, Meta loss averaged over last 500 steps = 2.8261e-01, PNorm = 142.3683, GNorm = 0.1911
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.8262e-01, PNorm = 142.3756, GNorm = 0.2273
Meta loss on this task batch = 3.0243e-01, Meta loss averaged over last 500 steps = 2.8265e-01, PNorm = 142.3824, GNorm = 0.2243
Meta loss on this task batch = 2.5206e-01, Meta loss averaged over last 500 steps = 2.8266e-01, PNorm = 142.3901, GNorm = 0.2098
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 2.8275e-01, PNorm = 142.3963, GNorm = 0.2438
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 142.4031, GNorm = 0.2539
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 142.4098, GNorm = 0.2042
Meta loss on this task batch = 2.3544e-01, Meta loss averaged over last 500 steps = 2.8285e-01, PNorm = 142.4166, GNorm = 0.2914
Meta loss on this task batch = 3.2675e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 142.4230, GNorm = 0.2281
Meta loss on this task batch = 2.7311e-01, Meta loss averaged over last 500 steps = 2.8303e-01, PNorm = 142.4293, GNorm = 0.2126
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 142.4349, GNorm = 0.2259
Meta loss on this task batch = 3.6270e-01, Meta loss averaged over last 500 steps = 2.8322e-01, PNorm = 142.4385, GNorm = 0.2719
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.8319e-01, PNorm = 142.4423, GNorm = 0.2041
Meta loss on this task batch = 3.0524e-01, Meta loss averaged over last 500 steps = 2.8323e-01, PNorm = 142.4473, GNorm = 0.2412
Meta loss on this task batch = 2.3801e-01, Meta loss averaged over last 500 steps = 2.8313e-01, PNorm = 142.4531, GNorm = 0.2026
Meta loss on this task batch = 2.9263e-01, Meta loss averaged over last 500 steps = 2.8306e-01, PNorm = 142.4596, GNorm = 0.2592
Meta loss on this task batch = 3.2639e-01, Meta loss averaged over last 500 steps = 2.8302e-01, PNorm = 142.4660, GNorm = 0.3073
Took 115.22351813316345 seconds to complete one epoch of meta training
Took 123.30672216415405 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480798
Epoch 545
Meta loss on this task batch = 2.7586e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 142.4733, GNorm = 0.2586
Meta loss on this task batch = 2.8363e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 142.4805, GNorm = 0.2235
Meta loss on this task batch = 2.5151e-01, Meta loss averaged over last 500 steps = 2.8288e-01, PNorm = 142.4881, GNorm = 0.2350
Meta loss on this task batch = 2.8717e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 142.4952, GNorm = 0.2492
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.8281e-01, PNorm = 142.5021, GNorm = 0.2168
Meta loss on this task batch = 2.8291e-01, Meta loss averaged over last 500 steps = 2.8282e-01, PNorm = 142.5091, GNorm = 0.2178
Meta loss on this task batch = 3.2222e-01, Meta loss averaged over last 500 steps = 2.8304e-01, PNorm = 142.5157, GNorm = 0.2202
Meta loss on this task batch = 2.7718e-01, Meta loss averaged over last 500 steps = 2.8301e-01, PNorm = 142.5218, GNorm = 0.2484
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 142.5276, GNorm = 0.2547
Meta loss on this task batch = 2.5826e-01, Meta loss averaged over last 500 steps = 2.8299e-01, PNorm = 142.5332, GNorm = 0.2417
Meta loss on this task batch = 2.1223e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 142.5395, GNorm = 0.1962
Meta loss on this task batch = 2.4595e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 142.5461, GNorm = 0.1939
Meta loss on this task batch = 2.6242e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 142.5530, GNorm = 0.2377
Meta loss on this task batch = 3.0920e-01, Meta loss averaged over last 500 steps = 2.8294e-01, PNorm = 142.5596, GNorm = 0.2418
Meta loss on this task batch = 2.2723e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 142.5665, GNorm = 0.2121
Meta loss on this task batch = 2.7638e-01, Meta loss averaged over last 500 steps = 2.8278e-01, PNorm = 142.5719, GNorm = 0.2301
Meta loss on this task batch = 2.8548e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 142.5772, GNorm = 0.2051
Meta loss on this task batch = 2.9337e-01, Meta loss averaged over last 500 steps = 2.8287e-01, PNorm = 142.5818, GNorm = 0.2298
Meta loss on this task batch = 3.2208e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 142.5867, GNorm = 0.3058
Took 130.6048548221588 seconds to complete one epoch of meta training
Took 138.66770124435425 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481494
Epoch 546
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 142.5915, GNorm = 0.2055
Meta loss on this task batch = 3.0491e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 142.5963, GNorm = 0.2220
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.8301e-01, PNorm = 142.6020, GNorm = 0.1863
Meta loss on this task batch = 3.0058e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 142.6081, GNorm = 0.2020
Meta loss on this task batch = 2.6910e-01, Meta loss averaged over last 500 steps = 2.8308e-01, PNorm = 142.6141, GNorm = 0.2198
Meta loss on this task batch = 2.4364e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 142.6207, GNorm = 0.1988
Meta loss on this task batch = 2.9411e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 142.6268, GNorm = 0.2097
Meta loss on this task batch = 2.2482e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 142.6329, GNorm = 0.1904
Meta loss on this task batch = 2.6356e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 142.6402, GNorm = 0.2201
Meta loss on this task batch = 2.6409e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 142.6470, GNorm = 0.2412
Meta loss on this task batch = 3.3161e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 142.6538, GNorm = 0.2584
Meta loss on this task batch = 2.7222e-01, Meta loss averaged over last 500 steps = 2.8300e-01, PNorm = 142.6601, GNorm = 0.2260
Meta loss on this task batch = 2.7613e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 142.6654, GNorm = 0.2283
Meta loss on this task batch = 2.7933e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 142.6713, GNorm = 0.2484
Meta loss on this task batch = 2.8946e-01, Meta loss averaged over last 500 steps = 2.8289e-01, PNorm = 142.6773, GNorm = 0.2382
Meta loss on this task batch = 2.6470e-01, Meta loss averaged over last 500 steps = 2.8295e-01, PNorm = 142.6838, GNorm = 0.2287
Meta loss on this task batch = 2.9335e-01, Meta loss averaged over last 500 steps = 2.8296e-01, PNorm = 142.6901, GNorm = 0.2231
Meta loss on this task batch = 2.9944e-01, Meta loss averaged over last 500 steps = 2.8309e-01, PNorm = 142.6970, GNorm = 0.2342
Meta loss on this task batch = 3.0123e-01, Meta loss averaged over last 500 steps = 2.8307e-01, PNorm = 142.7036, GNorm = 0.2603
Took 113.12446403503418 seconds to complete one epoch of meta training
Took 120.75736427307129 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495390
Epoch 547
Meta loss on this task batch = 2.1685e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 142.7104, GNorm = 0.1866
Meta loss on this task batch = 2.8309e-01, Meta loss averaged over last 500 steps = 2.8272e-01, PNorm = 142.7168, GNorm = 0.2456
Meta loss on this task batch = 2.7120e-01, Meta loss averaged over last 500 steps = 2.8266e-01, PNorm = 142.7230, GNorm = 0.2318
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 2.8270e-01, PNorm = 142.7284, GNorm = 0.2523
Meta loss on this task batch = 2.6935e-01, Meta loss averaged over last 500 steps = 2.8264e-01, PNorm = 142.7335, GNorm = 0.2303
Meta loss on this task batch = 3.1840e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 142.7382, GNorm = 0.2643
Meta loss on this task batch = 2.6762e-01, Meta loss averaged over last 500 steps = 2.8279e-01, PNorm = 142.7422, GNorm = 0.2574
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.8290e-01, PNorm = 142.7473, GNorm = 0.2486
Meta loss on this task batch = 2.9645e-01, Meta loss averaged over last 500 steps = 2.8292e-01, PNorm = 142.7533, GNorm = 0.2469
Meta loss on this task batch = 3.1180e-01, Meta loss averaged over last 500 steps = 2.8293e-01, PNorm = 142.7589, GNorm = 0.2360
Meta loss on this task batch = 2.9749e-01, Meta loss averaged over last 500 steps = 2.8298e-01, PNorm = 142.7646, GNorm = 0.2350
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.8294e-01, PNorm = 142.7699, GNorm = 0.2451
Meta loss on this task batch = 2.2233e-01, Meta loss averaged over last 500 steps = 2.8291e-01, PNorm = 142.7758, GNorm = 0.1912
Meta loss on this task batch = 2.7807e-01, Meta loss averaged over last 500 steps = 2.8283e-01, PNorm = 142.7820, GNorm = 0.2318
Meta loss on this task batch = 2.5755e-01, Meta loss averaged over last 500 steps = 2.8269e-01, PNorm = 142.7880, GNorm = 0.2174
Meta loss on this task batch = 2.2297e-01, Meta loss averaged over last 500 steps = 2.8255e-01, PNorm = 142.7947, GNorm = 0.2014
Meta loss on this task batch = 2.7793e-01, Meta loss averaged over last 500 steps = 2.8242e-01, PNorm = 142.8013, GNorm = 0.2171
Meta loss on this task batch = 3.4357e-01, Meta loss averaged over last 500 steps = 2.8244e-01, PNorm = 142.8074, GNorm = 0.2504
Meta loss on this task batch = 2.5696e-01, Meta loss averaged over last 500 steps = 2.8240e-01, PNorm = 142.8138, GNorm = 0.2186
Took 128.76032257080078 seconds to complete one epoch of meta training
Took 136.52192974090576 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497872
Epoch 548
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 2.8236e-01, PNorm = 142.8208, GNorm = 0.2299
Meta loss on this task batch = 2.5719e-01, Meta loss averaged over last 500 steps = 2.8238e-01, PNorm = 142.8284, GNorm = 0.2055
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 2.8240e-01, PNorm = 142.8359, GNorm = 0.2322
Meta loss on this task batch = 3.1738e-01, Meta loss averaged over last 500 steps = 2.8239e-01, PNorm = 142.8435, GNorm = 0.2358
Meta loss on this task batch = 3.2862e-01, Meta loss averaged over last 500 steps = 2.8246e-01, PNorm = 142.8504, GNorm = 0.2284
Meta loss on this task batch = 2.3568e-01, Meta loss averaged over last 500 steps = 2.8239e-01, PNorm = 142.8571, GNorm = 0.2383
Meta loss on this task batch = 2.2666e-01, Meta loss averaged over last 500 steps = 2.8234e-01, PNorm = 142.8642, GNorm = 0.1935
Meta loss on this task batch = 2.5582e-01, Meta loss averaged over last 500 steps = 2.8224e-01, PNorm = 142.8706, GNorm = 0.2379
Meta loss on this task batch = 2.2281e-01, Meta loss averaged over last 500 steps = 2.8214e-01, PNorm = 142.8775, GNorm = 0.2640
Meta loss on this task batch = 3.0279e-01, Meta loss averaged over last 500 steps = 2.8218e-01, PNorm = 142.8824, GNorm = 0.2441
Meta loss on this task batch = 3.1953e-01, Meta loss averaged over last 500 steps = 2.8240e-01, PNorm = 142.8873, GNorm = 0.2569
Meta loss on this task batch = 3.1913e-01, Meta loss averaged over last 500 steps = 2.8248e-01, PNorm = 142.8929, GNorm = 0.2510
Meta loss on this task batch = 3.7122e-01, Meta loss averaged over last 500 steps = 2.8264e-01, PNorm = 142.8978, GNorm = 0.2896
Meta loss on this task batch = 2.9573e-01, Meta loss averaged over last 500 steps = 2.8267e-01, PNorm = 142.9030, GNorm = 0.2446
Meta loss on this task batch = 2.5245e-01, Meta loss averaged over last 500 steps = 2.8252e-01, PNorm = 142.9084, GNorm = 0.2198
Meta loss on this task batch = 2.7494e-01, Meta loss averaged over last 500 steps = 2.8245e-01, PNorm = 142.9138, GNorm = 0.2401
Meta loss on this task batch = 2.9285e-01, Meta loss averaged over last 500 steps = 2.8253e-01, PNorm = 142.9194, GNorm = 0.2170
Meta loss on this task batch = 2.8789e-01, Meta loss averaged over last 500 steps = 2.8255e-01, PNorm = 142.9247, GNorm = 0.2562
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.8256e-01, PNorm = 142.9293, GNorm = 0.2771
Took 116.08041739463806 seconds to complete one epoch of meta training
Took 123.93388247489929 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486697
Epoch 549
Meta loss on this task batch = 3.1056e-01, Meta loss averaged over last 500 steps = 2.8253e-01, PNorm = 142.9332, GNorm = 0.2296
Meta loss on this task batch = 2.8486e-01, Meta loss averaged over last 500 steps = 2.8247e-01, PNorm = 142.9373, GNorm = 0.2305
Meta loss on this task batch = 3.3339e-01, Meta loss averaged over last 500 steps = 2.8267e-01, PNorm = 142.9420, GNorm = 0.2222
Meta loss on this task batch = 2.8824e-01, Meta loss averaged over last 500 steps = 2.8274e-01, PNorm = 142.9478, GNorm = 0.2294
Meta loss on this task batch = 2.7246e-01, Meta loss averaged over last 500 steps = 2.8258e-01, PNorm = 142.9536, GNorm = 0.2044
Meta loss on this task batch = 2.7851e-01, Meta loss averaged over last 500 steps = 2.8251e-01, PNorm = 142.9606, GNorm = 0.2155
Meta loss on this task batch = 2.7568e-01, Meta loss averaged over last 500 steps = 2.8240e-01, PNorm = 142.9678, GNorm = 0.2437
Meta loss on this task batch = 2.5160e-01, Meta loss averaged over last 500 steps = 2.8225e-01, PNorm = 142.9756, GNorm = 0.2317
Meta loss on this task batch = 2.9120e-01, Meta loss averaged over last 500 steps = 2.8234e-01, PNorm = 142.9834, GNorm = 0.2200
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.8229e-01, PNorm = 142.9911, GNorm = 0.2900
Meta loss on this task batch = 3.1393e-01, Meta loss averaged over last 500 steps = 2.8252e-01, PNorm = 142.9988, GNorm = 0.2274
Meta loss on this task batch = 2.9056e-01, Meta loss averaged over last 500 steps = 2.8250e-01, PNorm = 143.0064, GNorm = 0.2611
Meta loss on this task batch = 2.6993e-01, Meta loss averaged over last 500 steps = 2.8256e-01, PNorm = 143.0132, GNorm = 0.2542
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.8251e-01, PNorm = 143.0195, GNorm = 0.2256
Meta loss on this task batch = 2.9643e-01, Meta loss averaged over last 500 steps = 2.8245e-01, PNorm = 143.0244, GNorm = 0.3019
Meta loss on this task batch = 2.6692e-01, Meta loss averaged over last 500 steps = 2.8224e-01, PNorm = 143.0286, GNorm = 0.2508
Meta loss on this task batch = 2.5510e-01, Meta loss averaged over last 500 steps = 2.8224e-01, PNorm = 143.0323, GNorm = 0.2282
Meta loss on this task batch = 2.4170e-01, Meta loss averaged over last 500 steps = 2.8219e-01, PNorm = 143.0355, GNorm = 0.2358
Meta loss on this task batch = 2.7536e-01, Meta loss averaged over last 500 steps = 2.8223e-01, PNorm = 143.0398, GNorm = 0.2552
Took 114.09094738960266 seconds to complete one epoch of meta training
Took 121.8992235660553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470330
Epoch 550
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 2.8223e-01, PNorm = 143.0447, GNorm = 0.2128
Meta loss on this task batch = 2.6235e-01, Meta loss averaged over last 500 steps = 2.8224e-01, PNorm = 143.0502, GNorm = 0.2293
Meta loss on this task batch = 3.1712e-01, Meta loss averaged over last 500 steps = 2.8230e-01, PNorm = 143.0563, GNorm = 0.2357
Meta loss on this task batch = 3.1794e-01, Meta loss averaged over last 500 steps = 2.8235e-01, PNorm = 143.0617, GNorm = 0.2303
Meta loss on this task batch = 2.8733e-01, Meta loss averaged over last 500 steps = 2.8234e-01, PNorm = 143.0671, GNorm = 0.2585
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 2.8240e-01, PNorm = 143.0721, GNorm = 0.2413
Meta loss on this task batch = 3.8686e-01, Meta loss averaged over last 500 steps = 2.8257e-01, PNorm = 143.0765, GNorm = 0.2720
Meta loss on this task batch = 2.1054e-01, Meta loss averaged over last 500 steps = 2.8245e-01, PNorm = 143.0816, GNorm = 0.2154
Meta loss on this task batch = 2.4622e-01, Meta loss averaged over last 500 steps = 2.8238e-01, PNorm = 143.0883, GNorm = 0.2203
Meta loss on this task batch = 2.6415e-01, Meta loss averaged over last 500 steps = 2.8233e-01, PNorm = 143.0954, GNorm = 0.2561
Meta loss on this task batch = 2.9193e-01, Meta loss averaged over last 500 steps = 2.8233e-01, PNorm = 143.1026, GNorm = 0.2306
Meta loss on this task batch = 3.0528e-01, Meta loss averaged over last 500 steps = 2.8235e-01, PNorm = 143.1100, GNorm = 0.2305
Meta loss on this task batch = 2.7939e-01, Meta loss averaged over last 500 steps = 2.8240e-01, PNorm = 143.1160, GNorm = 0.2286
Meta loss on this task batch = 3.2114e-01, Meta loss averaged over last 500 steps = 2.8245e-01, PNorm = 143.1212, GNorm = 0.2895
Meta loss on this task batch = 2.3040e-01, Meta loss averaged over last 500 steps = 2.8226e-01, PNorm = 143.1271, GNorm = 0.2174
Meta loss on this task batch = 2.4279e-01, Meta loss averaged over last 500 steps = 2.8228e-01, PNorm = 143.1329, GNorm = 0.2102
Meta loss on this task batch = 3.3221e-01, Meta loss averaged over last 500 steps = 2.8235e-01, PNorm = 143.1384, GNorm = 0.2561
Meta loss on this task batch = 2.2150e-01, Meta loss averaged over last 500 steps = 2.8234e-01, PNorm = 143.1439, GNorm = 0.2058
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 2.8230e-01, PNorm = 143.1490, GNorm = 0.2580
Took 116.36895847320557 seconds to complete one epoch of meta training
Took 124.20424771308899 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490243
Epoch 551
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.8220e-01, PNorm = 143.1543, GNorm = 0.1946
Meta loss on this task batch = 2.8278e-01, Meta loss averaged over last 500 steps = 2.8215e-01, PNorm = 143.1596, GNorm = 0.2065
Meta loss on this task batch = 2.5434e-01, Meta loss averaged over last 500 steps = 2.8211e-01, PNorm = 143.1650, GNorm = 0.2064
Meta loss on this task batch = 3.0475e-01, Meta loss averaged over last 500 steps = 2.8213e-01, PNorm = 143.1702, GNorm = 0.2597
Meta loss on this task batch = 2.5086e-01, Meta loss averaged over last 500 steps = 2.8217e-01, PNorm = 143.1756, GNorm = 0.2010
Meta loss on this task batch = 3.1144e-01, Meta loss averaged over last 500 steps = 2.8228e-01, PNorm = 143.1819, GNorm = 0.2249
Meta loss on this task batch = 3.0943e-01, Meta loss averaged over last 500 steps = 2.8243e-01, PNorm = 143.1875, GNorm = 0.2359
Meta loss on this task batch = 2.7765e-01, Meta loss averaged over last 500 steps = 2.8241e-01, PNorm = 143.1933, GNorm = 0.2349
Meta loss on this task batch = 2.8018e-01, Meta loss averaged over last 500 steps = 2.8235e-01, PNorm = 143.1989, GNorm = 0.2476
Meta loss on this task batch = 2.9203e-01, Meta loss averaged over last 500 steps = 2.8237e-01, PNorm = 143.2050, GNorm = 0.2300
Meta loss on this task batch = 2.7987e-01, Meta loss averaged over last 500 steps = 2.8242e-01, PNorm = 143.2108, GNorm = 0.2033
Meta loss on this task batch = 2.7846e-01, Meta loss averaged over last 500 steps = 2.8248e-01, PNorm = 143.2162, GNorm = 0.2417
Meta loss on this task batch = 2.3545e-01, Meta loss averaged over last 500 steps = 2.8235e-01, PNorm = 143.2221, GNorm = 0.2084
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.8237e-01, PNorm = 143.2281, GNorm = 0.2418
Meta loss on this task batch = 2.7060e-01, Meta loss averaged over last 500 steps = 2.8237e-01, PNorm = 143.2336, GNorm = 0.2321
Meta loss on this task batch = 3.7098e-01, Meta loss averaged over last 500 steps = 2.8250e-01, PNorm = 143.2383, GNorm = 0.3026
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 2.8260e-01, PNorm = 143.2421, GNorm = 0.2459
Meta loss on this task batch = 2.5888e-01, Meta loss averaged over last 500 steps = 2.8252e-01, PNorm = 143.2451, GNorm = 0.2587
Meta loss on this task batch = 2.6336e-01, Meta loss averaged over last 500 steps = 2.8253e-01, PNorm = 143.2489, GNorm = 0.2408
Took 115.24775528907776 seconds to complete one epoch of meta training
Took 122.13564419746399 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472677
Epoch 552
Meta loss on this task batch = 3.1147e-01, Meta loss averaged over last 500 steps = 2.8258e-01, PNorm = 143.2520, GNorm = 0.2779
Meta loss on this task batch = 2.5167e-01, Meta loss averaged over last 500 steps = 2.8251e-01, PNorm = 143.2568, GNorm = 0.2196
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.8245e-01, PNorm = 143.2616, GNorm = 0.2253
Meta loss on this task batch = 2.5580e-01, Meta loss averaged over last 500 steps = 2.8233e-01, PNorm = 143.2669, GNorm = 0.1867
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.8230e-01, PNorm = 143.2728, GNorm = 0.1978
Meta loss on this task batch = 2.5511e-01, Meta loss averaged over last 500 steps = 2.8217e-01, PNorm = 143.2798, GNorm = 0.2000
Meta loss on this task batch = 3.0130e-01, Meta loss averaged over last 500 steps = 2.8217e-01, PNorm = 143.2875, GNorm = 0.2481
Meta loss on this task batch = 2.1251e-01, Meta loss averaged over last 500 steps = 2.8200e-01, PNorm = 143.2952, GNorm = 0.1930
Meta loss on this task batch = 2.9298e-01, Meta loss averaged over last 500 steps = 2.8205e-01, PNorm = 143.3021, GNorm = 0.2266
Meta loss on this task batch = 2.9378e-01, Meta loss averaged over last 500 steps = 2.8218e-01, PNorm = 143.3095, GNorm = 0.2164
Meta loss on this task batch = 2.4459e-01, Meta loss averaged over last 500 steps = 2.8209e-01, PNorm = 143.3164, GNorm = 0.2199
Meta loss on this task batch = 2.9272e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 143.3234, GNorm = 0.2602
Meta loss on this task batch = 2.3588e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 143.3300, GNorm = 0.2144
Meta loss on this task batch = 2.4531e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 143.3367, GNorm = 0.2499
Meta loss on this task batch = 2.3470e-01, Meta loss averaged over last 500 steps = 2.8165e-01, PNorm = 143.3442, GNorm = 0.1913
Meta loss on this task batch = 3.6286e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 143.3517, GNorm = 0.2779
Meta loss on this task batch = 2.7002e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 143.3579, GNorm = 0.2364
Meta loss on this task batch = 2.7756e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 143.3637, GNorm = 0.2258
Meta loss on this task batch = 3.2057e-01, Meta loss averaged over last 500 steps = 2.8169e-01, PNorm = 143.3686, GNorm = 0.2758
Took 114.06667804718018 seconds to complete one epoch of meta training
Took 121.67607235908508 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463583
Epoch 553
Meta loss on this task batch = 2.6060e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 143.3735, GNorm = 0.2180
Meta loss on this task batch = 2.8171e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 143.3792, GNorm = 0.2241
Meta loss on this task batch = 2.3209e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 143.3849, GNorm = 0.2246
Meta loss on this task batch = 3.0841e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 143.3891, GNorm = 0.2748
Meta loss on this task batch = 2.9661e-01, Meta loss averaged over last 500 steps = 2.8186e-01, PNorm = 143.3936, GNorm = 0.2091
Meta loss on this task batch = 2.3549e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 143.3993, GNorm = 0.2088
Meta loss on this task batch = 2.5698e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 143.4055, GNorm = 0.2201
Meta loss on this task batch = 3.0476e-01, Meta loss averaged over last 500 steps = 2.8166e-01, PNorm = 143.4113, GNorm = 0.2120
Meta loss on this task batch = 3.4234e-01, Meta loss averaged over last 500 steps = 2.8161e-01, PNorm = 143.4172, GNorm = 0.2145
Meta loss on this task batch = 2.7017e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 143.4233, GNorm = 0.2081
Meta loss on this task batch = 2.6214e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 143.4295, GNorm = 0.2030
Meta loss on this task batch = 2.9219e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 143.4366, GNorm = 0.2270
Meta loss on this task batch = 2.6234e-01, Meta loss averaged over last 500 steps = 2.8173e-01, PNorm = 143.4441, GNorm = 0.2516
Meta loss on this task batch = 3.2708e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 143.4520, GNorm = 0.2644
Meta loss on this task batch = 2.6304e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 143.4596, GNorm = 0.2112
Meta loss on this task batch = 2.4573e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 143.4670, GNorm = 0.2190
Meta loss on this task batch = 3.0095e-01, Meta loss averaged over last 500 steps = 2.8169e-01, PNorm = 143.4739, GNorm = 0.2285
Meta loss on this task batch = 2.8219e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 143.4806, GNorm = 0.2176
Meta loss on this task batch = 3.0272e-01, Meta loss averaged over last 500 steps = 2.8166e-01, PNorm = 143.4864, GNorm = 0.2461
Took 116.03339719772339 seconds to complete one epoch of meta training
Took 123.70483541488647 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473465
Epoch 554
Meta loss on this task batch = 1.9810e-01, Meta loss averaged over last 500 steps = 2.8150e-01, PNorm = 143.4914, GNorm = 0.1814
Meta loss on this task batch = 2.9886e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 143.4966, GNorm = 0.2262
Meta loss on this task batch = 2.7767e-01, Meta loss averaged over last 500 steps = 2.8158e-01, PNorm = 143.5017, GNorm = 0.2323
Meta loss on this task batch = 2.6150e-01, Meta loss averaged over last 500 steps = 2.8147e-01, PNorm = 143.5075, GNorm = 0.2227
Meta loss on this task batch = 2.6323e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 143.5132, GNorm = 0.2219
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.8148e-01, PNorm = 143.5187, GNorm = 0.2260
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 2.8140e-01, PNorm = 143.5235, GNorm = 0.2832
Meta loss on this task batch = 2.5639e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 143.5288, GNorm = 0.2156
Meta loss on this task batch = 2.4240e-01, Meta loss averaged over last 500 steps = 2.8126e-01, PNorm = 143.5342, GNorm = 0.2027
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 2.8139e-01, PNorm = 143.5397, GNorm = 0.2132
Meta loss on this task batch = 3.2354e-01, Meta loss averaged over last 500 steps = 2.8153e-01, PNorm = 143.5450, GNorm = 0.2378
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 143.5506, GNorm = 0.2220
Meta loss on this task batch = 2.9746e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 143.5572, GNorm = 0.2490
Meta loss on this task batch = 2.6663e-01, Meta loss averaged over last 500 steps = 2.8166e-01, PNorm = 143.5641, GNorm = 0.2229
Meta loss on this task batch = 3.0270e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 143.5705, GNorm = 0.2436
Meta loss on this task batch = 2.8346e-01, Meta loss averaged over last 500 steps = 2.8179e-01, PNorm = 143.5753, GNorm = 0.2889
Meta loss on this task batch = 3.0268e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 143.5801, GNorm = 0.2400
Meta loss on this task batch = 2.8209e-01, Meta loss averaged over last 500 steps = 2.8185e-01, PNorm = 143.5854, GNorm = 0.2303
Meta loss on this task batch = 3.3322e-01, Meta loss averaged over last 500 steps = 2.8187e-01, PNorm = 143.5907, GNorm = 0.2817
Took 113.75058460235596 seconds to complete one epoch of meta training
Took 121.769371509552 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470844
Epoch 555
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 143.5954, GNorm = 0.2498
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.8167e-01, PNorm = 143.5999, GNorm = 0.2419
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 143.6050, GNorm = 0.2468
Meta loss on this task batch = 2.8961e-01, Meta loss averaged over last 500 steps = 2.8166e-01, PNorm = 143.6112, GNorm = 0.2517
Meta loss on this task batch = 2.8393e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 143.6167, GNorm = 0.2182
Meta loss on this task batch = 3.0844e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 143.6220, GNorm = 0.2462
Meta loss on this task batch = 2.7815e-01, Meta loss averaged over last 500 steps = 2.8175e-01, PNorm = 143.6277, GNorm = 0.2295
Meta loss on this task batch = 2.6216e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 143.6332, GNorm = 0.2141
Meta loss on this task batch = 2.3000e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 143.6394, GNorm = 0.2115
Meta loss on this task batch = 2.4056e-01, Meta loss averaged over last 500 steps = 2.8171e-01, PNorm = 143.6462, GNorm = 0.1955
Meta loss on this task batch = 3.0105e-01, Meta loss averaged over last 500 steps = 2.8169e-01, PNorm = 143.6538, GNorm = 0.2428
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 143.6616, GNorm = 0.2128
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 143.6697, GNorm = 0.2315
Meta loss on this task batch = 2.6691e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 143.6768, GNorm = 0.2400
Meta loss on this task batch = 2.6349e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 143.6840, GNorm = 0.2710
Meta loss on this task batch = 3.2951e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 143.6909, GNorm = 0.2529
Meta loss on this task batch = 2.7610e-01, Meta loss averaged over last 500 steps = 2.8160e-01, PNorm = 143.6965, GNorm = 0.2502
Meta loss on this task batch = 3.5918e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 143.7016, GNorm = 0.2673
Meta loss on this task batch = 3.0081e-01, Meta loss averaged over last 500 steps = 2.8180e-01, PNorm = 143.7067, GNorm = 0.3077
Took 110.97506380081177 seconds to complete one epoch of meta training
Took 118.58443307876587 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469110
Epoch 556
Meta loss on this task batch = 2.7399e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 143.7128, GNorm = 0.3116
Meta loss on this task batch = 2.5482e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 143.7179, GNorm = 0.2250
Meta loss on this task batch = 2.8099e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 143.7238, GNorm = 0.2250
Meta loss on this task batch = 3.1203e-01, Meta loss averaged over last 500 steps = 2.8171e-01, PNorm = 143.7279, GNorm = 0.2536
Meta loss on this task batch = 2.8024e-01, Meta loss averaged over last 500 steps = 2.8173e-01, PNorm = 143.7323, GNorm = 0.2139
Meta loss on this task batch = 3.2322e-01, Meta loss averaged over last 500 steps = 2.8178e-01, PNorm = 143.7361, GNorm = 0.2438
Meta loss on this task batch = 2.5971e-01, Meta loss averaged over last 500 steps = 2.8172e-01, PNorm = 143.7411, GNorm = 0.2218
Meta loss on this task batch = 3.3244e-01, Meta loss averaged over last 500 steps = 2.8184e-01, PNorm = 143.7455, GNorm = 0.2466
Meta loss on this task batch = 2.4238e-01, Meta loss averaged over last 500 steps = 2.8185e-01, PNorm = 143.7511, GNorm = 0.2167
Meta loss on this task batch = 3.0216e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 143.7574, GNorm = 0.2507
Meta loss on this task batch = 2.8240e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 143.7640, GNorm = 0.2268
Meta loss on this task batch = 2.9133e-01, Meta loss averaged over last 500 steps = 2.8181e-01, PNorm = 143.7708, GNorm = 0.2215
Meta loss on this task batch = 2.4680e-01, Meta loss averaged over last 500 steps = 2.8160e-01, PNorm = 143.7781, GNorm = 0.1902
Meta loss on this task batch = 2.5423e-01, Meta loss averaged over last 500 steps = 2.8161e-01, PNorm = 143.7860, GNorm = 0.2367
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 2.8155e-01, PNorm = 143.7942, GNorm = 0.2005
Meta loss on this task batch = 2.5423e-01, Meta loss averaged over last 500 steps = 2.8149e-01, PNorm = 143.8024, GNorm = 0.2045
Meta loss on this task batch = 2.5349e-01, Meta loss averaged over last 500 steps = 2.8149e-01, PNorm = 143.8107, GNorm = 0.2108
Meta loss on this task batch = 2.3664e-01, Meta loss averaged over last 500 steps = 2.8152e-01, PNorm = 143.8191, GNorm = 0.2334
Meta loss on this task batch = 3.6902e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 143.8247, GNorm = 0.3862
Took 113.05854606628418 seconds to complete one epoch of meta training
Took 120.85208010673523 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475462
Epoch 557
Meta loss on this task batch = 3.3541e-01, Meta loss averaged over last 500 steps = 2.8178e-01, PNorm = 143.8298, GNorm = 0.2382
Meta loss on this task batch = 3.0720e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 143.8349, GNorm = 0.3301
Meta loss on this task batch = 2.7580e-01, Meta loss averaged over last 500 steps = 2.8179e-01, PNorm = 143.8396, GNorm = 0.2455
Meta loss on this task batch = 3.1358e-01, Meta loss averaged over last 500 steps = 2.8183e-01, PNorm = 143.8440, GNorm = 0.2742
Meta loss on this task batch = 2.7161e-01, Meta loss averaged over last 500 steps = 2.8178e-01, PNorm = 143.8486, GNorm = 0.2094
Meta loss on this task batch = 2.3343e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 143.8529, GNorm = 0.1981
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.8182e-01, PNorm = 143.8576, GNorm = 0.2435
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 2.8190e-01, PNorm = 143.8621, GNorm = 0.2419
Meta loss on this task batch = 2.8930e-01, Meta loss averaged over last 500 steps = 2.8197e-01, PNorm = 143.8664, GNorm = 0.2429
Meta loss on this task batch = 2.6062e-01, Meta loss averaged over last 500 steps = 2.8188e-01, PNorm = 143.8710, GNorm = 0.2531
Meta loss on this task batch = 2.5513e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 143.8760, GNorm = 0.2083
Meta loss on this task batch = 3.1567e-01, Meta loss averaged over last 500 steps = 2.8205e-01, PNorm = 143.8812, GNorm = 0.2585
Meta loss on this task batch = 3.1369e-01, Meta loss averaged over last 500 steps = 2.8212e-01, PNorm = 143.8860, GNorm = 0.2104
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 2.8211e-01, PNorm = 143.8902, GNorm = 0.2795
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.8208e-01, PNorm = 143.8949, GNorm = 0.2218
Meta loss on this task batch = 2.5260e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 143.9005, GNorm = 0.1999
Meta loss on this task batch = 3.0147e-01, Meta loss averaged over last 500 steps = 2.8207e-01, PNorm = 143.9062, GNorm = 0.2532
Meta loss on this task batch = 3.0444e-01, Meta loss averaged over last 500 steps = 2.8210e-01, PNorm = 143.9122, GNorm = 0.2576
Meta loss on this task batch = 2.5916e-01, Meta loss averaged over last 500 steps = 2.8204e-01, PNorm = 143.9189, GNorm = 0.3425
Took 117.6630117893219 seconds to complete one epoch of meta training
Took 125.19919514656067 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489774
Epoch 558
Meta loss on this task batch = 2.2795e-01, Meta loss averaged over last 500 steps = 2.8191e-01, PNorm = 143.9259, GNorm = 0.1617
Meta loss on this task batch = 2.4438e-01, Meta loss averaged over last 500 steps = 2.8176e-01, PNorm = 143.9332, GNorm = 0.2101
Meta loss on this task batch = 2.3976e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 143.9409, GNorm = 0.2207
Meta loss on this task batch = 3.1656e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 143.9476, GNorm = 0.2170
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.8149e-01, PNorm = 143.9545, GNorm = 0.2004
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 2.8136e-01, PNorm = 143.9608, GNorm = 0.2544
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.8133e-01, PNorm = 143.9675, GNorm = 0.2302
Meta loss on this task batch = 3.4702e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 143.9730, GNorm = 0.2462
Meta loss on this task batch = 2.8797e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 143.9781, GNorm = 0.2289
Meta loss on this task batch = 2.9815e-01, Meta loss averaged over last 500 steps = 2.8150e-01, PNorm = 143.9830, GNorm = 0.2733
Meta loss on this task batch = 3.0374e-01, Meta loss averaged over last 500 steps = 2.8157e-01, PNorm = 143.9871, GNorm = 0.2312
Meta loss on this task batch = 2.4533e-01, Meta loss averaged over last 500 steps = 2.8150e-01, PNorm = 143.9917, GNorm = 0.2064
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 143.9967, GNorm = 0.2311
Meta loss on this task batch = 3.0034e-01, Meta loss averaged over last 500 steps = 2.8170e-01, PNorm = 144.0010, GNorm = 0.2477
Meta loss on this task batch = 3.3783e-01, Meta loss averaged over last 500 steps = 2.8187e-01, PNorm = 144.0060, GNorm = 0.2475
Meta loss on this task batch = 3.4254e-01, Meta loss averaged over last 500 steps = 2.8196e-01, PNorm = 144.0121, GNorm = 0.2638
Meta loss on this task batch = 2.7537e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 144.0179, GNorm = 0.2524
Meta loss on this task batch = 2.6932e-01, Meta loss averaged over last 500 steps = 2.8194e-01, PNorm = 144.0240, GNorm = 0.2603
Meta loss on this task batch = 2.7828e-01, Meta loss averaged over last 500 steps = 2.8196e-01, PNorm = 144.0300, GNorm = 0.2615
Took 115.78108716011047 seconds to complete one epoch of meta training
Took 124.04102230072021 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500019
Epoch 559
Meta loss on this task batch = 3.1172e-01, Meta loss averaged over last 500 steps = 2.8192e-01, PNorm = 144.0343, GNorm = 0.2817
Meta loss on this task batch = 2.0964e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 144.0390, GNorm = 0.1970
Meta loss on this task batch = 2.9256e-01, Meta loss averaged over last 500 steps = 2.8187e-01, PNorm = 144.0437, GNorm = 0.2518
Meta loss on this task batch = 2.0161e-01, Meta loss averaged over last 500 steps = 2.8155e-01, PNorm = 144.0490, GNorm = 0.2012
Meta loss on this task batch = 2.6519e-01, Meta loss averaged over last 500 steps = 2.8143e-01, PNorm = 144.0542, GNorm = 0.2075
Meta loss on this task batch = 2.5987e-01, Meta loss averaged over last 500 steps = 2.8144e-01, PNorm = 144.0590, GNorm = 0.1925
Meta loss on this task batch = 2.6742e-01, Meta loss averaged over last 500 steps = 2.8134e-01, PNorm = 144.0634, GNorm = 0.2177
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.8128e-01, PNorm = 144.0683, GNorm = 0.2873
Meta loss on this task batch = 3.1408e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.0731, GNorm = 0.2466
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.8122e-01, PNorm = 144.0783, GNorm = 0.2142
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 2.8131e-01, PNorm = 144.0829, GNorm = 0.2289
Meta loss on this task batch = 2.6518e-01, Meta loss averaged over last 500 steps = 2.8130e-01, PNorm = 144.0878, GNorm = 0.2098
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 2.8131e-01, PNorm = 144.0925, GNorm = 0.2069
Meta loss on this task batch = 3.4580e-01, Meta loss averaged over last 500 steps = 2.8140e-01, PNorm = 144.0969, GNorm = 0.2551
Meta loss on this task batch = 3.0560e-01, Meta loss averaged over last 500 steps = 2.8146e-01, PNorm = 144.1000, GNorm = 0.2955
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 2.8144e-01, PNorm = 144.1037, GNorm = 0.1984
Meta loss on this task batch = 2.3444e-01, Meta loss averaged over last 500 steps = 2.8135e-01, PNorm = 144.1084, GNorm = 0.2200
Meta loss on this task batch = 3.4905e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 144.1130, GNorm = 0.2813
Meta loss on this task batch = 2.6772e-01, Meta loss averaged over last 500 steps = 2.8162e-01, PNorm = 144.1186, GNorm = 0.2796
Took 115.1800754070282 seconds to complete one epoch of meta training
Took 123.17854070663452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471086
Epoch 560
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 144.1250, GNorm = 0.2321
Meta loss on this task batch = 2.6812e-01, Meta loss averaged over last 500 steps = 2.8155e-01, PNorm = 144.1315, GNorm = 0.2183
Meta loss on this task batch = 2.4409e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 144.1391, GNorm = 0.2337
Meta loss on this task batch = 2.7765e-01, Meta loss averaged over last 500 steps = 2.8145e-01, PNorm = 144.1469, GNorm = 0.2104
Meta loss on this task batch = 2.7732e-01, Meta loss averaged over last 500 steps = 2.8150e-01, PNorm = 144.1548, GNorm = 0.2406
Meta loss on this task batch = 2.3954e-01, Meta loss averaged over last 500 steps = 2.8142e-01, PNorm = 144.1626, GNorm = 0.2093
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 2.8142e-01, PNorm = 144.1704, GNorm = 0.2715
Meta loss on this task batch = 3.0990e-01, Meta loss averaged over last 500 steps = 2.8138e-01, PNorm = 144.1782, GNorm = 0.2467
Meta loss on this task batch = 2.9925e-01, Meta loss averaged over last 500 steps = 2.8144e-01, PNorm = 144.1860, GNorm = 0.2801
Meta loss on this task batch = 2.4910e-01, Meta loss averaged over last 500 steps = 2.8147e-01, PNorm = 144.1943, GNorm = 0.2327
Meta loss on this task batch = 2.7210e-01, Meta loss averaged over last 500 steps = 2.8138e-01, PNorm = 144.2020, GNorm = 0.2195
Meta loss on this task batch = 2.6642e-01, Meta loss averaged over last 500 steps = 2.8130e-01, PNorm = 144.2097, GNorm = 0.2261
Meta loss on this task batch = 2.7132e-01, Meta loss averaged over last 500 steps = 2.8128e-01, PNorm = 144.2167, GNorm = 0.2505
Meta loss on this task batch = 2.7797e-01, Meta loss averaged over last 500 steps = 2.8132e-01, PNorm = 144.2228, GNorm = 0.2452
Meta loss on this task batch = 2.9722e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.2276, GNorm = 0.2825
Meta loss on this task batch = 2.4123e-01, Meta loss averaged over last 500 steps = 2.8139e-01, PNorm = 144.2324, GNorm = 0.1968
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.8128e-01, PNorm = 144.2368, GNorm = 0.2156
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.2406, GNorm = 0.3176
Meta loss on this task batch = 2.3109e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 144.2460, GNorm = 0.2470
Took 118.46444010734558 seconds to complete one epoch of meta training
Took 126.30586624145508 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492072
Epoch 561
Meta loss on this task batch = 2.9618e-01, Meta loss averaged over last 500 steps = 2.8117e-01, PNorm = 144.2517, GNorm = 0.2868
Meta loss on this task batch = 2.5307e-01, Meta loss averaged over last 500 steps = 2.8099e-01, PNorm = 144.2587, GNorm = 0.2117
Meta loss on this task batch = 2.8932e-01, Meta loss averaged over last 500 steps = 2.8101e-01, PNorm = 144.2640, GNorm = 0.2643
Meta loss on this task batch = 2.9692e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 144.2697, GNorm = 0.2258
Meta loss on this task batch = 2.4185e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 144.2753, GNorm = 0.2348
Meta loss on this task batch = 2.7247e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 144.2806, GNorm = 0.2072
Meta loss on this task batch = 3.1903e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 144.2860, GNorm = 0.2112
Meta loss on this task batch = 2.4641e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 144.2906, GNorm = 0.2382
Meta loss on this task batch = 2.8730e-01, Meta loss averaged over last 500 steps = 2.8095e-01, PNorm = 144.2946, GNorm = 0.2606
Meta loss on this task batch = 2.5808e-01, Meta loss averaged over last 500 steps = 2.8091e-01, PNorm = 144.2990, GNorm = 0.2107
Meta loss on this task batch = 2.7881e-01, Meta loss averaged over last 500 steps = 2.8090e-01, PNorm = 144.3034, GNorm = 0.2295
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 144.3069, GNorm = 0.3218
Meta loss on this task batch = 2.6818e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 144.3108, GNorm = 0.2167
Meta loss on this task batch = 3.1266e-01, Meta loss averaged over last 500 steps = 2.8094e-01, PNorm = 144.3138, GNorm = 0.2693
Meta loss on this task batch = 3.0554e-01, Meta loss averaged over last 500 steps = 2.8097e-01, PNorm = 144.3181, GNorm = 0.2657
Meta loss on this task batch = 2.8059e-01, Meta loss averaged over last 500 steps = 2.8096e-01, PNorm = 144.3226, GNorm = 0.2358
Meta loss on this task batch = 3.0974e-01, Meta loss averaged over last 500 steps = 2.8103e-01, PNorm = 144.3267, GNorm = 0.2170
Meta loss on this task batch = 3.0038e-01, Meta loss averaged over last 500 steps = 2.8114e-01, PNorm = 144.3310, GNorm = 0.2350
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 144.3353, GNorm = 0.2451
Took 114.68027257919312 seconds to complete one epoch of meta training
Took 122.11546397209167 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478063
Epoch 562
Meta loss on this task batch = 2.8076e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 144.3400, GNorm = 0.2303
Meta loss on this task batch = 3.0299e-01, Meta loss averaged over last 500 steps = 2.8121e-01, PNorm = 144.3448, GNorm = 0.2279
Meta loss on this task batch = 3.1502e-01, Meta loss averaged over last 500 steps = 2.8131e-01, PNorm = 144.3505, GNorm = 0.2502
Meta loss on this task batch = 2.7194e-01, Meta loss averaged over last 500 steps = 2.8130e-01, PNorm = 144.3560, GNorm = 0.2132
Meta loss on this task batch = 2.5212e-01, Meta loss averaged over last 500 steps = 2.8126e-01, PNorm = 144.3621, GNorm = 0.1932
Meta loss on this task batch = 2.8861e-01, Meta loss averaged over last 500 steps = 2.8132e-01, PNorm = 144.3674, GNorm = 0.2196
Meta loss on this task batch = 2.9348e-01, Meta loss averaged over last 500 steps = 2.8146e-01, PNorm = 144.3725, GNorm = 0.2129
Meta loss on this task batch = 3.1834e-01, Meta loss averaged over last 500 steps = 2.8153e-01, PNorm = 144.3776, GNorm = 0.2243
Meta loss on this task batch = 3.1332e-01, Meta loss averaged over last 500 steps = 2.8158e-01, PNorm = 144.3833, GNorm = 0.2249
Meta loss on this task batch = 2.4434e-01, Meta loss averaged over last 500 steps = 2.8154e-01, PNorm = 144.3887, GNorm = 0.2413
Meta loss on this task batch = 3.5940e-01, Meta loss averaged over last 500 steps = 2.8177e-01, PNorm = 144.3933, GNorm = 0.2455
Meta loss on this task batch = 2.8519e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 144.3990, GNorm = 0.2302
Meta loss on this task batch = 3.1191e-01, Meta loss averaged over last 500 steps = 2.8174e-01, PNorm = 144.4038, GNorm = 0.2213
Meta loss on this task batch = 2.5373e-01, Meta loss averaged over last 500 steps = 2.8157e-01, PNorm = 144.4091, GNorm = 0.1981
Meta loss on this task batch = 3.0578e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 144.4148, GNorm = 0.2588
Meta loss on this task batch = 2.4039e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 144.4215, GNorm = 0.2313
Meta loss on this task batch = 2.8195e-01, Meta loss averaged over last 500 steps = 2.8147e-01, PNorm = 144.4288, GNorm = 0.2346
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 2.8146e-01, PNorm = 144.4362, GNorm = 0.2177
Meta loss on this task batch = 2.4144e-01, Meta loss averaged over last 500 steps = 2.8133e-01, PNorm = 144.4443, GNorm = 0.2683
Took 115.0592303276062 seconds to complete one epoch of meta training
Took 122.57433009147644 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480113
Epoch 563
Meta loss on this task batch = 3.0916e-01, Meta loss averaged over last 500 steps = 2.8133e-01, PNorm = 144.4516, GNorm = 0.2537
Meta loss on this task batch = 2.8715e-01, Meta loss averaged over last 500 steps = 2.8138e-01, PNorm = 144.4585, GNorm = 0.2204
Meta loss on this task batch = 3.0373e-01, Meta loss averaged over last 500 steps = 2.8138e-01, PNorm = 144.4650, GNorm = 0.2647
Meta loss on this task batch = 2.5013e-01, Meta loss averaged over last 500 steps = 2.8123e-01, PNorm = 144.4717, GNorm = 0.2170
Meta loss on this task batch = 2.9017e-01, Meta loss averaged over last 500 steps = 2.8119e-01, PNorm = 144.4778, GNorm = 0.2790
Meta loss on this task batch = 2.7769e-01, Meta loss averaged over last 500 steps = 2.8129e-01, PNorm = 144.4841, GNorm = 0.2120
Meta loss on this task batch = 3.2099e-01, Meta loss averaged over last 500 steps = 2.8142e-01, PNorm = 144.4900, GNorm = 0.2411
Meta loss on this task batch = 2.5021e-01, Meta loss averaged over last 500 steps = 2.8136e-01, PNorm = 144.4956, GNorm = 0.2301
Meta loss on this task batch = 3.0303e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.5003, GNorm = 0.2495
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.8140e-01, PNorm = 144.5060, GNorm = 0.2307
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.8146e-01, PNorm = 144.5116, GNorm = 0.2204
Meta loss on this task batch = 2.8227e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.5172, GNorm = 0.2238
Meta loss on this task batch = 2.7297e-01, Meta loss averaged over last 500 steps = 2.8130e-01, PNorm = 144.5231, GNorm = 0.2113
Meta loss on this task batch = 3.3145e-01, Meta loss averaged over last 500 steps = 2.8151e-01, PNorm = 144.5290, GNorm = 0.2127
Meta loss on this task batch = 2.8884e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 144.5352, GNorm = 0.2010
Meta loss on this task batch = 2.8694e-01, Meta loss averaged over last 500 steps = 2.8156e-01, PNorm = 144.5414, GNorm = 0.2657
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.8140e-01, PNorm = 144.5487, GNorm = 0.2268
Meta loss on this task batch = 2.6775e-01, Meta loss averaged over last 500 steps = 2.8133e-01, PNorm = 144.5559, GNorm = 0.2023
Meta loss on this task batch = 2.1905e-01, Meta loss averaged over last 500 steps = 2.8125e-01, PNorm = 144.5635, GNorm = 0.2202
Took 114.06552982330322 seconds to complete one epoch of meta training
Took 122.01852226257324 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484105
Epoch 564
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.8128e-01, PNorm = 144.5702, GNorm = 0.2040
Meta loss on this task batch = 2.9225e-01, Meta loss averaged over last 500 steps = 2.8135e-01, PNorm = 144.5762, GNorm = 0.2254
Meta loss on this task batch = 2.8710e-01, Meta loss averaged over last 500 steps = 2.8139e-01, PNorm = 144.5822, GNorm = 0.2201
Meta loss on this task batch = 2.7319e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.5874, GNorm = 0.2305
Meta loss on this task batch = 2.4073e-01, Meta loss averaged over last 500 steps = 2.8136e-01, PNorm = 144.5924, GNorm = 0.2046
Meta loss on this task batch = 2.4171e-01, Meta loss averaged over last 500 steps = 2.8128e-01, PNorm = 144.5977, GNorm = 0.2367
Meta loss on this task batch = 3.0742e-01, Meta loss averaged over last 500 steps = 2.8122e-01, PNorm = 144.6021, GNorm = 0.2690
Meta loss on this task batch = 3.0529e-01, Meta loss averaged over last 500 steps = 2.8137e-01, PNorm = 144.6057, GNorm = 0.2468
Meta loss on this task batch = 2.8435e-01, Meta loss averaged over last 500 steps = 2.8143e-01, PNorm = 144.6098, GNorm = 0.2621
Meta loss on this task batch = 3.1925e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 144.6137, GNorm = 0.3425
Meta loss on this task batch = 2.4161e-01, Meta loss averaged over last 500 steps = 2.8136e-01, PNorm = 144.6183, GNorm = 0.1921
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.8127e-01, PNorm = 144.6233, GNorm = 0.2119
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.8117e-01, PNorm = 144.6282, GNorm = 0.1920
Meta loss on this task batch = 2.7027e-01, Meta loss averaged over last 500 steps = 2.8115e-01, PNorm = 144.6341, GNorm = 0.2414
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 144.6406, GNorm = 0.2060
Meta loss on this task batch = 2.7168e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 144.6463, GNorm = 0.2703
Meta loss on this task batch = 2.6618e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 144.6519, GNorm = 0.2174
Meta loss on this task batch = 2.6782e-01, Meta loss averaged over last 500 steps = 2.8108e-01, PNorm = 144.6573, GNorm = 0.1987
Meta loss on this task batch = 3.2162e-01, Meta loss averaged over last 500 steps = 2.8112e-01, PNorm = 144.6631, GNorm = 0.3182
Took 113.18475103378296 seconds to complete one epoch of meta training
Took 120.66817021369934 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488979
Epoch 565
Meta loss on this task batch = 2.7356e-01, Meta loss averaged over last 500 steps = 2.8110e-01, PNorm = 144.6694, GNorm = 0.2325
Meta loss on this task batch = 3.0678e-01, Meta loss averaged over last 500 steps = 2.8113e-01, PNorm = 144.6756, GNorm = 0.2571
Meta loss on this task batch = 2.0007e-01, Meta loss averaged over last 500 steps = 2.8098e-01, PNorm = 144.6813, GNorm = 0.2020
Meta loss on this task batch = 3.0663e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 144.6858, GNorm = 0.2420
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.8115e-01, PNorm = 144.6905, GNorm = 0.2310
Meta loss on this task batch = 3.3765e-01, Meta loss averaged over last 500 steps = 2.8119e-01, PNorm = 144.6953, GNorm = 0.2499
Meta loss on this task batch = 2.9105e-01, Meta loss averaged over last 500 steps = 2.8123e-01, PNorm = 144.6997, GNorm = 0.2236
Meta loss on this task batch = 3.2987e-01, Meta loss averaged over last 500 steps = 2.8139e-01, PNorm = 144.7045, GNorm = 0.2629
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 2.8149e-01, PNorm = 144.7097, GNorm = 0.2310
Meta loss on this task batch = 3.0567e-01, Meta loss averaged over last 500 steps = 2.8164e-01, PNorm = 144.7150, GNorm = 0.2397
Meta loss on this task batch = 2.3265e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 144.7212, GNorm = 0.2078
Meta loss on this task batch = 2.7589e-01, Meta loss averaged over last 500 steps = 2.8165e-01, PNorm = 144.7277, GNorm = 0.2464
Meta loss on this task batch = 2.7449e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 144.7326, GNorm = 0.3262
Meta loss on this task batch = 3.2561e-01, Meta loss averaged over last 500 steps = 2.8163e-01, PNorm = 144.7372, GNorm = 0.2287
Meta loss on this task batch = 2.4733e-01, Meta loss averaged over last 500 steps = 2.8158e-01, PNorm = 144.7423, GNorm = 0.2361
Meta loss on this task batch = 2.8909e-01, Meta loss averaged over last 500 steps = 2.8153e-01, PNorm = 144.7480, GNorm = 0.2220
Meta loss on this task batch = 3.1912e-01, Meta loss averaged over last 500 steps = 2.8161e-01, PNorm = 144.7527, GNorm = 0.2349
Meta loss on this task batch = 2.6608e-01, Meta loss averaged over last 500 steps = 2.8157e-01, PNorm = 144.7572, GNorm = 0.2067
Meta loss on this task batch = 2.4336e-01, Meta loss averaged over last 500 steps = 2.8141e-01, PNorm = 144.7612, GNorm = 0.2294
Took 115.94564056396484 seconds to complete one epoch of meta training
Took 123.68069887161255 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502289
Epoch 566
Meta loss on this task batch = 2.7288e-01, Meta loss averaged over last 500 steps = 2.8142e-01, PNorm = 144.7660, GNorm = 0.2291
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.8144e-01, PNorm = 144.7708, GNorm = 0.2265
Meta loss on this task batch = 3.2619e-01, Meta loss averaged over last 500 steps = 2.8159e-01, PNorm = 144.7765, GNorm = 0.2500
Meta loss on this task batch = 2.5236e-01, Meta loss averaged over last 500 steps = 2.8146e-01, PNorm = 144.7820, GNorm = 0.2127
Meta loss on this task batch = 2.1269e-01, Meta loss averaged over last 500 steps = 2.8127e-01, PNorm = 144.7886, GNorm = 0.1743
Meta loss on this task batch = 2.5807e-01, Meta loss averaged over last 500 steps = 2.8126e-01, PNorm = 144.7953, GNorm = 0.2041
Meta loss on this task batch = 2.9271e-01, Meta loss averaged over last 500 steps = 2.8124e-01, PNorm = 144.8005, GNorm = 0.2858
Meta loss on this task batch = 2.9599e-01, Meta loss averaged over last 500 steps = 2.8125e-01, PNorm = 144.8063, GNorm = 0.2032
Meta loss on this task batch = 2.9493e-01, Meta loss averaged over last 500 steps = 2.8120e-01, PNorm = 144.8122, GNorm = 0.2374
Meta loss on this task batch = 2.2092e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 144.8189, GNorm = 0.1964
Meta loss on this task batch = 2.9271e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 144.8246, GNorm = 0.2471
Meta loss on this task batch = 3.0165e-01, Meta loss averaged over last 500 steps = 2.8109e-01, PNorm = 144.8295, GNorm = 0.2638
Meta loss on this task batch = 2.6643e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 144.8345, GNorm = 0.2132
Meta loss on this task batch = 2.5515e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 144.8408, GNorm = 0.2045
Meta loss on this task batch = 2.7837e-01, Meta loss averaged over last 500 steps = 2.8085e-01, PNorm = 144.8477, GNorm = 0.2009
Meta loss on this task batch = 2.9044e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 144.8541, GNorm = 0.2238
Meta loss on this task batch = 3.1478e-01, Meta loss averaged over last 500 steps = 2.8084e-01, PNorm = 144.8611, GNorm = 0.2295
Meta loss on this task batch = 2.6766e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 144.8685, GNorm = 0.2228
Meta loss on this task batch = 2.5560e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 144.8751, GNorm = 0.2942
Took 163.29653310775757 seconds to complete one epoch of meta training
Took 170.91645741462708 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487050
Epoch 567
Meta loss on this task batch = 2.5402e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 144.8812, GNorm = 0.2026
Meta loss on this task batch = 2.5419e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 144.8870, GNorm = 0.1757
Meta loss on this task batch = 2.3563e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 144.8937, GNorm = 0.1945
Meta loss on this task batch = 2.4600e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 144.9010, GNorm = 0.1837
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.8051e-01, PNorm = 144.9082, GNorm = 0.1940
Meta loss on this task batch = 2.4914e-01, Meta loss averaged over last 500 steps = 2.8049e-01, PNorm = 144.9165, GNorm = 0.1962
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 2.8051e-01, PNorm = 144.9240, GNorm = 0.2394
Meta loss on this task batch = 3.4176e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 144.9300, GNorm = 0.3067
Meta loss on this task batch = 3.4716e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 144.9348, GNorm = 0.2950
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 144.9390, GNorm = 0.2529
Meta loss on this task batch = 2.8093e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 144.9436, GNorm = 0.2203
Meta loss on this task batch = 2.6554e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 144.9488, GNorm = 0.2220
Meta loss on this task batch = 2.9389e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 144.9541, GNorm = 0.2399
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 2.8085e-01, PNorm = 144.9594, GNorm = 0.2464
Meta loss on this task batch = 2.2162e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 144.9649, GNorm = 0.1775
Meta loss on this task batch = 3.1026e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 144.9700, GNorm = 0.2121
Meta loss on this task batch = 2.4358e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 144.9757, GNorm = 0.2027
Meta loss on this task batch = 2.8421e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 144.9810, GNorm = 0.2627
Meta loss on this task batch = 2.6835e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 144.9863, GNorm = 0.2632
Took 171.1986210346222 seconds to complete one epoch of meta training
Took 179.25792956352234 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480292
Epoch 568
Meta loss on this task batch = 2.5864e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 144.9916, GNorm = 0.2340
Meta loss on this task batch = 3.0709e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 144.9973, GNorm = 0.2486
Meta loss on this task batch = 2.4566e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 145.0035, GNorm = 0.2326
Meta loss on this task batch = 3.0243e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 145.0095, GNorm = 0.2422
Meta loss on this task batch = 2.8668e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 145.0154, GNorm = 0.2043
Meta loss on this task batch = 2.5812e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 145.0210, GNorm = 0.2148
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 145.0262, GNorm = 0.2325
Meta loss on this task batch = 2.9891e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 145.0315, GNorm = 0.2266
Meta loss on this task batch = 3.5976e-01, Meta loss averaged over last 500 steps = 2.8107e-01, PNorm = 145.0364, GNorm = 0.2502
Meta loss on this task batch = 2.8471e-01, Meta loss averaged over last 500 steps = 2.8111e-01, PNorm = 145.0414, GNorm = 0.2632
Meta loss on this task batch = 2.2312e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 145.0468, GNorm = 0.2076
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.8106e-01, PNorm = 145.0516, GNorm = 0.2540
Meta loss on this task batch = 2.8143e-01, Meta loss averaged over last 500 steps = 2.8110e-01, PNorm = 145.0561, GNorm = 0.2308
Meta loss on this task batch = 2.4678e-01, Meta loss averaged over last 500 steps = 2.8114e-01, PNorm = 145.0607, GNorm = 0.2160
Meta loss on this task batch = 2.3765e-01, Meta loss averaged over last 500 steps = 2.8109e-01, PNorm = 145.0654, GNorm = 0.1979
Meta loss on this task batch = 2.2885e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 145.0709, GNorm = 0.2126
Meta loss on this task batch = 2.7809e-01, Meta loss averaged over last 500 steps = 2.8080e-01, PNorm = 145.0772, GNorm = 0.2772
Meta loss on this task batch = 3.4504e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 145.0826, GNorm = 0.2567
Meta loss on this task batch = 2.4969e-01, Meta loss averaged over last 500 steps = 2.8087e-01, PNorm = 145.0882, GNorm = 0.2427
Took 158.7938413619995 seconds to complete one epoch of meta training
Took 166.33163857460022 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487566
Epoch 569
Meta loss on this task batch = 3.1664e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 145.0937, GNorm = 0.2426
Meta loss on this task batch = 2.6073e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 145.0997, GNorm = 0.2207
Meta loss on this task batch = 2.9009e-01, Meta loss averaged over last 500 steps = 2.8065e-01, PNorm = 145.1052, GNorm = 0.2587
Meta loss on this task batch = 2.8016e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 145.1094, GNorm = 0.2751
Meta loss on this task batch = 2.8119e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 145.1136, GNorm = 0.2313
Meta loss on this task batch = 2.4626e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 145.1191, GNorm = 0.2203
Meta loss on this task batch = 2.7404e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 145.1244, GNorm = 0.2175
Meta loss on this task batch = 2.1916e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 145.1301, GNorm = 0.2008
Meta loss on this task batch = 2.6737e-01, Meta loss averaged over last 500 steps = 2.8057e-01, PNorm = 145.1360, GNorm = 0.2217
Meta loss on this task batch = 3.0850e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 145.1421, GNorm = 0.2296
Meta loss on this task batch = 2.8701e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 145.1481, GNorm = 0.2653
Meta loss on this task batch = 2.6921e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 145.1542, GNorm = 0.2191
Meta loss on this task batch = 3.1910e-01, Meta loss averaged over last 500 steps = 2.8079e-01, PNorm = 145.1603, GNorm = 0.2289
Meta loss on this task batch = 2.4448e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 145.1667, GNorm = 0.1877
Meta loss on this task batch = 2.6502e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 145.1746, GNorm = 0.2183
Meta loss on this task batch = 2.5295e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 145.1824, GNorm = 0.2208
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 145.1899, GNorm = 0.2453
Meta loss on this task batch = 2.7306e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 145.1976, GNorm = 0.2104
Meta loss on this task batch = 2.8862e-01, Meta loss averaged over last 500 steps = 2.8071e-01, PNorm = 145.2058, GNorm = 0.2556
Took 114.34675765037537 seconds to complete one epoch of meta training
Took 122.42940187454224 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486876
Epoch 570
Meta loss on this task batch = 3.2157e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 145.2122, GNorm = 0.2600
Meta loss on this task batch = 2.9446e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 145.2188, GNorm = 0.2794
Meta loss on this task batch = 2.9887e-01, Meta loss averaged over last 500 steps = 2.8074e-01, PNorm = 145.2243, GNorm = 0.2875
Meta loss on this task batch = 2.3178e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 145.2291, GNorm = 0.2142
Meta loss on this task batch = 2.2088e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 145.2346, GNorm = 0.2199
Meta loss on this task batch = 2.4588e-01, Meta loss averaged over last 500 steps = 2.8037e-01, PNorm = 145.2402, GNorm = 0.2182
Meta loss on this task batch = 2.9566e-01, Meta loss averaged over last 500 steps = 2.8048e-01, PNorm = 145.2461, GNorm = 0.2099
Meta loss on this task batch = 2.9325e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 145.2518, GNorm = 0.2625
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 145.2575, GNorm = 0.2615
Meta loss on this task batch = 3.0790e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 145.2629, GNorm = 0.2179
Meta loss on this task batch = 2.6486e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 145.2692, GNorm = 0.2136
Meta loss on this task batch = 2.6312e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 145.2763, GNorm = 0.2084
Meta loss on this task batch = 2.7840e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 145.2834, GNorm = 0.2274
Meta loss on this task batch = 2.4406e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 145.2893, GNorm = 0.2318
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 145.2953, GNorm = 0.2474
Meta loss on this task batch = 2.6856e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 145.3008, GNorm = 0.2336
Meta loss on this task batch = 3.2269e-01, Meta loss averaged over last 500 steps = 2.8059e-01, PNorm = 145.3061, GNorm = 0.2333
Meta loss on this task batch = 3.1247e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 145.3112, GNorm = 0.2197
Meta loss on this task batch = 2.7012e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 145.3162, GNorm = 0.2343
Took 116.02806448936462 seconds to complete one epoch of meta training
Took 124.0444598197937 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492945
Epoch 571
Meta loss on this task batch = 2.7811e-01, Meta loss averaged over last 500 steps = 2.8050e-01, PNorm = 145.3211, GNorm = 0.2289
Meta loss on this task batch = 3.4891e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 145.3258, GNorm = 0.2350
Meta loss on this task batch = 2.7772e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 145.3309, GNorm = 0.2052
Meta loss on this task batch = 1.9455e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 145.3372, GNorm = 0.1715
Meta loss on this task batch = 2.3619e-01, Meta loss averaged over last 500 steps = 2.8042e-01, PNorm = 145.3433, GNorm = 0.2072
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.8030e-01, PNorm = 145.3501, GNorm = 0.2007
Meta loss on this task batch = 3.1385e-01, Meta loss averaged over last 500 steps = 2.8037e-01, PNorm = 145.3564, GNorm = 0.2216
Meta loss on this task batch = 2.5347e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 145.3625, GNorm = 0.2260
Meta loss on this task batch = 2.3396e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 145.3699, GNorm = 0.1854
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 145.3771, GNorm = 0.2465
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 2.8038e-01, PNorm = 145.3844, GNorm = 0.2344
Meta loss on this task batch = 2.9424e-01, Meta loss averaged over last 500 steps = 2.8040e-01, PNorm = 145.3918, GNorm = 0.2874
Meta loss on this task batch = 2.8554e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 145.3988, GNorm = 0.2363
Meta loss on this task batch = 2.7035e-01, Meta loss averaged over last 500 steps = 2.8032e-01, PNorm = 145.4045, GNorm = 0.2236
Meta loss on this task batch = 3.6426e-01, Meta loss averaged over last 500 steps = 2.8045e-01, PNorm = 145.4078, GNorm = 0.2790
Meta loss on this task batch = 2.7399e-01, Meta loss averaged over last 500 steps = 2.8048e-01, PNorm = 145.4111, GNorm = 0.2464
Meta loss on this task batch = 2.7349e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 145.4147, GNorm = 0.2227
Meta loss on this task batch = 2.8008e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 145.4161, GNorm = 0.2545
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 145.4168, GNorm = 0.2735
Took 115.19805240631104 seconds to complete one epoch of meta training
Took 123.7446858882904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507198
Epoch 572
Meta loss on this task batch = 2.6084e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 145.4195, GNorm = 0.2772
Meta loss on this task batch = 2.9599e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 145.4219, GNorm = 0.2412
Meta loss on this task batch = 3.2886e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 145.4257, GNorm = 0.2680
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 2.8085e-01, PNorm = 145.4300, GNorm = 0.2414
Meta loss on this task batch = 3.0570e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 145.4354, GNorm = 0.2388
Meta loss on this task batch = 2.3087e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 145.4420, GNorm = 0.2323
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 2.8067e-01, PNorm = 145.4485, GNorm = 0.2350
Meta loss on this task batch = 3.1677e-01, Meta loss averaged over last 500 steps = 2.8069e-01, PNorm = 145.4557, GNorm = 0.2177
Meta loss on this task batch = 3.1226e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 145.4632, GNorm = 0.2321
Meta loss on this task batch = 2.3424e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 145.4709, GNorm = 0.2045
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 145.4784, GNorm = 0.1780
Meta loss on this task batch = 2.6971e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 145.4856, GNorm = 0.1927
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 2.8059e-01, PNorm = 145.4926, GNorm = 0.2076
Meta loss on this task batch = 3.0193e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 145.4980, GNorm = 0.2813
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 145.5023, GNorm = 0.2536
Meta loss on this task batch = 2.4388e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 145.5072, GNorm = 0.2166
Meta loss on this task batch = 3.0561e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 145.5124, GNorm = 0.2123
Meta loss on this task batch = 2.8549e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 145.5177, GNorm = 0.2273
Meta loss on this task batch = 2.9403e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 145.5221, GNorm = 0.3052
Took 118.235023021698 seconds to complete one epoch of meta training
Took 125.64292049407959 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473271
Epoch 573
Meta loss on this task batch = 2.6749e-01, Meta loss averaged over last 500 steps = 2.8076e-01, PNorm = 145.5265, GNorm = 0.2200
Meta loss on this task batch = 2.6706e-01, Meta loss averaged over last 500 steps = 2.8072e-01, PNorm = 145.5314, GNorm = 0.2228
Meta loss on this task batch = 2.7145e-01, Meta loss averaged over last 500 steps = 2.8073e-01, PNorm = 145.5375, GNorm = 0.2226
Meta loss on this task batch = 2.7138e-01, Meta loss averaged over last 500 steps = 2.8068e-01, PNorm = 145.5447, GNorm = 0.2195
Meta loss on this task batch = 2.6044e-01, Meta loss averaged over last 500 steps = 2.8061e-01, PNorm = 145.5520, GNorm = 0.2161
Meta loss on this task batch = 2.7551e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 145.5596, GNorm = 0.2486
Meta loss on this task batch = 3.4192e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 145.5672, GNorm = 0.2565
Meta loss on this task batch = 3.4042e-01, Meta loss averaged over last 500 steps = 2.8092e-01, PNorm = 145.5733, GNorm = 0.2359
Meta loss on this task batch = 2.7677e-01, Meta loss averaged over last 500 steps = 2.8093e-01, PNorm = 145.5813, GNorm = 0.2351
Meta loss on this task batch = 2.5674e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 145.5902, GNorm = 0.2010
Meta loss on this task batch = 2.6837e-01, Meta loss averaged over last 500 steps = 2.8088e-01, PNorm = 145.5986, GNorm = 0.2291
Meta loss on this task batch = 2.9396e-01, Meta loss averaged over last 500 steps = 2.8083e-01, PNorm = 145.6066, GNorm = 0.2197
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.8082e-01, PNorm = 145.6146, GNorm = 0.2207
Meta loss on this task batch = 2.6656e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 145.6220, GNorm = 0.2334
Meta loss on this task batch = 3.0011e-01, Meta loss averaged over last 500 steps = 2.8078e-01, PNorm = 145.6291, GNorm = 0.2427
Meta loss on this task batch = 2.7094e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 145.6349, GNorm = 0.2695
Meta loss on this task batch = 2.1480e-01, Meta loss averaged over last 500 steps = 2.8053e-01, PNorm = 145.6410, GNorm = 0.1885
Meta loss on this task batch = 2.7221e-01, Meta loss averaged over last 500 steps = 2.8049e-01, PNorm = 145.6470, GNorm = 0.2721
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.8056e-01, PNorm = 145.6534, GNorm = 0.2949
Took 112.46661615371704 seconds to complete one epoch of meta training
Took 120.24485421180725 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485779
Epoch 574
Meta loss on this task batch = 2.6862e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 145.6601, GNorm = 0.2378
Meta loss on this task batch = 2.7360e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 145.6665, GNorm = 0.2539
Meta loss on this task batch = 2.3557e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 145.6733, GNorm = 0.1946
Meta loss on this task batch = 2.9040e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 145.6800, GNorm = 0.2369
Meta loss on this task batch = 2.9953e-01, Meta loss averaged over last 500 steps = 2.8054e-01, PNorm = 145.6863, GNorm = 0.2246
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 2.8060e-01, PNorm = 145.6920, GNorm = 0.2251
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 145.6978, GNorm = 0.2388
Meta loss on this task batch = 2.6370e-01, Meta loss averaged over last 500 steps = 2.8059e-01, PNorm = 145.7030, GNorm = 0.2461
Meta loss on this task batch = 3.2388e-01, Meta loss averaged over last 500 steps = 2.8070e-01, PNorm = 145.7086, GNorm = 0.2455
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 2.8064e-01, PNorm = 145.7138, GNorm = 0.2394
Meta loss on this task batch = 3.0212e-01, Meta loss averaged over last 500 steps = 2.8058e-01, PNorm = 145.7193, GNorm = 0.2466
Meta loss on this task batch = 2.7478e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 145.7243, GNorm = 0.2474
Meta loss on this task batch = 2.7030e-01, Meta loss averaged over last 500 steps = 2.8075e-01, PNorm = 145.7290, GNorm = 0.2363
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.8081e-01, PNorm = 145.7338, GNorm = 0.2338
Meta loss on this task batch = 3.4169e-01, Meta loss averaged over last 500 steps = 2.8105e-01, PNorm = 145.7391, GNorm = 0.2209
Meta loss on this task batch = 2.5772e-01, Meta loss averaged over last 500 steps = 2.8096e-01, PNorm = 145.7441, GNorm = 0.2266
Meta loss on this task batch = 2.7357e-01, Meta loss averaged over last 500 steps = 2.8086e-01, PNorm = 145.7499, GNorm = 0.2491
Meta loss on this task batch = 2.7247e-01, Meta loss averaged over last 500 steps = 2.8077e-01, PNorm = 145.7559, GNorm = 0.2381
Meta loss on this task batch = 2.9555e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 145.7616, GNorm = 0.2856
Took 114.32330703735352 seconds to complete one epoch of meta training
Took 122.28413271903992 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466625
Epoch 575
Meta loss on this task batch = 2.9738e-01, Meta loss averaged over last 500 steps = 2.8062e-01, PNorm = 145.7678, GNorm = 0.2388
Meta loss on this task batch = 2.5577e-01, Meta loss averaged over last 500 steps = 2.8063e-01, PNorm = 145.7739, GNorm = 0.1953
Meta loss on this task batch = 2.9298e-01, Meta loss averaged over last 500 steps = 2.8066e-01, PNorm = 145.7802, GNorm = 0.2471
Meta loss on this task batch = 2.3487e-01, Meta loss averaged over last 500 steps = 2.8055e-01, PNorm = 145.7870, GNorm = 0.1931
Meta loss on this task batch = 2.1471e-01, Meta loss averaged over last 500 steps = 2.8040e-01, PNorm = 145.7944, GNorm = 0.1943
Meta loss on this task batch = 2.4876e-01, Meta loss averaged over last 500 steps = 2.8031e-01, PNorm = 145.8026, GNorm = 0.2214
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 145.8111, GNorm = 0.2568
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 145.8205, GNorm = 0.2593
Meta loss on this task batch = 2.5937e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 145.8298, GNorm = 0.2105
Meta loss on this task batch = 2.8247e-01, Meta loss averaged over last 500 steps = 2.8007e-01, PNorm = 145.8378, GNorm = 0.2389
Meta loss on this task batch = 2.6628e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 145.8453, GNorm = 0.2327
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 145.8521, GNorm = 0.2399
Meta loss on this task batch = 2.5859e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 145.8586, GNorm = 0.2224
Meta loss on this task batch = 3.3408e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 145.8639, GNorm = 0.2557
Meta loss on this task batch = 3.0491e-01, Meta loss averaged over last 500 steps = 2.8026e-01, PNorm = 145.8687, GNorm = 0.2244
Meta loss on this task batch = 2.7309e-01, Meta loss averaged over last 500 steps = 2.8022e-01, PNorm = 145.8730, GNorm = 0.2129
Meta loss on this task batch = 2.3396e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 145.8777, GNorm = 0.2149
Meta loss on this task batch = 2.4602e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 145.8827, GNorm = 0.2325
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 145.8880, GNorm = 0.2746
Took 114.61101460456848 seconds to complete one epoch of meta training
Took 121.19760012626648 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480156
Epoch 576
Meta loss on this task batch = 2.7711e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 145.8941, GNorm = 0.2337
Meta loss on this task batch = 3.3828e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 145.9004, GNorm = 0.2935
Meta loss on this task batch = 2.1101e-01, Meta loss averaged over last 500 steps = 2.7999e-01, PNorm = 145.9082, GNorm = 0.1997
Meta loss on this task batch = 3.0488e-01, Meta loss averaged over last 500 steps = 2.8009e-01, PNorm = 145.9158, GNorm = 0.2420
Meta loss on this task batch = 3.0998e-01, Meta loss averaged over last 500 steps = 2.8023e-01, PNorm = 145.9235, GNorm = 0.2329
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.8022e-01, PNorm = 145.9317, GNorm = 0.2339
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 2.8028e-01, PNorm = 145.9387, GNorm = 0.2402
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.8029e-01, PNorm = 145.9461, GNorm = 0.2022
Meta loss on this task batch = 2.5342e-01, Meta loss averaged over last 500 steps = 2.8016e-01, PNorm = 145.9540, GNorm = 0.2322
Meta loss on this task batch = 2.7686e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 145.9610, GNorm = 0.2601
Meta loss on this task batch = 3.0436e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 145.9658, GNorm = 0.2787
Meta loss on this task batch = 2.7197e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 145.9694, GNorm = 0.2381
Meta loss on this task batch = 2.6528e-01, Meta loss averaged over last 500 steps = 2.7986e-01, PNorm = 145.9740, GNorm = 0.2086
Meta loss on this task batch = 2.5873e-01, Meta loss averaged over last 500 steps = 2.7996e-01, PNorm = 145.9783, GNorm = 0.2175
Meta loss on this task batch = 2.5520e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 145.9825, GNorm = 0.2191
Meta loss on this task batch = 3.2259e-01, Meta loss averaged over last 500 steps = 2.8009e-01, PNorm = 145.9867, GNorm = 0.2552
Meta loss on this task batch = 2.1035e-01, Meta loss averaged over last 500 steps = 2.7993e-01, PNorm = 145.9917, GNorm = 0.2057
Meta loss on this task batch = 2.8082e-01, Meta loss averaged over last 500 steps = 2.7988e-01, PNorm = 145.9964, GNorm = 0.2371
Meta loss on this task batch = 2.6501e-01, Meta loss averaged over last 500 steps = 2.7985e-01, PNorm = 146.0017, GNorm = 0.2331
Took 114.9168004989624 seconds to complete one epoch of meta training
Took 123.11325597763062 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461700
Epoch 577
Meta loss on this task batch = 2.6085e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 146.0075, GNorm = 0.1876
Meta loss on this task batch = 3.0292e-01, Meta loss averaged over last 500 steps = 2.7987e-01, PNorm = 146.0124, GNorm = 0.2331
Meta loss on this task batch = 3.0400e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 146.0168, GNorm = 0.2388
Meta loss on this task batch = 2.9197e-01, Meta loss averaged over last 500 steps = 2.7992e-01, PNorm = 146.0210, GNorm = 0.2334
Meta loss on this task batch = 2.9385e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 146.0257, GNorm = 0.2216
Meta loss on this task batch = 2.5085e-01, Meta loss averaged over last 500 steps = 2.7998e-01, PNorm = 146.0309, GNorm = 0.2176
Meta loss on this task batch = 2.7038e-01, Meta loss averaged over last 500 steps = 2.7999e-01, PNorm = 146.0368, GNorm = 0.2370
Meta loss on this task batch = 2.5005e-01, Meta loss averaged over last 500 steps = 2.7992e-01, PNorm = 146.0438, GNorm = 0.2231
Meta loss on this task batch = 2.9375e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 146.0509, GNorm = 0.1936
Meta loss on this task batch = 2.7593e-01, Meta loss averaged over last 500 steps = 2.7994e-01, PNorm = 146.0584, GNorm = 0.2919
Meta loss on this task batch = 2.4767e-01, Meta loss averaged over last 500 steps = 2.7994e-01, PNorm = 146.0666, GNorm = 0.2142
Meta loss on this task batch = 3.1355e-01, Meta loss averaged over last 500 steps = 2.7994e-01, PNorm = 146.0749, GNorm = 0.2644
Meta loss on this task batch = 2.9175e-01, Meta loss averaged over last 500 steps = 2.7991e-01, PNorm = 146.0822, GNorm = 0.2592
Meta loss on this task batch = 3.1164e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 146.0886, GNorm = 0.2466
Meta loss on this task batch = 3.0497e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 146.0942, GNorm = 0.2661
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 146.0991, GNorm = 0.1991
Meta loss on this task batch = 2.2633e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 146.1049, GNorm = 0.2078
Meta loss on this task batch = 3.3309e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 146.1100, GNorm = 0.2470
Meta loss on this task batch = 3.0296e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 146.1148, GNorm = 0.2505
Took 113.5370032787323 seconds to complete one epoch of meta training
Took 121.29953694343567 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483004
Epoch 578
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 146.1205, GNorm = 0.2181
Meta loss on this task batch = 2.4856e-01, Meta loss averaged over last 500 steps = 2.8007e-01, PNorm = 146.1271, GNorm = 0.2153
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.7987e-01, PNorm = 146.1335, GNorm = 0.2060
Meta loss on this task batch = 2.6745e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 146.1398, GNorm = 0.2481
Meta loss on this task batch = 2.8856e-01, Meta loss averaged over last 500 steps = 2.7987e-01, PNorm = 146.1460, GNorm = 0.2259
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.7991e-01, PNorm = 146.1512, GNorm = 0.2246
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 146.1574, GNorm = 0.2007
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 2.7984e-01, PNorm = 146.1631, GNorm = 0.2190
Meta loss on this task batch = 2.4851e-01, Meta loss averaged over last 500 steps = 2.7975e-01, PNorm = 146.1692, GNorm = 0.1965
Meta loss on this task batch = 3.1916e-01, Meta loss averaged over last 500 steps = 2.7988e-01, PNorm = 146.1750, GNorm = 0.2479
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.7994e-01, PNorm = 146.1815, GNorm = 0.2807
Meta loss on this task batch = 3.1340e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 146.1887, GNorm = 0.3066
Meta loss on this task batch = 2.4819e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 146.1957, GNorm = 0.2411
Meta loss on this task batch = 2.6800e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 146.2025, GNorm = 0.2126
Meta loss on this task batch = 2.3826e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 146.2093, GNorm = 0.2193
Meta loss on this task batch = 2.7227e-01, Meta loss averaged over last 500 steps = 2.7991e-01, PNorm = 146.2154, GNorm = 0.2257
Meta loss on this task batch = 3.0384e-01, Meta loss averaged over last 500 steps = 2.8003e-01, PNorm = 146.2210, GNorm = 0.2273
Meta loss on this task batch = 2.9362e-01, Meta loss averaged over last 500 steps = 2.8003e-01, PNorm = 146.2258, GNorm = 0.2246
Meta loss on this task batch = 2.9360e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 146.2308, GNorm = 0.3237
Took 117.9487407207489 seconds to complete one epoch of meta training
Took 125.95410418510437 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479840
Epoch 579
Meta loss on this task batch = 2.5626e-01, Meta loss averaged over last 500 steps = 2.8017e-01, PNorm = 146.2363, GNorm = 0.2201
Meta loss on this task batch = 3.1483e-01, Meta loss averaged over last 500 steps = 2.8033e-01, PNorm = 146.2420, GNorm = 0.2486
Meta loss on this task batch = 2.4044e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 146.2482, GNorm = 0.1886
Meta loss on this task batch = 2.7830e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 146.2535, GNorm = 0.2695
Meta loss on this task batch = 2.9500e-01, Meta loss averaged over last 500 steps = 2.8013e-01, PNorm = 146.2589, GNorm = 0.2661
Meta loss on this task batch = 3.1412e-01, Meta loss averaged over last 500 steps = 2.8012e-01, PNorm = 146.2640, GNorm = 0.2784
Meta loss on this task batch = 2.6419e-01, Meta loss averaged over last 500 steps = 2.8013e-01, PNorm = 146.2690, GNorm = 0.2451
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 146.2734, GNorm = 0.2399
Meta loss on this task batch = 2.6250e-01, Meta loss averaged over last 500 steps = 2.8016e-01, PNorm = 146.2785, GNorm = 0.2573
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 2.8013e-01, PNorm = 146.2838, GNorm = 0.2515
Meta loss on this task batch = 2.7208e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 146.2885, GNorm = 0.2366
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.8014e-01, PNorm = 146.2931, GNorm = 0.2317
Meta loss on this task batch = 2.8842e-01, Meta loss averaged over last 500 steps = 2.8020e-01, PNorm = 146.2986, GNorm = 0.2138
Meta loss on this task batch = 2.9285e-01, Meta loss averaged over last 500 steps = 2.8018e-01, PNorm = 146.3033, GNorm = 0.2411
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 2.8006e-01, PNorm = 146.3071, GNorm = 0.2458
Meta loss on this task batch = 2.4417e-01, Meta loss averaged over last 500 steps = 2.8000e-01, PNorm = 146.3109, GNorm = 0.1898
Meta loss on this task batch = 2.6527e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 146.3151, GNorm = 0.2361
Meta loss on this task batch = 2.3427e-01, Meta loss averaged over last 500 steps = 2.7989e-01, PNorm = 146.3198, GNorm = 0.2470
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 2.7997e-01, PNorm = 146.3251, GNorm = 0.2427
Took 116.69947934150696 seconds to complete one epoch of meta training
Took 124.59710240364075 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488407
Epoch 580
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 146.3303, GNorm = 0.2056
Meta loss on this task batch = 3.2507e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 146.3356, GNorm = 0.2897
Meta loss on this task batch = 2.7616e-01, Meta loss averaged over last 500 steps = 2.8008e-01, PNorm = 146.3412, GNorm = 0.2271
Meta loss on this task batch = 2.1011e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 146.3469, GNorm = 0.1991
Meta loss on this task batch = 2.3676e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 146.3540, GNorm = 0.2441
Meta loss on this task batch = 2.9051e-01, Meta loss averaged over last 500 steps = 2.7978e-01, PNorm = 146.3609, GNorm = 0.2561
Meta loss on this task batch = 3.0900e-01, Meta loss averaged over last 500 steps = 2.8001e-01, PNorm = 146.3676, GNorm = 0.2520
Meta loss on this task batch = 2.5896e-01, Meta loss averaged over last 500 steps = 2.7993e-01, PNorm = 146.3744, GNorm = 0.2183
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 2.7994e-01, PNorm = 146.3809, GNorm = 0.2324
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 146.3866, GNorm = 0.2386
Meta loss on this task batch = 2.5979e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 146.3917, GNorm = 0.2328
Meta loss on this task batch = 2.4947e-01, Meta loss averaged over last 500 steps = 2.7990e-01, PNorm = 146.3978, GNorm = 0.2295
Meta loss on this task batch = 2.5787e-01, Meta loss averaged over last 500 steps = 2.7985e-01, PNorm = 146.4036, GNorm = 0.2073
Meta loss on this task batch = 3.3947e-01, Meta loss averaged over last 500 steps = 2.8002e-01, PNorm = 146.4098, GNorm = 0.2549
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 2.8010e-01, PNorm = 146.4149, GNorm = 0.2529
Meta loss on this task batch = 2.4371e-01, Meta loss averaged over last 500 steps = 2.7995e-01, PNorm = 146.4195, GNorm = 0.2174
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 2.7991e-01, PNorm = 146.4232, GNorm = 0.2795
Meta loss on this task batch = 2.6590e-01, Meta loss averaged over last 500 steps = 2.7985e-01, PNorm = 146.4257, GNorm = 0.2421
Meta loss on this task batch = 2.7840e-01, Meta loss averaged over last 500 steps = 2.7981e-01, PNorm = 146.4283, GNorm = 0.2553
Took 114.77256059646606 seconds to complete one epoch of meta training
Took 122.67096853256226 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499078
Epoch 581
Meta loss on this task batch = 2.2986e-01, Meta loss averaged over last 500 steps = 2.7974e-01, PNorm = 146.4309, GNorm = 0.2111
Meta loss on this task batch = 2.9828e-01, Meta loss averaged over last 500 steps = 2.7973e-01, PNorm = 146.4343, GNorm = 0.2198
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.7969e-01, PNorm = 146.4384, GNorm = 0.2100
Meta loss on this task batch = 3.1725e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 146.4432, GNorm = 0.3134
Meta loss on this task batch = 2.8245e-01, Meta loss averaged over last 500 steps = 2.7972e-01, PNorm = 146.4487, GNorm = 0.2231
Meta loss on this task batch = 2.9232e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 146.4543, GNorm = 0.2403
Meta loss on this task batch = 2.8769e-01, Meta loss averaged over last 500 steps = 2.7962e-01, PNorm = 146.4606, GNorm = 0.2512
Meta loss on this task batch = 3.0093e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 146.4668, GNorm = 0.2102
Meta loss on this task batch = 3.1852e-01, Meta loss averaged over last 500 steps = 2.7971e-01, PNorm = 146.4729, GNorm = 0.2038
Meta loss on this task batch = 3.4415e-01, Meta loss averaged over last 500 steps = 2.7982e-01, PNorm = 146.4791, GNorm = 0.2700
Meta loss on this task batch = 2.4653e-01, Meta loss averaged over last 500 steps = 2.7974e-01, PNorm = 146.4863, GNorm = 0.2552
Meta loss on this task batch = 2.1775e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 146.4937, GNorm = 0.2235
Meta loss on this task batch = 2.4864e-01, Meta loss averaged over last 500 steps = 2.7950e-01, PNorm = 146.5017, GNorm = 0.2146
Meta loss on this task batch = 2.8605e-01, Meta loss averaged over last 500 steps = 2.7955e-01, PNorm = 146.5096, GNorm = 0.2447
Meta loss on this task batch = 2.8470e-01, Meta loss averaged over last 500 steps = 2.7966e-01, PNorm = 146.5172, GNorm = 0.2435
Meta loss on this task batch = 2.3264e-01, Meta loss averaged over last 500 steps = 2.7964e-01, PNorm = 146.5247, GNorm = 0.2188
Meta loss on this task batch = 2.6135e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 146.5319, GNorm = 0.2339
Meta loss on this task batch = 3.0825e-01, Meta loss averaged over last 500 steps = 2.7967e-01, PNorm = 146.5383, GNorm = 0.2436
Meta loss on this task batch = 2.6073e-01, Meta loss averaged over last 500 steps = 2.7961e-01, PNorm = 146.5440, GNorm = 0.2575
Took 115.36707377433777 seconds to complete one epoch of meta training
Took 123.51999068260193 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471915
Epoch 582
Meta loss on this task batch = 2.6107e-01, Meta loss averaged over last 500 steps = 2.7959e-01, PNorm = 146.5495, GNorm = 0.2678
Meta loss on this task batch = 2.4776e-01, Meta loss averaged over last 500 steps = 2.7956e-01, PNorm = 146.5549, GNorm = 0.2152
Meta loss on this task batch = 2.4947e-01, Meta loss averaged over last 500 steps = 2.7940e-01, PNorm = 146.5606, GNorm = 0.1996
Meta loss on this task batch = 2.8315e-01, Meta loss averaged over last 500 steps = 2.7942e-01, PNorm = 146.5668, GNorm = 0.2268
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.7924e-01, PNorm = 146.5727, GNorm = 0.2262
Meta loss on this task batch = 2.9739e-01, Meta loss averaged over last 500 steps = 2.7923e-01, PNorm = 146.5772, GNorm = 0.2510
Meta loss on this task batch = 2.7606e-01, Meta loss averaged over last 500 steps = 2.7924e-01, PNorm = 146.5814, GNorm = 0.2287
Meta loss on this task batch = 2.5701e-01, Meta loss averaged over last 500 steps = 2.7924e-01, PNorm = 146.5862, GNorm = 0.1854
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 2.7927e-01, PNorm = 146.5905, GNorm = 0.2161
Meta loss on this task batch = 2.0399e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 146.5957, GNorm = 0.1794
Meta loss on this task batch = 2.9437e-01, Meta loss averaged over last 500 steps = 2.7908e-01, PNorm = 146.6012, GNorm = 0.2284
Meta loss on this task batch = 2.4148e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 146.6063, GNorm = 0.2222
Meta loss on this task batch = 3.5147e-01, Meta loss averaged over last 500 steps = 2.7910e-01, PNorm = 146.6128, GNorm = 0.2972
Meta loss on this task batch = 2.7653e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 146.6194, GNorm = 0.2473
Meta loss on this task batch = 3.1979e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 146.6256, GNorm = 0.2649
Meta loss on this task batch = 2.3972e-01, Meta loss averaged over last 500 steps = 2.7902e-01, PNorm = 146.6315, GNorm = 0.2174
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.7898e-01, PNorm = 146.6371, GNorm = 0.2295
Meta loss on this task batch = 3.0279e-01, Meta loss averaged over last 500 steps = 2.7900e-01, PNorm = 146.6428, GNorm = 0.2227
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 2.7907e-01, PNorm = 146.6490, GNorm = 0.2506
Took 116.07690072059631 seconds to complete one epoch of meta training
Took 124.16975975036621 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485520
Epoch 583
Meta loss on this task batch = 2.2298e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 146.6551, GNorm = 0.2025
Meta loss on this task batch = 2.4213e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 146.6614, GNorm = 0.2296
Meta loss on this task batch = 3.3805e-01, Meta loss averaged over last 500 steps = 2.7911e-01, PNorm = 146.6657, GNorm = 0.2701
Meta loss on this task batch = 2.6808e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 146.6703, GNorm = 0.2184
Meta loss on this task batch = 2.6790e-01, Meta loss averaged over last 500 steps = 2.7921e-01, PNorm = 146.6736, GNorm = 0.2513
Meta loss on this task batch = 2.3737e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 146.6772, GNorm = 0.2073
Meta loss on this task batch = 3.0192e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 146.6809, GNorm = 0.2347
Meta loss on this task batch = 2.5679e-01, Meta loss averaged over last 500 steps = 2.7877e-01, PNorm = 146.6860, GNorm = 0.2113
Meta loss on this task batch = 3.3103e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 146.6905, GNorm = 0.2607
Meta loss on this task batch = 2.8049e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 146.6962, GNorm = 0.2421
Meta loss on this task batch = 2.6449e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 146.7026, GNorm = 0.2256
Meta loss on this task batch = 2.5920e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 146.7094, GNorm = 0.2547
Meta loss on this task batch = 2.9492e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 146.7164, GNorm = 0.2344
Meta loss on this task batch = 2.4456e-01, Meta loss averaged over last 500 steps = 2.7876e-01, PNorm = 146.7239, GNorm = 0.2146
Meta loss on this task batch = 2.5476e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 146.7311, GNorm = 0.2240
Meta loss on this task batch = 3.2176e-01, Meta loss averaged over last 500 steps = 2.7881e-01, PNorm = 146.7372, GNorm = 0.2550
Meta loss on this task batch = 3.0849e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 146.7425, GNorm = 0.2540
Meta loss on this task batch = 2.8026e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 146.7478, GNorm = 0.2435
Meta loss on this task batch = 3.1718e-01, Meta loss averaged over last 500 steps = 2.7886e-01, PNorm = 146.7529, GNorm = 0.3042
Took 113.72227811813354 seconds to complete one epoch of meta training
Took 121.49196791648865 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478393
Epoch 584
Meta loss on this task batch = 3.1564e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 146.7580, GNorm = 0.2259
Meta loss on this task batch = 2.6722e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 146.7630, GNorm = 0.2263
Meta loss on this task batch = 2.6146e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 146.7684, GNorm = 0.1966
Meta loss on this task batch = 3.1033e-01, Meta loss averaged over last 500 steps = 2.7896e-01, PNorm = 146.7739, GNorm = 0.2542
Meta loss on this task batch = 2.9523e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 146.7795, GNorm = 0.2154
Meta loss on this task batch = 2.3271e-01, Meta loss averaged over last 500 steps = 2.7889e-01, PNorm = 146.7854, GNorm = 0.2189
Meta loss on this task batch = 3.3183e-01, Meta loss averaged over last 500 steps = 2.7910e-01, PNorm = 146.7919, GNorm = 0.2207
Meta loss on this task batch = 2.9681e-01, Meta loss averaged over last 500 steps = 2.7920e-01, PNorm = 146.7977, GNorm = 0.2210
Meta loss on this task batch = 2.8161e-01, Meta loss averaged over last 500 steps = 2.7929e-01, PNorm = 146.8035, GNorm = 0.2675
Meta loss on this task batch = 2.4482e-01, Meta loss averaged over last 500 steps = 2.7914e-01, PNorm = 146.8074, GNorm = 0.3676
Meta loss on this task batch = 2.7119e-01, Meta loss averaged over last 500 steps = 2.7916e-01, PNorm = 146.8117, GNorm = 0.2004
Meta loss on this task batch = 2.8217e-01, Meta loss averaged over last 500 steps = 2.7916e-01, PNorm = 146.8169, GNorm = 0.1982
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.7922e-01, PNorm = 146.8213, GNorm = 0.2323
Meta loss on this task batch = 2.5474e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 146.8262, GNorm = 0.2392
Meta loss on this task batch = 2.9672e-01, Meta loss averaged over last 500 steps = 2.7905e-01, PNorm = 146.8312, GNorm = 0.2068
Meta loss on this task batch = 2.1075e-01, Meta loss averaged over last 500 steps = 2.7888e-01, PNorm = 146.8369, GNorm = 0.1796
Meta loss on this task batch = 3.2694e-01, Meta loss averaged over last 500 steps = 2.7893e-01, PNorm = 146.8418, GNorm = 0.2664
Meta loss on this task batch = 2.7960e-01, Meta loss averaged over last 500 steps = 2.7899e-01, PNorm = 146.8464, GNorm = 0.2154
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 146.8513, GNorm = 0.2675
Took 115.00553894042969 seconds to complete one epoch of meta training
Took 123.02429556846619 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497682
Epoch 585
Meta loss on this task batch = 2.4501e-01, Meta loss averaged over last 500 steps = 2.7883e-01, PNorm = 146.8563, GNorm = 0.2251
Meta loss on this task batch = 2.8712e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 146.8613, GNorm = 0.2346
Meta loss on this task batch = 2.4450e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 146.8672, GNorm = 0.1800
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 146.8724, GNorm = 0.2176
Meta loss on this task batch = 3.1495e-01, Meta loss averaged over last 500 steps = 2.7863e-01, PNorm = 146.8780, GNorm = 0.2563
Meta loss on this task batch = 2.6606e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 146.8826, GNorm = 0.2419
Meta loss on this task batch = 2.4416e-01, Meta loss averaged over last 500 steps = 2.7847e-01, PNorm = 146.8870, GNorm = 0.2090
Meta loss on this task batch = 3.2925e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 146.8916, GNorm = 0.2592
Meta loss on this task batch = 2.7696e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 146.8961, GNorm = 0.2313
Meta loss on this task batch = 2.2551e-01, Meta loss averaged over last 500 steps = 2.7873e-01, PNorm = 146.9020, GNorm = 0.2205
Meta loss on this task batch = 3.6302e-01, Meta loss averaged over last 500 steps = 2.7892e-01, PNorm = 146.9087, GNorm = 0.2657
Meta loss on this task batch = 3.1971e-01, Meta loss averaged over last 500 steps = 2.7904e-01, PNorm = 146.9155, GNorm = 0.2161
Meta loss on this task batch = 2.5355e-01, Meta loss averaged over last 500 steps = 2.7901e-01, PNorm = 146.9229, GNorm = 0.2045
Meta loss on this task batch = 2.4636e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 146.9300, GNorm = 0.2427
Meta loss on this task batch = 3.2362e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 146.9368, GNorm = 0.2199
Meta loss on this task batch = 3.0347e-01, Meta loss averaged over last 500 steps = 2.7903e-01, PNorm = 146.9433, GNorm = 0.2181
Meta loss on this task batch = 2.7378e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 146.9503, GNorm = 0.1951
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 146.9565, GNorm = 0.2355
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 146.9610, GNorm = 0.2908
Took 116.91588759422302 seconds to complete one epoch of meta training
Took 124.51839590072632 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498541
Epoch 586
Meta loss on this task batch = 2.7339e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 146.9651, GNorm = 0.2240
Meta loss on this task batch = 2.3698e-01, Meta loss averaged over last 500 steps = 2.7866e-01, PNorm = 146.9693, GNorm = 0.1900
Meta loss on this task batch = 2.7206e-01, Meta loss averaged over last 500 steps = 2.7865e-01, PNorm = 146.9740, GNorm = 0.1902
Meta loss on this task batch = 2.5727e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 146.9785, GNorm = 0.1970
Meta loss on this task batch = 2.6450e-01, Meta loss averaged over last 500 steps = 2.7852e-01, PNorm = 146.9837, GNorm = 0.2518
Meta loss on this task batch = 2.1336e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 146.9894, GNorm = 0.2032
Meta loss on this task batch = 2.3060e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 146.9959, GNorm = 0.1937
Meta loss on this task batch = 2.9242e-01, Meta loss averaged over last 500 steps = 2.7836e-01, PNorm = 147.0030, GNorm = 0.2060
Meta loss on this task batch = 2.8370e-01, Meta loss averaged over last 500 steps = 2.7844e-01, PNorm = 147.0103, GNorm = 0.2210
Meta loss on this task batch = 3.2720e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 147.0173, GNorm = 0.2352
Meta loss on this task batch = 2.3293e-01, Meta loss averaged over last 500 steps = 2.7845e-01, PNorm = 147.0234, GNorm = 0.2362
Meta loss on this task batch = 3.0032e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 147.0295, GNorm = 0.2174
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 147.0359, GNorm = 0.2249
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.7855e-01, PNorm = 147.0435, GNorm = 0.2480
Meta loss on this task batch = 3.0973e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 147.0503, GNorm = 0.2596
Meta loss on this task batch = 3.2122e-01, Meta loss averaged over last 500 steps = 2.7871e-01, PNorm = 147.0560, GNorm = 0.2874
Meta loss on this task batch = 2.6397e-01, Meta loss averaged over last 500 steps = 2.7869e-01, PNorm = 147.0615, GNorm = 0.2341
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 147.0671, GNorm = 0.2305
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 147.0717, GNorm = 0.2949
Took 116.07773661613464 seconds to complete one epoch of meta training
Took 123.96741509437561 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470981
Epoch 587
Meta loss on this task batch = 3.1698e-01, Meta loss averaged over last 500 steps = 2.7875e-01, PNorm = 147.0764, GNorm = 0.2334
Meta loss on this task batch = 3.1609e-01, Meta loss averaged over last 500 steps = 2.7879e-01, PNorm = 147.0812, GNorm = 0.2199
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 147.0866, GNorm = 0.2221
Meta loss on this task batch = 2.4707e-01, Meta loss averaged over last 500 steps = 2.7882e-01, PNorm = 147.0922, GNorm = 0.2043
Meta loss on this task batch = 2.4852e-01, Meta loss averaged over last 500 steps = 2.7868e-01, PNorm = 147.0984, GNorm = 0.2449
Meta loss on this task batch = 3.1704e-01, Meta loss averaged over last 500 steps = 2.7885e-01, PNorm = 147.1038, GNorm = 0.2565
Meta loss on this task batch = 2.9109e-01, Meta loss averaged over last 500 steps = 2.7884e-01, PNorm = 147.1090, GNorm = 0.2466
Meta loss on this task batch = 3.0213e-01, Meta loss averaged over last 500 steps = 2.7894e-01, PNorm = 147.1143, GNorm = 0.2265
Meta loss on this task batch = 2.5538e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 147.1197, GNorm = 0.2160
Meta loss on this task batch = 2.9501e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 147.1245, GNorm = 0.2671
Meta loss on this task batch = 2.9129e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 147.1290, GNorm = 0.2547
Meta loss on this task batch = 2.6348e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 147.1333, GNorm = 0.1951
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 2.7891e-01, PNorm = 147.1379, GNorm = 0.2126
Meta loss on this task batch = 2.7789e-01, Meta loss averaged over last 500 steps = 2.7897e-01, PNorm = 147.1427, GNorm = 0.2032
Meta loss on this task batch = 2.5217e-01, Meta loss averaged over last 500 steps = 2.7890e-01, PNorm = 147.1475, GNorm = 0.2177
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.7895e-01, PNorm = 147.1526, GNorm = 0.2375
Meta loss on this task batch = 2.3510e-01, Meta loss averaged over last 500 steps = 2.7887e-01, PNorm = 147.1581, GNorm = 0.2554
Meta loss on this task batch = 2.3864e-01, Meta loss averaged over last 500 steps = 2.7880e-01, PNorm = 147.1638, GNorm = 0.2061
Meta loss on this task batch = 2.5980e-01, Meta loss averaged over last 500 steps = 2.7878e-01, PNorm = 147.1693, GNorm = 0.2362
Took 115.80010175704956 seconds to complete one epoch of meta training
Took 123.6374101638794 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502387
Epoch 588
Meta loss on this task batch = 2.5724e-01, Meta loss averaged over last 500 steps = 2.7867e-01, PNorm = 147.1752, GNorm = 0.2085
Meta loss on this task batch = 2.7722e-01, Meta loss averaged over last 500 steps = 2.7861e-01, PNorm = 147.1816, GNorm = 0.2434
Meta loss on this task batch = 2.6003e-01, Meta loss averaged over last 500 steps = 2.7857e-01, PNorm = 147.1885, GNorm = 0.2139
Meta loss on this task batch = 2.9484e-01, Meta loss averaged over last 500 steps = 2.7854e-01, PNorm = 147.1951, GNorm = 0.2711
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.7851e-01, PNorm = 147.2007, GNorm = 0.2416
Meta loss on this task batch = 2.2688e-01, Meta loss averaged over last 500 steps = 2.7837e-01, PNorm = 147.2056, GNorm = 0.2479
Meta loss on this task batch = 3.0627e-01, Meta loss averaged over last 500 steps = 2.7842e-01, PNorm = 147.2106, GNorm = 0.2665
Meta loss on this task batch = 2.3671e-01, Meta loss averaged over last 500 steps = 2.7829e-01, PNorm = 147.2160, GNorm = 0.2144
Meta loss on this task batch = 3.1054e-01, Meta loss averaged over last 500 steps = 2.7828e-01, PNorm = 147.2216, GNorm = 0.2669
Meta loss on this task batch = 2.8598e-01, Meta loss averaged over last 500 steps = 2.7831e-01, PNorm = 147.2273, GNorm = 0.2232
Meta loss on this task batch = 3.1117e-01, Meta loss averaged over last 500 steps = 2.7843e-01, PNorm = 147.2335, GNorm = 0.2588
Meta loss on this task batch = 2.4932e-01, Meta loss averaged over last 500 steps = 2.7835e-01, PNorm = 147.2403, GNorm = 0.2199
Meta loss on this task batch = 2.9101e-01, Meta loss averaged over last 500 steps = 2.7834e-01, PNorm = 147.2473, GNorm = 0.2558
Meta loss on this task batch = 2.5087e-01, Meta loss averaged over last 500 steps = 2.7821e-01, PNorm = 147.2539, GNorm = 0.2368
Meta loss on this task batch = 2.9609e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 147.2612, GNorm = 0.2444
Meta loss on this task batch = 2.4899e-01, Meta loss averaged over last 500 steps = 2.7818e-01, PNorm = 147.2682, GNorm = 0.2029
Meta loss on this task batch = 3.1235e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 147.2753, GNorm = 0.2400
Meta loss on this task batch = 3.3306e-01, Meta loss averaged over last 500 steps = 2.7819e-01, PNorm = 147.2823, GNorm = 0.2535
Meta loss on this task batch = 2.0634e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 147.2899, GNorm = 0.2452
Took 114.56432509422302 seconds to complete one epoch of meta training
Took 121.88301420211792 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461781
Epoch 589
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 147.2974, GNorm = 0.2215
Meta loss on this task batch = 2.6005e-01, Meta loss averaged over last 500 steps = 2.7793e-01, PNorm = 147.3039, GNorm = 0.2157
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.7804e-01, PNorm = 147.3088, GNorm = 0.2654
Meta loss on this task batch = 2.5954e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 147.3136, GNorm = 0.2064
Meta loss on this task batch = 2.3907e-01, Meta loss averaged over last 500 steps = 2.7790e-01, PNorm = 147.3190, GNorm = 0.1952
Meta loss on this task batch = 2.8053e-01, Meta loss averaged over last 500 steps = 2.7798e-01, PNorm = 147.3246, GNorm = 0.2409
Meta loss on this task batch = 2.7689e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 147.3300, GNorm = 0.2544
Meta loss on this task batch = 2.4099e-01, Meta loss averaged over last 500 steps = 2.7782e-01, PNorm = 147.3355, GNorm = 0.1949
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 147.3402, GNorm = 0.2295
Meta loss on this task batch = 2.5492e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 147.3458, GNorm = 0.2140
Meta loss on this task batch = 2.7067e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 147.3512, GNorm = 0.2345
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 147.3573, GNorm = 0.2189
Meta loss on this task batch = 3.9885e-01, Meta loss averaged over last 500 steps = 2.7788e-01, PNorm = 147.3633, GNorm = 0.2873
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 147.3690, GNorm = 0.2167
Meta loss on this task batch = 1.9020e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 147.3752, GNorm = 0.1728
Meta loss on this task batch = 3.1500e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 147.3811, GNorm = 0.2468
Meta loss on this task batch = 2.5422e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 147.3879, GNorm = 0.2166
Meta loss on this task batch = 2.2318e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 147.3950, GNorm = 0.2150
Meta loss on this task batch = 3.2912e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 147.4016, GNorm = 0.3020
Took 114.94142246246338 seconds to complete one epoch of meta training
Took 122.47604131698608 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478302
Epoch 590
Meta loss on this task batch = 2.5426e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 147.4087, GNorm = 0.2269
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 2.7762e-01, PNorm = 147.4150, GNorm = 0.2403
Meta loss on this task batch = 2.9406e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 147.4209, GNorm = 0.2314
Meta loss on this task batch = 2.4292e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 147.4267, GNorm = 0.2028
Meta loss on this task batch = 2.6599e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 147.4330, GNorm = 0.2278
Meta loss on this task batch = 2.1768e-01, Meta loss averaged over last 500 steps = 2.7760e-01, PNorm = 147.4399, GNorm = 0.2084
Meta loss on this task batch = 2.9233e-01, Meta loss averaged over last 500 steps = 2.7762e-01, PNorm = 147.4462, GNorm = 0.2329
Meta loss on this task batch = 2.4537e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 147.4523, GNorm = 0.2319
Meta loss on this task batch = 2.7570e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 147.4583, GNorm = 0.2177
Meta loss on this task batch = 2.8708e-01, Meta loss averaged over last 500 steps = 2.7754e-01, PNorm = 147.4653, GNorm = 0.2576
Meta loss on this task batch = 3.3424e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 147.4718, GNorm = 0.2629
Meta loss on this task batch = 2.3025e-01, Meta loss averaged over last 500 steps = 2.7770e-01, PNorm = 147.4782, GNorm = 0.1884
Meta loss on this task batch = 2.3181e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 147.4857, GNorm = 0.2556
Meta loss on this task batch = 2.7764e-01, Meta loss averaged over last 500 steps = 2.7749e-01, PNorm = 147.4924, GNorm = 0.2604
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 2.7744e-01, PNorm = 147.4992, GNorm = 0.2258
Meta loss on this task batch = 3.7081e-01, Meta loss averaged over last 500 steps = 2.7754e-01, PNorm = 147.5045, GNorm = 0.3016
Meta loss on this task batch = 2.6679e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 147.5092, GNorm = 0.2416
Meta loss on this task batch = 2.8953e-01, Meta loss averaged over last 500 steps = 2.7762e-01, PNorm = 147.5136, GNorm = 0.2551
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 147.5186, GNorm = 0.2403
Took 115.60342812538147 seconds to complete one epoch of meta training
Took 123.4199571609497 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499046
Epoch 591
Meta loss on this task batch = 2.9577e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 147.5236, GNorm = 0.2393
Meta loss on this task batch = 2.5018e-01, Meta loss averaged over last 500 steps = 2.7760e-01, PNorm = 147.5289, GNorm = 0.2243
Meta loss on this task batch = 2.9121e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 147.5343, GNorm = 0.2554
Meta loss on this task batch = 2.6484e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 147.5393, GNorm = 0.2144
Meta loss on this task batch = 2.4565e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 147.5456, GNorm = 0.2049
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 2.7745e-01, PNorm = 147.5504, GNorm = 0.2382
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 147.5553, GNorm = 0.2339
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 2.7742e-01, PNorm = 147.5598, GNorm = 0.2469
Meta loss on this task batch = 3.2027e-01, Meta loss averaged over last 500 steps = 2.7766e-01, PNorm = 147.5643, GNorm = 0.2115
Meta loss on this task batch = 2.4465e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 147.5690, GNorm = 0.1976
Meta loss on this task batch = 2.5028e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 147.5745, GNorm = 0.1872
Meta loss on this task batch = 2.2703e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 147.5788, GNorm = 0.2017
Meta loss on this task batch = 2.2408e-01, Meta loss averaged over last 500 steps = 2.7714e-01, PNorm = 147.5834, GNorm = 0.1886
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 147.5880, GNorm = 0.2501
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 147.5937, GNorm = 0.2358
Meta loss on this task batch = 3.3361e-01, Meta loss averaged over last 500 steps = 2.7716e-01, PNorm = 147.5998, GNorm = 0.2433
Meta loss on this task batch = 3.3231e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 147.6059, GNorm = 0.2332
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.7735e-01, PNorm = 147.6120, GNorm = 0.1888
Meta loss on this task batch = 3.5149e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 147.6162, GNorm = 0.3324
Took 116.09687566757202 seconds to complete one epoch of meta training
Took 123.76911473274231 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492663
Epoch 592
Meta loss on this task batch = 2.5672e-01, Meta loss averaged over last 500 steps = 2.7737e-01, PNorm = 147.6209, GNorm = 0.2245
Meta loss on this task batch = 3.2108e-01, Meta loss averaged over last 500 steps = 2.7752e-01, PNorm = 147.6255, GNorm = 0.2338
Meta loss on this task batch = 2.7287e-01, Meta loss averaged over last 500 steps = 2.7749e-01, PNorm = 147.6302, GNorm = 0.2174
Meta loss on this task batch = 3.3031e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 147.6351, GNorm = 0.2585
Meta loss on this task batch = 2.7095e-01, Meta loss averaged over last 500 steps = 2.7752e-01, PNorm = 147.6411, GNorm = 0.2372
Meta loss on this task batch = 2.5046e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 147.6468, GNorm = 0.2410
Meta loss on this task batch = 2.7647e-01, Meta loss averaged over last 500 steps = 2.7754e-01, PNorm = 147.6527, GNorm = 0.1882
Meta loss on this task batch = 2.8239e-01, Meta loss averaged over last 500 steps = 2.7749e-01, PNorm = 147.6597, GNorm = 0.2464
Meta loss on this task batch = 3.0932e-01, Meta loss averaged over last 500 steps = 2.7745e-01, PNorm = 147.6667, GNorm = 0.3342
Meta loss on this task batch = 3.0788e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 147.6737, GNorm = 0.2013
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 147.6807, GNorm = 0.2417
Meta loss on this task batch = 2.9123e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 147.6872, GNorm = 0.2471
Meta loss on this task batch = 2.6314e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 147.6931, GNorm = 0.2199
Meta loss on this task batch = 2.2009e-01, Meta loss averaged over last 500 steps = 2.7752e-01, PNorm = 147.6993, GNorm = 0.2219
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 2.7754e-01, PNorm = 147.7049, GNorm = 0.2724
Meta loss on this task batch = 2.4042e-01, Meta loss averaged over last 500 steps = 2.7758e-01, PNorm = 147.7106, GNorm = 0.1899
Meta loss on this task batch = 3.0096e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 147.7145, GNorm = 0.2637
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 147.7180, GNorm = 0.2469
Meta loss on this task batch = 2.9370e-01, Meta loss averaged over last 500 steps = 2.7758e-01, PNorm = 147.7219, GNorm = 0.2755
Took 115.42931365966797 seconds to complete one epoch of meta training
Took 123.4502375125885 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495005
Epoch 593
Meta loss on this task batch = 2.4350e-01, Meta loss averaged over last 500 steps = 2.7756e-01, PNorm = 147.7260, GNorm = 0.2410
Meta loss on this task batch = 2.7546e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 147.7304, GNorm = 0.2051
Meta loss on this task batch = 3.2711e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 147.7355, GNorm = 0.2391
Meta loss on this task batch = 2.4156e-01, Meta loss averaged over last 500 steps = 2.7748e-01, PNorm = 147.7416, GNorm = 0.1791
Meta loss on this task batch = 2.6769e-01, Meta loss averaged over last 500 steps = 2.7748e-01, PNorm = 147.7477, GNorm = 0.2312
Meta loss on this task batch = 2.6897e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 147.7545, GNorm = 0.2630
Meta loss on this task batch = 2.7474e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 147.7612, GNorm = 0.2602
Meta loss on this task batch = 2.8185e-01, Meta loss averaged over last 500 steps = 2.7760e-01, PNorm = 147.7679, GNorm = 0.2606
Meta loss on this task batch = 3.1011e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 147.7734, GNorm = 0.2402
Meta loss on this task batch = 2.8820e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 147.7783, GNorm = 0.2563
Meta loss on this task batch = 3.4516e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 147.7832, GNorm = 0.2513
Meta loss on this task batch = 3.0967e-01, Meta loss averaged over last 500 steps = 2.7813e-01, PNorm = 147.7881, GNorm = 0.2262
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 147.7929, GNorm = 0.1915
Meta loss on this task batch = 2.7004e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 147.7977, GNorm = 0.2338
Meta loss on this task batch = 2.8292e-01, Meta loss averaged over last 500 steps = 2.7773e-01, PNorm = 147.8025, GNorm = 0.2173
Meta loss on this task batch = 2.4893e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 147.8075, GNorm = 0.2329
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 147.8136, GNorm = 0.2884
Meta loss on this task batch = 3.2422e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 147.8200, GNorm = 0.2229
Meta loss on this task batch = 2.7313e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 147.8254, GNorm = 0.2567
Took 113.1660635471344 seconds to complete one epoch of meta training
Took 120.60731363296509 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513878
Epoch 594
Meta loss on this task batch = 2.5638e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 147.8311, GNorm = 0.2219
Meta loss on this task batch = 2.4288e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 147.8362, GNorm = 0.2305
Meta loss on this task batch = 2.6071e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 147.8408, GNorm = 0.2044
Meta loss on this task batch = 2.7200e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 147.8452, GNorm = 0.2182
Meta loss on this task batch = 3.4134e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 147.8487, GNorm = 0.2400
Meta loss on this task batch = 2.6485e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 147.8533, GNorm = 0.2383
Meta loss on this task batch = 2.5323e-01, Meta loss averaged over last 500 steps = 2.7775e-01, PNorm = 147.8583, GNorm = 0.2001
Meta loss on this task batch = 2.2423e-01, Meta loss averaged over last 500 steps = 2.7758e-01, PNorm = 147.8641, GNorm = 0.2487
Meta loss on this task batch = 3.0527e-01, Meta loss averaged over last 500 steps = 2.7770e-01, PNorm = 147.8691, GNorm = 0.2113
Meta loss on this task batch = 2.8995e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 147.8744, GNorm = 0.2224
Meta loss on this task batch = 2.7351e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 147.8795, GNorm = 0.2295
Meta loss on this task batch = 3.2285e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 147.8854, GNorm = 0.2425
Meta loss on this task batch = 3.2255e-01, Meta loss averaged over last 500 steps = 2.7781e-01, PNorm = 147.8911, GNorm = 0.2567
Meta loss on this task batch = 2.1283e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 147.8980, GNorm = 0.1949
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 147.9040, GNorm = 0.2863
Meta loss on this task batch = 2.7381e-01, Meta loss averaged over last 500 steps = 2.7749e-01, PNorm = 147.9100, GNorm = 0.2296
Meta loss on this task batch = 2.8272e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 147.9169, GNorm = 0.2480
Meta loss on this task batch = 3.0849e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 147.9232, GNorm = 0.2569
Meta loss on this task batch = 3.0162e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 147.9289, GNorm = 0.2763
Took 112.70437955856323 seconds to complete one epoch of meta training
Took 120.2515480518341 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500372
Epoch 595
Meta loss on this task batch = 2.4856e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 147.9343, GNorm = 0.2371
Meta loss on this task batch = 2.4293e-01, Meta loss averaged over last 500 steps = 2.7773e-01, PNorm = 147.9395, GNorm = 0.2041
Meta loss on this task batch = 2.5880e-01, Meta loss averaged over last 500 steps = 2.7779e-01, PNorm = 147.9446, GNorm = 0.2073
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 147.9503, GNorm = 0.2031
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 147.9559, GNorm = 0.2246
Meta loss on this task batch = 2.5971e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 147.9611, GNorm = 0.2417
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.7760e-01, PNorm = 147.9670, GNorm = 0.2721
Meta loss on this task batch = 2.6820e-01, Meta loss averaged over last 500 steps = 2.7762e-01, PNorm = 147.9723, GNorm = 0.2036
Meta loss on this task batch = 2.6786e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 147.9785, GNorm = 0.2064
Meta loss on this task batch = 2.6904e-01, Meta loss averaged over last 500 steps = 2.7755e-01, PNorm = 147.9845, GNorm = 0.2376
Meta loss on this task batch = 2.6199e-01, Meta loss averaged over last 500 steps = 2.7751e-01, PNorm = 147.9894, GNorm = 0.2425
Meta loss on this task batch = 3.1424e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 147.9939, GNorm = 0.2652
Meta loss on this task batch = 2.5805e-01, Meta loss averaged over last 500 steps = 2.7761e-01, PNorm = 147.9992, GNorm = 0.2015
Meta loss on this task batch = 2.6899e-01, Meta loss averaged over last 500 steps = 2.7771e-01, PNorm = 148.0044, GNorm = 0.2491
Meta loss on this task batch = 2.7202e-01, Meta loss averaged over last 500 steps = 2.7772e-01, PNorm = 148.0105, GNorm = 0.2328
Meta loss on this task batch = 2.8976e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 148.0170, GNorm = 0.2146
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 148.0230, GNorm = 0.2118
Meta loss on this task batch = 2.9861e-01, Meta loss averaged over last 500 steps = 2.7771e-01, PNorm = 148.0296, GNorm = 0.2384
Meta loss on this task batch = 3.7807e-01, Meta loss averaged over last 500 steps = 2.7783e-01, PNorm = 148.0361, GNorm = 0.3077
Took 118.59284782409668 seconds to complete one epoch of meta training
Took 126.46841764450073 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482396
Epoch 596
Meta loss on this task batch = 3.3279e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 148.0424, GNorm = 0.2845
Meta loss on this task batch = 2.8128e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 148.0485, GNorm = 0.2288
Meta loss on this task batch = 2.8656e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 148.0555, GNorm = 0.2253
Meta loss on this task batch = 2.4842e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 148.0628, GNorm = 0.2092
Meta loss on this task batch = 3.0608e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 148.0706, GNorm = 0.2114
Meta loss on this task batch = 2.9482e-01, Meta loss averaged over last 500 steps = 2.7810e-01, PNorm = 148.0774, GNorm = 0.2407
Meta loss on this task batch = 2.8718e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 148.0824, GNorm = 0.2661
Meta loss on this task batch = 2.5253e-01, Meta loss averaged over last 500 steps = 2.7795e-01, PNorm = 148.0879, GNorm = 0.1979
Meta loss on this task batch = 2.8558e-01, Meta loss averaged over last 500 steps = 2.7792e-01, PNorm = 148.0933, GNorm = 0.2210
Meta loss on this task batch = 2.1272e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 148.0998, GNorm = 0.1863
Meta loss on this task batch = 2.2102e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 148.1063, GNorm = 0.2129
Meta loss on this task batch = 3.4886e-01, Meta loss averaged over last 500 steps = 2.7809e-01, PNorm = 148.1110, GNorm = 0.2982
Meta loss on this task batch = 2.5625e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 148.1159, GNorm = 0.2208
Meta loss on this task batch = 2.5437e-01, Meta loss averaged over last 500 steps = 2.7794e-01, PNorm = 148.1216, GNorm = 0.2242
Meta loss on this task batch = 2.5166e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 148.1272, GNorm = 0.2010
Meta loss on this task batch = 3.0772e-01, Meta loss averaged over last 500 steps = 2.7784e-01, PNorm = 148.1323, GNorm = 0.2742
Meta loss on this task batch = 2.8014e-01, Meta loss averaged over last 500 steps = 2.7787e-01, PNorm = 148.1360, GNorm = 0.3289
Meta loss on this task batch = 1.9523e-01, Meta loss averaged over last 500 steps = 2.7773e-01, PNorm = 148.1404, GNorm = 0.1762
Meta loss on this task batch = 3.6537e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 148.1452, GNorm = 0.4314
Took 112.19755506515503 seconds to complete one epoch of meta training
Took 120.51995134353638 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470751
Epoch 597
Meta loss on this task batch = 2.9228e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 148.1497, GNorm = 0.2281
Meta loss on this task batch = 2.7660e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 148.1545, GNorm = 0.2410
Meta loss on this task batch = 2.6014e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 148.1591, GNorm = 0.2313
Meta loss on this task batch = 3.4599e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 148.1635, GNorm = 0.2916
Meta loss on this task batch = 2.6695e-01, Meta loss averaged over last 500 steps = 2.7796e-01, PNorm = 148.1685, GNorm = 0.2091
Meta loss on this task batch = 2.4572e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 148.1731, GNorm = 0.2484
Meta loss on this task batch = 2.3921e-01, Meta loss averaged over last 500 steps = 2.7783e-01, PNorm = 148.1783, GNorm = 0.2190
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.7769e-01, PNorm = 148.1830, GNorm = 0.2135
Meta loss on this task batch = 3.0278e-01, Meta loss averaged over last 500 steps = 2.7774e-01, PNorm = 148.1880, GNorm = 0.2279
Meta loss on this task batch = 3.0846e-01, Meta loss averaged over last 500 steps = 2.7797e-01, PNorm = 148.1933, GNorm = 0.2741
Meta loss on this task batch = 2.6532e-01, Meta loss averaged over last 500 steps = 2.7803e-01, PNorm = 148.1991, GNorm = 0.2232
Meta loss on this task batch = 2.9128e-01, Meta loss averaged over last 500 steps = 2.7808e-01, PNorm = 148.2049, GNorm = 0.2265
Meta loss on this task batch = 2.9978e-01, Meta loss averaged over last 500 steps = 2.7805e-01, PNorm = 148.2113, GNorm = 0.2702
Meta loss on this task batch = 2.3885e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 148.2184, GNorm = 0.2160
Meta loss on this task batch = 2.3394e-01, Meta loss averaged over last 500 steps = 2.7802e-01, PNorm = 148.2250, GNorm = 0.1946
Meta loss on this task batch = 2.9507e-01, Meta loss averaged over last 500 steps = 2.7801e-01, PNorm = 148.2319, GNorm = 0.2736
Meta loss on this task batch = 2.9567e-01, Meta loss averaged over last 500 steps = 2.7800e-01, PNorm = 148.2373, GNorm = 0.2350
Meta loss on this task batch = 2.3815e-01, Meta loss averaged over last 500 steps = 2.7789e-01, PNorm = 148.2423, GNorm = 0.2494
Meta loss on this task batch = 2.9706e-01, Meta loss averaged over last 500 steps = 2.7791e-01, PNorm = 148.2470, GNorm = 0.2796
Took 114.78245329856873 seconds to complete one epoch of meta training
Took 122.84063529968262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493561
Epoch 598
Meta loss on this task batch = 2.0860e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 148.2525, GNorm = 0.1808
Meta loss on this task batch = 2.9654e-01, Meta loss averaged over last 500 steps = 2.7765e-01, PNorm = 148.2578, GNorm = 0.2409
Meta loss on this task batch = 2.4469e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 148.2642, GNorm = 0.2113
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.7763e-01, PNorm = 148.2701, GNorm = 0.2326
Meta loss on this task batch = 2.9828e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 148.2753, GNorm = 0.2174
Meta loss on this task batch = 2.9262e-01, Meta loss averaged over last 500 steps = 2.7767e-01, PNorm = 148.2801, GNorm = 0.2362
Meta loss on this task batch = 2.6347e-01, Meta loss averaged over last 500 steps = 2.7768e-01, PNorm = 148.2848, GNorm = 0.2222
Meta loss on this task batch = 3.1052e-01, Meta loss averaged over last 500 steps = 2.7771e-01, PNorm = 148.2893, GNorm = 0.2679
Meta loss on this task batch = 2.5981e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 148.2942, GNorm = 0.2313
Meta loss on this task batch = 2.8461e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 148.2994, GNorm = 0.2113
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 148.3049, GNorm = 0.2234
Meta loss on this task batch = 3.4855e-01, Meta loss averaged over last 500 steps = 2.7777e-01, PNorm = 148.3097, GNorm = 0.2613
Meta loss on this task batch = 2.8787e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 148.3147, GNorm = 0.2365
Meta loss on this task batch = 3.1323e-01, Meta loss averaged over last 500 steps = 2.7778e-01, PNorm = 148.3201, GNorm = 0.2354
Meta loss on this task batch = 2.4257e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 148.3263, GNorm = 0.2027
Meta loss on this task batch = 2.9703e-01, Meta loss averaged over last 500 steps = 2.7776e-01, PNorm = 148.3315, GNorm = 0.2472
Meta loss on this task batch = 2.9520e-01, Meta loss averaged over last 500 steps = 2.7786e-01, PNorm = 148.3368, GNorm = 0.2272
Meta loss on this task batch = 2.6055e-01, Meta loss averaged over last 500 steps = 2.7785e-01, PNorm = 148.3427, GNorm = 0.2112
Meta loss on this task batch = 1.7079e-01, Meta loss averaged over last 500 steps = 2.7764e-01, PNorm = 148.3487, GNorm = 0.2465
Took 116.96290516853333 seconds to complete one epoch of meta training
Took 125.02712798118591 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504529
Epoch 599
Meta loss on this task batch = 2.6996e-01, Meta loss averaged over last 500 steps = 2.7757e-01, PNorm = 148.3554, GNorm = 0.2132
Meta loss on this task batch = 2.5970e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 148.3631, GNorm = 0.2660
Meta loss on this task batch = 2.3246e-01, Meta loss averaged over last 500 steps = 2.7748e-01, PNorm = 148.3708, GNorm = 0.2339
Meta loss on this task batch = 2.5624e-01, Meta loss averaged over last 500 steps = 2.7738e-01, PNorm = 148.3781, GNorm = 0.2511
Meta loss on this task batch = 2.3096e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 148.3856, GNorm = 0.2351
Meta loss on this task batch = 2.2314e-01, Meta loss averaged over last 500 steps = 2.7713e-01, PNorm = 148.3932, GNorm = 0.2019
Meta loss on this task batch = 3.1585e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 148.3997, GNorm = 0.2656
Meta loss on this task batch = 1.9525e-01, Meta loss averaged over last 500 steps = 2.7708e-01, PNorm = 148.4055, GNorm = 0.2034
Meta loss on this task batch = 2.4562e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 148.4114, GNorm = 0.2212
Meta loss on this task batch = 3.0101e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 148.4175, GNorm = 0.2415
Meta loss on this task batch = 2.6139e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 148.4234, GNorm = 0.2527
Meta loss on this task batch = 3.0184e-01, Meta loss averaged over last 500 steps = 2.7714e-01, PNorm = 148.4288, GNorm = 0.2206
Meta loss on this task batch = 3.1287e-01, Meta loss averaged over last 500 steps = 2.7709e-01, PNorm = 148.4337, GNorm = 0.2341
Meta loss on this task batch = 3.1582e-01, Meta loss averaged over last 500 steps = 2.7704e-01, PNorm = 148.4382, GNorm = 0.2530
Meta loss on this task batch = 3.1328e-01, Meta loss averaged over last 500 steps = 2.7711e-01, PNorm = 148.4426, GNorm = 0.2691
Meta loss on this task batch = 2.5077e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 148.4466, GNorm = 0.2349
Meta loss on this task batch = 2.8531e-01, Meta loss averaged over last 500 steps = 2.7713e-01, PNorm = 148.4517, GNorm = 0.2298
Meta loss on this task batch = 2.8537e-01, Meta loss averaged over last 500 steps = 2.7712e-01, PNorm = 148.4575, GNorm = 0.2268
Meta loss on this task batch = 2.6033e-01, Meta loss averaged over last 500 steps = 2.7711e-01, PNorm = 148.4637, GNorm = 0.2488
Took 112.3892810344696 seconds to complete one epoch of meta training
Took 119.83362102508545 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497432
Epoch 600
Meta loss on this task batch = 2.4593e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 148.4715, GNorm = 0.2216
Meta loss on this task batch = 3.1657e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 148.4773, GNorm = 0.2554
Meta loss on this task batch = 2.6956e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 148.4839, GNorm = 0.2395
Meta loss on this task batch = 2.9487e-01, Meta loss averaged over last 500 steps = 2.7726e-01, PNorm = 148.4910, GNorm = 0.2187
Meta loss on this task batch = 2.6270e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 148.4966, GNorm = 0.2308
Meta loss on this task batch = 3.2829e-01, Meta loss averaged over last 500 steps = 2.7737e-01, PNorm = 148.5001, GNorm = 0.2796
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.7741e-01, PNorm = 148.5034, GNorm = 0.2246
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 2.7746e-01, PNorm = 148.5073, GNorm = 0.2147
Meta loss on this task batch = 2.4099e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 148.5118, GNorm = 0.2084
Meta loss on this task batch = 2.9650e-01, Meta loss averaged over last 500 steps = 2.7749e-01, PNorm = 148.5154, GNorm = 0.2647
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 148.5198, GNorm = 0.2463
Meta loss on this task batch = 2.4327e-01, Meta loss averaged over last 500 steps = 2.7742e-01, PNorm = 148.5255, GNorm = 0.1885
Meta loss on this task batch = 2.5546e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 148.5309, GNorm = 0.2371
Meta loss on this task batch = 2.5376e-01, Meta loss averaged over last 500 steps = 2.7734e-01, PNorm = 148.5371, GNorm = 0.2980
Meta loss on this task batch = 3.1063e-01, Meta loss averaged over last 500 steps = 2.7732e-01, PNorm = 148.5425, GNorm = 0.2849
Meta loss on this task batch = 2.1033e-01, Meta loss averaged over last 500 steps = 2.7716e-01, PNorm = 148.5479, GNorm = 0.2001
Meta loss on this task batch = 2.5476e-01, Meta loss averaged over last 500 steps = 2.7707e-01, PNorm = 148.5533, GNorm = 0.1929
Meta loss on this task batch = 3.7754e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 148.5587, GNorm = 0.3683
Meta loss on this task batch = 2.1919e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 148.5645, GNorm = 0.2610
Took 112.26563549041748 seconds to complete one epoch of meta training
Took 119.78558349609375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477852
Epoch 601
Meta loss on this task batch = 2.7096e-01, Meta loss averaged over last 500 steps = 2.7714e-01, PNorm = 148.5709, GNorm = 0.2111
Meta loss on this task batch = 2.0964e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 148.5781, GNorm = 0.2141
Meta loss on this task batch = 2.5491e-01, Meta loss averaged over last 500 steps = 2.7687e-01, PNorm = 148.5852, GNorm = 0.2172
Meta loss on this task batch = 2.6646e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 148.5922, GNorm = 0.2393
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 148.5983, GNorm = 0.2654
Meta loss on this task batch = 2.8065e-01, Meta loss averaged over last 500 steps = 2.7688e-01, PNorm = 148.6029, GNorm = 0.2430
Meta loss on this task batch = 2.8784e-01, Meta loss averaged over last 500 steps = 2.7686e-01, PNorm = 148.6076, GNorm = 0.2338
Meta loss on this task batch = 2.4320e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 148.6131, GNorm = 0.2276
Meta loss on this task batch = 3.1815e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 148.6179, GNorm = 0.2570
Meta loss on this task batch = 2.4715e-01, Meta loss averaged over last 500 steps = 2.7691e-01, PNorm = 148.6229, GNorm = 0.1997
Meta loss on this task batch = 3.3396e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 148.6268, GNorm = 0.2609
Meta loss on this task batch = 2.4728e-01, Meta loss averaged over last 500 steps = 2.7715e-01, PNorm = 148.6311, GNorm = 0.2028
Meta loss on this task batch = 3.1823e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 148.6354, GNorm = 0.2463
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 148.6397, GNorm = 0.2491
Meta loss on this task batch = 2.8189e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 148.6443, GNorm = 0.2684
Meta loss on this task batch = 2.4900e-01, Meta loss averaged over last 500 steps = 2.7720e-01, PNorm = 148.6506, GNorm = 0.2443
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 148.6561, GNorm = 0.2883
Meta loss on this task batch = 3.3175e-01, Meta loss averaged over last 500 steps = 2.7735e-01, PNorm = 148.6612, GNorm = 0.2229
Meta loss on this task batch = 3.2340e-01, Meta loss averaged over last 500 steps = 2.7748e-01, PNorm = 148.6661, GNorm = 0.2461
Took 115.32064938545227 seconds to complete one epoch of meta training
Took 123.01892447471619 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481603
Epoch 602
Meta loss on this task batch = 3.0069e-01, Meta loss averaged over last 500 steps = 2.7742e-01, PNorm = 148.6706, GNorm = 0.2329
Meta loss on this task batch = 2.5914e-01, Meta loss averaged over last 500 steps = 2.7733e-01, PNorm = 148.6749, GNorm = 0.2234
Meta loss on this task batch = 2.2966e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 148.6801, GNorm = 0.1968
Meta loss on this task batch = 2.6850e-01, Meta loss averaged over last 500 steps = 2.7731e-01, PNorm = 148.6861, GNorm = 0.2131
Meta loss on this task batch = 2.4389e-01, Meta loss averaged over last 500 steps = 2.7730e-01, PNorm = 148.6920, GNorm = 0.2092
Meta loss on this task batch = 2.5656e-01, Meta loss averaged over last 500 steps = 2.7725e-01, PNorm = 148.6980, GNorm = 0.2113
Meta loss on this task batch = 3.1879e-01, Meta loss averaged over last 500 steps = 2.7733e-01, PNorm = 148.7039, GNorm = 0.2430
Meta loss on this task batch = 3.1825e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 148.7094, GNorm = 0.2349
Meta loss on this task batch = 3.5969e-01, Meta loss averaged over last 500 steps = 2.7759e-01, PNorm = 148.7137, GNorm = 0.3062
Meta loss on this task batch = 2.3292e-01, Meta loss averaged over last 500 steps = 2.7744e-01, PNorm = 148.7187, GNorm = 0.2086
Meta loss on this task batch = 2.6600e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 148.7239, GNorm = 0.2156
Meta loss on this task batch = 2.5790e-01, Meta loss averaged over last 500 steps = 2.7733e-01, PNorm = 148.7290, GNorm = 0.2320
Meta loss on this task batch = 2.9770e-01, Meta loss averaged over last 500 steps = 2.7731e-01, PNorm = 148.7339, GNorm = 0.2436
Meta loss on this task batch = 3.4953e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 148.7390, GNorm = 0.2505
Meta loss on this task batch = 2.5184e-01, Meta loss averaged over last 500 steps = 2.7746e-01, PNorm = 148.7440, GNorm = 0.2099
Meta loss on this task batch = 2.8021e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 148.7487, GNorm = 0.2399
Meta loss on this task batch = 2.4231e-01, Meta loss averaged over last 500 steps = 2.7734e-01, PNorm = 148.7538, GNorm = 0.1988
Meta loss on this task batch = 2.3612e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 148.7595, GNorm = 0.2148
Meta loss on this task batch = 3.0557e-01, Meta loss averaged over last 500 steps = 2.7735e-01, PNorm = 148.7652, GNorm = 0.2832
Took 113.12380719184875 seconds to complete one epoch of meta training
Took 121.17345762252808 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487609
Epoch 603
Meta loss on this task batch = 2.6085e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 148.7709, GNorm = 0.2939
Meta loss on this task batch = 2.5705e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 148.7774, GNorm = 0.2079
Meta loss on this task batch = 2.8620e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 148.7841, GNorm = 0.2214
Meta loss on this task batch = 2.9956e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 148.7907, GNorm = 0.2218
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.7749e-01, PNorm = 148.7975, GNorm = 0.2131
Meta loss on this task batch = 2.0816e-01, Meta loss averaged over last 500 steps = 2.7737e-01, PNorm = 148.8042, GNorm = 0.1914
Meta loss on this task batch = 2.6618e-01, Meta loss averaged over last 500 steps = 2.7738e-01, PNorm = 148.8095, GNorm = 0.2277
Meta loss on this task batch = 2.7272e-01, Meta loss averaged over last 500 steps = 2.7732e-01, PNorm = 148.8152, GNorm = 0.2291
Meta loss on this task batch = 2.6591e-01, Meta loss averaged over last 500 steps = 2.7725e-01, PNorm = 148.8218, GNorm = 0.2213
Meta loss on this task batch = 3.3267e-01, Meta loss averaged over last 500 steps = 2.7733e-01, PNorm = 148.8277, GNorm = 0.2514
Meta loss on this task batch = 2.9613e-01, Meta loss averaged over last 500 steps = 2.7733e-01, PNorm = 148.8324, GNorm = 0.2510
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.7739e-01, PNorm = 148.8374, GNorm = 0.2089
Meta loss on this task batch = 2.8636e-01, Meta loss averaged over last 500 steps = 2.7742e-01, PNorm = 148.8424, GNorm = 0.2136
Meta loss on this task batch = 3.0449e-01, Meta loss averaged over last 500 steps = 2.7753e-01, PNorm = 148.8473, GNorm = 0.2630
Meta loss on this task batch = 2.8020e-01, Meta loss averaged over last 500 steps = 2.7750e-01, PNorm = 148.8514, GNorm = 0.2152
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.7748e-01, PNorm = 148.8567, GNorm = 0.2544
Meta loss on this task batch = 3.1399e-01, Meta loss averaged over last 500 steps = 2.7762e-01, PNorm = 148.8611, GNorm = 0.2643
Meta loss on this task batch = 2.3853e-01, Meta loss averaged over last 500 steps = 2.7747e-01, PNorm = 148.8659, GNorm = 0.1934
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.7743e-01, PNorm = 148.8709, GNorm = 0.2979
Took 115.88304829597473 seconds to complete one epoch of meta training
Took 122.81735348701477 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467803
Epoch 604
Meta loss on this task batch = 2.8566e-01, Meta loss averaged over last 500 steps = 2.7738e-01, PNorm = 148.8763, GNorm = 0.2417
Meta loss on this task batch = 2.9732e-01, Meta loss averaged over last 500 steps = 2.7736e-01, PNorm = 148.8829, GNorm = 0.2671
Meta loss on this task batch = 2.1450e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 148.8905, GNorm = 0.2105
Meta loss on this task batch = 2.4186e-01, Meta loss averaged over last 500 steps = 2.7726e-01, PNorm = 148.8978, GNorm = 0.2082
Meta loss on this task batch = 3.2336e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 148.9044, GNorm = 0.2532
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 2.7725e-01, PNorm = 148.9104, GNorm = 0.2461
Meta loss on this task batch = 2.6730e-01, Meta loss averaged over last 500 steps = 2.7722e-01, PNorm = 148.9161, GNorm = 0.2339
Meta loss on this task batch = 2.2395e-01, Meta loss averaged over last 500 steps = 2.7717e-01, PNorm = 148.9208, GNorm = 0.2088
Meta loss on this task batch = 3.0769e-01, Meta loss averaged over last 500 steps = 2.7725e-01, PNorm = 148.9253, GNorm = 0.2551
Meta loss on this task batch = 2.5576e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 148.9307, GNorm = 0.2125
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 2.7722e-01, PNorm = 148.9354, GNorm = 0.2369
Meta loss on this task batch = 2.9423e-01, Meta loss averaged over last 500 steps = 2.7724e-01, PNorm = 148.9402, GNorm = 0.2571
Meta loss on this task batch = 2.7551e-01, Meta loss averaged over last 500 steps = 2.7727e-01, PNorm = 148.9453, GNorm = 0.2527
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.7729e-01, PNorm = 148.9506, GNorm = 0.2293
Meta loss on this task batch = 2.6209e-01, Meta loss averaged over last 500 steps = 2.7732e-01, PNorm = 148.9554, GNorm = 0.2096
Meta loss on this task batch = 2.7537e-01, Meta loss averaged over last 500 steps = 2.7723e-01, PNorm = 148.9599, GNorm = 0.2176
Meta loss on this task batch = 2.6666e-01, Meta loss averaged over last 500 steps = 2.7716e-01, PNorm = 148.9652, GNorm = 0.2175
Meta loss on this task batch = 2.6188e-01, Meta loss averaged over last 500 steps = 2.7706e-01, PNorm = 148.9699, GNorm = 0.2245
Meta loss on this task batch = 2.3580e-01, Meta loss averaged over last 500 steps = 2.7703e-01, PNorm = 148.9755, GNorm = 0.2484
Took 114.04222416877747 seconds to complete one epoch of meta training
Took 121.85636472702026 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483743
Epoch 605
Meta loss on this task batch = 3.1064e-01, Meta loss averaged over last 500 steps = 2.7712e-01, PNorm = 148.9803, GNorm = 0.2210
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 2.7720e-01, PNorm = 148.9851, GNorm = 0.2459
Meta loss on this task batch = 2.2124e-01, Meta loss averaged over last 500 steps = 2.7710e-01, PNorm = 148.9895, GNorm = 0.2413
Meta loss on this task batch = 2.4971e-01, Meta loss averaged over last 500 steps = 2.7699e-01, PNorm = 148.9940, GNorm = 0.2084
Meta loss on this task batch = 2.4100e-01, Meta loss averaged over last 500 steps = 2.7689e-01, PNorm = 148.9986, GNorm = 0.2220
Meta loss on this task batch = 2.5031e-01, Meta loss averaged over last 500 steps = 2.7680e-01, PNorm = 149.0037, GNorm = 0.2201
Meta loss on this task batch = 2.2029e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 149.0095, GNorm = 0.2307
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 149.0157, GNorm = 0.2117
Meta loss on this task batch = 2.4483e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 149.0224, GNorm = 0.2289
Meta loss on this task batch = 2.7106e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 149.0283, GNorm = 0.2343
Meta loss on this task batch = 2.7867e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 149.0338, GNorm = 0.1898
Meta loss on this task batch = 2.4511e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 149.0395, GNorm = 0.2121
Meta loss on this task batch = 2.5122e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 149.0457, GNorm = 0.2243
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 149.0512, GNorm = 0.2268
Meta loss on this task batch = 3.1067e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 149.0571, GNorm = 0.2523
Meta loss on this task batch = 2.9939e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 149.0621, GNorm = 0.2994
Meta loss on this task batch = 3.0364e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 149.0675, GNorm = 0.2200
Meta loss on this task batch = 3.4309e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 149.0719, GNorm = 0.2595
Meta loss on this task batch = 2.3845e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 149.0771, GNorm = 0.2650
Took 117.89831256866455 seconds to complete one epoch of meta training
Took 126.16436862945557 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.461576
Epoch 606
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 2.7667e-01, PNorm = 149.0835, GNorm = 0.2674
Meta loss on this task batch = 2.8777e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 149.0899, GNorm = 0.2347
Meta loss on this task batch = 2.4593e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 149.0959, GNorm = 0.2325
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 149.1019, GNorm = 0.2399
Meta loss on this task batch = 2.8786e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 149.1076, GNorm = 0.2446
Meta loss on this task batch = 2.7801e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 149.1133, GNorm = 0.2052
Meta loss on this task batch = 2.2401e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 149.1190, GNorm = 0.2179
Meta loss on this task batch = 3.0800e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 149.1242, GNorm = 0.2348
Meta loss on this task batch = 2.1005e-01, Meta loss averaged over last 500 steps = 2.7647e-01, PNorm = 149.1307, GNorm = 0.2127
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 149.1369, GNorm = 0.2205
Meta loss on this task batch = 2.3891e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 149.1432, GNorm = 0.1981
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 149.1493, GNorm = 0.2309
Meta loss on this task batch = 2.8208e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 149.1551, GNorm = 0.2320
Meta loss on this task batch = 2.7677e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 149.1608, GNorm = 0.2102
Meta loss on this task batch = 2.9823e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 149.1674, GNorm = 0.2166
Meta loss on this task batch = 2.5806e-01, Meta loss averaged over last 500 steps = 2.7655e-01, PNorm = 149.1735, GNorm = 0.2025
Meta loss on this task batch = 3.0731e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 149.1790, GNorm = 0.2561
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 149.1842, GNorm = 0.2297
Meta loss on this task batch = 2.9449e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 149.1885, GNorm = 0.3393
Took 114.68424654006958 seconds to complete one epoch of meta training
Took 122.59955430030823 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.460121
Epoch 607
Meta loss on this task batch = 3.1253e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 149.1921, GNorm = 0.2351
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 149.1968, GNorm = 0.2087
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 149.2022, GNorm = 0.2178
Meta loss on this task batch = 2.5143e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 149.2085, GNorm = 0.2171
Meta loss on this task batch = 3.3680e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 149.2145, GNorm = 0.2204
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 149.2211, GNorm = 0.2349
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.7678e-01, PNorm = 149.2279, GNorm = 0.2433
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.7675e-01, PNorm = 149.2358, GNorm = 0.2351
Meta loss on this task batch = 3.0993e-01, Meta loss averaged over last 500 steps = 2.7684e-01, PNorm = 149.2436, GNorm = 0.2253
Meta loss on this task batch = 3.0305e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 149.2512, GNorm = 0.2489
Meta loss on this task batch = 2.9213e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 149.2568, GNorm = 0.2902
Meta loss on this task batch = 2.6401e-01, Meta loss averaged over last 500 steps = 2.7677e-01, PNorm = 149.2622, GNorm = 0.2123
Meta loss on this task batch = 2.6468e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 149.2673, GNorm = 0.2202
Meta loss on this task batch = 2.4264e-01, Meta loss averaged over last 500 steps = 2.7661e-01, PNorm = 149.2725, GNorm = 0.2092
Meta loss on this task batch = 2.4075e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 149.2770, GNorm = 0.2399
Meta loss on this task batch = 2.7035e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 149.2812, GNorm = 0.2169
Meta loss on this task batch = 2.9804e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 149.2857, GNorm = 0.2881
Meta loss on this task batch = 2.9359e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 149.2904, GNorm = 0.2641
Meta loss on this task batch = 1.9355e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 149.2955, GNorm = 0.2200
Took 116.20736956596375 seconds to complete one epoch of meta training
Took 122.94820380210876 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502974
Epoch 608
Meta loss on this task batch = 3.2328e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 149.3000, GNorm = 0.3112
Meta loss on this task batch = 2.4577e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 149.3047, GNorm = 0.2093
Meta loss on this task batch = 2.1873e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 149.3103, GNorm = 0.1923
Meta loss on this task batch = 2.5620e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 149.3157, GNorm = 0.2062
Meta loss on this task batch = 2.7369e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 149.3217, GNorm = 0.1956
Meta loss on this task batch = 2.7782e-01, Meta loss averaged over last 500 steps = 2.7637e-01, PNorm = 149.3280, GNorm = 0.2253
Meta loss on this task batch = 2.4488e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 149.3346, GNorm = 0.1999
Meta loss on this task batch = 2.8008e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 149.3416, GNorm = 0.2421
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 149.3486, GNorm = 0.2272
Meta loss on this task batch = 3.2012e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 149.3557, GNorm = 0.2423
Meta loss on this task batch = 2.6888e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 149.3636, GNorm = 0.2200
Meta loss on this task batch = 3.1788e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 149.3710, GNorm = 0.2550
Meta loss on this task batch = 2.5170e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 149.3789, GNorm = 0.2547
Meta loss on this task batch = 2.8564e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 149.3863, GNorm = 0.2440
Meta loss on this task batch = 2.7527e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 149.3924, GNorm = 0.2955
Meta loss on this task batch = 2.9592e-01, Meta loss averaged over last 500 steps = 2.7672e-01, PNorm = 149.3974, GNorm = 0.2152
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 149.4019, GNorm = 0.2501
Meta loss on this task batch = 3.2141e-01, Meta loss averaged over last 500 steps = 2.7682e-01, PNorm = 149.4053, GNorm = 0.3325
Meta loss on this task batch = 2.6927e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 149.4089, GNorm = 0.2736
Took 115.30352711677551 seconds to complete one epoch of meta training
Took 123.60916090011597 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.527673
Epoch 609
Meta loss on this task batch = 2.7191e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 149.4127, GNorm = 0.2073
Meta loss on this task batch = 2.6183e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 149.4167, GNorm = 0.2131
Meta loss on this task batch = 2.3656e-01, Meta loss averaged over last 500 steps = 2.7652e-01, PNorm = 149.4220, GNorm = 0.2298
Meta loss on this task batch = 2.6730e-01, Meta loss averaged over last 500 steps = 2.7653e-01, PNorm = 149.4286, GNorm = 0.2287
Meta loss on this task batch = 3.4978e-01, Meta loss averaged over last 500 steps = 2.7662e-01, PNorm = 149.4344, GNorm = 0.2747
Meta loss on this task batch = 3.1120e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 149.4408, GNorm = 0.2154
Meta loss on this task batch = 2.6120e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 149.4471, GNorm = 0.2516
Meta loss on this task batch = 2.6787e-01, Meta loss averaged over last 500 steps = 2.7681e-01, PNorm = 149.4538, GNorm = 0.2369
Meta loss on this task batch = 2.5914e-01, Meta loss averaged over last 500 steps = 2.7665e-01, PNorm = 149.4601, GNorm = 0.2519
Meta loss on this task batch = 3.1034e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 149.4662, GNorm = 0.2606
Meta loss on this task batch = 3.1561e-01, Meta loss averaged over last 500 steps = 2.7683e-01, PNorm = 149.4719, GNorm = 0.2311
Meta loss on this task batch = 2.9998e-01, Meta loss averaged over last 500 steps = 2.7695e-01, PNorm = 149.4775, GNorm = 0.2587
Meta loss on this task batch = 3.0517e-01, Meta loss averaged over last 500 steps = 2.7696e-01, PNorm = 149.4828, GNorm = 0.2630
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.7694e-01, PNorm = 149.4885, GNorm = 0.2057
Meta loss on this task batch = 2.4492e-01, Meta loss averaged over last 500 steps = 2.7676e-01, PNorm = 149.4939, GNorm = 0.2036
Meta loss on this task batch = 2.4741e-01, Meta loss averaged over last 500 steps = 2.7670e-01, PNorm = 149.4989, GNorm = 0.2293
Meta loss on this task batch = 2.5500e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 149.5046, GNorm = 0.1960
Meta loss on this task batch = 2.8639e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 149.5106, GNorm = 0.2490
Meta loss on this task batch = 2.6999e-01, Meta loss averaged over last 500 steps = 2.7668e-01, PNorm = 149.5167, GNorm = 0.2477
Took 114.622225522995 seconds to complete one epoch of meta training
Took 122.11496353149414 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.522517
Epoch 610
Meta loss on this task batch = 2.4706e-01, Meta loss averaged over last 500 steps = 2.7669e-01, PNorm = 149.5230, GNorm = 0.2390
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 149.5296, GNorm = 0.2172
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 149.5359, GNorm = 0.1965
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 149.5425, GNorm = 0.2034
Meta loss on this task batch = 2.6876e-01, Meta loss averaged over last 500 steps = 2.7656e-01, PNorm = 149.5497, GNorm = 0.2376
Meta loss on this task batch = 3.3904e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 149.5563, GNorm = 0.2324
Meta loss on this task batch = 3.0979e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 149.5622, GNorm = 0.2409
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 149.5677, GNorm = 0.2121
Meta loss on this task batch = 3.3357e-01, Meta loss averaged over last 500 steps = 2.7673e-01, PNorm = 149.5726, GNorm = 0.2995
Meta loss on this task batch = 2.6624e-01, Meta loss averaged over last 500 steps = 2.7664e-01, PNorm = 149.5783, GNorm = 0.2746
Meta loss on this task batch = 2.4512e-01, Meta loss averaged over last 500 steps = 2.7654e-01, PNorm = 149.5837, GNorm = 0.2017
Meta loss on this task batch = 2.4675e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 149.5884, GNorm = 0.1864
Meta loss on this task batch = 2.5941e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 149.5934, GNorm = 0.2353
Meta loss on this task batch = 2.5132e-01, Meta loss averaged over last 500 steps = 2.7633e-01, PNorm = 149.5984, GNorm = 0.2212
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 149.6040, GNorm = 0.2097
Meta loss on this task batch = 3.0654e-01, Meta loss averaged over last 500 steps = 2.7640e-01, PNorm = 149.6096, GNorm = 0.2353
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 149.6136, GNorm = 0.2600
Meta loss on this task batch = 2.8252e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 149.6177, GNorm = 0.2330
Meta loss on this task batch = 2.5171e-01, Meta loss averaged over last 500 steps = 2.7634e-01, PNorm = 149.6233, GNorm = 0.2890
Took 113.96767282485962 seconds to complete one epoch of meta training
Took 122.06294822692871 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486450
Epoch 611
Meta loss on this task batch = 3.0034e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 149.6289, GNorm = 0.2158
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 149.6347, GNorm = 0.2242
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 149.6399, GNorm = 0.2201
Meta loss on this task batch = 2.2782e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 149.6453, GNorm = 0.1742
Meta loss on this task batch = 2.6252e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 149.6514, GNorm = 0.2408
Meta loss on this task batch = 2.4270e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 149.6575, GNorm = 0.2184
Meta loss on this task batch = 3.4971e-01, Meta loss averaged over last 500 steps = 2.7646e-01, PNorm = 149.6624, GNorm = 0.2506
Meta loss on this task batch = 3.0569e-01, Meta loss averaged over last 500 steps = 2.7650e-01, PNorm = 149.6672, GNorm = 0.2382
Meta loss on this task batch = 2.9303e-01, Meta loss averaged over last 500 steps = 2.7659e-01, PNorm = 149.6726, GNorm = 0.2120
Meta loss on this task batch = 3.3444e-01, Meta loss averaged over last 500 steps = 2.7671e-01, PNorm = 149.6770, GNorm = 0.2267
Meta loss on this task batch = 2.8811e-01, Meta loss averaged over last 500 steps = 2.7666e-01, PNorm = 149.6816, GNorm = 0.2669
Meta loss on this task batch = 2.3646e-01, Meta loss averaged over last 500 steps = 2.7660e-01, PNorm = 149.6870, GNorm = 0.1851
Meta loss on this task batch = 2.3360e-01, Meta loss averaged over last 500 steps = 2.7658e-01, PNorm = 149.6930, GNorm = 0.2283
Meta loss on this task batch = 2.4939e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 149.6997, GNorm = 0.2192
Meta loss on this task batch = 2.7692e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 149.7062, GNorm = 0.2305
Meta loss on this task batch = 3.0256e-01, Meta loss averaged over last 500 steps = 2.7657e-01, PNorm = 149.7127, GNorm = 0.2227
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.7639e-01, PNorm = 149.7191, GNorm = 0.2175
Meta loss on this task batch = 2.5088e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 149.7247, GNorm = 0.2369
Meta loss on this task batch = 2.3277e-01, Meta loss averaged over last 500 steps = 2.7621e-01, PNorm = 149.7308, GNorm = 0.2635
Took 113.75038456916809 seconds to complete one epoch of meta training
Took 120.94562125205994 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517324
Epoch 612
Meta loss on this task batch = 3.1348e-01, Meta loss averaged over last 500 steps = 2.7635e-01, PNorm = 149.7353, GNorm = 0.2839
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 2.7632e-01, PNorm = 149.7392, GNorm = 0.2434
Meta loss on this task batch = 2.8678e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 149.7428, GNorm = 0.2220
Meta loss on this task batch = 2.4566e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 149.7461, GNorm = 0.1964
Meta loss on this task batch = 2.8932e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 149.7494, GNorm = 0.2427
Meta loss on this task batch = 2.5010e-01, Meta loss averaged over last 500 steps = 2.7619e-01, PNorm = 149.7548, GNorm = 0.2110
Meta loss on this task batch = 2.5379e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 149.7607, GNorm = 0.2070
Meta loss on this task batch = 2.6971e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 149.7674, GNorm = 0.2411
Meta loss on this task batch = 2.6164e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 149.7749, GNorm = 0.2150
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 149.7823, GNorm = 0.2099
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 149.7890, GNorm = 0.2323
Meta loss on this task batch = 2.4171e-01, Meta loss averaged over last 500 steps = 2.7628e-01, PNorm = 149.7949, GNorm = 0.2403
Meta loss on this task batch = 3.0246e-01, Meta loss averaged over last 500 steps = 2.7643e-01, PNorm = 149.8006, GNorm = 0.3063
Meta loss on this task batch = 3.3246e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 149.8049, GNorm = 0.2851
Meta loss on this task batch = 2.8560e-01, Meta loss averaged over last 500 steps = 2.7651e-01, PNorm = 149.8092, GNorm = 0.2276
Meta loss on this task batch = 2.6151e-01, Meta loss averaged over last 500 steps = 2.7638e-01, PNorm = 149.8134, GNorm = 0.2085
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.7645e-01, PNorm = 149.8178, GNorm = 0.2126
Meta loss on this task batch = 2.8359e-01, Meta loss averaged over last 500 steps = 2.7642e-01, PNorm = 149.8216, GNorm = 0.2426
Meta loss on this task batch = 2.9687e-01, Meta loss averaged over last 500 steps = 2.7641e-01, PNorm = 149.8242, GNorm = 0.2850
Took 113.45589971542358 seconds to complete one epoch of meta training
Took 121.4750235080719 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498697
Epoch 613
Meta loss on this task batch = 2.4716e-01, Meta loss averaged over last 500 steps = 2.7629e-01, PNorm = 149.8268, GNorm = 0.2249
Meta loss on this task batch = 2.9770e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 149.8298, GNorm = 0.2513
Meta loss on this task batch = 2.3378e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 149.8330, GNorm = 0.2245
Meta loss on this task batch = 3.1544e-01, Meta loss averaged over last 500 steps = 2.7620e-01, PNorm = 149.8368, GNorm = 0.2089
Meta loss on this task batch = 2.3535e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 149.8409, GNorm = 0.1949
Meta loss on this task batch = 3.4033e-01, Meta loss averaged over last 500 steps = 2.7630e-01, PNorm = 149.8450, GNorm = 0.2624
Meta loss on this task batch = 2.5541e-01, Meta loss averaged over last 500 steps = 2.7617e-01, PNorm = 149.8496, GNorm = 0.2265
Meta loss on this task batch = 2.6773e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 149.8546, GNorm = 0.2109
Meta loss on this task batch = 3.0483e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 149.8593, GNorm = 0.2144
Meta loss on this task batch = 2.6540e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 149.8643, GNorm = 0.2286
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 149.8698, GNorm = 0.2014
Meta loss on this task batch = 2.7277e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 149.8743, GNorm = 0.2187
Meta loss on this task batch = 2.5414e-01, Meta loss averaged over last 500 steps = 2.7608e-01, PNorm = 149.8787, GNorm = 0.2115
Meta loss on this task batch = 2.4366e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 149.8838, GNorm = 0.2054
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 149.8891, GNorm = 0.2357
Meta loss on this task batch = 2.4964e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 149.8947, GNorm = 0.2327
Meta loss on this task batch = 2.4178e-01, Meta loss averaged over last 500 steps = 2.7579e-01, PNorm = 149.9008, GNorm = 0.2448
Meta loss on this task batch = 3.1691e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 149.9063, GNorm = 0.2852
Meta loss on this task batch = 3.2716e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 149.9105, GNorm = 0.3110
Took 115.75990438461304 seconds to complete one epoch of meta training
Took 123.21068477630615 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490837
Epoch 614
Meta loss on this task batch = 2.2258e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 149.9149, GNorm = 0.1893
Meta loss on this task batch = 2.5769e-01, Meta loss averaged over last 500 steps = 2.7586e-01, PNorm = 149.9196, GNorm = 0.2379
Meta loss on this task batch = 2.8376e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 149.9245, GNorm = 0.2247
Meta loss on this task batch = 3.0325e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 149.9288, GNorm = 0.2252
Meta loss on this task batch = 2.3913e-01, Meta loss averaged over last 500 steps = 2.7599e-01, PNorm = 149.9338, GNorm = 0.2275
Meta loss on this task batch = 2.7465e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 149.9387, GNorm = 0.1981
Meta loss on this task batch = 2.7599e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 149.9442, GNorm = 0.2311
Meta loss on this task batch = 2.3413e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 149.9501, GNorm = 0.2178
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 149.9560, GNorm = 0.2430
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 149.9610, GNorm = 0.2231
Meta loss on this task batch = 2.1451e-01, Meta loss averaged over last 500 steps = 2.7583e-01, PNorm = 149.9661, GNorm = 0.2158
Meta loss on this task batch = 3.2007e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 149.9708, GNorm = 0.2441
Meta loss on this task batch = 2.7474e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 149.9760, GNorm = 0.2349
Meta loss on this task batch = 2.9683e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 149.9811, GNorm = 0.2507
Meta loss on this task batch = 2.2831e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 149.9871, GNorm = 0.2253
Meta loss on this task batch = 2.8819e-01, Meta loss averaged over last 500 steps = 2.7591e-01, PNorm = 149.9935, GNorm = 0.2474
Meta loss on this task batch = 2.8765e-01, Meta loss averaged over last 500 steps = 2.7587e-01, PNorm = 149.9994, GNorm = 0.2436
Meta loss on this task batch = 2.9748e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 150.0038, GNorm = 0.2642
Meta loss on this task batch = 2.5854e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 150.0094, GNorm = 0.2737
Took 116.38454985618591 seconds to complete one epoch of meta training
Took 124.44949769973755 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491697
Epoch 615
Meta loss on this task batch = 3.0815e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 150.0137, GNorm = 0.2667
Meta loss on this task batch = 2.5269e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 150.0186, GNorm = 0.2108
Meta loss on this task batch = 2.9767e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 150.0232, GNorm = 0.2425
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 150.0273, GNorm = 0.2132
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.7585e-01, PNorm = 150.0318, GNorm = 0.2065
Meta loss on this task batch = 2.6349e-01, Meta loss averaged over last 500 steps = 2.7596e-01, PNorm = 150.0363, GNorm = 0.2358
Meta loss on this task batch = 2.3788e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 150.0407, GNorm = 0.2302
Meta loss on this task batch = 3.0061e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 150.0444, GNorm = 0.2487
Meta loss on this task batch = 2.5524e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 150.0475, GNorm = 0.2326
Meta loss on this task batch = 2.6603e-01, Meta loss averaged over last 500 steps = 2.7590e-01, PNorm = 150.0516, GNorm = 0.2278
Meta loss on this task batch = 2.3691e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 150.0575, GNorm = 0.2643
Meta loss on this task batch = 3.2028e-01, Meta loss averaged over last 500 steps = 2.7597e-01, PNorm = 150.0639, GNorm = 0.2177
Meta loss on this task batch = 3.2562e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 150.0717, GNorm = 0.2781
Meta loss on this task batch = 2.2371e-01, Meta loss averaged over last 500 steps = 2.7604e-01, PNorm = 150.0800, GNorm = 0.2272
Meta loss on this task batch = 2.7797e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 150.0889, GNorm = 0.2160
Meta loss on this task batch = 2.9759e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 150.0971, GNorm = 0.2824
Meta loss on this task batch = 2.7888e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 150.1044, GNorm = 0.2117
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 150.1117, GNorm = 0.2175
Meta loss on this task batch = 3.0403e-01, Meta loss averaged over last 500 steps = 2.7594e-01, PNorm = 150.1189, GNorm = 0.2793
Took 113.7792592048645 seconds to complete one epoch of meta training
Took 121.28730297088623 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505666
Epoch 616
Meta loss on this task batch = 2.8258e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 150.1254, GNorm = 0.2020
Meta loss on this task batch = 2.9829e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 150.1316, GNorm = 0.2469
Meta loss on this task batch = 2.2642e-01, Meta loss averaged over last 500 steps = 2.7595e-01, PNorm = 150.1372, GNorm = 0.2352
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 150.1434, GNorm = 0.2353
Meta loss on this task batch = 3.3512e-01, Meta loss averaged over last 500 steps = 2.7622e-01, PNorm = 150.1490, GNorm = 0.2442
Meta loss on this task batch = 2.2710e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 150.1547, GNorm = 0.2285
Meta loss on this task batch = 2.3482e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 150.1605, GNorm = 0.2158
Meta loss on this task batch = 3.0461e-01, Meta loss averaged over last 500 steps = 2.7602e-01, PNorm = 150.1664, GNorm = 0.2161
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 2.7598e-01, PNorm = 150.1714, GNorm = 0.2392
Meta loss on this task batch = 3.0054e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 150.1758, GNorm = 0.2580
Meta loss on this task batch = 2.6924e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 150.1803, GNorm = 0.2413
Meta loss on this task batch = 2.7796e-01, Meta loss averaged over last 500 steps = 2.7623e-01, PNorm = 150.1856, GNorm = 0.2651
Meta loss on this task batch = 2.9988e-01, Meta loss averaged over last 500 steps = 2.7624e-01, PNorm = 150.1910, GNorm = 0.2411
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.7631e-01, PNorm = 150.1966, GNorm = 0.2182
Meta loss on this task batch = 2.5665e-01, Meta loss averaged over last 500 steps = 2.7627e-01, PNorm = 150.2021, GNorm = 0.2101
Meta loss on this task batch = 2.7597e-01, Meta loss averaged over last 500 steps = 2.7625e-01, PNorm = 150.2068, GNorm = 0.2732
Meta loss on this task batch = 2.6380e-01, Meta loss averaged over last 500 steps = 2.7611e-01, PNorm = 150.2115, GNorm = 0.2161
Meta loss on this task batch = 2.5758e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 150.2156, GNorm = 0.2663
Meta loss on this task batch = 2.3001e-01, Meta loss averaged over last 500 steps = 2.7616e-01, PNorm = 150.2206, GNorm = 0.2183
Took 116.18376684188843 seconds to complete one epoch of meta training
Took 124.11454391479492 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499240
Epoch 617
Meta loss on this task batch = 2.5813e-01, Meta loss averaged over last 500 steps = 2.7612e-01, PNorm = 150.2260, GNorm = 0.2207
Meta loss on this task batch = 2.2432e-01, Meta loss averaged over last 500 steps = 2.7605e-01, PNorm = 150.2316, GNorm = 0.2130
Meta loss on this task batch = 2.3626e-01, Meta loss averaged over last 500 steps = 2.7578e-01, PNorm = 150.2381, GNorm = 0.2123
Meta loss on this task batch = 2.4345e-01, Meta loss averaged over last 500 steps = 2.7574e-01, PNorm = 150.2448, GNorm = 0.1983
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.7573e-01, PNorm = 150.2502, GNorm = 0.2718
Meta loss on this task batch = 2.8404e-01, Meta loss averaged over last 500 steps = 2.7577e-01, PNorm = 150.2555, GNorm = 0.2519
Meta loss on this task batch = 2.5533e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 150.2611, GNorm = 0.2424
Meta loss on this task batch = 3.1605e-01, Meta loss averaged over last 500 steps = 2.7582e-01, PNorm = 150.2656, GNorm = 0.2636
Meta loss on this task batch = 2.4141e-01, Meta loss averaged over last 500 steps = 2.7572e-01, PNorm = 150.2712, GNorm = 0.2227
Meta loss on this task batch = 2.8447e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 150.2766, GNorm = 0.2123
Meta loss on this task batch = 3.2357e-01, Meta loss averaged over last 500 steps = 2.7592e-01, PNorm = 150.2816, GNorm = 0.2575
Meta loss on this task batch = 3.1222e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 150.2861, GNorm = 0.2187
Meta loss on this task batch = 2.9735e-01, Meta loss averaged over last 500 steps = 2.7607e-01, PNorm = 150.2898, GNorm = 0.3036
Meta loss on this task batch = 3.0048e-01, Meta loss averaged over last 500 steps = 2.7610e-01, PNorm = 150.2949, GNorm = 0.2694
Meta loss on this task batch = 2.9679e-01, Meta loss averaged over last 500 steps = 2.7606e-01, PNorm = 150.3003, GNorm = 0.2070
Meta loss on this task batch = 2.6186e-01, Meta loss averaged over last 500 steps = 2.7609e-01, PNorm = 150.3058, GNorm = 0.2322
Meta loss on this task batch = 2.0630e-01, Meta loss averaged over last 500 steps = 2.7600e-01, PNorm = 150.3111, GNorm = 0.1872
Meta loss on this task batch = 2.3975e-01, Meta loss averaged over last 500 steps = 2.7603e-01, PNorm = 150.3150, GNorm = 0.2438
Meta loss on this task batch = 2.7417e-01, Meta loss averaged over last 500 steps = 2.7613e-01, PNorm = 150.3191, GNorm = 0.2860
Took 114.02432680130005 seconds to complete one epoch of meta training
Took 121.55152368545532 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508196
Epoch 618
Meta loss on this task batch = 3.0713e-01, Meta loss averaged over last 500 steps = 2.7615e-01, PNorm = 150.3232, GNorm = 0.2305
Meta loss on this task batch = 2.1039e-01, Meta loss averaged over last 500 steps = 2.7601e-01, PNorm = 150.3273, GNorm = 0.1945
Meta loss on this task batch = 2.7746e-01, Meta loss averaged over last 500 steps = 2.7589e-01, PNorm = 150.3307, GNorm = 0.2126
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 2.7576e-01, PNorm = 150.3345, GNorm = 0.2068
Meta loss on this task batch = 2.8706e-01, Meta loss averaged over last 500 steps = 2.7579e-01, PNorm = 150.3382, GNorm = 0.2072
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 2.7569e-01, PNorm = 150.3424, GNorm = 0.2334
Meta loss on this task batch = 2.7303e-01, Meta loss averaged over last 500 steps = 2.7572e-01, PNorm = 150.3455, GNorm = 0.2321
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.7561e-01, PNorm = 150.3496, GNorm = 0.1890
Meta loss on this task batch = 2.3233e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 150.3545, GNorm = 0.2210
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 150.3602, GNorm = 0.2452
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 150.3658, GNorm = 0.2520
Meta loss on this task batch = 2.7460e-01, Meta loss averaged over last 500 steps = 2.7557e-01, PNorm = 150.3717, GNorm = 0.2370
Meta loss on this task batch = 2.5176e-01, Meta loss averaged over last 500 steps = 2.7552e-01, PNorm = 150.3775, GNorm = 0.2048
Meta loss on this task batch = 2.9428e-01, Meta loss averaged over last 500 steps = 2.7554e-01, PNorm = 150.3810, GNorm = 0.3363
Meta loss on this task batch = 2.8039e-01, Meta loss averaged over last 500 steps = 2.7548e-01, PNorm = 150.3848, GNorm = 0.2430
Meta loss on this task batch = 2.8205e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 150.3888, GNorm = 0.2141
Meta loss on this task batch = 1.9851e-01, Meta loss averaged over last 500 steps = 2.7530e-01, PNorm = 150.3939, GNorm = 0.2014
Meta loss on this task batch = 3.3486e-01, Meta loss averaged over last 500 steps = 2.7539e-01, PNorm = 150.3982, GNorm = 0.2127
Meta loss on this task batch = 2.2323e-01, Meta loss averaged over last 500 steps = 2.7531e-01, PNorm = 150.4032, GNorm = 0.2833
Took 112.54338669776917 seconds to complete one epoch of meta training
Took 120.80699014663696 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508457
Epoch 619
Meta loss on this task batch = 2.9069e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 150.4095, GNorm = 0.2334
Meta loss on this task batch = 2.6441e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 150.4158, GNorm = 0.2152
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 150.4227, GNorm = 0.2444
Meta loss on this task batch = 2.7058e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 150.4295, GNorm = 0.2445
Meta loss on this task batch = 2.8541e-01, Meta loss averaged over last 500 steps = 2.7537e-01, PNorm = 150.4366, GNorm = 0.2270
Meta loss on this task batch = 2.7453e-01, Meta loss averaged over last 500 steps = 2.7534e-01, PNorm = 150.4438, GNorm = 0.2226
Meta loss on this task batch = 2.7931e-01, Meta loss averaged over last 500 steps = 2.7541e-01, PNorm = 150.4502, GNorm = 0.2247
Meta loss on this task batch = 2.7952e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 150.4554, GNorm = 0.2667
Meta loss on this task batch = 3.1121e-01, Meta loss averaged over last 500 steps = 2.7538e-01, PNorm = 150.4603, GNorm = 0.2740
Meta loss on this task batch = 2.5040e-01, Meta loss averaged over last 500 steps = 2.7540e-01, PNorm = 150.4653, GNorm = 0.2552
Meta loss on this task batch = 2.7563e-01, Meta loss averaged over last 500 steps = 2.7542e-01, PNorm = 150.4705, GNorm = 0.2133
Meta loss on this task batch = 3.0933e-01, Meta loss averaged over last 500 steps = 2.7550e-01, PNorm = 150.4750, GNorm = 0.3059
Meta loss on this task batch = 2.4832e-01, Meta loss averaged over last 500 steps = 2.7545e-01, PNorm = 150.4793, GNorm = 0.2948
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 150.4839, GNorm = 0.2792
Meta loss on this task batch = 3.0779e-01, Meta loss averaged over last 500 steps = 2.7543e-01, PNorm = 150.4892, GNorm = 0.2369
Meta loss on this task batch = 2.0777e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 150.4940, GNorm = 0.2405
Meta loss on this task batch = 2.8478e-01, Meta loss averaged over last 500 steps = 2.7514e-01, PNorm = 150.4988, GNorm = 0.2465
Meta loss on this task batch = 3.3905e-01, Meta loss averaged over last 500 steps = 2.7520e-01, PNorm = 150.5043, GNorm = 0.2726
Meta loss on this task batch = 2.4661e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 150.5095, GNorm = 0.2518
Took 115.37676787376404 seconds to complete one epoch of meta training
Took 123.15481400489807 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497515
Epoch 620
Meta loss on this task batch = 2.9149e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 150.5153, GNorm = 0.2432
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.7522e-01, PNorm = 150.5203, GNorm = 0.2113
Meta loss on this task batch = 2.5863e-01, Meta loss averaged over last 500 steps = 2.7524e-01, PNorm = 150.5258, GNorm = 0.2142
Meta loss on this task batch = 3.0396e-01, Meta loss averaged over last 500 steps = 2.7527e-01, PNorm = 150.5318, GNorm = 0.2026
Meta loss on this task batch = 2.8407e-01, Meta loss averaged over last 500 steps = 2.7519e-01, PNorm = 150.5379, GNorm = 0.2274
Meta loss on this task batch = 2.6655e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 150.5443, GNorm = 0.2203
Meta loss on this task batch = 2.2806e-01, Meta loss averaged over last 500 steps = 2.7512e-01, PNorm = 150.5513, GNorm = 0.1915
Meta loss on this task batch = 2.7192e-01, Meta loss averaged over last 500 steps = 2.7518e-01, PNorm = 150.5570, GNorm = 0.2275
Meta loss on this task batch = 3.0050e-01, Meta loss averaged over last 500 steps = 2.7526e-01, PNorm = 150.5631, GNorm = 0.2628
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.7523e-01, PNorm = 150.5695, GNorm = 0.2226
Meta loss on this task batch = 2.8040e-01, Meta loss averaged over last 500 steps = 2.7511e-01, PNorm = 150.5769, GNorm = 0.2298
Meta loss on this task batch = 2.2396e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 150.5843, GNorm = 0.2175
Meta loss on this task batch = 2.4646e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 150.5914, GNorm = 0.2259
Meta loss on this task batch = 2.4724e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 150.5996, GNorm = 0.2368
Meta loss on this task batch = 2.8356e-01, Meta loss averaged over last 500 steps = 2.7502e-01, PNorm = 150.6081, GNorm = 0.2737
Meta loss on this task batch = 3.1748e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 150.6156, GNorm = 0.2654
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.7506e-01, PNorm = 150.6222, GNorm = 0.2394
Meta loss on this task batch = 2.7663e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 150.6279, GNorm = 0.2378
Meta loss on this task batch = 3.0921e-01, Meta loss averaged over last 500 steps = 2.7495e-01, PNorm = 150.6325, GNorm = 0.3612
Took 112.36036515235901 seconds to complete one epoch of meta training
Took 119.86672711372375 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507824
Epoch 621
Meta loss on this task batch = 2.4470e-01, Meta loss averaged over last 500 steps = 2.7501e-01, PNorm = 150.6376, GNorm = 0.2221
Meta loss on this task batch = 2.7357e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 150.6433, GNorm = 0.2677
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.7494e-01, PNorm = 150.6499, GNorm = 0.2366
Meta loss on this task batch = 2.9527e-01, Meta loss averaged over last 500 steps = 2.7497e-01, PNorm = 150.6561, GNorm = 0.2790
Meta loss on this task batch = 2.5920e-01, Meta loss averaged over last 500 steps = 2.7487e-01, PNorm = 150.6623, GNorm = 0.2126
Meta loss on this task batch = 2.2954e-01, Meta loss averaged over last 500 steps = 2.7472e-01, PNorm = 150.6689, GNorm = 0.2197
Meta loss on this task batch = 3.1203e-01, Meta loss averaged over last 500 steps = 2.7485e-01, PNorm = 150.6754, GNorm = 0.2222
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 150.6823, GNorm = 0.2151
Meta loss on this task batch = 3.2305e-01, Meta loss averaged over last 500 steps = 2.7496e-01, PNorm = 150.6893, GNorm = 0.2499
Meta loss on this task batch = 3.4700e-01, Meta loss averaged over last 500 steps = 2.7514e-01, PNorm = 150.6957, GNorm = 0.2550
Meta loss on this task batch = 2.7248e-01, Meta loss averaged over last 500 steps = 2.7508e-01, PNorm = 150.7017, GNorm = 0.2041
Meta loss on this task batch = 2.3752e-01, Meta loss averaged over last 500 steps = 2.7504e-01, PNorm = 150.7076, GNorm = 0.2193
Meta loss on this task batch = 3.0559e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 150.7127, GNorm = 0.2444
Meta loss on this task batch = 2.6457e-01, Meta loss averaged over last 500 steps = 2.7510e-01, PNorm = 150.7179, GNorm = 0.2359
Meta loss on this task batch = 2.5450e-01, Meta loss averaged over last 500 steps = 2.7507e-01, PNorm = 150.7229, GNorm = 0.2191
Meta loss on this task batch = 2.5180e-01, Meta loss averaged over last 500 steps = 2.7503e-01, PNorm = 150.7278, GNorm = 0.2190
Meta loss on this task batch = 2.3550e-01, Meta loss averaged over last 500 steps = 2.7498e-01, PNorm = 150.7331, GNorm = 0.1987
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.7488e-01, PNorm = 150.7379, GNorm = 0.2346
Meta loss on this task batch = 1.9955e-01, Meta loss averaged over last 500 steps = 2.7476e-01, PNorm = 150.7435, GNorm = 0.2275
Took 114.9929895401001 seconds to complete one epoch of meta training
Took 122.90650177001953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489259
Epoch 622
Meta loss on this task batch = 3.0602e-01, Meta loss averaged over last 500 steps = 2.7484e-01, PNorm = 150.7490, GNorm = 0.2359
Meta loss on this task batch = 2.6763e-01, Meta loss averaged over last 500 steps = 2.7483e-01, PNorm = 150.7547, GNorm = 0.2072
Meta loss on this task batch = 2.7102e-01, Meta loss averaged over last 500 steps = 2.7479e-01, PNorm = 150.7605, GNorm = 0.2519
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 2.7478e-01, PNorm = 150.7667, GNorm = 0.2396
Meta loss on this task batch = 2.4799e-01, Meta loss averaged over last 500 steps = 2.7468e-01, PNorm = 150.7732, GNorm = 0.2422
Meta loss on this task batch = 2.7535e-01, Meta loss averaged over last 500 steps = 2.7448e-01, PNorm = 150.7802, GNorm = 0.2386
Meta loss on this task batch = 2.4584e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 150.7870, GNorm = 0.2182
Meta loss on this task batch = 3.0770e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 150.7940, GNorm = 0.2708
Meta loss on this task batch = 2.6790e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 150.8004, GNorm = 0.2297
Meta loss on this task batch = 2.2195e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 150.8065, GNorm = 0.2068
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 150.8125, GNorm = 0.2222
Meta loss on this task batch = 2.6005e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 150.8177, GNorm = 0.2639
Meta loss on this task batch = 2.6759e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 150.8235, GNorm = 0.2071
Meta loss on this task batch = 3.0646e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 150.8286, GNorm = 0.2537
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 150.8337, GNorm = 0.2379
Meta loss on this task batch = 2.8827e-01, Meta loss averaged over last 500 steps = 2.7437e-01, PNorm = 150.8390, GNorm = 0.2741
Meta loss on this task batch = 2.2919e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 150.8439, GNorm = 0.2182
Meta loss on this task batch = 2.6464e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 150.8492, GNorm = 0.2090
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 2.7432e-01, PNorm = 150.8535, GNorm = 0.3732
Took 115.81812739372253 seconds to complete one epoch of meta training
Took 124.0431387424469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486711
Epoch 623
Meta loss on this task batch = 2.3947e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 150.8580, GNorm = 0.1821
Meta loss on this task batch = 2.9365e-01, Meta loss averaged over last 500 steps = 2.7438e-01, PNorm = 150.8632, GNorm = 0.2178
Meta loss on this task batch = 2.8342e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 150.8689, GNorm = 0.2057
Meta loss on this task batch = 2.5312e-01, Meta loss averaged over last 500 steps = 2.7427e-01, PNorm = 150.8749, GNorm = 0.2011
Meta loss on this task batch = 2.3970e-01, Meta loss averaged over last 500 steps = 2.7436e-01, PNorm = 150.8811, GNorm = 0.2899
Meta loss on this task batch = 2.3872e-01, Meta loss averaged over last 500 steps = 2.7411e-01, PNorm = 150.8870, GNorm = 0.2084
Meta loss on this task batch = 3.6068e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 150.8924, GNorm = 0.2510
Meta loss on this task batch = 2.8266e-01, Meta loss averaged over last 500 steps = 2.7426e-01, PNorm = 150.8980, GNorm = 0.2288
Meta loss on this task batch = 2.5791e-01, Meta loss averaged over last 500 steps = 2.7425e-01, PNorm = 150.9045, GNorm = 0.2136
Meta loss on this task batch = 3.0686e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 150.9104, GNorm = 0.2499
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 150.9164, GNorm = 0.2450
Meta loss on this task batch = 2.2437e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 150.9223, GNorm = 0.2076
Meta loss on this task batch = 2.9811e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 150.9287, GNorm = 0.2612
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.7421e-01, PNorm = 150.9357, GNorm = 0.2189
Meta loss on this task batch = 2.7030e-01, Meta loss averaged over last 500 steps = 2.7414e-01, PNorm = 150.9425, GNorm = 0.2480
Meta loss on this task batch = 2.4822e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 150.9490, GNorm = 0.2178
Meta loss on this task batch = 2.3981e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 150.9548, GNorm = 0.1992
Meta loss on this task batch = 3.0052e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 150.9601, GNorm = 0.2356
Meta loss on this task batch = 2.3879e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 150.9651, GNorm = 0.2236
Took 115.35996389389038 seconds to complete one epoch of meta training
Took 123.46366286277771 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481544
Epoch 624
Meta loss on this task batch = 2.9093e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 150.9693, GNorm = 0.2502
Meta loss on this task batch = 2.6124e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 150.9737, GNorm = 0.2152
Meta loss on this task batch = 3.4519e-01, Meta loss averaged over last 500 steps = 2.7412e-01, PNorm = 150.9777, GNorm = 0.2394
Meta loss on this task batch = 3.4233e-01, Meta loss averaged over last 500 steps = 2.7422e-01, PNorm = 150.9811, GNorm = 0.2767
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.7430e-01, PNorm = 150.9842, GNorm = 0.2581
Meta loss on this task batch = 2.2417e-01, Meta loss averaged over last 500 steps = 2.7415e-01, PNorm = 150.9878, GNorm = 0.2102
Meta loss on this task batch = 3.4153e-01, Meta loss averaged over last 500 steps = 2.7442e-01, PNorm = 150.9912, GNorm = 0.2561
Meta loss on this task batch = 2.5139e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 150.9947, GNorm = 0.2473
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.7439e-01, PNorm = 150.9989, GNorm = 0.2082
Meta loss on this task batch = 2.6948e-01, Meta loss averaged over last 500 steps = 2.7434e-01, PNorm = 151.0034, GNorm = 0.2326
Meta loss on this task batch = 2.7390e-01, Meta loss averaged over last 500 steps = 2.7429e-01, PNorm = 151.0086, GNorm = 0.1943
Meta loss on this task batch = 2.6020e-01, Meta loss averaged over last 500 steps = 2.7423e-01, PNorm = 151.0138, GNorm = 0.2389
Meta loss on this task batch = 2.9118e-01, Meta loss averaged over last 500 steps = 2.7428e-01, PNorm = 151.0191, GNorm = 0.2382
Meta loss on this task batch = 2.1422e-01, Meta loss averaged over last 500 steps = 2.7409e-01, PNorm = 151.0249, GNorm = 0.1760
Meta loss on this task batch = 2.9684e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 151.0315, GNorm = 0.2084
Meta loss on this task batch = 2.3117e-01, Meta loss averaged over last 500 steps = 2.7406e-01, PNorm = 151.0384, GNorm = 0.1985
Meta loss on this task batch = 2.9056e-01, Meta loss averaged over last 500 steps = 2.7408e-01, PNorm = 151.0456, GNorm = 0.2249
Meta loss on this task batch = 2.5483e-01, Meta loss averaged over last 500 steps = 2.7390e-01, PNorm = 151.0521, GNorm = 0.2225
Meta loss on this task batch = 2.9693e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 151.0582, GNorm = 0.2528
Took 113.75891637802124 seconds to complete one epoch of meta training
Took 121.13396096229553 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482348
Epoch 625
Meta loss on this task batch = 2.4324e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 151.0646, GNorm = 0.1877
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 151.0703, GNorm = 0.2136
Meta loss on this task batch = 2.4126e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 151.0742, GNorm = 0.2682
Meta loss on this task batch = 2.7069e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 151.0777, GNorm = 0.2247
Meta loss on this task batch = 2.7386e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 151.0831, GNorm = 0.2888
Meta loss on this task batch = 2.8735e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 151.0881, GNorm = 0.2902
Meta loss on this task batch = 2.2047e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 151.0938, GNorm = 0.2023
Meta loss on this task batch = 2.2056e-01, Meta loss averaged over last 500 steps = 2.7379e-01, PNorm = 151.1002, GNorm = 0.1821
Meta loss on this task batch = 3.3090e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 151.1068, GNorm = 0.2720
Meta loss on this task batch = 2.3763e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 151.1146, GNorm = 0.2205
Meta loss on this task batch = 2.4321e-01, Meta loss averaged over last 500 steps = 2.7397e-01, PNorm = 151.1217, GNorm = 0.1945
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 151.1293, GNorm = 0.2345
Meta loss on this task batch = 2.9434e-01, Meta loss averaged over last 500 steps = 2.7402e-01, PNorm = 151.1364, GNorm = 0.2215
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.7416e-01, PNorm = 151.1443, GNorm = 0.2493
Meta loss on this task batch = 3.3278e-01, Meta loss averaged over last 500 steps = 2.7433e-01, PNorm = 151.1523, GNorm = 0.2721
Meta loss on this task batch = 2.5797e-01, Meta loss averaged over last 500 steps = 2.7424e-01, PNorm = 151.1596, GNorm = 0.2447
Meta loss on this task batch = 3.1350e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 151.1655, GNorm = 0.2922
Meta loss on this task batch = 3.0460e-01, Meta loss averaged over last 500 steps = 2.7435e-01, PNorm = 151.1708, GNorm = 0.2576
Meta loss on this task batch = 2.2065e-01, Meta loss averaged over last 500 steps = 2.7417e-01, PNorm = 151.1764, GNorm = 0.2178
Took 126.9322247505188 seconds to complete one epoch of meta training
Took 135.0871181488037 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469440
Epoch 626
Meta loss on this task batch = 2.8280e-01, Meta loss averaged over last 500 steps = 2.7410e-01, PNorm = 151.1819, GNorm = 0.2259
Meta loss on this task batch = 2.5477e-01, Meta loss averaged over last 500 steps = 2.7399e-01, PNorm = 151.1878, GNorm = 0.2502
Meta loss on this task batch = 2.7018e-01, Meta loss averaged over last 500 steps = 2.7403e-01, PNorm = 151.1932, GNorm = 0.2155
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 2.7407e-01, PNorm = 151.1986, GNorm = 0.2211
Meta loss on this task batch = 1.8275e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 151.2045, GNorm = 0.1879
Meta loss on this task batch = 2.1607e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 151.2106, GNorm = 0.2052
Meta loss on this task batch = 3.1177e-01, Meta loss averaged over last 500 steps = 2.7391e-01, PNorm = 151.2159, GNorm = 0.2414
Meta loss on this task batch = 2.8901e-01, Meta loss averaged over last 500 steps = 2.7385e-01, PNorm = 151.2213, GNorm = 0.2229
Meta loss on this task batch = 2.4945e-01, Meta loss averaged over last 500 steps = 2.7381e-01, PNorm = 151.2276, GNorm = 0.2566
Meta loss on this task batch = 2.6021e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 151.2341, GNorm = 0.1980
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 151.2397, GNorm = 0.2394
Meta loss on this task batch = 2.6761e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 151.2455, GNorm = 0.2461
Meta loss on this task batch = 2.5407e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 151.2515, GNorm = 0.2179
Meta loss on this task batch = 2.5466e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 151.2578, GNorm = 0.2159
Meta loss on this task batch = 2.9538e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 151.2646, GNorm = 0.2296
Meta loss on this task batch = 2.9453e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 151.2711, GNorm = 0.2076
Meta loss on this task batch = 3.2612e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 151.2759, GNorm = 0.2645
Meta loss on this task batch = 2.7159e-01, Meta loss averaged over last 500 steps = 2.7369e-01, PNorm = 151.2809, GNorm = 0.2216
Meta loss on this task batch = 2.7352e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 151.2863, GNorm = 0.2675
Took 127.68061399459839 seconds to complete one epoch of meta training
Took 135.67000317573547 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517990
Epoch 627
Meta loss on this task batch = 2.6506e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 151.2916, GNorm = 0.2144
Meta loss on this task batch = 2.4168e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 151.2980, GNorm = 0.1946
Meta loss on this task batch = 2.9240e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 151.3042, GNorm = 0.2355
Meta loss on this task batch = 2.3751e-01, Meta loss averaged over last 500 steps = 2.7374e-01, PNorm = 151.3106, GNorm = 0.2099
Meta loss on this task batch = 2.5642e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 151.3166, GNorm = 0.2013
Meta loss on this task batch = 3.5098e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 151.3209, GNorm = 0.2844
Meta loss on this task batch = 2.8036e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 151.3251, GNorm = 0.2423
Meta loss on this task batch = 2.6307e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 151.3287, GNorm = 0.2388
Meta loss on this task batch = 3.0039e-01, Meta loss averaged over last 500 steps = 2.7398e-01, PNorm = 151.3318, GNorm = 0.2439
Meta loss on this task batch = 2.5076e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 151.3362, GNorm = 0.2072
Meta loss on this task batch = 3.0009e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 151.3411, GNorm = 0.2335
Meta loss on this task batch = 2.3832e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 151.3471, GNorm = 0.2358
Meta loss on this task batch = 2.9056e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 151.3530, GNorm = 0.2423
Meta loss on this task batch = 2.3768e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 151.3591, GNorm = 0.2004
Meta loss on this task batch = 2.5393e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 151.3643, GNorm = 0.2241
Meta loss on this task batch = 3.1721e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 151.3684, GNorm = 0.2762
Meta loss on this task batch = 3.4408e-01, Meta loss averaged over last 500 steps = 2.7389e-01, PNorm = 151.3724, GNorm = 0.2518
Meta loss on this task batch = 2.7663e-01, Meta loss averaged over last 500 steps = 2.7395e-01, PNorm = 151.3768, GNorm = 0.2510
Meta loss on this task batch = 2.6727e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 151.3812, GNorm = 0.2702
Took 115.39940929412842 seconds to complete one epoch of meta training
Took 122.58053946495056 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497286
Epoch 628
Meta loss on this task batch = 2.7626e-01, Meta loss averaged over last 500 steps = 2.7384e-01, PNorm = 151.3865, GNorm = 0.2600
Meta loss on this task batch = 2.1051e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 151.3923, GNorm = 0.2079
Meta loss on this task batch = 2.4933e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 151.3981, GNorm = 0.1912
Meta loss on this task batch = 2.5138e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 151.4046, GNorm = 0.2203
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 151.4106, GNorm = 0.2129
Meta loss on this task batch = 3.3801e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 151.4168, GNorm = 0.2614
Meta loss on this task batch = 2.3854e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 151.4230, GNorm = 0.2391
Meta loss on this task batch = 3.5392e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 151.4300, GNorm = 0.3025
Meta loss on this task batch = 2.8895e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 151.4353, GNorm = 0.2382
Meta loss on this task batch = 3.1843e-01, Meta loss averaged over last 500 steps = 2.7378e-01, PNorm = 151.4403, GNorm = 0.2529
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 151.4452, GNorm = 0.2051
Meta loss on this task batch = 2.4210e-01, Meta loss averaged over last 500 steps = 2.7383e-01, PNorm = 151.4504, GNorm = 0.1884
Meta loss on this task batch = 3.3823e-01, Meta loss averaged over last 500 steps = 2.7387e-01, PNorm = 151.4554, GNorm = 0.2525
Meta loss on this task batch = 2.1190e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 151.4605, GNorm = 0.1874
Meta loss on this task batch = 2.8335e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 151.4647, GNorm = 0.2326
Meta loss on this task batch = 2.8648e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 151.4687, GNorm = 0.2564
Meta loss on this task batch = 2.2802e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 151.4730, GNorm = 0.1837
Meta loss on this task batch = 2.7882e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 151.4767, GNorm = 0.2438
Meta loss on this task batch = 3.4555e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 151.4788, GNorm = 0.3091
Took 116.81400418281555 seconds to complete one epoch of meta training
Took 124.5264675617218 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493573
Epoch 629
Meta loss on this task batch = 2.3479e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 151.4814, GNorm = 0.2026
Meta loss on this task batch = 2.9924e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 151.4835, GNorm = 0.2111
Meta loss on this task batch = 2.4699e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 151.4865, GNorm = 0.2059
Meta loss on this task batch = 2.4316e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 151.4899, GNorm = 0.2163
Meta loss on this task batch = 2.6098e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 151.4936, GNorm = 0.2001
Meta loss on this task batch = 3.1320e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 151.4976, GNorm = 0.2386
Meta loss on this task batch = 2.5606e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 151.5022, GNorm = 0.2325
Meta loss on this task batch = 2.4015e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 151.5069, GNorm = 0.1745
Meta loss on this task batch = 2.3418e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 151.5118, GNorm = 0.1813
Meta loss on this task batch = 3.2934e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 151.5168, GNorm = 0.2597
Meta loss on this task batch = 2.7165e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 151.5228, GNorm = 0.2279
Meta loss on this task batch = 3.1203e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 151.5286, GNorm = 0.2533
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 151.5340, GNorm = 0.2303
Meta loss on this task batch = 2.4094e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 151.5402, GNorm = 0.1945
Meta loss on this task batch = 3.3676e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 151.5449, GNorm = 0.2645
Meta loss on this task batch = 2.5595e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 151.5506, GNorm = 0.2431
Meta loss on this task batch = 2.4511e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 151.5576, GNorm = 0.2569
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 151.5643, GNorm = 0.2738
Meta loss on this task batch = 2.4250e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 151.5710, GNorm = 0.2608
Took 119.6298463344574 seconds to complete one epoch of meta training
Took 126.32366466522217 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515307
Epoch 630
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 151.5776, GNorm = 0.2553
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 151.5830, GNorm = 0.2994
Meta loss on this task batch = 2.8544e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 151.5885, GNorm = 0.2564
Meta loss on this task batch = 2.4131e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 151.5948, GNorm = 0.2025
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 151.6003, GNorm = 0.2760
Meta loss on this task batch = 2.6958e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 151.6046, GNorm = 0.2496
Meta loss on this task batch = 2.3752e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 151.6086, GNorm = 0.2193
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 151.6132, GNorm = 0.2698
Meta loss on this task batch = 3.3500e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 151.6176, GNorm = 0.3265
Meta loss on this task batch = 2.8625e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 151.6229, GNorm = 0.2417
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 151.6290, GNorm = 0.2631
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 151.6362, GNorm = 0.2292
Meta loss on this task batch = 1.9777e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 151.6438, GNorm = 0.1877
Meta loss on this task batch = 2.7270e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 151.6512, GNorm = 0.2303
Meta loss on this task batch = 2.3486e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 151.6583, GNorm = 0.2152
Meta loss on this task batch = 2.7541e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 151.6653, GNorm = 0.2585
Meta loss on this task batch = 3.4893e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 151.6718, GNorm = 0.2564
Meta loss on this task batch = 2.3730e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 151.6786, GNorm = 0.2400
Meta loss on this task batch = 2.7379e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 151.6857, GNorm = 0.2786
Took 115.54757404327393 seconds to complete one epoch of meta training
Took 122.92488551139832 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484624
Epoch 631
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 151.6912, GNorm = 0.2363
Meta loss on this task batch = 2.9684e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 151.6964, GNorm = 0.2445
Meta loss on this task batch = 2.3963e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 151.7017, GNorm = 0.2183
Meta loss on this task batch = 2.3156e-01, Meta loss averaged over last 500 steps = 2.7315e-01, PNorm = 151.7061, GNorm = 0.2576
Meta loss on this task batch = 3.4219e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 151.7104, GNorm = 0.2822
Meta loss on this task batch = 2.6519e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 151.7148, GNorm = 0.2386
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 151.7187, GNorm = 0.2461
Meta loss on this task batch = 2.6016e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 151.7223, GNorm = 0.2178
Meta loss on this task batch = 2.7743e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 151.7259, GNorm = 0.2682
Meta loss on this task batch = 2.4314e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 151.7303, GNorm = 0.2079
Meta loss on this task batch = 2.8853e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 151.7355, GNorm = 0.2493
Meta loss on this task batch = 3.0793e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 151.7403, GNorm = 0.2448
Meta loss on this task batch = 2.3436e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 151.7448, GNorm = 0.2077
Meta loss on this task batch = 2.8210e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 151.7486, GNorm = 0.2428
Meta loss on this task batch = 2.7459e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 151.7531, GNorm = 0.2706
Meta loss on this task batch = 2.7628e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 151.7587, GNorm = 0.2266
Meta loss on this task batch = 2.9299e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 151.7641, GNorm = 0.2224
Meta loss on this task batch = 2.8761e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 151.7694, GNorm = 0.2465
Meta loss on this task batch = 3.0647e-01, Meta loss averaged over last 500 steps = 2.7386e-01, PNorm = 151.7748, GNorm = 0.3386
Took 115.16914963722229 seconds to complete one epoch of meta training
Took 123.24265336990356 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487610
Epoch 632
Meta loss on this task batch = 1.9637e-01, Meta loss averaged over last 500 steps = 2.7375e-01, PNorm = 151.7808, GNorm = 0.2207
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 151.7864, GNorm = 0.2776
Meta loss on this task batch = 3.1448e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 151.7919, GNorm = 0.2279
Meta loss on this task batch = 3.3398e-01, Meta loss averaged over last 500 steps = 2.7377e-01, PNorm = 151.7967, GNorm = 0.2634
Meta loss on this task batch = 2.8159e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 151.8015, GNorm = 0.2181
Meta loss on this task batch = 2.5488e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 151.8073, GNorm = 0.2138
Meta loss on this task batch = 2.1522e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 151.8140, GNorm = 0.2142
Meta loss on this task batch = 3.5856e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 151.8207, GNorm = 0.2803
Meta loss on this task batch = 2.3687e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 151.8277, GNorm = 0.2250
Meta loss on this task batch = 2.5621e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 151.8351, GNorm = 0.2433
Meta loss on this task batch = 2.5271e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 151.8424, GNorm = 0.2181
Meta loss on this task batch = 2.6767e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 151.8491, GNorm = 0.2317
Meta loss on this task batch = 2.7778e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 151.8560, GNorm = 0.2520
Meta loss on this task batch = 2.5151e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 151.8632, GNorm = 0.2427
Meta loss on this task batch = 2.7444e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 151.8704, GNorm = 0.2578
Meta loss on this task batch = 2.3617e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 151.8775, GNorm = 0.2121
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 151.8836, GNorm = 0.2340
Meta loss on this task batch = 2.9187e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 151.8889, GNorm = 0.2419
Meta loss on this task batch = 3.2175e-01, Meta loss averaged over last 500 steps = 2.7380e-01, PNorm = 151.8932, GNorm = 0.2873
Took 114.57046508789062 seconds to complete one epoch of meta training
Took 122.33097386360168 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483815
Epoch 633
Meta loss on this task batch = 2.4265e-01, Meta loss averaged over last 500 steps = 2.7373e-01, PNorm = 151.8971, GNorm = 0.2331
Meta loss on this task batch = 2.3359e-01, Meta loss averaged over last 500 steps = 2.7360e-01, PNorm = 151.9010, GNorm = 0.2261
Meta loss on this task batch = 2.8160e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 151.9048, GNorm = 0.2162
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 151.9092, GNorm = 0.2067
Meta loss on this task batch = 3.3222e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 151.9132, GNorm = 0.2295
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 151.9172, GNorm = 0.2196
Meta loss on this task batch = 3.0497e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 151.9214, GNorm = 0.2484
Meta loss on this task batch = 2.3206e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 151.9265, GNorm = 0.2150
Meta loss on this task batch = 2.9263e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 151.9316, GNorm = 0.2564
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.7371e-01, PNorm = 151.9364, GNorm = 0.2311
Meta loss on this task batch = 2.4465e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 151.9411, GNorm = 0.1916
Meta loss on this task batch = 2.6355e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 151.9459, GNorm = 0.2119
Meta loss on this task batch = 3.4522e-01, Meta loss averaged over last 500 steps = 2.7369e-01, PNorm = 151.9506, GNorm = 0.2692
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.7369e-01, PNorm = 151.9553, GNorm = 0.2323
Meta loss on this task batch = 2.9309e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 151.9602, GNorm = 0.2360
Meta loss on this task batch = 2.7325e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 151.9652, GNorm = 0.2343
Meta loss on this task batch = 3.2714e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 151.9693, GNorm = 0.2387
Meta loss on this task batch = 2.3983e-01, Meta loss averaged over last 500 steps = 2.7362e-01, PNorm = 151.9735, GNorm = 0.2308
Meta loss on this task batch = 2.3516e-01, Meta loss averaged over last 500 steps = 2.7356e-01, PNorm = 151.9777, GNorm = 0.2289
Took 113.36144351959229 seconds to complete one epoch of meta training
Took 120.94711756706238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502997
Epoch 634
Meta loss on this task batch = 2.9204e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 151.9821, GNorm = 0.2233
Meta loss on this task batch = 2.5118e-01, Meta loss averaged over last 500 steps = 2.7368e-01, PNorm = 151.9869, GNorm = 0.2110
Meta loss on this task batch = 2.9002e-01, Meta loss averaged over last 500 steps = 2.7372e-01, PNorm = 151.9923, GNorm = 0.2432
Meta loss on this task batch = 2.2429e-01, Meta loss averaged over last 500 steps = 2.7357e-01, PNorm = 151.9984, GNorm = 0.2416
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.7355e-01, PNorm = 152.0050, GNorm = 0.2053
Meta loss on this task batch = 2.2493e-01, Meta loss averaged over last 500 steps = 2.7361e-01, PNorm = 152.0119, GNorm = 0.2090
Meta loss on this task batch = 2.2687e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 152.0189, GNorm = 0.1977
Meta loss on this task batch = 2.6519e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 152.0252, GNorm = 0.2251
Meta loss on this task batch = 2.2243e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 152.0309, GNorm = 0.2853
Meta loss on this task batch = 2.3724e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 152.0369, GNorm = 0.2256
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 152.0428, GNorm = 0.2557
Meta loss on this task batch = 3.0382e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 152.0484, GNorm = 0.2320
Meta loss on this task batch = 3.3363e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 152.0541, GNorm = 0.2491
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 2.7363e-01, PNorm = 152.0591, GNorm = 0.2215
Meta loss on this task batch = 3.0912e-01, Meta loss averaged over last 500 steps = 2.7370e-01, PNorm = 152.0647, GNorm = 0.2797
Meta loss on this task batch = 2.9834e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 152.0690, GNorm = 0.2566
Meta loss on this task batch = 2.0907e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 152.0739, GNorm = 0.1905
Meta loss on this task batch = 3.0074e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 152.0773, GNorm = 0.2527
Meta loss on this task batch = 3.2008e-01, Meta loss averaged over last 500 steps = 2.7364e-01, PNorm = 152.0797, GNorm = 0.2891
Took 113.63018894195557 seconds to complete one epoch of meta training
Took 120.987380027771 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488181
Epoch 635
Meta loss on this task batch = 2.8899e-01, Meta loss averaged over last 500 steps = 2.7365e-01, PNorm = 152.0824, GNorm = 0.2482
Meta loss on this task batch = 2.4337e-01, Meta loss averaged over last 500 steps = 2.7358e-01, PNorm = 152.0855, GNorm = 0.2069
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 152.0897, GNorm = 0.2131
Meta loss on this task batch = 2.2802e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 152.0938, GNorm = 0.2255
Meta loss on this task batch = 3.1488e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 152.0980, GNorm = 0.2105
Meta loss on this task batch = 2.5129e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 152.1027, GNorm = 0.2310
Meta loss on this task batch = 3.0084e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 152.1082, GNorm = 0.3213
Meta loss on this task batch = 2.4592e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 152.1137, GNorm = 0.2228
Meta loss on this task batch = 2.5578e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 152.1188, GNorm = 0.2204
Meta loss on this task batch = 2.7551e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 152.1227, GNorm = 0.2344
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.1265, GNorm = 0.2364
Meta loss on this task batch = 3.3680e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.1302, GNorm = 0.2624
Meta loss on this task batch = 2.9504e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 152.1335, GNorm = 0.2098
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 152.1378, GNorm = 0.2283
Meta loss on this task batch = 2.7034e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 152.1417, GNorm = 0.2377
Meta loss on this task batch = 2.9951e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 152.1445, GNorm = 0.2443
Meta loss on this task batch = 2.7502e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.1476, GNorm = 0.2192
Meta loss on this task batch = 2.6659e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 152.1514, GNorm = 0.1889
Meta loss on this task batch = 2.6951e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 152.1555, GNorm = 0.2569
Took 114.92315411567688 seconds to complete one epoch of meta training
Took 121.93083500862122 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516251
Epoch 636
Meta loss on this task batch = 2.9469e-01, Meta loss averaged over last 500 steps = 2.7330e-01, PNorm = 152.1595, GNorm = 0.2580
Meta loss on this task batch = 2.4068e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.1644, GNorm = 0.2356
Meta loss on this task batch = 2.9779e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 152.1690, GNorm = 0.2131
Meta loss on this task batch = 3.1013e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 152.1728, GNorm = 0.2501
Meta loss on this task batch = 2.2711e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 152.1772, GNorm = 0.1948
Meta loss on this task batch = 2.7057e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 152.1823, GNorm = 0.1989
Meta loss on this task batch = 2.4102e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 152.1879, GNorm = 0.2139
Meta loss on this task batch = 2.8174e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 152.1937, GNorm = 0.2569
Meta loss on this task batch = 3.2799e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 152.1994, GNorm = 0.2873
Meta loss on this task batch = 2.7358e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 152.2055, GNorm = 0.2448
Meta loss on this task batch = 2.4057e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 152.2114, GNorm = 0.2023
Meta loss on this task batch = 2.6931e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 152.2169, GNorm = 0.2330
Meta loss on this task batch = 3.4511e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 152.2216, GNorm = 0.2634
Meta loss on this task batch = 2.2570e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 152.2255, GNorm = 0.2168
Meta loss on this task batch = 2.8242e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 152.2293, GNorm = 0.2429
Meta loss on this task batch = 2.7293e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 152.2330, GNorm = 0.2237
Meta loss on this task batch = 2.4675e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 152.2372, GNorm = 0.1966
Meta loss on this task batch = 2.8476e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 152.2416, GNorm = 0.2745
Meta loss on this task batch = 3.0341e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 152.2462, GNorm = 0.2908
Took 219.74533677101135 seconds to complete one epoch of meta training
Took 227.40101718902588 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514407
Epoch 637
Meta loss on this task batch = 3.0906e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 152.2511, GNorm = 0.2449
Meta loss on this task batch = 2.6731e-01, Meta loss averaged over last 500 steps = 2.7349e-01, PNorm = 152.2563, GNorm = 0.2218
Meta loss on this task batch = 2.8463e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 152.2602, GNorm = 0.2925
Meta loss on this task batch = 2.6687e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 152.2644, GNorm = 0.2124
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 152.2695, GNorm = 0.2399
Meta loss on this task batch = 2.4077e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 152.2744, GNorm = 0.2034
Meta loss on this task batch = 2.6640e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.2784, GNorm = 0.2288
Meta loss on this task batch = 3.1293e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 152.2825, GNorm = 0.2434
Meta loss on this task batch = 2.8868e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 152.2869, GNorm = 0.2376
Meta loss on this task batch = 2.6462e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 152.2923, GNorm = 0.2359
Meta loss on this task batch = 2.5725e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 152.2975, GNorm = 0.2261
Meta loss on this task batch = 3.0401e-01, Meta loss averaged over last 500 steps = 2.7359e-01, PNorm = 152.3024, GNorm = 0.2273
Meta loss on this task batch = 2.5583e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 152.3074, GNorm = 0.2219
Meta loss on this task batch = 2.8241e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 152.3108, GNorm = 0.2503
Meta loss on this task batch = 2.5380e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 152.3150, GNorm = 0.1997
Meta loss on this task batch = 2.7917e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 152.3193, GNorm = 0.2407
Meta loss on this task batch = 3.1010e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 152.3240, GNorm = 0.2412
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 152.3286, GNorm = 0.2153
Meta loss on this task batch = 3.0899e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 152.3332, GNorm = 0.3155
Took 226.70109105110168 seconds to complete one epoch of meta training
Took 234.9074878692627 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499726
Epoch 638
Meta loss on this task batch = 3.0511e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 152.3384, GNorm = 0.3029
Meta loss on this task batch = 2.2857e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 152.3430, GNorm = 0.2397
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 152.3477, GNorm = 0.2078
Meta loss on this task batch = 2.3501e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.3532, GNorm = 0.2116
Meta loss on this task batch = 3.2072e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 152.3580, GNorm = 0.2388
Meta loss on this task batch = 2.3031e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 152.3634, GNorm = 0.2214
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 152.3693, GNorm = 0.2082
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 152.3748, GNorm = 0.2343
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 152.3797, GNorm = 0.2572
Meta loss on this task batch = 2.5194e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 152.3850, GNorm = 0.1954
Meta loss on this task batch = 2.7251e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.3905, GNorm = 0.2289
Meta loss on this task batch = 2.4933e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 152.3959, GNorm = 0.2378
Meta loss on this task batch = 2.1801e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 152.4031, GNorm = 0.2214
Meta loss on this task batch = 2.3359e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 152.4104, GNorm = 0.2200
Meta loss on this task batch = 2.4301e-01, Meta loss averaged over last 500 steps = 2.7310e-01, PNorm = 152.4172, GNorm = 0.2111
Meta loss on this task batch = 2.8135e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 152.4225, GNorm = 0.2871
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 152.4263, GNorm = 0.2204
Meta loss on this task batch = 2.7373e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 152.4298, GNorm = 0.2588
Meta loss on this task batch = 3.3464e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.4330, GNorm = 0.3030
Took 113.2612624168396 seconds to complete one epoch of meta training
Took 121.64195847511292 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480520
Epoch 639
Meta loss on this task batch = 2.7755e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 152.4370, GNorm = 0.2187
Meta loss on this task batch = 2.9251e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 152.4416, GNorm = 0.2620
Meta loss on this task batch = 3.1631e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 152.4466, GNorm = 0.2308
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 152.4517, GNorm = 0.2654
Meta loss on this task batch = 2.7995e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 152.4574, GNorm = 0.2559
Meta loss on this task batch = 3.1071e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 152.4632, GNorm = 0.2726
Meta loss on this task batch = 2.3855e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 152.4684, GNorm = 0.2027
Meta loss on this task batch = 2.2393e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 152.4739, GNorm = 0.2174
Meta loss on this task batch = 2.2552e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 152.4796, GNorm = 0.2112
Meta loss on this task batch = 2.6084e-01, Meta loss averaged over last 500 steps = 2.7309e-01, PNorm = 152.4857, GNorm = 0.2273
Meta loss on this task batch = 2.5552e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 152.4924, GNorm = 0.2134
Meta loss on this task batch = 2.6275e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.4997, GNorm = 0.2137
Meta loss on this task batch = 2.7997e-01, Meta loss averaged over last 500 steps = 2.7302e-01, PNorm = 152.5070, GNorm = 0.2125
Meta loss on this task batch = 2.7094e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 152.5131, GNorm = 0.2282
Meta loss on this task batch = 2.8857e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 152.5191, GNorm = 0.2264
Meta loss on this task batch = 3.6135e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 152.5242, GNorm = 0.2782
Meta loss on this task batch = 2.8484e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 152.5299, GNorm = 0.2380
Meta loss on this task batch = 2.7525e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 152.5354, GNorm = 0.2207
Meta loss on this task batch = 2.2779e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 152.5407, GNorm = 0.2675
Took 161.73063278198242 seconds to complete one epoch of meta training
Took 169.58232593536377 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503049
Epoch 640
Meta loss on this task batch = 2.3595e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 152.5450, GNorm = 0.2201
Meta loss on this task batch = 2.5277e-01, Meta loss averaged over last 500 steps = 2.7309e-01, PNorm = 152.5499, GNorm = 0.2162
Meta loss on this task batch = 3.0633e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 152.5544, GNorm = 0.2815
Meta loss on this task batch = 2.1763e-01, Meta loss averaged over last 500 steps = 2.7315e-01, PNorm = 152.5591, GNorm = 0.1941
Meta loss on this task batch = 3.0980e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 152.5629, GNorm = 0.2612
Meta loss on this task batch = 2.5179e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 152.5667, GNorm = 0.2331
Meta loss on this task batch = 2.1948e-01, Meta loss averaged over last 500 steps = 2.7298e-01, PNorm = 152.5718, GNorm = 0.2001
Meta loss on this task batch = 2.6523e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 152.5772, GNorm = 0.2165
Meta loss on this task batch = 2.7007e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.5830, GNorm = 0.2202
Meta loss on this task batch = 3.0336e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.5885, GNorm = 0.2231
Meta loss on this task batch = 2.3925e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.5945, GNorm = 0.2309
Meta loss on this task batch = 2.6891e-01, Meta loss averaged over last 500 steps = 2.7296e-01, PNorm = 152.6004, GNorm = 0.2153
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.7295e-01, PNorm = 152.6064, GNorm = 0.2116
Meta loss on this task batch = 3.2674e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 152.6128, GNorm = 0.3171
Meta loss on this task batch = 2.3734e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 152.6189, GNorm = 0.2022
Meta loss on this task batch = 3.3049e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 152.6251, GNorm = 0.2855
Meta loss on this task batch = 2.6100e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 152.6316, GNorm = 0.2449
Meta loss on this task batch = 2.8629e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 152.6378, GNorm = 0.2587
Meta loss on this task batch = 2.8377e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 152.6443, GNorm = 0.2876
Took 134.12607407569885 seconds to complete one epoch of meta training
Took 142.27216267585754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478028
Epoch 641
Meta loss on this task batch = 2.9405e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 152.6508, GNorm = 0.2579
Meta loss on this task batch = 2.5256e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 152.6569, GNorm = 0.2564
Meta loss on this task batch = 3.0223e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 152.6625, GNorm = 0.2587
Meta loss on this task batch = 2.4398e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 152.6680, GNorm = 0.2059
Meta loss on this task batch = 2.5003e-01, Meta loss averaged over last 500 steps = 2.7310e-01, PNorm = 152.6730, GNorm = 0.3073
Meta loss on this task batch = 2.6461e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 152.6782, GNorm = 0.2512
Meta loss on this task batch = 2.5352e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 152.6829, GNorm = 0.2189
Meta loss on this task batch = 3.1503e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 152.6876, GNorm = 0.2116
Meta loss on this task batch = 2.6329e-01, Meta loss averaged over last 500 steps = 2.7305e-01, PNorm = 152.6922, GNorm = 0.2046
Meta loss on this task batch = 2.4909e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.6965, GNorm = 0.2261
Meta loss on this task batch = 2.5599e-01, Meta loss averaged over last 500 steps = 2.7295e-01, PNorm = 152.7007, GNorm = 0.2318
Meta loss on this task batch = 3.0377e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 152.7043, GNorm = 0.3509
Meta loss on this task batch = 3.4606e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 152.7070, GNorm = 0.2256
Meta loss on this task batch = 2.8620e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 152.7101, GNorm = 0.2295
Meta loss on this task batch = 2.6513e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 152.7139, GNorm = 0.2354
Meta loss on this task batch = 2.1264e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 152.7188, GNorm = 0.1790
Meta loss on this task batch = 2.8305e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 152.7247, GNorm = 0.2241
Meta loss on this task batch = 2.9081e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 152.7310, GNorm = 0.2488
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 152.7354, GNorm = 0.3297
Took 115.73073697090149 seconds to complete one epoch of meta training
Took 123.41542315483093 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502528
Epoch 642
Meta loss on this task batch = 2.9003e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 152.7395, GNorm = 0.2305
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 152.7433, GNorm = 0.2087
Meta loss on this task batch = 2.6229e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 152.7467, GNorm = 0.2164
Meta loss on this task batch = 3.0581e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 152.7489, GNorm = 0.2983
Meta loss on this task batch = 2.4382e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 152.7516, GNorm = 0.1998
Meta loss on this task batch = 2.8270e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 152.7543, GNorm = 0.2136
Meta loss on this task batch = 2.9864e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 152.7575, GNorm = 0.2376
Meta loss on this task batch = 1.9927e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.7615, GNorm = 0.1921
Meta loss on this task batch = 2.5092e-01, Meta loss averaged over last 500 steps = 2.7302e-01, PNorm = 152.7665, GNorm = 0.2196
Meta loss on this task batch = 3.5851e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 152.7717, GNorm = 0.2643
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 152.7770, GNorm = 0.2283
Meta loss on this task batch = 2.3982e-01, Meta loss averaged over last 500 steps = 2.7305e-01, PNorm = 152.7830, GNorm = 0.1898
Meta loss on this task batch = 2.6404e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 152.7878, GNorm = 0.2215
Meta loss on this task batch = 2.6571e-01, Meta loss averaged over last 500 steps = 2.7304e-01, PNorm = 152.7929, GNorm = 0.2299
Meta loss on this task batch = 2.9969e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 152.7978, GNorm = 0.2693
Meta loss on this task batch = 2.7851e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 152.8029, GNorm = 0.2446
Meta loss on this task batch = 2.7972e-01, Meta loss averaged over last 500 steps = 2.7306e-01, PNorm = 152.8076, GNorm = 0.2334
Meta loss on this task batch = 2.4553e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 152.8123, GNorm = 0.2068
Meta loss on this task batch = 2.8976e-01, Meta loss averaged over last 500 steps = 2.7297e-01, PNorm = 152.8163, GNorm = 0.3119
Took 114.43981957435608 seconds to complete one epoch of meta training
Took 122.04570436477661 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515413
Epoch 643
Meta loss on this task batch = 2.8624e-01, Meta loss averaged over last 500 steps = 2.7298e-01, PNorm = 152.8203, GNorm = 0.2327
Meta loss on this task batch = 2.2259e-01, Meta loss averaged over last 500 steps = 2.7292e-01, PNorm = 152.8255, GNorm = 0.1993
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.7292e-01, PNorm = 152.8308, GNorm = 0.2313
Meta loss on this task batch = 2.4216e-01, Meta loss averaged over last 500 steps = 2.7288e-01, PNorm = 152.8361, GNorm = 0.2235
Meta loss on this task batch = 2.4032e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 152.8412, GNorm = 0.2393
Meta loss on this task batch = 3.3067e-01, Meta loss averaged over last 500 steps = 2.7304e-01, PNorm = 152.8441, GNorm = 0.3632
Meta loss on this task batch = 2.3938e-01, Meta loss averaged over last 500 steps = 2.7301e-01, PNorm = 152.8469, GNorm = 0.2392
Meta loss on this task batch = 2.8122e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 152.8499, GNorm = 0.2273
Meta loss on this task batch = 3.1975e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.8531, GNorm = 0.2668
Meta loss on this task batch = 2.6928e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.8564, GNorm = 0.2430
Meta loss on this task batch = 2.8172e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 152.8605, GNorm = 0.2231
Meta loss on this task batch = 2.9843e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 152.8650, GNorm = 0.2270
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 152.8693, GNorm = 0.2511
Meta loss on this task batch = 2.4301e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.8745, GNorm = 0.2161
Meta loss on this task batch = 2.9040e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 152.8797, GNorm = 0.2630
Meta loss on this task batch = 3.1545e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 152.8858, GNorm = 0.2465
Meta loss on this task batch = 2.3147e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 152.8922, GNorm = 0.2375
Meta loss on this task batch = 3.0279e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 152.8979, GNorm = 0.2546
Meta loss on this task batch = 2.4567e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 152.9044, GNorm = 0.2411
Took 117.0657012462616 seconds to complete one epoch of meta training
Took 124.63337016105652 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496561
Epoch 644
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 152.9108, GNorm = 0.2113
Meta loss on this task batch = 2.7003e-01, Meta loss averaged over last 500 steps = 2.7306e-01, PNorm = 152.9166, GNorm = 0.2372
Meta loss on this task batch = 2.4944e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 152.9216, GNorm = 0.2201
Meta loss on this task batch = 2.8883e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 152.9262, GNorm = 0.2483
Meta loss on this task batch = 2.6844e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 152.9314, GNorm = 0.2134
Meta loss on this task batch = 2.8466e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 152.9367, GNorm = 0.3236
Meta loss on this task batch = 2.8661e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 152.9416, GNorm = 0.2472
Meta loss on this task batch = 2.6557e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.9465, GNorm = 0.2075
Meta loss on this task batch = 3.0366e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 152.9516, GNorm = 0.2226
Meta loss on this task batch = 2.8890e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 152.9562, GNorm = 0.2712
Meta loss on this task batch = 2.3113e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 152.9610, GNorm = 0.2042
Meta loss on this task batch = 2.7591e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 152.9660, GNorm = 0.2412
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 152.9704, GNorm = 0.2592
Meta loss on this task batch = 2.7188e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 152.9747, GNorm = 0.2115
Meta loss on this task batch = 2.6071e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.9793, GNorm = 0.2149
Meta loss on this task batch = 3.0222e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 152.9829, GNorm = 0.2514
Meta loss on this task batch = 2.3796e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 152.9872, GNorm = 0.2115
Meta loss on this task batch = 2.7590e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 152.9918, GNorm = 0.2395
Meta loss on this task batch = 2.8355e-01, Meta loss averaged over last 500 steps = 2.7330e-01, PNorm = 152.9964, GNorm = 0.2690
Took 113.38420033454895 seconds to complete one epoch of meta training
Took 121.4624617099762 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493495
Epoch 645
Meta loss on this task batch = 2.8165e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 153.0005, GNorm = 0.2613
Meta loss on this task batch = 2.4797e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.0053, GNorm = 0.2073
Meta loss on this task batch = 2.4446e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 153.0110, GNorm = 0.2224
Meta loss on this task batch = 3.1128e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 153.0157, GNorm = 0.2629
Meta loss on this task batch = 2.5991e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.0206, GNorm = 0.2128
Meta loss on this task batch = 2.5276e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 153.0260, GNorm = 0.2173
Meta loss on this task batch = 2.9312e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 153.0307, GNorm = 0.2569
Meta loss on this task batch = 2.9904e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 153.0353, GNorm = 0.2366
Meta loss on this task batch = 2.4471e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 153.0391, GNorm = 0.2301
Meta loss on this task batch = 2.3662e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 153.0436, GNorm = 0.2063
Meta loss on this task batch = 2.0115e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 153.0486, GNorm = 0.2080
Meta loss on this task batch = 2.2782e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 153.0532, GNorm = 0.2286
Meta loss on this task batch = 2.5541e-01, Meta loss averaged over last 500 steps = 2.7294e-01, PNorm = 153.0582, GNorm = 0.2411
Meta loss on this task batch = 2.5790e-01, Meta loss averaged over last 500 steps = 2.7290e-01, PNorm = 153.0628, GNorm = 0.2284
Meta loss on this task batch = 3.0037e-01, Meta loss averaged over last 500 steps = 2.7288e-01, PNorm = 153.0681, GNorm = 0.2678
Meta loss on this task batch = 3.7333e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 153.0735, GNorm = 0.3198
Meta loss on this task batch = 3.1895e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.0782, GNorm = 0.2448
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 153.0832, GNorm = 0.2262
Meta loss on this task batch = 3.3174e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 153.0871, GNorm = 0.3007
Took 113.57449698448181 seconds to complete one epoch of meta training
Took 120.84424781799316 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503939
Epoch 646
Meta loss on this task batch = 2.4722e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 153.0918, GNorm = 0.2095
Meta loss on this task batch = 2.9648e-01, Meta loss averaged over last 500 steps = 2.7327e-01, PNorm = 153.0962, GNorm = 0.2227
Meta loss on this task batch = 2.9207e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 153.1005, GNorm = 0.2539
Meta loss on this task batch = 2.8915e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 153.1054, GNorm = 0.2291
Meta loss on this task batch = 3.0156e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.1087, GNorm = 0.2980
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.1125, GNorm = 0.2168
Meta loss on this task batch = 3.1303e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 153.1159, GNorm = 0.2214
Meta loss on this task batch = 2.5886e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.1191, GNorm = 0.2150
Meta loss on this task batch = 2.5073e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 153.1232, GNorm = 0.2082
Meta loss on this task batch = 2.8871e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 153.1289, GNorm = 0.2779
Meta loss on this task batch = 2.8356e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 153.1352, GNorm = 0.2197
Meta loss on this task batch = 2.8409e-01, Meta loss averaged over last 500 steps = 2.7336e-01, PNorm = 153.1416, GNorm = 0.2206
Meta loss on this task batch = 2.2064e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 153.1478, GNorm = 0.2032
Meta loss on this task batch = 2.5415e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 153.1542, GNorm = 0.1975
Meta loss on this task batch = 2.3039e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 153.1601, GNorm = 0.2417
Meta loss on this task batch = 2.4068e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 153.1654, GNorm = 0.2205
Meta loss on this task batch = 3.4492e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 153.1696, GNorm = 0.2748
Meta loss on this task batch = 2.7762e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.1736, GNorm = 0.2222
Meta loss on this task batch = 2.9666e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 153.1756, GNorm = 0.3114
Took 114.77971768379211 seconds to complete one epoch of meta training
Took 122.60202884674072 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477704
Epoch 647
Meta loss on this task batch = 2.6764e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 153.1779, GNorm = 0.2212
Meta loss on this task batch = 2.8591e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 153.1789, GNorm = 0.2839
Meta loss on this task batch = 2.9251e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 153.1814, GNorm = 0.2172
Meta loss on this task batch = 2.7887e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 153.1842, GNorm = 0.2349
Meta loss on this task batch = 2.9868e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 153.1874, GNorm = 0.2731
Meta loss on this task batch = 2.5503e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 153.1913, GNorm = 0.2021
Meta loss on this task batch = 2.9126e-01, Meta loss averaged over last 500 steps = 2.7351e-01, PNorm = 153.1963, GNorm = 0.2368
Meta loss on this task batch = 2.5705e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 153.2018, GNorm = 0.2143
Meta loss on this task batch = 2.0948e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.2077, GNorm = 0.2040
Meta loss on this task batch = 3.0935e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 153.2138, GNorm = 0.2592
Meta loss on this task batch = 2.8800e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 153.2210, GNorm = 0.2358
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.7350e-01, PNorm = 153.2286, GNorm = 0.2161
Meta loss on this task batch = 2.2252e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 153.2357, GNorm = 0.2490
Meta loss on this task batch = 2.5901e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.2420, GNorm = 0.2700
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 153.2475, GNorm = 0.2574
Meta loss on this task batch = 3.0209e-01, Meta loss averaged over last 500 steps = 2.7314e-01, PNorm = 153.2525, GNorm = 0.2492
Meta loss on this task batch = 2.8875e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 153.2582, GNorm = 0.2714
Meta loss on this task batch = 2.5678e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.2643, GNorm = 0.2092
Meta loss on this task batch = 2.3613e-01, Meta loss averaged over last 500 steps = 2.7307e-01, PNorm = 153.2701, GNorm = 0.2993
Took 114.47089290618896 seconds to complete one epoch of meta training
Took 122.65388655662537 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499557
Epoch 648
Meta loss on this task batch = 3.2543e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 153.2751, GNorm = 0.2566
Meta loss on this task batch = 2.4936e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 153.2799, GNorm = 0.2186
Meta loss on this task batch = 2.4040e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 153.2848, GNorm = 0.2298
Meta loss on this task batch = 2.6771e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 153.2899, GNorm = 0.2354
Meta loss on this task batch = 3.5990e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 153.2941, GNorm = 0.2579
Meta loss on this task batch = 2.5916e-01, Meta loss averaged over last 500 steps = 2.7354e-01, PNorm = 153.2975, GNorm = 0.2045
Meta loss on this task batch = 2.5611e-01, Meta loss averaged over last 500 steps = 2.7344e-01, PNorm = 153.3014, GNorm = 0.2330
Meta loss on this task batch = 2.2472e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 153.3063, GNorm = 0.2273
Meta loss on this task batch = 3.3323e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 153.3111, GNorm = 0.2370
Meta loss on this task batch = 2.5293e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 153.3154, GNorm = 0.2287
Meta loss on this task batch = 2.2841e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 153.3201, GNorm = 0.2156
Meta loss on this task batch = 2.5447e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.3256, GNorm = 0.2176
Meta loss on this task batch = 2.9535e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 153.3311, GNorm = 0.2422
Meta loss on this task batch = 2.7729e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 153.3363, GNorm = 0.2166
Meta loss on this task batch = 2.6812e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 153.3415, GNorm = 0.2468
Meta loss on this task batch = 2.4480e-01, Meta loss averaged over last 500 steps = 2.7346e-01, PNorm = 153.3476, GNorm = 0.2212
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 153.3539, GNorm = 0.2325
Meta loss on this task batch = 2.5991e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 153.3603, GNorm = 0.2354
Meta loss on this task batch = 2.0869e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 153.3670, GNorm = 0.2313
Took 113.66103744506836 seconds to complete one epoch of meta training
Took 121.47029709815979 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483621
Epoch 649
Meta loss on this task batch = 2.6310e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 153.3724, GNorm = 0.2628
Meta loss on this task batch = 2.8934e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.3775, GNorm = 0.2273
Meta loss on this task batch = 2.6070e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 153.3810, GNorm = 0.2311
Meta loss on this task batch = 2.4052e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.3843, GNorm = 0.2634
Meta loss on this task batch = 2.2795e-01, Meta loss averaged over last 500 steps = 2.7313e-01, PNorm = 153.3883, GNorm = 0.2270
Meta loss on this task batch = 2.8316e-01, Meta loss averaged over last 500 steps = 2.7309e-01, PNorm = 153.3919, GNorm = 0.2481
Meta loss on this task batch = 2.9660e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 153.3962, GNorm = 0.2660
Meta loss on this task batch = 2.5034e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 153.4005, GNorm = 0.2323
Meta loss on this task batch = 3.3380e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 153.4041, GNorm = 0.2778
Meta loss on this task batch = 2.5099e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.4085, GNorm = 0.2385
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 153.4144, GNorm = 0.2402
Meta loss on this task batch = 2.5404e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.4201, GNorm = 0.2051
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.4266, GNorm = 0.2593
Meta loss on this task batch = 3.1114e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 153.4321, GNorm = 0.2634
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 153.4381, GNorm = 0.2443
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.4445, GNorm = 0.1965
Meta loss on this task batch = 2.5880e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.4520, GNorm = 0.2073
Meta loss on this task batch = 2.6429e-01, Meta loss averaged over last 500 steps = 2.7332e-01, PNorm = 153.4599, GNorm = 0.2402
Meta loss on this task batch = 2.8447e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 153.4670, GNorm = 0.2559
Took 115.32285070419312 seconds to complete one epoch of meta training
Took 123.14057731628418 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490104
Epoch 650
Meta loss on this task batch = 2.8454e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 153.4739, GNorm = 0.2153
Meta loss on this task batch = 3.0340e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 153.4801, GNorm = 0.2552
Meta loss on this task batch = 2.4030e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 153.4856, GNorm = 0.2327
Meta loss on this task batch = 2.8204e-01, Meta loss averaged over last 500 steps = 2.7347e-01, PNorm = 153.4914, GNorm = 0.2327
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 2.7348e-01, PNorm = 153.4968, GNorm = 0.2415
Meta loss on this task batch = 3.7795e-01, Meta loss averaged over last 500 steps = 2.7376e-01, PNorm = 153.5008, GNorm = 0.3153
Meta loss on this task batch = 2.4287e-01, Meta loss averaged over last 500 steps = 2.7366e-01, PNorm = 153.5055, GNorm = 0.2434
Meta loss on this task batch = 2.6469e-01, Meta loss averaged over last 500 steps = 2.7367e-01, PNorm = 153.5105, GNorm = 0.2182
Meta loss on this task batch = 2.7191e-01, Meta loss averaged over last 500 steps = 2.7352e-01, PNorm = 153.5158, GNorm = 0.2708
Meta loss on this task batch = 2.6622e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.5215, GNorm = 0.2132
Meta loss on this task batch = 2.6647e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 153.5276, GNorm = 0.2114
Meta loss on this task batch = 2.5521e-01, Meta loss averaged over last 500 steps = 2.7341e-01, PNorm = 153.5340, GNorm = 0.2180
Meta loss on this task batch = 2.2059e-01, Meta loss averaged over last 500 steps = 2.7317e-01, PNorm = 153.5402, GNorm = 0.2069
Meta loss on this task batch = 2.6176e-01, Meta loss averaged over last 500 steps = 2.7319e-01, PNorm = 153.5458, GNorm = 0.2414
Meta loss on this task batch = 2.7834e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 153.5502, GNorm = 0.2387
Meta loss on this task batch = 2.8137e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 153.5551, GNorm = 0.2371
Meta loss on this task batch = 1.9199e-01, Meta loss averaged over last 500 steps = 2.7306e-01, PNorm = 153.5609, GNorm = 0.2094
Meta loss on this task batch = 2.1354e-01, Meta loss averaged over last 500 steps = 2.7296e-01, PNorm = 153.5666, GNorm = 0.1960
Meta loss on this task batch = 2.8274e-01, Meta loss averaged over last 500 steps = 2.7295e-01, PNorm = 153.5700, GNorm = 0.3304
Took 115.85651016235352 seconds to complete one epoch of meta training
Took 123.76478695869446 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500319
Epoch 651
Meta loss on this task batch = 2.8931e-01, Meta loss averaged over last 500 steps = 2.7310e-01, PNorm = 153.5732, GNorm = 0.2236
Meta loss on this task batch = 3.2766e-01, Meta loss averaged over last 500 steps = 2.7316e-01, PNorm = 153.5771, GNorm = 0.2747
Meta loss on this task batch = 2.6037e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 153.5815, GNorm = 0.2185
Meta loss on this task batch = 2.4123e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 153.5856, GNorm = 0.2349
Meta loss on this task batch = 3.5321e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 153.5888, GNorm = 0.2672
Meta loss on this task batch = 2.5117e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 153.5925, GNorm = 0.2110
Meta loss on this task batch = 2.4335e-01, Meta loss averaged over last 500 steps = 2.7322e-01, PNorm = 153.5971, GNorm = 0.2489
Meta loss on this task batch = 2.7938e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 153.6019, GNorm = 0.2219
Meta loss on this task batch = 2.3934e-01, Meta loss averaged over last 500 steps = 2.7320e-01, PNorm = 153.6075, GNorm = 0.2062
Meta loss on this task batch = 2.9952e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 153.6134, GNorm = 0.2537
Meta loss on this task batch = 2.6223e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 153.6193, GNorm = 0.2483
Meta loss on this task batch = 2.9003e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.6247, GNorm = 0.2273
Meta loss on this task batch = 2.6775e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 153.6311, GNorm = 0.2180
Meta loss on this task batch = 2.5323e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 153.6367, GNorm = 0.2215
Meta loss on this task batch = 3.1985e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 153.6414, GNorm = 0.2334
Meta loss on this task batch = 2.2183e-01, Meta loss averaged over last 500 steps = 2.7335e-01, PNorm = 153.6465, GNorm = 0.1845
Meta loss on this task batch = 2.6106e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 153.6518, GNorm = 0.2075
Meta loss on this task batch = 2.6371e-01, Meta loss averaged over last 500 steps = 2.7337e-01, PNorm = 153.6573, GNorm = 0.1929
Meta loss on this task batch = 2.5336e-01, Meta loss averaged over last 500 steps = 2.7329e-01, PNorm = 153.6627, GNorm = 0.2815
Took 114.57831144332886 seconds to complete one epoch of meta training
Took 122.52998161315918 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507288
Epoch 652
Meta loss on this task batch = 2.7203e-01, Meta loss averaged over last 500 steps = 2.7331e-01, PNorm = 153.6675, GNorm = 0.2384
Meta loss on this task batch = 3.1016e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 153.6724, GNorm = 0.2149
Meta loss on this task batch = 2.9373e-01, Meta loss averaged over last 500 steps = 2.7334e-01, PNorm = 153.6768, GNorm = 0.2343
Meta loss on this task batch = 2.6060e-01, Meta loss averaged over last 500 steps = 2.7323e-01, PNorm = 153.6809, GNorm = 0.2016
Meta loss on this task batch = 2.7919e-01, Meta loss averaged over last 500 steps = 2.7318e-01, PNorm = 153.6850, GNorm = 0.2288
Meta loss on this task batch = 2.5128e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.6897, GNorm = 0.2876
Meta loss on this task batch = 3.0509e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 153.6937, GNorm = 0.2491
Meta loss on this task batch = 2.1628e-01, Meta loss averaged over last 500 steps = 2.7321e-01, PNorm = 153.6986, GNorm = 0.1821
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 153.7040, GNorm = 0.2238
Meta loss on this task batch = 3.0395e-01, Meta loss averaged over last 500 steps = 2.7324e-01, PNorm = 153.7098, GNorm = 0.2548
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.7340e-01, PNorm = 153.7158, GNorm = 0.2389
Meta loss on this task batch = 2.3832e-01, Meta loss averaged over last 500 steps = 2.7345e-01, PNorm = 153.7216, GNorm = 0.2180
Meta loss on this task batch = 2.7672e-01, Meta loss averaged over last 500 steps = 2.7338e-01, PNorm = 153.7274, GNorm = 0.2317
Meta loss on this task batch = 2.6426e-01, Meta loss averaged over last 500 steps = 2.7333e-01, PNorm = 153.7330, GNorm = 0.2252
Meta loss on this task batch = 2.8003e-01, Meta loss averaged over last 500 steps = 2.7339e-01, PNorm = 153.7385, GNorm = 0.2133
Meta loss on this task batch = 3.3120e-01, Meta loss averaged over last 500 steps = 2.7353e-01, PNorm = 153.7432, GNorm = 0.2600
Meta loss on this task batch = 2.2514e-01, Meta loss averaged over last 500 steps = 2.7343e-01, PNorm = 153.7486, GNorm = 0.2120
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.7342e-01, PNorm = 153.7539, GNorm = 0.2371
Meta loss on this task batch = 1.7359e-01, Meta loss averaged over last 500 steps = 2.7326e-01, PNorm = 153.7595, GNorm = 0.1972
Took 113.22757315635681 seconds to complete one epoch of meta training
Took 120.73133707046509 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489676
Epoch 653
Meta loss on this task batch = 2.5088e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 153.7648, GNorm = 0.2567
Meta loss on this task batch = 3.1112e-01, Meta loss averaged over last 500 steps = 2.7328e-01, PNorm = 153.7686, GNorm = 0.2561
Meta loss on this task batch = 2.7773e-01, Meta loss averaged over last 500 steps = 2.7325e-01, PNorm = 153.7719, GNorm = 0.2463
Meta loss on this task batch = 2.5673e-01, Meta loss averaged over last 500 steps = 2.7311e-01, PNorm = 153.7756, GNorm = 0.2288
Meta loss on this task batch = 2.1026e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 153.7802, GNorm = 0.2027
Meta loss on this task batch = 2.7786e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 153.7848, GNorm = 0.2551
Meta loss on this task batch = 2.7223e-01, Meta loss averaged over last 500 steps = 2.7301e-01, PNorm = 153.7899, GNorm = 0.2597
Meta loss on this task batch = 2.9836e-01, Meta loss averaged over last 500 steps = 2.7312e-01, PNorm = 153.7947, GNorm = 0.2909
Meta loss on this task batch = 2.6872e-01, Meta loss averaged over last 500 steps = 2.7308e-01, PNorm = 153.7988, GNorm = 0.2292
Meta loss on this task batch = 2.0217e-01, Meta loss averaged over last 500 steps = 2.7301e-01, PNorm = 153.8030, GNorm = 0.1860
Meta loss on this task batch = 2.7255e-01, Meta loss averaged over last 500 steps = 2.7304e-01, PNorm = 153.8072, GNorm = 0.2410
Meta loss on this task batch = 2.3247e-01, Meta loss averaged over last 500 steps = 2.7280e-01, PNorm = 153.8116, GNorm = 0.2098
Meta loss on this task batch = 2.9858e-01, Meta loss averaged over last 500 steps = 2.7284e-01, PNorm = 153.8163, GNorm = 0.2322
Meta loss on this task batch = 2.4118e-01, Meta loss averaged over last 500 steps = 2.7279e-01, PNorm = 153.8214, GNorm = 0.2143
Meta loss on this task batch = 3.4242e-01, Meta loss averaged over last 500 steps = 2.7288e-01, PNorm = 153.8267, GNorm = 0.2469
Meta loss on this task batch = 2.8119e-01, Meta loss averaged over last 500 steps = 2.7294e-01, PNorm = 153.8320, GNorm = 0.2289
Meta loss on this task batch = 2.5459e-01, Meta loss averaged over last 500 steps = 2.7285e-01, PNorm = 153.8379, GNorm = 0.2183
Meta loss on this task batch = 2.8350e-01, Meta loss averaged over last 500 steps = 2.7294e-01, PNorm = 153.8436, GNorm = 0.2461
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 2.7291e-01, PNorm = 153.8495, GNorm = 0.2645
Took 113.38851833343506 seconds to complete one epoch of meta training
Took 121.57588219642639 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490932
Epoch 654
Meta loss on this task batch = 2.7322e-01, Meta loss averaged over last 500 steps = 2.7298e-01, PNorm = 153.8556, GNorm = 0.2202
Meta loss on this task batch = 2.6456e-01, Meta loss averaged over last 500 steps = 2.7300e-01, PNorm = 153.8620, GNorm = 0.1877
Meta loss on this task batch = 2.4007e-01, Meta loss averaged over last 500 steps = 2.7285e-01, PNorm = 153.8679, GNorm = 0.2237
Meta loss on this task batch = 2.5363e-01, Meta loss averaged over last 500 steps = 2.7267e-01, PNorm = 153.8732, GNorm = 0.2177
Meta loss on this task batch = 2.5019e-01, Meta loss averaged over last 500 steps = 2.7261e-01, PNorm = 153.8788, GNorm = 0.2215
Meta loss on this task batch = 2.7949e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 153.8851, GNorm = 0.2080
Meta loss on this task batch = 3.0823e-01, Meta loss averaged over last 500 steps = 2.7270e-01, PNorm = 153.8909, GNorm = 0.2546
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.7287e-01, PNorm = 153.8963, GNorm = 0.2366
Meta loss on this task batch = 3.2630e-01, Meta loss averaged over last 500 steps = 2.7302e-01, PNorm = 153.9004, GNorm = 0.2665
Meta loss on this task batch = 2.5374e-01, Meta loss averaged over last 500 steps = 2.7303e-01, PNorm = 153.9046, GNorm = 0.2462
Meta loss on this task batch = 2.5346e-01, Meta loss averaged over last 500 steps = 2.7299e-01, PNorm = 153.9084, GNorm = 0.2134
Meta loss on this task batch = 2.9290e-01, Meta loss averaged over last 500 steps = 2.7290e-01, PNorm = 153.9110, GNorm = 0.2491
Meta loss on this task batch = 2.3027e-01, Meta loss averaged over last 500 steps = 2.7289e-01, PNorm = 153.9144, GNorm = 0.2126
Meta loss on this task batch = 2.5647e-01, Meta loss averaged over last 500 steps = 2.7269e-01, PNorm = 153.9183, GNorm = 0.2084
Meta loss on this task batch = 2.6299e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 153.9223, GNorm = 0.2485
Meta loss on this task batch = 2.3364e-01, Meta loss averaged over last 500 steps = 2.7247e-01, PNorm = 153.9263, GNorm = 0.1987
Meta loss on this task batch = 2.5049e-01, Meta loss averaged over last 500 steps = 2.7240e-01, PNorm = 153.9309, GNorm = 0.1958
Meta loss on this task batch = 3.0842e-01, Meta loss averaged over last 500 steps = 2.7253e-01, PNorm = 153.9359, GNorm = 0.2408
Meta loss on this task batch = 2.7868e-01, Meta loss averaged over last 500 steps = 2.7241e-01, PNorm = 153.9401, GNorm = 0.2880
Took 118.23113107681274 seconds to complete one epoch of meta training
Took 126.0543372631073 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493908
Epoch 655
Meta loss on this task batch = 2.6223e-01, Meta loss averaged over last 500 steps = 2.7251e-01, PNorm = 153.9453, GNorm = 0.2420
Meta loss on this task batch = 3.1351e-01, Meta loss averaged over last 500 steps = 2.7257e-01, PNorm = 153.9507, GNorm = 0.2380
Meta loss on this task batch = 2.4063e-01, Meta loss averaged over last 500 steps = 2.7248e-01, PNorm = 153.9571, GNorm = 0.1934
Meta loss on this task batch = 2.4895e-01, Meta loss averaged over last 500 steps = 2.7252e-01, PNorm = 153.9631, GNorm = 0.2115
Meta loss on this task batch = 3.4968e-01, Meta loss averaged over last 500 steps = 2.7267e-01, PNorm = 153.9690, GNorm = 0.2675
Meta loss on this task batch = 3.0551e-01, Meta loss averaged over last 500 steps = 2.7259e-01, PNorm = 153.9748, GNorm = 0.2397
Meta loss on this task batch = 2.9600e-01, Meta loss averaged over last 500 steps = 2.7271e-01, PNorm = 153.9804, GNorm = 0.2306
Meta loss on this task batch = 2.4979e-01, Meta loss averaged over last 500 steps = 2.7261e-01, PNorm = 153.9859, GNorm = 0.2293
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 153.9919, GNorm = 0.2433
Meta loss on this task batch = 3.0155e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 153.9960, GNorm = 0.2758
Meta loss on this task batch = 2.3443e-01, Meta loss averaged over last 500 steps = 2.7271e-01, PNorm = 154.0008, GNorm = 0.2123
Meta loss on this task batch = 2.2866e-01, Meta loss averaged over last 500 steps = 2.7254e-01, PNorm = 154.0059, GNorm = 0.2044
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 2.7258e-01, PNorm = 154.0107, GNorm = 0.2435
Meta loss on this task batch = 2.7265e-01, Meta loss averaged over last 500 steps = 2.7264e-01, PNorm = 154.0151, GNorm = 0.2648
Meta loss on this task batch = 2.9144e-01, Meta loss averaged over last 500 steps = 2.7276e-01, PNorm = 154.0193, GNorm = 0.2213
Meta loss on this task batch = 2.2184e-01, Meta loss averaged over last 500 steps = 2.7254e-01, PNorm = 154.0240, GNorm = 0.2058
Meta loss on this task batch = 3.0221e-01, Meta loss averaged over last 500 steps = 2.7260e-01, PNorm = 154.0292, GNorm = 0.2272
Meta loss on this task batch = 2.7388e-01, Meta loss averaged over last 500 steps = 2.7253e-01, PNorm = 154.0342, GNorm = 0.2440
Meta loss on this task batch = 2.2898e-01, Meta loss averaged over last 500 steps = 2.7247e-01, PNorm = 154.0392, GNorm = 0.2681
Took 112.74234557151794 seconds to complete one epoch of meta training
Took 120.54374861717224 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475350
Epoch 656
Meta loss on this task batch = 2.8296e-01, Meta loss averaged over last 500 steps = 2.7255e-01, PNorm = 154.0447, GNorm = 0.2461
Meta loss on this task batch = 2.2112e-01, Meta loss averaged over last 500 steps = 2.7232e-01, PNorm = 154.0506, GNorm = 0.1954
Meta loss on this task batch = 3.0164e-01, Meta loss averaged over last 500 steps = 2.7241e-01, PNorm = 154.0566, GNorm = 0.2420
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.7245e-01, PNorm = 154.0628, GNorm = 0.2498
Meta loss on this task batch = 2.7696e-01, Meta loss averaged over last 500 steps = 2.7240e-01, PNorm = 154.0683, GNorm = 0.2389
Meta loss on this task batch = 2.7201e-01, Meta loss averaged over last 500 steps = 2.7246e-01, PNorm = 154.0741, GNorm = 0.2503
Meta loss on this task batch = 2.0738e-01, Meta loss averaged over last 500 steps = 2.7237e-01, PNorm = 154.0797, GNorm = 0.2161
Meta loss on this task batch = 2.2676e-01, Meta loss averaged over last 500 steps = 2.7223e-01, PNorm = 154.0845, GNorm = 0.2285
Meta loss on this task batch = 3.2793e-01, Meta loss averaged over last 500 steps = 2.7232e-01, PNorm = 154.0879, GNorm = 0.2574
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.7238e-01, PNorm = 154.0913, GNorm = 0.2340
Meta loss on this task batch = 2.8071e-01, Meta loss averaged over last 500 steps = 2.7233e-01, PNorm = 154.0942, GNorm = 0.2335
Meta loss on this task batch = 2.5970e-01, Meta loss averaged over last 500 steps = 2.7231e-01, PNorm = 154.0975, GNorm = 0.2109
Meta loss on this task batch = 2.2370e-01, Meta loss averaged over last 500 steps = 2.7229e-01, PNorm = 154.1014, GNorm = 0.2462
Meta loss on this task batch = 2.4065e-01, Meta loss averaged over last 500 steps = 2.7224e-01, PNorm = 154.1057, GNorm = 0.2433
Meta loss on this task batch = 3.0464e-01, Meta loss averaged over last 500 steps = 2.7218e-01, PNorm = 154.1099, GNorm = 0.2646
Meta loss on this task batch = 2.6846e-01, Meta loss averaged over last 500 steps = 2.7215e-01, PNorm = 154.1155, GNorm = 0.2380
Meta loss on this task batch = 2.6847e-01, Meta loss averaged over last 500 steps = 2.7210e-01, PNorm = 154.1217, GNorm = 0.2811
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.7204e-01, PNorm = 154.1279, GNorm = 0.2440
Meta loss on this task batch = 2.7973e-01, Meta loss averaged over last 500 steps = 2.7221e-01, PNorm = 154.1339, GNorm = 0.2794
Took 114.43557500839233 seconds to complete one epoch of meta training
Took 122.21117234230042 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480863
Epoch 657
Meta loss on this task batch = 2.4891e-01, Meta loss averaged over last 500 steps = 2.7216e-01, PNorm = 154.1397, GNorm = 0.2121
Meta loss on this task batch = 2.6765e-01, Meta loss averaged over last 500 steps = 2.7222e-01, PNorm = 154.1457, GNorm = 0.2040
Meta loss on this task batch = 2.3947e-01, Meta loss averaged over last 500 steps = 2.7215e-01, PNorm = 154.1524, GNorm = 0.2327
Meta loss on this task batch = 2.3519e-01, Meta loss averaged over last 500 steps = 2.7192e-01, PNorm = 154.1582, GNorm = 0.2313
Meta loss on this task batch = 2.5932e-01, Meta loss averaged over last 500 steps = 2.7197e-01, PNorm = 154.1640, GNorm = 0.2035
Meta loss on this task batch = 2.7237e-01, Meta loss averaged over last 500 steps = 2.7197e-01, PNorm = 154.1691, GNorm = 0.2335
Meta loss on this task batch = 2.2491e-01, Meta loss averaged over last 500 steps = 2.7190e-01, PNorm = 154.1736, GNorm = 0.2415
Meta loss on this task batch = 2.9721e-01, Meta loss averaged over last 500 steps = 2.7190e-01, PNorm = 154.1780, GNorm = 0.2929
Meta loss on this task batch = 2.6201e-01, Meta loss averaged over last 500 steps = 2.7195e-01, PNorm = 154.1820, GNorm = 0.2286
Meta loss on this task batch = 2.4075e-01, Meta loss averaged over last 500 steps = 2.7197e-01, PNorm = 154.1863, GNorm = 0.2252
Meta loss on this task batch = 2.6105e-01, Meta loss averaged over last 500 steps = 2.7181e-01, PNorm = 154.1909, GNorm = 0.2082
Meta loss on this task batch = 2.9122e-01, Meta loss averaged over last 500 steps = 2.7186e-01, PNorm = 154.1951, GNorm = 0.2091
Meta loss on this task batch = 2.8480e-01, Meta loss averaged over last 500 steps = 2.7189e-01, PNorm = 154.1990, GNorm = 0.2134
Meta loss on this task batch = 2.5550e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 154.2033, GNorm = 0.2370
Meta loss on this task batch = 2.7907e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 154.2076, GNorm = 0.2146
Meta loss on this task batch = 3.0059e-01, Meta loss averaged over last 500 steps = 2.7200e-01, PNorm = 154.2128, GNorm = 0.2017
Meta loss on this task batch = 2.8436e-01, Meta loss averaged over last 500 steps = 2.7199e-01, PNorm = 154.2191, GNorm = 0.2264
Meta loss on this task batch = 2.6865e-01, Meta loss averaged over last 500 steps = 2.7191e-01, PNorm = 154.2254, GNorm = 0.2148
Meta loss on this task batch = 2.5331e-01, Meta loss averaged over last 500 steps = 2.7195e-01, PNorm = 154.2319, GNorm = 0.2515
Took 111.95496320724487 seconds to complete one epoch of meta training
Took 120.26858234405518 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472531
Epoch 658
Meta loss on this task batch = 2.5161e-01, Meta loss averaged over last 500 steps = 2.7189e-01, PNorm = 154.2379, GNorm = 0.2224
Meta loss on this task batch = 2.3795e-01, Meta loss averaged over last 500 steps = 2.7181e-01, PNorm = 154.2447, GNorm = 0.2215
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.7181e-01, PNorm = 154.2507, GNorm = 0.2564
Meta loss on this task batch = 2.2958e-01, Meta loss averaged over last 500 steps = 2.7168e-01, PNorm = 154.2569, GNorm = 0.2254
Meta loss on this task batch = 3.0169e-01, Meta loss averaged over last 500 steps = 2.7171e-01, PNorm = 154.2630, GNorm = 0.2461
Meta loss on this task batch = 2.8949e-01, Meta loss averaged over last 500 steps = 2.7168e-01, PNorm = 154.2688, GNorm = 0.2549
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 2.7186e-01, PNorm = 154.2742, GNorm = 0.2734
Meta loss on this task batch = 2.4747e-01, Meta loss averaged over last 500 steps = 2.7180e-01, PNorm = 154.2789, GNorm = 0.2368
Meta loss on this task batch = 2.9230e-01, Meta loss averaged over last 500 steps = 2.7176e-01, PNorm = 154.2829, GNorm = 0.2369
Meta loss on this task batch = 3.0929e-01, Meta loss averaged over last 500 steps = 2.7171e-01, PNorm = 154.2871, GNorm = 0.3021
Meta loss on this task batch = 2.4469e-01, Meta loss averaged over last 500 steps = 2.7163e-01, PNorm = 154.2920, GNorm = 0.2323
Meta loss on this task batch = 2.6517e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.2968, GNorm = 0.2200
Meta loss on this task batch = 3.1525e-01, Meta loss averaged over last 500 steps = 2.7185e-01, PNorm = 154.2979, GNorm = 0.3736
Meta loss on this task batch = 2.9455e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 154.2990, GNorm = 0.2327
Meta loss on this task batch = 2.5443e-01, Meta loss averaged over last 500 steps = 2.7176e-01, PNorm = 154.3000, GNorm = 0.2045
Meta loss on this task batch = 2.9590e-01, Meta loss averaged over last 500 steps = 2.7184e-01, PNorm = 154.3018, GNorm = 0.2294
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.7189e-01, PNorm = 154.3036, GNorm = 0.2011
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 2.7190e-01, PNorm = 154.3065, GNorm = 0.2394
Meta loss on this task batch = 3.5795e-01, Meta loss averaged over last 500 steps = 2.7206e-01, PNorm = 154.3075, GNorm = 0.3135
Took 117.58164477348328 seconds to complete one epoch of meta training
Took 125.40811371803284 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480095
Epoch 659
Meta loss on this task batch = 2.7946e-01, Meta loss averaged over last 500 steps = 2.7212e-01, PNorm = 154.3097, GNorm = 0.2374
Meta loss on this task batch = 2.4560e-01, Meta loss averaged over last 500 steps = 2.7206e-01, PNorm = 154.3134, GNorm = 0.2276
Meta loss on this task batch = 2.4926e-01, Meta loss averaged over last 500 steps = 2.7209e-01, PNorm = 154.3181, GNorm = 0.2247
Meta loss on this task batch = 2.2373e-01, Meta loss averaged over last 500 steps = 2.7195e-01, PNorm = 154.3241, GNorm = 0.1999
Meta loss on this task batch = 2.9752e-01, Meta loss averaged over last 500 steps = 2.7196e-01, PNorm = 154.3307, GNorm = 0.2271
Meta loss on this task batch = 2.5308e-01, Meta loss averaged over last 500 steps = 2.7182e-01, PNorm = 154.3374, GNorm = 0.2188
Meta loss on this task batch = 2.7089e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 154.3441, GNorm = 0.2281
Meta loss on this task batch = 2.6513e-01, Meta loss averaged over last 500 steps = 2.7194e-01, PNorm = 154.3500, GNorm = 0.2497
Meta loss on this task batch = 2.8531e-01, Meta loss averaged over last 500 steps = 2.7195e-01, PNorm = 154.3547, GNorm = 0.2794
Meta loss on this task batch = 2.8075e-01, Meta loss averaged over last 500 steps = 2.7199e-01, PNorm = 154.3585, GNorm = 0.2778
Meta loss on this task batch = 3.3117e-01, Meta loss averaged over last 500 steps = 2.7199e-01, PNorm = 154.3611, GNorm = 0.3101
Meta loss on this task batch = 3.2434e-01, Meta loss averaged over last 500 steps = 2.7208e-01, PNorm = 154.3640, GNorm = 0.2665
Meta loss on this task batch = 2.8274e-01, Meta loss averaged over last 500 steps = 2.7203e-01, PNorm = 154.3678, GNorm = 0.2679
Meta loss on this task batch = 2.6475e-01, Meta loss averaged over last 500 steps = 2.7210e-01, PNorm = 154.3722, GNorm = 0.2146
Meta loss on this task batch = 2.3959e-01, Meta loss averaged over last 500 steps = 2.7199e-01, PNorm = 154.3770, GNorm = 0.2155
Meta loss on this task batch = 3.2490e-01, Meta loss averaged over last 500 steps = 2.7205e-01, PNorm = 154.3819, GNorm = 0.2487
Meta loss on this task batch = 2.9908e-01, Meta loss averaged over last 500 steps = 2.7216e-01, PNorm = 154.3868, GNorm = 0.2670
Meta loss on this task batch = 1.9318e-01, Meta loss averaged over last 500 steps = 2.7202e-01, PNorm = 154.3916, GNorm = 0.1882
Meta loss on this task batch = 2.3551e-01, Meta loss averaged over last 500 steps = 2.7180e-01, PNorm = 154.3971, GNorm = 0.2240
Took 117.09079718589783 seconds to complete one epoch of meta training
Took 124.58713793754578 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489139
Epoch 660
Meta loss on this task batch = 2.7579e-01, Meta loss averaged over last 500 steps = 2.7180e-01, PNorm = 154.4032, GNorm = 0.2396
Meta loss on this task batch = 3.1416e-01, Meta loss averaged over last 500 steps = 2.7184e-01, PNorm = 154.4088, GNorm = 0.2532
Meta loss on this task batch = 2.3409e-01, Meta loss averaged over last 500 steps = 2.7176e-01, PNorm = 154.4152, GNorm = 0.2428
Meta loss on this task batch = 2.1123e-01, Meta loss averaged over last 500 steps = 2.7153e-01, PNorm = 154.4219, GNorm = 0.1861
Meta loss on this task batch = 3.0864e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 154.4282, GNorm = 0.2368
Meta loss on this task batch = 3.0177e-01, Meta loss averaged over last 500 steps = 2.7180e-01, PNorm = 154.4337, GNorm = 0.2240
Meta loss on this task batch = 2.5830e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 154.4392, GNorm = 0.2310
Meta loss on this task batch = 2.5543e-01, Meta loss averaged over last 500 steps = 2.7174e-01, PNorm = 154.4446, GNorm = 0.2321
Meta loss on this task batch = 2.7068e-01, Meta loss averaged over last 500 steps = 2.7170e-01, PNorm = 154.4501, GNorm = 0.2235
Meta loss on this task batch = 2.3884e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 154.4559, GNorm = 0.2383
Meta loss on this task batch = 2.8920e-01, Meta loss averaged over last 500 steps = 2.7174e-01, PNorm = 154.4609, GNorm = 0.2497
Meta loss on this task batch = 2.9320e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 154.4642, GNorm = 0.2731
Meta loss on this task batch = 2.6697e-01, Meta loss averaged over last 500 steps = 2.7196e-01, PNorm = 154.4669, GNorm = 0.1919
Meta loss on this task batch = 2.3595e-01, Meta loss averaged over last 500 steps = 2.7190e-01, PNorm = 154.4702, GNorm = 0.2130
Meta loss on this task batch = 2.3569e-01, Meta loss averaged over last 500 steps = 2.7193e-01, PNorm = 154.4741, GNorm = 0.2030
Meta loss on this task batch = 2.3223e-01, Meta loss averaged over last 500 steps = 2.7192e-01, PNorm = 154.4780, GNorm = 0.2106
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 2.7188e-01, PNorm = 154.4823, GNorm = 0.2179
Meta loss on this task batch = 3.2725e-01, Meta loss averaged over last 500 steps = 2.7193e-01, PNorm = 154.4861, GNorm = 0.2552
Meta loss on this task batch = 2.7190e-01, Meta loss averaged over last 500 steps = 2.7181e-01, PNorm = 154.4893, GNorm = 0.2935
Took 116.8414716720581 seconds to complete one epoch of meta training
Took 125.08200979232788 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474324
Epoch 661
Meta loss on this task batch = 3.2326e-01, Meta loss averaged over last 500 steps = 2.7193e-01, PNorm = 154.4931, GNorm = 0.2491
Meta loss on this task batch = 2.6304e-01, Meta loss averaged over last 500 steps = 2.7184e-01, PNorm = 154.4971, GNorm = 0.2470
Meta loss on this task batch = 2.5641e-01, Meta loss averaged over last 500 steps = 2.7175e-01, PNorm = 154.5020, GNorm = 0.2454
Meta loss on this task batch = 2.6703e-01, Meta loss averaged over last 500 steps = 2.7187e-01, PNorm = 154.5074, GNorm = 0.2613
Meta loss on this task batch = 3.2635e-01, Meta loss averaged over last 500 steps = 2.7192e-01, PNorm = 154.5134, GNorm = 0.2660
Meta loss on this task batch = 2.6929e-01, Meta loss averaged over last 500 steps = 2.7182e-01, PNorm = 154.5196, GNorm = 0.2227
Meta loss on this task batch = 2.0685e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.5263, GNorm = 0.1985
Meta loss on this task batch = 2.2432e-01, Meta loss averaged over last 500 steps = 2.7162e-01, PNorm = 154.5338, GNorm = 0.2142
Meta loss on this task batch = 2.5453e-01, Meta loss averaged over last 500 steps = 2.7163e-01, PNorm = 154.5415, GNorm = 0.2161
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 2.7176e-01, PNorm = 154.5475, GNorm = 0.2759
Meta loss on this task batch = 2.7771e-01, Meta loss averaged over last 500 steps = 2.7169e-01, PNorm = 154.5535, GNorm = 0.2592
Meta loss on this task batch = 2.6937e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 154.5592, GNorm = 0.2403
Meta loss on this task batch = 2.4649e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 154.5639, GNorm = 0.2315
Meta loss on this task batch = 2.5734e-01, Meta loss averaged over last 500 steps = 2.7164e-01, PNorm = 154.5680, GNorm = 0.2714
Meta loss on this task batch = 2.7455e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 154.5725, GNorm = 0.2487
Meta loss on this task batch = 2.9052e-01, Meta loss averaged over last 500 steps = 2.7170e-01, PNorm = 154.5764, GNorm = 0.2879
Meta loss on this task batch = 2.7607e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 154.5806, GNorm = 0.2200
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.5850, GNorm = 0.2507
Meta loss on this task batch = 2.9598e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.5900, GNorm = 0.2809
Took 119.47110915184021 seconds to complete one epoch of meta training
Took 127.24963402748108 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503778
Epoch 662
Meta loss on this task batch = 2.4944e-01, Meta loss averaged over last 500 steps = 2.7160e-01, PNorm = 154.5945, GNorm = 0.2272
Meta loss on this task batch = 2.7294e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 154.5987, GNorm = 0.2290
Meta loss on this task batch = 2.6840e-01, Meta loss averaged over last 500 steps = 2.7154e-01, PNorm = 154.6038, GNorm = 0.2320
Meta loss on this task batch = 2.9221e-01, Meta loss averaged over last 500 steps = 2.7158e-01, PNorm = 154.6071, GNorm = 0.2991
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.7156e-01, PNorm = 154.6099, GNorm = 0.2407
Meta loss on this task batch = 2.5381e-01, Meta loss averaged over last 500 steps = 2.7153e-01, PNorm = 154.6129, GNorm = 0.2107
Meta loss on this task batch = 2.7186e-01, Meta loss averaged over last 500 steps = 2.7148e-01, PNorm = 154.6158, GNorm = 0.2200
Meta loss on this task batch = 3.1560e-01, Meta loss averaged over last 500 steps = 2.7163e-01, PNorm = 154.6189, GNorm = 0.2362
Meta loss on this task batch = 2.9070e-01, Meta loss averaged over last 500 steps = 2.7162e-01, PNorm = 154.6220, GNorm = 0.2341
Meta loss on this task batch = 2.9810e-01, Meta loss averaged over last 500 steps = 2.7159e-01, PNorm = 154.6246, GNorm = 0.2388
Meta loss on this task batch = 2.4068e-01, Meta loss averaged over last 500 steps = 2.7162e-01, PNorm = 154.6283, GNorm = 0.2496
Meta loss on this task batch = 2.9418e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 154.6327, GNorm = 0.2233
Meta loss on this task batch = 2.1987e-01, Meta loss averaged over last 500 steps = 2.7162e-01, PNorm = 154.6373, GNorm = 0.1796
Meta loss on this task batch = 2.5214e-01, Meta loss averaged over last 500 steps = 2.7157e-01, PNorm = 154.6422, GNorm = 0.2223
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.7149e-01, PNorm = 154.6476, GNorm = 0.2537
Meta loss on this task batch = 2.4846e-01, Meta loss averaged over last 500 steps = 2.7144e-01, PNorm = 154.6538, GNorm = 0.2743
Meta loss on this task batch = 2.3095e-01, Meta loss averaged over last 500 steps = 2.7142e-01, PNorm = 154.6600, GNorm = 0.2385
Meta loss on this task batch = 3.3732e-01, Meta loss averaged over last 500 steps = 2.7156e-01, PNorm = 154.6659, GNorm = 0.2651
Meta loss on this task batch = 2.7623e-01, Meta loss averaged over last 500 steps = 2.7142e-01, PNorm = 154.6720, GNorm = 0.2959
Took 115.39812183380127 seconds to complete one epoch of meta training
Took 123.6883111000061 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.531518
Epoch 663
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 2.7158e-01, PNorm = 154.6772, GNorm = 0.2306
Meta loss on this task batch = 3.2577e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 154.6821, GNorm = 0.2472
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.7172e-01, PNorm = 154.6862, GNorm = 0.2659
Meta loss on this task batch = 2.0773e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.6900, GNorm = 0.2238
Meta loss on this task batch = 2.8969e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.6940, GNorm = 0.2227
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.6976, GNorm = 0.2248
Meta loss on this task batch = 2.5154e-01, Meta loss averaged over last 500 steps = 2.7153e-01, PNorm = 154.7016, GNorm = 0.2337
Meta loss on this task batch = 3.0606e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 154.7050, GNorm = 0.2561
Meta loss on this task batch = 2.0273e-01, Meta loss averaged over last 500 steps = 2.7145e-01, PNorm = 154.7086, GNorm = 0.1820
Meta loss on this task batch = 2.7140e-01, Meta loss averaged over last 500 steps = 2.7146e-01, PNorm = 154.7126, GNorm = 0.1940
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.7149e-01, PNorm = 154.7168, GNorm = 0.2368
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.7150e-01, PNorm = 154.7231, GNorm = 0.2527
Meta loss on this task batch = 2.6240e-01, Meta loss averaged over last 500 steps = 2.7150e-01, PNorm = 154.7304, GNorm = 0.2639
Meta loss on this task batch = 2.6442e-01, Meta loss averaged over last 500 steps = 2.7140e-01, PNorm = 154.7380, GNorm = 0.2406
Meta loss on this task batch = 3.5552e-01, Meta loss averaged over last 500 steps = 2.7153e-01, PNorm = 154.7457, GNorm = 0.2698
Meta loss on this task batch = 2.5453e-01, Meta loss averaged over last 500 steps = 2.7151e-01, PNorm = 154.7529, GNorm = 0.2027
Meta loss on this task batch = 2.6212e-01, Meta loss averaged over last 500 steps = 2.7152e-01, PNorm = 154.7605, GNorm = 0.2211
Meta loss on this task batch = 3.0368e-01, Meta loss averaged over last 500 steps = 2.7152e-01, PNorm = 154.7684, GNorm = 0.2388
Meta loss on this task batch = 2.9444e-01, Meta loss averaged over last 500 steps = 2.7160e-01, PNorm = 154.7756, GNorm = 0.3165
Took 115.77374863624573 seconds to complete one epoch of meta training
Took 123.86230564117432 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517552
Epoch 664
Meta loss on this task batch = 2.8589e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 154.7830, GNorm = 0.2430
Meta loss on this task batch = 2.3130e-01, Meta loss averaged over last 500 steps = 2.7156e-01, PNorm = 154.7900, GNorm = 0.1971
Meta loss on this task batch = 2.6272e-01, Meta loss averaged over last 500 steps = 2.7153e-01, PNorm = 154.7957, GNorm = 0.2846
Meta loss on this task batch = 2.2752e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 154.8020, GNorm = 0.2149
Meta loss on this task batch = 2.9367e-01, Meta loss averaged over last 500 steps = 2.7144e-01, PNorm = 154.8081, GNorm = 0.2300
Meta loss on this task batch = 2.5619e-01, Meta loss averaged over last 500 steps = 2.7133e-01, PNorm = 154.8145, GNorm = 0.2295
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.7126e-01, PNorm = 154.8213, GNorm = 0.2262
Meta loss on this task batch = 3.2610e-01, Meta loss averaged over last 500 steps = 2.7146e-01, PNorm = 154.8279, GNorm = 0.2561
Meta loss on this task batch = 2.3955e-01, Meta loss averaged over last 500 steps = 2.7133e-01, PNorm = 154.8350, GNorm = 0.2191
Meta loss on this task batch = 2.7273e-01, Meta loss averaged over last 500 steps = 2.7140e-01, PNorm = 154.8421, GNorm = 0.1997
Meta loss on this task batch = 2.9307e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 154.8485, GNorm = 0.2687
Meta loss on this task batch = 2.6327e-01, Meta loss averaged over last 500 steps = 2.7141e-01, PNorm = 154.8537, GNorm = 0.2332
Meta loss on this task batch = 2.8055e-01, Meta loss averaged over last 500 steps = 2.7143e-01, PNorm = 154.8586, GNorm = 0.2545
Meta loss on this task batch = 3.0803e-01, Meta loss averaged over last 500 steps = 2.7150e-01, PNorm = 154.8631, GNorm = 0.2661
Meta loss on this task batch = 3.1598e-01, Meta loss averaged over last 500 steps = 2.7156e-01, PNorm = 154.8676, GNorm = 0.2517
Meta loss on this task batch = 2.7518e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 154.8727, GNorm = 0.2229
Meta loss on this task batch = 2.2427e-01, Meta loss averaged over last 500 steps = 2.7151e-01, PNorm = 154.8790, GNorm = 0.2412
Meta loss on this task batch = 2.6273e-01, Meta loss averaged over last 500 steps = 2.7154e-01, PNorm = 154.8850, GNorm = 0.2195
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.8912, GNorm = 0.2673
Took 116.19718623161316 seconds to complete one epoch of meta training
Took 124.34042596817017 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483428
Epoch 665
Meta loss on this task batch = 2.7420e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 154.8973, GNorm = 0.2303
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 2.7187e-01, PNorm = 154.9024, GNorm = 0.2470
Meta loss on this task batch = 2.9702e-01, Meta loss averaged over last 500 steps = 2.7190e-01, PNorm = 154.9073, GNorm = 0.2724
Meta loss on this task batch = 2.4607e-01, Meta loss averaged over last 500 steps = 2.7182e-01, PNorm = 154.9122, GNorm = 0.2436
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.7183e-01, PNorm = 154.9171, GNorm = 0.2256
Meta loss on this task batch = 2.7504e-01, Meta loss averaged over last 500 steps = 2.7171e-01, PNorm = 154.9219, GNorm = 0.2225
Meta loss on this task batch = 2.1926e-01, Meta loss averaged over last 500 steps = 2.7160e-01, PNorm = 154.9267, GNorm = 0.2100
Meta loss on this task batch = 3.1686e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.9318, GNorm = 0.2368
Meta loss on this task batch = 2.5003e-01, Meta loss averaged over last 500 steps = 2.7151e-01, PNorm = 154.9366, GNorm = 0.2102
Meta loss on this task batch = 2.9591e-01, Meta loss averaged over last 500 steps = 2.7152e-01, PNorm = 154.9410, GNorm = 0.2148
Meta loss on this task batch = 2.7811e-01, Meta loss averaged over last 500 steps = 2.7151e-01, PNorm = 154.9453, GNorm = 0.2270
Meta loss on this task batch = 2.8685e-01, Meta loss averaged over last 500 steps = 2.7147e-01, PNorm = 154.9503, GNorm = 0.2258
Meta loss on this task batch = 2.6710e-01, Meta loss averaged over last 500 steps = 2.7152e-01, PNorm = 154.9560, GNorm = 0.2781
Meta loss on this task batch = 2.5509e-01, Meta loss averaged over last 500 steps = 2.7159e-01, PNorm = 154.9625, GNorm = 0.2168
Meta loss on this task batch = 2.7756e-01, Meta loss averaged over last 500 steps = 2.7169e-01, PNorm = 154.9692, GNorm = 0.2292
Meta loss on this task batch = 3.2883e-01, Meta loss averaged over last 500 steps = 2.7183e-01, PNorm = 154.9748, GNorm = 0.2736
Meta loss on this task batch = 2.7287e-01, Meta loss averaged over last 500 steps = 2.7186e-01, PNorm = 154.9801, GNorm = 0.2268
Meta loss on this task batch = 2.3188e-01, Meta loss averaged over last 500 steps = 2.7180e-01, PNorm = 154.9857, GNorm = 0.2010
Meta loss on this task batch = 2.2830e-01, Meta loss averaged over last 500 steps = 2.7169e-01, PNorm = 154.9922, GNorm = 0.2379
Took 116.19199323654175 seconds to complete one epoch of meta training
Took 124.27893280982971 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504412
Epoch 666
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 154.9985, GNorm = 0.2095
Meta loss on this task batch = 2.4714e-01, Meta loss averaged over last 500 steps = 2.7157e-01, PNorm = 155.0051, GNorm = 0.2133
Meta loss on this task batch = 3.0236e-01, Meta loss averaged over last 500 steps = 2.7145e-01, PNorm = 155.0104, GNorm = 0.2603
Meta loss on this task batch = 2.6836e-01, Meta loss averaged over last 500 steps = 2.7142e-01, PNorm = 155.0157, GNorm = 0.2330
Meta loss on this task batch = 2.6752e-01, Meta loss averaged over last 500 steps = 2.7140e-01, PNorm = 155.0206, GNorm = 0.2159
Meta loss on this task batch = 2.7719e-01, Meta loss averaged over last 500 steps = 2.7150e-01, PNorm = 155.0250, GNorm = 0.2285
Meta loss on this task batch = 2.9137e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 155.0296, GNorm = 0.2296
Meta loss on this task batch = 2.3717e-01, Meta loss averaged over last 500 steps = 2.7158e-01, PNorm = 155.0347, GNorm = 0.2309
Meta loss on this task batch = 2.6962e-01, Meta loss averaged over last 500 steps = 2.7151e-01, PNorm = 155.0398, GNorm = 0.2321
Meta loss on this task batch = 3.3456e-01, Meta loss averaged over last 500 steps = 2.7174e-01, PNorm = 155.0444, GNorm = 0.2338
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 155.0497, GNorm = 0.2099
Meta loss on this task batch = 2.9141e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 155.0546, GNorm = 0.2290
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.7174e-01, PNorm = 155.0599, GNorm = 0.1858
Meta loss on this task batch = 3.0557e-01, Meta loss averaged over last 500 steps = 2.7182e-01, PNorm = 155.0647, GNorm = 0.2579
Meta loss on this task batch = 2.3825e-01, Meta loss averaged over last 500 steps = 2.7175e-01, PNorm = 155.0698, GNorm = 0.2185
Meta loss on this task batch = 2.4953e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 155.0748, GNorm = 0.2749
Meta loss on this task batch = 3.0350e-01, Meta loss averaged over last 500 steps = 2.7177e-01, PNorm = 155.0807, GNorm = 0.2258
Meta loss on this task batch = 2.4914e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 155.0860, GNorm = 0.2174
Meta loss on this task batch = 2.8622e-01, Meta loss averaged over last 500 steps = 2.7176e-01, PNorm = 155.0915, GNorm = 0.2806
Took 112.18177390098572 seconds to complete one epoch of meta training
Took 120.16388654708862 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509626
Epoch 667
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.7162e-01, PNorm = 155.0971, GNorm = 0.1978
Meta loss on this task batch = 3.1858e-01, Meta loss averaged over last 500 steps = 2.7178e-01, PNorm = 155.1025, GNorm = 0.2365
Meta loss on this task batch = 3.3520e-01, Meta loss averaged over last 500 steps = 2.7179e-01, PNorm = 155.1068, GNorm = 0.2958
Meta loss on this task batch = 2.5385e-01, Meta loss averaged over last 500 steps = 2.7177e-01, PNorm = 155.1112, GNorm = 0.1926
Meta loss on this task batch = 2.4720e-01, Meta loss averaged over last 500 steps = 2.7169e-01, PNorm = 155.1163, GNorm = 0.2149
Meta loss on this task batch = 2.5996e-01, Meta loss averaged over last 500 steps = 2.7165e-01, PNorm = 155.1209, GNorm = 0.2458
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.7163e-01, PNorm = 155.1256, GNorm = 0.2550
Meta loss on this task batch = 2.8293e-01, Meta loss averaged over last 500 steps = 2.7169e-01, PNorm = 155.1298, GNorm = 0.2230
Meta loss on this task batch = 2.4851e-01, Meta loss averaged over last 500 steps = 2.7158e-01, PNorm = 155.1342, GNorm = 0.1934
Meta loss on this task batch = 2.8919e-01, Meta loss averaged over last 500 steps = 2.7167e-01, PNorm = 155.1391, GNorm = 0.2245
Meta loss on this task batch = 2.3371e-01, Meta loss averaged over last 500 steps = 2.7164e-01, PNorm = 155.1440, GNorm = 0.1832
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.7170e-01, PNorm = 155.1494, GNorm = 0.2403
Meta loss on this task batch = 2.8579e-01, Meta loss averaged over last 500 steps = 2.7177e-01, PNorm = 155.1548, GNorm = 0.2336
Meta loss on this task batch = 2.9426e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 155.1594, GNorm = 0.2430
Meta loss on this task batch = 2.5646e-01, Meta loss averaged over last 500 steps = 2.7171e-01, PNorm = 155.1645, GNorm = 0.2540
Meta loss on this task batch = 2.6274e-01, Meta loss averaged over last 500 steps = 2.7174e-01, PNorm = 155.1693, GNorm = 0.2091
Meta loss on this task batch = 2.5405e-01, Meta loss averaged over last 500 steps = 2.7173e-01, PNorm = 155.1750, GNorm = 0.2264
Meta loss on this task batch = 2.4174e-01, Meta loss averaged over last 500 steps = 2.7161e-01, PNorm = 155.1809, GNorm = 0.2337
Meta loss on this task batch = 2.6094e-01, Meta loss averaged over last 500 steps = 2.7144e-01, PNorm = 155.1868, GNorm = 0.2959
Took 116.7280752658844 seconds to complete one epoch of meta training
Took 124.75392365455627 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513304
Epoch 668
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.7144e-01, PNorm = 155.1930, GNorm = 0.2227
Meta loss on this task batch = 2.8002e-01, Meta loss averaged over last 500 steps = 2.7147e-01, PNorm = 155.1988, GNorm = 0.2235
Meta loss on this task batch = 2.1473e-01, Meta loss averaged over last 500 steps = 2.7147e-01, PNorm = 155.2053, GNorm = 0.2030
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 2.7144e-01, PNorm = 155.2122, GNorm = 0.2121
Meta loss on this task batch = 2.5471e-01, Meta loss averaged over last 500 steps = 2.7137e-01, PNorm = 155.2183, GNorm = 0.2486
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 155.2244, GNorm = 0.2510
Meta loss on this task batch = 2.5978e-01, Meta loss averaged over last 500 steps = 2.7129e-01, PNorm = 155.2302, GNorm = 0.2382
Meta loss on this task batch = 2.5288e-01, Meta loss averaged over last 500 steps = 2.7127e-01, PNorm = 155.2358, GNorm = 0.1929
Meta loss on this task batch = 2.8126e-01, Meta loss averaged over last 500 steps = 2.7130e-01, PNorm = 155.2412, GNorm = 0.2491
Meta loss on this task batch = 3.5736e-01, Meta loss averaged over last 500 steps = 2.7141e-01, PNorm = 155.2462, GNorm = 0.2786
Meta loss on this task batch = 2.4602e-01, Meta loss averaged over last 500 steps = 2.7141e-01, PNorm = 155.2511, GNorm = 0.2263
Meta loss on this task batch = 2.5143e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 155.2560, GNorm = 0.2345
Meta loss on this task batch = 2.3321e-01, Meta loss averaged over last 500 steps = 2.7122e-01, PNorm = 155.2614, GNorm = 0.2109
Meta loss on this task batch = 2.8881e-01, Meta loss averaged over last 500 steps = 2.7140e-01, PNorm = 155.2662, GNorm = 0.2980
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.7148e-01, PNorm = 155.2711, GNorm = 0.2423
Meta loss on this task batch = 2.3669e-01, Meta loss averaged over last 500 steps = 2.7124e-01, PNorm = 155.2763, GNorm = 0.1883
Meta loss on this task batch = 2.3111e-01, Meta loss averaged over last 500 steps = 2.7119e-01, PNorm = 155.2816, GNorm = 0.2254
Meta loss on this task batch = 2.9798e-01, Meta loss averaged over last 500 steps = 2.7130e-01, PNorm = 155.2862, GNorm = 0.2307
Meta loss on this task batch = 2.5963e-01, Meta loss averaged over last 500 steps = 2.7129e-01, PNorm = 155.2913, GNorm = 0.2756
Took 116.96122241020203 seconds to complete one epoch of meta training
Took 124.07303261756897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514204
Epoch 669
Meta loss on this task batch = 3.2156e-01, Meta loss averaged over last 500 steps = 2.7141e-01, PNorm = 155.2965, GNorm = 0.2146
Meta loss on this task batch = 3.0395e-01, Meta loss averaged over last 500 steps = 2.7142e-01, PNorm = 155.3009, GNorm = 0.2356
Meta loss on this task batch = 2.5051e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 155.3058, GNorm = 0.1977
Meta loss on this task batch = 3.1277e-01, Meta loss averaged over last 500 steps = 2.7143e-01, PNorm = 155.3101, GNorm = 0.2792
Meta loss on this task batch = 2.8514e-01, Meta loss averaged over last 500 steps = 2.7150e-01, PNorm = 155.3142, GNorm = 0.2785
Meta loss on this task batch = 2.5437e-01, Meta loss averaged over last 500 steps = 2.7143e-01, PNorm = 155.3187, GNorm = 0.2353
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.7140e-01, PNorm = 155.3241, GNorm = 0.2049
Meta loss on this task batch = 2.6397e-01, Meta loss averaged over last 500 steps = 2.7148e-01, PNorm = 155.3310, GNorm = 0.2277
Meta loss on this task batch = 2.5141e-01, Meta loss averaged over last 500 steps = 2.7143e-01, PNorm = 155.3381, GNorm = 0.2356
Meta loss on this task batch = 2.3098e-01, Meta loss averaged over last 500 steps = 2.7141e-01, PNorm = 155.3457, GNorm = 0.2152
Meta loss on this task batch = 2.0891e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 155.3541, GNorm = 0.1926
Meta loss on this task batch = 2.9364e-01, Meta loss averaged over last 500 steps = 2.7127e-01, PNorm = 155.3624, GNorm = 0.2513
Meta loss on this task batch = 2.8354e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 155.3698, GNorm = 0.2215
Meta loss on this task batch = 2.8558e-01, Meta loss averaged over last 500 steps = 2.7137e-01, PNorm = 155.3774, GNorm = 0.2933
Meta loss on this task batch = 2.8807e-01, Meta loss averaged over last 500 steps = 2.7130e-01, PNorm = 155.3851, GNorm = 0.3771
Meta loss on this task batch = 2.9332e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 155.3918, GNorm = 0.2464
Meta loss on this task batch = 2.7129e-01, Meta loss averaged over last 500 steps = 2.7133e-01, PNorm = 155.3973, GNorm = 0.2587
Meta loss on this task batch = 2.7577e-01, Meta loss averaged over last 500 steps = 2.7129e-01, PNorm = 155.4024, GNorm = 0.2088
Meta loss on this task batch = 2.4903e-01, Meta loss averaged over last 500 steps = 2.7120e-01, PNorm = 155.4080, GNorm = 0.2617
Took 112.80665755271912 seconds to complete one epoch of meta training
Took 120.46056652069092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497677
Epoch 670
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.7124e-01, PNorm = 155.4130, GNorm = 0.2562
Meta loss on this task batch = 2.4469e-01, Meta loss averaged over last 500 steps = 2.7115e-01, PNorm = 155.4176, GNorm = 0.1888
Meta loss on this task batch = 2.5420e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 155.4225, GNorm = 0.2290
Meta loss on this task batch = 1.9557e-01, Meta loss averaged over last 500 steps = 2.7096e-01, PNorm = 155.4268, GNorm = 0.2148
Meta loss on this task batch = 2.8425e-01, Meta loss averaged over last 500 steps = 2.7092e-01, PNorm = 155.4310, GNorm = 0.2612
Meta loss on this task batch = 2.7768e-01, Meta loss averaged over last 500 steps = 2.7098e-01, PNorm = 155.4360, GNorm = 0.2469
Meta loss on this task batch = 3.2610e-01, Meta loss averaged over last 500 steps = 2.7106e-01, PNorm = 155.4396, GNorm = 0.2783
Meta loss on this task batch = 2.5182e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 155.4441, GNorm = 0.1960
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 2.7104e-01, PNorm = 155.4493, GNorm = 0.2105
Meta loss on this task batch = 3.1611e-01, Meta loss averaged over last 500 steps = 2.7110e-01, PNorm = 155.4548, GNorm = 0.2180
Meta loss on this task batch = 2.6677e-01, Meta loss averaged over last 500 steps = 2.7109e-01, PNorm = 155.4610, GNorm = 0.2317
Meta loss on this task batch = 2.5151e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 155.4680, GNorm = 0.2295
Meta loss on this task batch = 2.9543e-01, Meta loss averaged over last 500 steps = 2.7104e-01, PNorm = 155.4740, GNorm = 0.2379
Meta loss on this task batch = 2.1849e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 155.4806, GNorm = 0.2239
Meta loss on this task batch = 2.1154e-01, Meta loss averaged over last 500 steps = 2.7077e-01, PNorm = 155.4872, GNorm = 0.1830
Meta loss on this task batch = 3.2809e-01, Meta loss averaged over last 500 steps = 2.7084e-01, PNorm = 155.4930, GNorm = 0.2627
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.7089e-01, PNorm = 155.5001, GNorm = 0.2209
Meta loss on this task batch = 3.2955e-01, Meta loss averaged over last 500 steps = 2.7099e-01, PNorm = 155.5068, GNorm = 0.2345
Meta loss on this task batch = 2.8422e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 155.5135, GNorm = 0.2425
Took 116.79398584365845 seconds to complete one epoch of meta training
Took 123.30332446098328 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480896
Epoch 671
Meta loss on this task batch = 2.7105e-01, Meta loss averaged over last 500 steps = 2.7102e-01, PNorm = 155.5203, GNorm = 0.2336
Meta loss on this task batch = 2.3542e-01, Meta loss averaged over last 500 steps = 2.7097e-01, PNorm = 155.5267, GNorm = 0.1936
Meta loss on this task batch = 3.0610e-01, Meta loss averaged over last 500 steps = 2.7098e-01, PNorm = 155.5308, GNorm = 0.2610
Meta loss on this task batch = 3.0414e-01, Meta loss averaged over last 500 steps = 2.7111e-01, PNorm = 155.5348, GNorm = 0.2319
Meta loss on this task batch = 2.6829e-01, Meta loss averaged over last 500 steps = 2.7110e-01, PNorm = 155.5385, GNorm = 0.2429
Meta loss on this task batch = 2.5452e-01, Meta loss averaged over last 500 steps = 2.7104e-01, PNorm = 155.5436, GNorm = 0.2109
Meta loss on this task batch = 2.5114e-01, Meta loss averaged over last 500 steps = 2.7098e-01, PNorm = 155.5498, GNorm = 0.2317
Meta loss on this task batch = 2.5695e-01, Meta loss averaged over last 500 steps = 2.7100e-01, PNorm = 155.5555, GNorm = 0.2175
Meta loss on this task batch = 2.1772e-01, Meta loss averaged over last 500 steps = 2.7094e-01, PNorm = 155.5614, GNorm = 0.2082
Meta loss on this task batch = 2.4496e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 155.5680, GNorm = 0.2319
Meta loss on this task batch = 2.2713e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 155.5753, GNorm = 0.2132
Meta loss on this task batch = 2.2321e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 155.5832, GNorm = 0.2039
Meta loss on this task batch = 2.7542e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 155.5907, GNorm = 0.2174
Meta loss on this task batch = 3.0694e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 155.5977, GNorm = 0.2825
Meta loss on this task batch = 3.6838e-01, Meta loss averaged over last 500 steps = 2.7091e-01, PNorm = 155.6042, GNorm = 0.2721
Meta loss on this task batch = 2.7499e-01, Meta loss averaged over last 500 steps = 2.7099e-01, PNorm = 155.6110, GNorm = 0.2335
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.7115e-01, PNorm = 155.6175, GNorm = 0.2465
Meta loss on this task batch = 3.1482e-01, Meta loss averaged over last 500 steps = 2.7132e-01, PNorm = 155.6246, GNorm = 0.2558
Meta loss on this task batch = 2.7655e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 155.6316, GNorm = 0.2733
Took 115.14327716827393 seconds to complete one epoch of meta training
Took 122.1639130115509 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488295
Epoch 672
Meta loss on this task batch = 2.4780e-01, Meta loss averaged over last 500 steps = 2.7134e-01, PNorm = 155.6387, GNorm = 0.2026
Meta loss on this task batch = 2.4305e-01, Meta loss averaged over last 500 steps = 2.7123e-01, PNorm = 155.6454, GNorm = 0.2617
Meta loss on this task batch = 2.7369e-01, Meta loss averaged over last 500 steps = 2.7103e-01, PNorm = 155.6512, GNorm = 0.2389
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 155.6552, GNorm = 0.2784
Meta loss on this task batch = 2.5240e-01, Meta loss averaged over last 500 steps = 2.7087e-01, PNorm = 155.6594, GNorm = 0.1918
Meta loss on this task batch = 2.9460e-01, Meta loss averaged over last 500 steps = 2.7080e-01, PNorm = 155.6639, GNorm = 0.2432
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 155.6691, GNorm = 0.2586
Meta loss on this task batch = 2.2378e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 155.6743, GNorm = 0.2133
Meta loss on this task batch = 2.5943e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 155.6799, GNorm = 0.2068
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.7059e-01, PNorm = 155.6845, GNorm = 0.2335
Meta loss on this task batch = 2.6030e-01, Meta loss averaged over last 500 steps = 2.7051e-01, PNorm = 155.6910, GNorm = 0.2285
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 2.7057e-01, PNorm = 155.6982, GNorm = 0.2136
Meta loss on this task batch = 3.2823e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 155.7054, GNorm = 0.2824
Meta loss on this task batch = 3.0301e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 155.7121, GNorm = 0.2458
Meta loss on this task batch = 2.3644e-01, Meta loss averaged over last 500 steps = 2.7066e-01, PNorm = 155.7193, GNorm = 0.2062
Meta loss on this task batch = 2.9806e-01, Meta loss averaged over last 500 steps = 2.7068e-01, PNorm = 155.7262, GNorm = 0.2902
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 155.7328, GNorm = 0.2703
Meta loss on this task batch = 2.5290e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 155.7396, GNorm = 0.2207
Meta loss on this task batch = 2.6507e-01, Meta loss averaged over last 500 steps = 2.7072e-01, PNorm = 155.7471, GNorm = 0.2570
Took 117.12601280212402 seconds to complete one epoch of meta training
Took 125.22143054008484 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502914
Epoch 673
Meta loss on this task batch = 2.9014e-01, Meta loss averaged over last 500 steps = 2.7079e-01, PNorm = 155.7537, GNorm = 0.2502
Meta loss on this task batch = 2.0017e-01, Meta loss averaged over last 500 steps = 2.7073e-01, PNorm = 155.7603, GNorm = 0.1743
Meta loss on this task batch = 2.2900e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 155.7676, GNorm = 0.1900
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 2.7061e-01, PNorm = 155.7743, GNorm = 0.2182
Meta loss on this task batch = 2.7267e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 155.7814, GNorm = 0.2436
Meta loss on this task batch = 2.6109e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 155.7885, GNorm = 0.2064
Meta loss on this task batch = 2.7105e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 155.7956, GNorm = 0.1897
Meta loss on this task batch = 2.3455e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 155.8014, GNorm = 0.2190
Meta loss on this task batch = 3.1996e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 155.8066, GNorm = 0.2350
Meta loss on this task batch = 2.7789e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 155.8113, GNorm = 0.2648
Meta loss on this task batch = 2.4504e-01, Meta loss averaged over last 500 steps = 2.7038e-01, PNorm = 155.8157, GNorm = 0.2239
Meta loss on this task batch = 2.7872e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 155.8194, GNorm = 0.2180
Meta loss on this task batch = 2.9069e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 155.8239, GNorm = 0.2315
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 155.8290, GNorm = 0.2119
Meta loss on this task batch = 3.1278e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 155.8332, GNorm = 0.2553
Meta loss on this task batch = 2.5779e-01, Meta loss averaged over last 500 steps = 2.7057e-01, PNorm = 155.8378, GNorm = 0.2048
Meta loss on this task batch = 2.3952e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 155.8426, GNorm = 0.1889
Meta loss on this task batch = 2.2282e-01, Meta loss averaged over last 500 steps = 2.7041e-01, PNorm = 155.8478, GNorm = 0.2056
Meta loss on this task batch = 3.2871e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 155.8524, GNorm = 0.3764
Took 115.0147659778595 seconds to complete one epoch of meta training
Took 122.40208101272583 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502936
Epoch 674
Meta loss on this task batch = 2.3076e-01, Meta loss averaged over last 500 steps = 2.7057e-01, PNorm = 155.8573, GNorm = 0.2032
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 155.8630, GNorm = 0.2420
Meta loss on this task batch = 2.9173e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 155.8688, GNorm = 0.2327
Meta loss on this task batch = 3.0199e-01, Meta loss averaged over last 500 steps = 2.7066e-01, PNorm = 155.8748, GNorm = 0.2588
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 155.8811, GNorm = 0.2114
Meta loss on this task batch = 2.4429e-01, Meta loss averaged over last 500 steps = 2.7066e-01, PNorm = 155.8881, GNorm = 0.2093
Meta loss on this task batch = 2.6618e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 155.8950, GNorm = 0.2392
Meta loss on this task batch = 2.7127e-01, Meta loss averaged over last 500 steps = 2.7059e-01, PNorm = 155.9016, GNorm = 0.2151
Meta loss on this task batch = 3.0934e-01, Meta loss averaged over last 500 steps = 2.7073e-01, PNorm = 155.9075, GNorm = 0.2294
Meta loss on this task batch = 2.6202e-01, Meta loss averaged over last 500 steps = 2.7072e-01, PNorm = 155.9133, GNorm = 0.2299
Meta loss on this task batch = 2.5993e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 155.9194, GNorm = 0.2214
Meta loss on this task batch = 2.2659e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 155.9251, GNorm = 0.2031
Meta loss on this task batch = 2.7324e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 155.9302, GNorm = 0.2133
Meta loss on this task batch = 2.7552e-01, Meta loss averaged over last 500 steps = 2.7059e-01, PNorm = 155.9339, GNorm = 0.3129
Meta loss on this task batch = 3.0045e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 155.9378, GNorm = 0.2710
Meta loss on this task batch = 2.3372e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 155.9419, GNorm = 0.1885
Meta loss on this task batch = 2.3285e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 155.9465, GNorm = 0.2247
Meta loss on this task batch = 2.9713e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 155.9516, GNorm = 0.2129
Meta loss on this task batch = 2.3877e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 155.9566, GNorm = 0.2480
Took 110.03193855285645 seconds to complete one epoch of meta training
Took 117.85297632217407 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493793
Epoch 675
Meta loss on this task batch = 2.7260e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 155.9619, GNorm = 0.2283
Meta loss on this task batch = 2.1544e-01, Meta loss averaged over last 500 steps = 2.7035e-01, PNorm = 155.9674, GNorm = 0.2058
Meta loss on this task batch = 2.1503e-01, Meta loss averaged over last 500 steps = 2.7029e-01, PNorm = 155.9731, GNorm = 0.1923
Meta loss on this task batch = 2.4869e-01, Meta loss averaged over last 500 steps = 2.7023e-01, PNorm = 155.9789, GNorm = 0.2002
Meta loss on this task batch = 2.6656e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 155.9846, GNorm = 0.2305
Meta loss on this task batch = 3.2573e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 155.9902, GNorm = 0.2537
Meta loss on this task batch = 2.3160e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 155.9956, GNorm = 0.1963
Meta loss on this task batch = 2.7930e-01, Meta loss averaged over last 500 steps = 2.7040e-01, PNorm = 156.0007, GNorm = 0.2478
Meta loss on this task batch = 3.2453e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.0054, GNorm = 0.2780
Meta loss on this task batch = 2.9507e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 156.0105, GNorm = 0.2252
Meta loss on this task batch = 2.5935e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 156.0162, GNorm = 0.2021
Meta loss on this task batch = 2.8140e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 156.0216, GNorm = 0.2261
Meta loss on this task batch = 3.0490e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 156.0270, GNorm = 0.2671
Meta loss on this task batch = 3.1554e-01, Meta loss averaged over last 500 steps = 2.7084e-01, PNorm = 156.0326, GNorm = 0.2296
Meta loss on this task batch = 2.5691e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 156.0376, GNorm = 0.2536
Meta loss on this task batch = 1.9852e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 156.0433, GNorm = 0.2101
Meta loss on this task batch = 2.4727e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 156.0486, GNorm = 0.2548
Meta loss on this task batch = 2.9572e-01, Meta loss averaged over last 500 steps = 2.7056e-01, PNorm = 156.0537, GNorm = 0.2131
Meta loss on this task batch = 3.1505e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 156.0582, GNorm = 0.2832
Took 108.47521185874939 seconds to complete one epoch of meta training
Took 116.57635951042175 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484816
Epoch 676
Meta loss on this task batch = 2.5113e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 156.0639, GNorm = 0.2089
Meta loss on this task batch = 2.8192e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 156.0695, GNorm = 0.2166
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 156.0749, GNorm = 0.2534
Meta loss on this task batch = 2.6784e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 156.0804, GNorm = 0.1895
Meta loss on this task batch = 2.6633e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 156.0851, GNorm = 0.2125
Meta loss on this task batch = 2.6628e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 156.0901, GNorm = 0.2073
Meta loss on this task batch = 2.2612e-01, Meta loss averaged over last 500 steps = 2.7035e-01, PNorm = 156.0960, GNorm = 0.2078
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 156.1019, GNorm = 0.2322
Meta loss on this task batch = 2.8195e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 156.1082, GNorm = 0.2229
Meta loss on this task batch = 2.8877e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 156.1147, GNorm = 0.3645
Meta loss on this task batch = 1.9940e-01, Meta loss averaged over last 500 steps = 2.7022e-01, PNorm = 156.1210, GNorm = 0.1828
Meta loss on this task batch = 2.7284e-01, Meta loss averaged over last 500 steps = 2.7001e-01, PNorm = 156.1268, GNorm = 0.2385
Meta loss on this task batch = 2.6895e-01, Meta loss averaged over last 500 steps = 2.7006e-01, PNorm = 156.1327, GNorm = 0.2487
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 2.7009e-01, PNorm = 156.1376, GNorm = 0.2397
Meta loss on this task batch = 3.0363e-01, Meta loss averaged over last 500 steps = 2.7016e-01, PNorm = 156.1423, GNorm = 0.2822
Meta loss on this task batch = 2.5185e-01, Meta loss averaged over last 500 steps = 2.7013e-01, PNorm = 156.1473, GNorm = 0.2399
Meta loss on this task batch = 2.5015e-01, Meta loss averaged over last 500 steps = 2.7010e-01, PNorm = 156.1527, GNorm = 0.2162
Meta loss on this task batch = 2.5951e-01, Meta loss averaged over last 500 steps = 2.7010e-01, PNorm = 156.1592, GNorm = 0.2401
Meta loss on this task batch = 2.3881e-01, Meta loss averaged over last 500 steps = 2.7014e-01, PNorm = 156.1658, GNorm = 0.3359
Took 108.8903295993805 seconds to complete one epoch of meta training
Took 116.24304127693176 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486694
Epoch 677
Meta loss on this task batch = 2.7731e-01, Meta loss averaged over last 500 steps = 2.7017e-01, PNorm = 156.1726, GNorm = 0.2515
Meta loss on this task batch = 2.9457e-01, Meta loss averaged over last 500 steps = 2.7020e-01, PNorm = 156.1790, GNorm = 0.2612
Meta loss on this task batch = 2.6333e-01, Meta loss averaged over last 500 steps = 2.7017e-01, PNorm = 156.1850, GNorm = 0.2450
Meta loss on this task batch = 2.2377e-01, Meta loss averaged over last 500 steps = 2.7023e-01, PNorm = 156.1917, GNorm = 0.2264
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 156.1987, GNorm = 0.2311
Meta loss on this task batch = 2.9506e-01, Meta loss averaged over last 500 steps = 2.7036e-01, PNorm = 156.2048, GNorm = 0.2452
Meta loss on this task batch = 2.5546e-01, Meta loss averaged over last 500 steps = 2.7029e-01, PNorm = 156.2112, GNorm = 0.2889
Meta loss on this task batch = 2.8443e-01, Meta loss averaged over last 500 steps = 2.7020e-01, PNorm = 156.2171, GNorm = 0.2378
Meta loss on this task batch = 3.0678e-01, Meta loss averaged over last 500 steps = 2.7029e-01, PNorm = 156.2226, GNorm = 0.2205
Meta loss on this task batch = 3.1931e-01, Meta loss averaged over last 500 steps = 2.7045e-01, PNorm = 156.2280, GNorm = 0.2228
Meta loss on this task batch = 2.8536e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 156.2342, GNorm = 0.2380
Meta loss on this task batch = 2.5471e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 156.2402, GNorm = 0.2260
Meta loss on this task batch = 2.6908e-01, Meta loss averaged over last 500 steps = 2.7037e-01, PNorm = 156.2452, GNorm = 0.2649
Meta loss on this task batch = 2.2518e-01, Meta loss averaged over last 500 steps = 2.7027e-01, PNorm = 156.2499, GNorm = 0.1856
Meta loss on this task batch = 3.0494e-01, Meta loss averaged over last 500 steps = 2.7040e-01, PNorm = 156.2548, GNorm = 0.2538
Meta loss on this task batch = 2.4751e-01, Meta loss averaged over last 500 steps = 2.7029e-01, PNorm = 156.2594, GNorm = 0.2008
Meta loss on this task batch = 2.3927e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 156.2648, GNorm = 0.2170
Meta loss on this task batch = 2.8677e-01, Meta loss averaged over last 500 steps = 2.7024e-01, PNorm = 156.2690, GNorm = 0.2336
Meta loss on this task batch = 2.9719e-01, Meta loss averaged over last 500 steps = 2.7030e-01, PNorm = 156.2731, GNorm = 0.2555
Took 110.22694754600525 seconds to complete one epoch of meta training
Took 118.13214588165283 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483450
Epoch 678
Meta loss on this task batch = 2.9747e-01, Meta loss averaged over last 500 steps = 2.7039e-01, PNorm = 156.2777, GNorm = 0.2567
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.7026e-01, PNorm = 156.2821, GNorm = 0.2470
Meta loss on this task batch = 3.0648e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 156.2871, GNorm = 0.2313
Meta loss on this task batch = 1.5780e-01, Meta loss averaged over last 500 steps = 2.7023e-01, PNorm = 156.2928, GNorm = 0.2228
Meta loss on this task batch = 2.1383e-01, Meta loss averaged over last 500 steps = 2.7013e-01, PNorm = 156.2988, GNorm = 0.1881
Meta loss on this task batch = 2.3853e-01, Meta loss averaged over last 500 steps = 2.7010e-01, PNorm = 156.3044, GNorm = 0.2765
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.7006e-01, PNorm = 156.3104, GNorm = 0.2100
Meta loss on this task batch = 2.6954e-01, Meta loss averaged over last 500 steps = 2.6998e-01, PNorm = 156.3160, GNorm = 0.2572
Meta loss on this task batch = 2.8544e-01, Meta loss averaged over last 500 steps = 2.6996e-01, PNorm = 156.3215, GNorm = 0.2185
Meta loss on this task batch = 2.8058e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 156.3262, GNorm = 0.2601
Meta loss on this task batch = 2.6251e-01, Meta loss averaged over last 500 steps = 2.6997e-01, PNorm = 156.3305, GNorm = 0.2858
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 2.7001e-01, PNorm = 156.3349, GNorm = 0.2425
Meta loss on this task batch = 2.9392e-01, Meta loss averaged over last 500 steps = 2.6999e-01, PNorm = 156.3391, GNorm = 0.2616
Meta loss on this task batch = 2.9711e-01, Meta loss averaged over last 500 steps = 2.7015e-01, PNorm = 156.3438, GNorm = 0.2899
Meta loss on this task batch = 2.9249e-01, Meta loss averaged over last 500 steps = 2.7016e-01, PNorm = 156.3487, GNorm = 0.2454
Meta loss on this task batch = 3.0668e-01, Meta loss averaged over last 500 steps = 2.7016e-01, PNorm = 156.3534, GNorm = 0.2129
Meta loss on this task batch = 2.7909e-01, Meta loss averaged over last 500 steps = 2.7019e-01, PNorm = 156.3582, GNorm = 0.2111
Meta loss on this task batch = 2.8661e-01, Meta loss averaged over last 500 steps = 2.7029e-01, PNorm = 156.3616, GNorm = 0.2752
Meta loss on this task batch = 3.4543e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 156.3642, GNorm = 0.3931
Took 117.13546061515808 seconds to complete one epoch of meta training
Took 125.08070755004883 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486212
Epoch 679
Meta loss on this task batch = 2.5771e-01, Meta loss averaged over last 500 steps = 2.7041e-01, PNorm = 156.3676, GNorm = 0.1942
Meta loss on this task batch = 2.6015e-01, Meta loss averaged over last 500 steps = 2.7037e-01, PNorm = 156.3716, GNorm = 0.2115
Meta loss on this task batch = 2.6597e-01, Meta loss averaged over last 500 steps = 2.7024e-01, PNorm = 156.3754, GNorm = 0.2252
Meta loss on this task batch = 2.6957e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 156.3794, GNorm = 0.2143
Meta loss on this task batch = 2.2548e-01, Meta loss averaged over last 500 steps = 2.7026e-01, PNorm = 156.3840, GNorm = 0.2053
Meta loss on this task batch = 2.8184e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 156.3896, GNorm = 0.2091
Meta loss on this task batch = 2.9091e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 156.3949, GNorm = 0.2405
Meta loss on this task batch = 2.7120e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 156.3997, GNorm = 0.2372
Meta loss on this task batch = 2.6394e-01, Meta loss averaged over last 500 steps = 2.7044e-01, PNorm = 156.4047, GNorm = 0.2220
Meta loss on this task batch = 2.2262e-01, Meta loss averaged over last 500 steps = 2.7038e-01, PNorm = 156.4099, GNorm = 0.2025
Meta loss on this task batch = 3.0007e-01, Meta loss averaged over last 500 steps = 2.7056e-01, PNorm = 156.4147, GNorm = 0.2116
Meta loss on this task batch = 2.4313e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 156.4195, GNorm = 0.2299
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 156.4247, GNorm = 0.2078
Meta loss on this task batch = 2.8276e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 156.4296, GNorm = 0.2387
Meta loss on this task batch = 2.6493e-01, Meta loss averaged over last 500 steps = 2.7042e-01, PNorm = 156.4346, GNorm = 0.2377
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 156.4394, GNorm = 0.2337
Meta loss on this task batch = 2.3577e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 156.4446, GNorm = 0.2180
Meta loss on this task batch = 2.7293e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 156.4499, GNorm = 0.2374
Meta loss on this task batch = 3.6974e-01, Meta loss averaged over last 500 steps = 2.7078e-01, PNorm = 156.4531, GNorm = 0.3559
Took 107.17734742164612 seconds to complete one epoch of meta training
Took 114.83263969421387 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478803
Epoch 680
Meta loss on this task batch = 2.6736e-01, Meta loss averaged over last 500 steps = 2.7083e-01, PNorm = 156.4564, GNorm = 0.2360
Meta loss on this task batch = 2.8510e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 156.4589, GNorm = 0.2635
Meta loss on this task batch = 1.8936e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.4620, GNorm = 0.1853
Meta loss on this task batch = 2.9748e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 156.4645, GNorm = 0.2551
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 156.4673, GNorm = 0.2234
Meta loss on this task batch = 2.8530e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 156.4699, GNorm = 0.2542
Meta loss on this task batch = 2.1940e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 156.4734, GNorm = 0.1985
Meta loss on this task batch = 2.6541e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 156.4781, GNorm = 0.2249
Meta loss on this task batch = 2.7540e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 156.4831, GNorm = 0.2204
Meta loss on this task batch = 3.2572e-01, Meta loss averaged over last 500 steps = 2.7076e-01, PNorm = 156.4883, GNorm = 0.2828
Meta loss on this task batch = 2.7592e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 156.4941, GNorm = 0.2194
Meta loss on this task batch = 2.6281e-01, Meta loss averaged over last 500 steps = 2.7078e-01, PNorm = 156.5011, GNorm = 0.1918
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 156.5083, GNorm = 0.2231
Meta loss on this task batch = 3.0427e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 156.5154, GNorm = 0.2163
Meta loss on this task batch = 2.6405e-01, Meta loss averaged over last 500 steps = 2.7056e-01, PNorm = 156.5218, GNorm = 0.2646
Meta loss on this task batch = 2.3570e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.5285, GNorm = 0.2005
Meta loss on this task batch = 2.7380e-01, Meta loss averaged over last 500 steps = 2.7057e-01, PNorm = 156.5356, GNorm = 0.2236
Meta loss on this task batch = 2.7419e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.5430, GNorm = 0.2201
Meta loss on this task batch = 2.4904e-01, Meta loss averaged over last 500 steps = 2.7057e-01, PNorm = 156.5505, GNorm = 0.2123
Took 109.25691771507263 seconds to complete one epoch of meta training
Took 116.58840823173523 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482153
Epoch 681
Meta loss on this task batch = 2.4659e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 156.5578, GNorm = 0.2167
Meta loss on this task batch = 2.7776e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 156.5657, GNorm = 0.2441
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 156.5734, GNorm = 0.2397
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.7072e-01, PNorm = 156.5806, GNorm = 0.2223
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 156.5871, GNorm = 0.2132
Meta loss on this task batch = 2.6491e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 156.5929, GNorm = 0.2837
Meta loss on this task batch = 2.7585e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 156.5985, GNorm = 0.2509
Meta loss on this task batch = 2.6193e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.6033, GNorm = 0.2738
Meta loss on this task batch = 2.4910e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 156.6077, GNorm = 0.2245
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 156.6119, GNorm = 0.2431
Meta loss on this task batch = 3.0189e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 156.6154, GNorm = 0.2601
Meta loss on this task batch = 3.2902e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 156.6189, GNorm = 0.2472
Meta loss on this task batch = 2.9989e-01, Meta loss averaged over last 500 steps = 2.7056e-01, PNorm = 156.6228, GNorm = 0.2214
Meta loss on this task batch = 2.4178e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 156.6272, GNorm = 0.2467
Meta loss on this task batch = 2.5429e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 156.6324, GNorm = 0.2361
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 156.6374, GNorm = 0.2354
Meta loss on this task batch = 2.5233e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.6420, GNorm = 0.2291
Meta loss on this task batch = 2.6899e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 156.6469, GNorm = 0.2098
Meta loss on this task batch = 2.3464e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 156.6521, GNorm = 0.2292
Took 108.10888934135437 seconds to complete one epoch of meta training
Took 114.98269009590149 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473251
Epoch 682
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.7051e-01, PNorm = 156.6583, GNorm = 0.2288
Meta loss on this task batch = 2.7480e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 156.6628, GNorm = 0.2330
Meta loss on this task batch = 2.1181e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 156.6683, GNorm = 0.2165
Meta loss on this task batch = 2.0160e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 156.6742, GNorm = 0.2015
Meta loss on this task batch = 1.8609e-01, Meta loss averaged over last 500 steps = 2.7008e-01, PNorm = 156.6799, GNorm = 0.2039
Meta loss on this task batch = 2.1185e-01, Meta loss averaged over last 500 steps = 2.7005e-01, PNorm = 156.6866, GNorm = 0.2085
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 2.7005e-01, PNorm = 156.6927, GNorm = 0.2424
Meta loss on this task batch = 2.8554e-01, Meta loss averaged over last 500 steps = 2.7018e-01, PNorm = 156.6982, GNorm = 0.2248
Meta loss on this task batch = 2.8107e-01, Meta loss averaged over last 500 steps = 2.7014e-01, PNorm = 156.7040, GNorm = 0.2380
Meta loss on this task batch = 3.2458e-01, Meta loss averaged over last 500 steps = 2.7026e-01, PNorm = 156.7089, GNorm = 0.2473
Meta loss on this task batch = 3.2421e-01, Meta loss averaged over last 500 steps = 2.7036e-01, PNorm = 156.7129, GNorm = 0.2707
Meta loss on this task batch = 2.8802e-01, Meta loss averaged over last 500 steps = 2.7039e-01, PNorm = 156.7168, GNorm = 0.2355
Meta loss on this task batch = 2.7626e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 156.7207, GNorm = 0.2402
Meta loss on this task batch = 2.8600e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 156.7245, GNorm = 0.2304
Meta loss on this task batch = 3.3770e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 156.7279, GNorm = 0.2334
Meta loss on this task batch = 2.6724e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 156.7315, GNorm = 0.2379
Meta loss on this task batch = 3.0284e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 156.7352, GNorm = 0.2144
Meta loss on this task batch = 2.7584e-01, Meta loss averaged over last 500 steps = 2.7073e-01, PNorm = 156.7395, GNorm = 0.2271
Meta loss on this task batch = 2.7271e-01, Meta loss averaged over last 500 steps = 2.7083e-01, PNorm = 156.7428, GNorm = 0.3197
Took 106.84858798980713 seconds to complete one epoch of meta training
Took 114.11233687400818 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486344
Epoch 683
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.7087e-01, PNorm = 156.7463, GNorm = 0.1975
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 2.7079e-01, PNorm = 156.7505, GNorm = 0.2173
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 2.7077e-01, PNorm = 156.7553, GNorm = 0.2334
Meta loss on this task batch = 2.5262e-01, Meta loss averaged over last 500 steps = 2.7074e-01, PNorm = 156.7613, GNorm = 0.2432
Meta loss on this task batch = 3.0889e-01, Meta loss averaged over last 500 steps = 2.7082e-01, PNorm = 156.7656, GNorm = 0.2338
Meta loss on this task batch = 2.8029e-01, Meta loss averaged over last 500 steps = 2.7082e-01, PNorm = 156.7698, GNorm = 0.2756
Meta loss on this task batch = 2.7083e-01, Meta loss averaged over last 500 steps = 2.7087e-01, PNorm = 156.7723, GNorm = 0.2839
Meta loss on this task batch = 2.7066e-01, Meta loss averaged over last 500 steps = 2.7087e-01, PNorm = 156.7750, GNorm = 0.2256
Meta loss on this task batch = 2.8321e-01, Meta loss averaged over last 500 steps = 2.7096e-01, PNorm = 156.7777, GNorm = 0.2225
Meta loss on this task batch = 2.8939e-01, Meta loss averaged over last 500 steps = 2.7107e-01, PNorm = 156.7802, GNorm = 0.2377
Meta loss on this task batch = 2.8415e-01, Meta loss averaged over last 500 steps = 2.7112e-01, PNorm = 156.7826, GNorm = 0.2147
Meta loss on this task batch = 2.5014e-01, Meta loss averaged over last 500 steps = 2.7108e-01, PNorm = 156.7861, GNorm = 0.2175
Meta loss on this task batch = 2.9360e-01, Meta loss averaged over last 500 steps = 2.7121e-01, PNorm = 156.7899, GNorm = 0.2435
Meta loss on this task batch = 2.6334e-01, Meta loss averaged over last 500 steps = 2.7114e-01, PNorm = 156.7935, GNorm = 0.2361
Meta loss on this task batch = 2.9827e-01, Meta loss averaged over last 500 steps = 2.7122e-01, PNorm = 156.7970, GNorm = 0.2457
Meta loss on this task batch = 2.8534e-01, Meta loss averaged over last 500 steps = 2.7131e-01, PNorm = 156.8007, GNorm = 0.2338
Meta loss on this task batch = 2.6147e-01, Meta loss averaged over last 500 steps = 2.7131e-01, PNorm = 156.8042, GNorm = 0.1977
Meta loss on this task batch = 2.8449e-01, Meta loss averaged over last 500 steps = 2.7129e-01, PNorm = 156.8085, GNorm = 0.2500
Meta loss on this task batch = 2.6356e-01, Meta loss averaged over last 500 steps = 2.7125e-01, PNorm = 156.8141, GNorm = 0.3106
Took 107.88410234451294 seconds to complete one epoch of meta training
Took 115.1545181274414 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474789
Epoch 684
Meta loss on this task batch = 2.5356e-01, Meta loss averaged over last 500 steps = 2.7125e-01, PNorm = 156.8198, GNorm = 0.2110
Meta loss on this task batch = 2.5609e-01, Meta loss averaged over last 500 steps = 2.7120e-01, PNorm = 156.8259, GNorm = 0.2324
Meta loss on this task batch = 2.4910e-01, Meta loss averaged over last 500 steps = 2.7110e-01, PNorm = 156.8326, GNorm = 0.2327
Meta loss on this task batch = 3.1267e-01, Meta loss averaged over last 500 steps = 2.7116e-01, PNorm = 156.8394, GNorm = 0.2978
Meta loss on this task batch = 2.6268e-01, Meta loss averaged over last 500 steps = 2.7114e-01, PNorm = 156.8462, GNorm = 0.2636
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 2.7119e-01, PNorm = 156.8530, GNorm = 0.2214
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.7122e-01, PNorm = 156.8595, GNorm = 0.1940
Meta loss on this task batch = 3.1249e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 156.8654, GNorm = 0.2160
Meta loss on this task batch = 3.2869e-01, Meta loss averaged over last 500 steps = 2.7147e-01, PNorm = 156.8699, GNorm = 0.2644
Meta loss on this task batch = 2.6229e-01, Meta loss averaged over last 500 steps = 2.7154e-01, PNorm = 156.8738, GNorm = 0.2497
Meta loss on this task batch = 2.6051e-01, Meta loss averaged over last 500 steps = 2.7146e-01, PNorm = 156.8768, GNorm = 0.2675
Meta loss on this task batch = 2.5549e-01, Meta loss averaged over last 500 steps = 2.7139e-01, PNorm = 156.8800, GNorm = 0.1854
Meta loss on this task batch = 2.8345e-01, Meta loss averaged over last 500 steps = 2.7138e-01, PNorm = 156.8833, GNorm = 0.2049
Meta loss on this task batch = 2.7527e-01, Meta loss averaged over last 500 steps = 2.7143e-01, PNorm = 156.8877, GNorm = 0.2529
Meta loss on this task batch = 2.5780e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 156.8924, GNorm = 0.1944
Meta loss on this task batch = 3.0049e-01, Meta loss averaged over last 500 steps = 2.7135e-01, PNorm = 156.8982, GNorm = 0.2320
Meta loss on this task batch = 2.4948e-01, Meta loss averaged over last 500 steps = 2.7136e-01, PNorm = 156.9041, GNorm = 0.2104
Meta loss on this task batch = 2.8164e-01, Meta loss averaged over last 500 steps = 2.7139e-01, PNorm = 156.9099, GNorm = 0.2671
Meta loss on this task batch = 2.7358e-01, Meta loss averaged over last 500 steps = 2.7131e-01, PNorm = 156.9163, GNorm = 0.2279
Took 108.1607608795166 seconds to complete one epoch of meta training
Took 115.7764503955841 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500418
Epoch 685
Meta loss on this task batch = 2.3798e-01, Meta loss averaged over last 500 steps = 2.7119e-01, PNorm = 156.9232, GNorm = 0.1983
Meta loss on this task batch = 2.2183e-01, Meta loss averaged over last 500 steps = 2.7113e-01, PNorm = 156.9303, GNorm = 0.2311
Meta loss on this task batch = 2.6961e-01, Meta loss averaged over last 500 steps = 2.7107e-01, PNorm = 156.9369, GNorm = 0.2062
Meta loss on this task batch = 2.4528e-01, Meta loss averaged over last 500 steps = 2.7100e-01, PNorm = 156.9440, GNorm = 0.2073
Meta loss on this task batch = 2.7534e-01, Meta loss averaged over last 500 steps = 2.7101e-01, PNorm = 156.9507, GNorm = 0.2135
Meta loss on this task batch = 2.2282e-01, Meta loss averaged over last 500 steps = 2.7074e-01, PNorm = 156.9576, GNorm = 0.2275
Meta loss on this task batch = 2.5681e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 156.9644, GNorm = 0.2446
Meta loss on this task batch = 3.0523e-01, Meta loss averaged over last 500 steps = 2.7082e-01, PNorm = 156.9704, GNorm = 0.2270
Meta loss on this task batch = 2.9658e-01, Meta loss averaged over last 500 steps = 2.7091e-01, PNorm = 156.9758, GNorm = 0.2380
Meta loss on this task batch = 3.0119e-01, Meta loss averaged over last 500 steps = 2.7107e-01, PNorm = 156.9801, GNorm = 0.2597
Meta loss on this task batch = 2.2638e-01, Meta loss averaged over last 500 steps = 2.7092e-01, PNorm = 156.9844, GNorm = 0.2204
Meta loss on this task batch = 2.6744e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 156.9890, GNorm = 0.2528
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.7098e-01, PNorm = 156.9938, GNorm = 0.2251
Meta loss on this task batch = 2.7279e-01, Meta loss averaged over last 500 steps = 2.7099e-01, PNorm = 156.9983, GNorm = 0.2377
Meta loss on this task batch = 2.6492e-01, Meta loss averaged over last 500 steps = 2.7095e-01, PNorm = 157.0033, GNorm = 0.2303
Meta loss on this task batch = 2.8452e-01, Meta loss averaged over last 500 steps = 2.7096e-01, PNorm = 157.0081, GNorm = 0.2395
Meta loss on this task batch = 2.8719e-01, Meta loss averaged over last 500 steps = 2.7087e-01, PNorm = 157.0126, GNorm = 0.2413
Meta loss on this task batch = 2.9944e-01, Meta loss averaged over last 500 steps = 2.7082e-01, PNorm = 157.0168, GNorm = 0.2717
Meta loss on this task batch = 2.6206e-01, Meta loss averaged over last 500 steps = 2.7078e-01, PNorm = 157.0210, GNorm = 0.2532
Took 108.45759987831116 seconds to complete one epoch of meta training
Took 115.7895565032959 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484153
Epoch 686
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 2.7079e-01, PNorm = 157.0249, GNorm = 0.2385
Meta loss on this task batch = 2.5060e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 157.0291, GNorm = 0.2286
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 157.0336, GNorm = 0.2607
Meta loss on this task batch = 2.4327e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 157.0384, GNorm = 0.2266
Meta loss on this task batch = 2.4307e-01, Meta loss averaged over last 500 steps = 2.7074e-01, PNorm = 157.0434, GNorm = 0.2345
Meta loss on this task batch = 2.3903e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 157.0479, GNorm = 0.2260
Meta loss on this task batch = 2.7584e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 157.0519, GNorm = 0.2192
Meta loss on this task batch = 2.5352e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 157.0563, GNorm = 0.1922
Meta loss on this task batch = 2.6868e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 157.0611, GNorm = 0.1983
Meta loss on this task batch = 2.4002e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 157.0656, GNorm = 0.2091
Meta loss on this task batch = 2.4274e-01, Meta loss averaged over last 500 steps = 2.7062e-01, PNorm = 157.0702, GNorm = 0.2026
Meta loss on this task batch = 2.3347e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 157.0753, GNorm = 0.2178
Meta loss on this task batch = 2.5124e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 157.0805, GNorm = 0.2287
Meta loss on this task batch = 2.6864e-01, Meta loss averaged over last 500 steps = 2.7049e-01, PNorm = 157.0861, GNorm = 0.2240
Meta loss on this task batch = 2.9503e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 157.0912, GNorm = 0.2840
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 157.0962, GNorm = 0.2592
Meta loss on this task batch = 2.8816e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 157.1009, GNorm = 0.2249
Meta loss on this task batch = 2.8045e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 157.1054, GNorm = 0.2061
Meta loss on this task batch = 3.0144e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 157.1101, GNorm = 0.3556
Took 113.91034698486328 seconds to complete one epoch of meta training
Took 121.50809717178345 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504184
Epoch 687
Meta loss on this task batch = 3.1584e-01, Meta loss averaged over last 500 steps = 2.7083e-01, PNorm = 157.1146, GNorm = 0.2932
Meta loss on this task batch = 2.3450e-01, Meta loss averaged over last 500 steps = 2.7083e-01, PNorm = 157.1193, GNorm = 0.2653
Meta loss on this task batch = 2.6955e-01, Meta loss averaged over last 500 steps = 2.7091e-01, PNorm = 157.1248, GNorm = 0.2622
Meta loss on this task batch = 2.1325e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 157.1301, GNorm = 0.2029
Meta loss on this task batch = 3.2175e-01, Meta loss averaged over last 500 steps = 2.7080e-01, PNorm = 157.1346, GNorm = 0.2905
Meta loss on this task batch = 2.7121e-01, Meta loss averaged over last 500 steps = 2.7080e-01, PNorm = 157.1393, GNorm = 0.2516
Meta loss on this task batch = 2.4935e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 157.1440, GNorm = 0.2089
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 157.1492, GNorm = 0.2516
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 157.1531, GNorm = 0.2633
Meta loss on this task batch = 2.9431e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 157.1574, GNorm = 0.2334
Meta loss on this task batch = 2.4963e-01, Meta loss averaged over last 500 steps = 2.7054e-01, PNorm = 157.1623, GNorm = 0.2253
Meta loss on this task batch = 2.5839e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 157.1673, GNorm = 0.2249
Meta loss on this task batch = 2.2234e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 157.1729, GNorm = 0.2224
Meta loss on this task batch = 2.9910e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 157.1781, GNorm = 0.2969
Meta loss on this task batch = 2.5806e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 157.1838, GNorm = 0.2137
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.7068e-01, PNorm = 157.1898, GNorm = 0.2334
Meta loss on this task batch = 2.8543e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 157.1956, GNorm = 0.2401
Meta loss on this task batch = 2.7748e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 157.2012, GNorm = 0.2445
Meta loss on this task batch = 2.8975e-01, Meta loss averaged over last 500 steps = 2.7080e-01, PNorm = 157.2061, GNorm = 0.2693
Took 108.01450324058533 seconds to complete one epoch of meta training
Took 115.55143427848816 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493482
Epoch 688
Meta loss on this task batch = 2.6076e-01, Meta loss averaged over last 500 steps = 2.7081e-01, PNorm = 157.2109, GNorm = 0.2706
Meta loss on this task batch = 2.4390e-01, Meta loss averaged over last 500 steps = 2.7075e-01, PNorm = 157.2152, GNorm = 0.2677
Meta loss on this task batch = 3.0561e-01, Meta loss averaged over last 500 steps = 2.7078e-01, PNorm = 157.2191, GNorm = 0.2456
Meta loss on this task batch = 2.4863e-01, Meta loss averaged over last 500 steps = 2.7072e-01, PNorm = 157.2236, GNorm = 0.2117
Meta loss on this task batch = 2.2906e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 157.2279, GNorm = 0.2047
Meta loss on this task batch = 2.9452e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 157.2320, GNorm = 0.2234
Meta loss on this task batch = 3.0817e-01, Meta loss averaged over last 500 steps = 2.7070e-01, PNorm = 157.2353, GNorm = 0.2524
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 157.2391, GNorm = 0.2358
Meta loss on this task batch = 2.4525e-01, Meta loss averaged over last 500 steps = 2.7066e-01, PNorm = 157.2433, GNorm = 0.2089
Meta loss on this task batch = 2.5000e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 157.2483, GNorm = 0.2256
Meta loss on this task batch = 2.9007e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 157.2531, GNorm = 0.2326
Meta loss on this task batch = 2.6636e-01, Meta loss averaged over last 500 steps = 2.7067e-01, PNorm = 157.2581, GNorm = 0.2236
Meta loss on this task batch = 2.9053e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 157.2630, GNorm = 0.2083
Meta loss on this task batch = 2.8629e-01, Meta loss averaged over last 500 steps = 2.7065e-01, PNorm = 157.2671, GNorm = 0.2415
Meta loss on this task batch = 2.8499e-01, Meta loss averaged over last 500 steps = 2.7064e-01, PNorm = 157.2719, GNorm = 0.2645
Meta loss on this task batch = 2.4625e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 157.2768, GNorm = 0.2201
Meta loss on this task batch = 3.2016e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 157.2817, GNorm = 0.2379
Meta loss on this task batch = 2.4038e-01, Meta loss averaged over last 500 steps = 2.7058e-01, PNorm = 157.2866, GNorm = 0.2202
Meta loss on this task batch = 2.8160e-01, Meta loss averaged over last 500 steps = 2.7071e-01, PNorm = 157.2923, GNorm = 0.2592
Took 108.93825674057007 seconds to complete one epoch of meta training
Took 116.31358408927917 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491984
Epoch 689
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 2.7078e-01, PNorm = 157.2971, GNorm = 0.2533
Meta loss on this task batch = 2.4139e-01, Meta loss averaged over last 500 steps = 2.7068e-01, PNorm = 157.3024, GNorm = 0.2302
Meta loss on this task batch = 2.2500e-01, Meta loss averaged over last 500 steps = 2.7063e-01, PNorm = 157.3076, GNorm = 0.2207
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.7069e-01, PNorm = 157.3121, GNorm = 0.2387
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 2.7060e-01, PNorm = 157.3165, GNorm = 0.2581
Meta loss on this task batch = 3.0703e-01, Meta loss averaged over last 500 steps = 2.7066e-01, PNorm = 157.3194, GNorm = 0.2712
Meta loss on this task batch = 2.5838e-01, Meta loss averaged over last 500 steps = 2.7056e-01, PNorm = 157.3211, GNorm = 0.2638
Meta loss on this task batch = 2.1786e-01, Meta loss averaged over last 500 steps = 2.7034e-01, PNorm = 157.3234, GNorm = 0.1839
Meta loss on this task batch = 2.9046e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 157.3262, GNorm = 0.2292
Meta loss on this task batch = 2.8413e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 157.3297, GNorm = 0.2433
Meta loss on this task batch = 2.9290e-01, Meta loss averaged over last 500 steps = 2.7048e-01, PNorm = 157.3336, GNorm = 0.2312
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 157.3367, GNorm = 0.2410
Meta loss on this task batch = 2.1694e-01, Meta loss averaged over last 500 steps = 2.7039e-01, PNorm = 157.3400, GNorm = 0.2078
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 2.7034e-01, PNorm = 157.3438, GNorm = 0.2471
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.7051e-01, PNorm = 157.3482, GNorm = 0.2459
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 2.7050e-01, PNorm = 157.3533, GNorm = 0.2330
Meta loss on this task batch = 2.0594e-01, Meta loss averaged over last 500 steps = 2.7035e-01, PNorm = 157.3591, GNorm = 0.1901
Meta loss on this task batch = 3.0583e-01, Meta loss averaged over last 500 steps = 2.7046e-01, PNorm = 157.3649, GNorm = 0.3017
Meta loss on this task batch = 3.0644e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 157.3697, GNorm = 0.2887
Took 111.35465788841248 seconds to complete one epoch of meta training
Took 119.77454328536987 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503095
Epoch 690
Meta loss on this task batch = 2.6898e-01, Meta loss averaged over last 500 steps = 2.7056e-01, PNorm = 157.3753, GNorm = 0.2037
Meta loss on this task batch = 2.7211e-01, Meta loss averaged over last 500 steps = 2.7039e-01, PNorm = 157.3807, GNorm = 0.2267
Meta loss on this task batch = 2.8232e-01, Meta loss averaged over last 500 steps = 2.7044e-01, PNorm = 157.3858, GNorm = 0.2378
Meta loss on this task batch = 2.4000e-01, Meta loss averaged over last 500 steps = 2.7040e-01, PNorm = 157.3907, GNorm = 0.2342
Meta loss on this task batch = 3.3960e-01, Meta loss averaged over last 500 steps = 2.7047e-01, PNorm = 157.3943, GNorm = 0.2673
Meta loss on this task batch = 2.6447e-01, Meta loss averaged over last 500 steps = 2.7041e-01, PNorm = 157.3978, GNorm = 0.2609
Meta loss on this task batch = 3.0104e-01, Meta loss averaged over last 500 steps = 2.7044e-01, PNorm = 157.4014, GNorm = 0.2587
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 157.4047, GNorm = 0.2138
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 157.4088, GNorm = 0.2588
Meta loss on this task batch = 2.4958e-01, Meta loss averaged over last 500 steps = 2.7059e-01, PNorm = 157.4129, GNorm = 0.1985
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.7055e-01, PNorm = 157.4177, GNorm = 0.2303
Meta loss on this task batch = 2.4345e-01, Meta loss averaged over last 500 steps = 2.7052e-01, PNorm = 157.4233, GNorm = 0.2334
Meta loss on this task batch = 2.7298e-01, Meta loss averaged over last 500 steps = 2.7053e-01, PNorm = 157.4289, GNorm = 0.2430
Meta loss on this task batch = 2.4143e-01, Meta loss averaged over last 500 steps = 2.7036e-01, PNorm = 157.4349, GNorm = 0.2189
Meta loss on this task batch = 2.8184e-01, Meta loss averaged over last 500 steps = 2.7044e-01, PNorm = 157.4403, GNorm = 0.2463
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 157.4464, GNorm = 0.2355
Meta loss on this task batch = 2.9121e-01, Meta loss averaged over last 500 steps = 2.7043e-01, PNorm = 157.4520, GNorm = 0.2341
Meta loss on this task batch = 2.5544e-01, Meta loss averaged over last 500 steps = 2.7041e-01, PNorm = 157.4579, GNorm = 0.2054
Meta loss on this task batch = 2.5296e-01, Meta loss averaged over last 500 steps = 2.7036e-01, PNorm = 157.4623, GNorm = 0.3227
Took 109.40324234962463 seconds to complete one epoch of meta training
Took 116.73353672027588 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475040
Epoch 691
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 2.7032e-01, PNorm = 157.4664, GNorm = 0.2626
Meta loss on this task batch = 2.4750e-01, Meta loss averaged over last 500 steps = 2.7018e-01, PNorm = 157.4709, GNorm = 0.2106
Meta loss on this task batch = 2.9941e-01, Meta loss averaged over last 500 steps = 2.7023e-01, PNorm = 157.4760, GNorm = 0.2332
Meta loss on this task batch = 2.7191e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 157.4812, GNorm = 0.2305
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.7030e-01, PNorm = 157.4872, GNorm = 0.2465
Meta loss on this task batch = 2.4691e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 157.4935, GNorm = 0.2244
Meta loss on this task batch = 2.6446e-01, Meta loss averaged over last 500 steps = 2.7023e-01, PNorm = 157.4998, GNorm = 0.2159
Meta loss on this task batch = 2.7187e-01, Meta loss averaged over last 500 steps = 2.7015e-01, PNorm = 157.5058, GNorm = 0.2365
Meta loss on this task batch = 2.4938e-01, Meta loss averaged over last 500 steps = 2.7006e-01, PNorm = 157.5113, GNorm = 0.2085
Meta loss on this task batch = 2.7179e-01, Meta loss averaged over last 500 steps = 2.7011e-01, PNorm = 157.5166, GNorm = 0.2594
Meta loss on this task batch = 3.2368e-01, Meta loss averaged over last 500 steps = 2.7019e-01, PNorm = 157.5212, GNorm = 0.2560
Meta loss on this task batch = 2.1896e-01, Meta loss averaged over last 500 steps = 2.7008e-01, PNorm = 157.5257, GNorm = 0.2004
Meta loss on this task batch = 3.4577e-01, Meta loss averaged over last 500 steps = 2.7033e-01, PNorm = 157.5294, GNorm = 0.2852
Meta loss on this task batch = 2.7344e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 157.5325, GNorm = 0.2268
Meta loss on this task batch = 2.4895e-01, Meta loss averaged over last 500 steps = 2.7025e-01, PNorm = 157.5363, GNorm = 0.2773
Meta loss on this task batch = 2.2586e-01, Meta loss averaged over last 500 steps = 2.7011e-01, PNorm = 157.5403, GNorm = 0.1992
Meta loss on this task batch = 2.4839e-01, Meta loss averaged over last 500 steps = 2.7005e-01, PNorm = 157.5452, GNorm = 0.2199
Meta loss on this task batch = 2.6256e-01, Meta loss averaged over last 500 steps = 2.7000e-01, PNorm = 157.5506, GNorm = 0.2211
Meta loss on this task batch = 2.6315e-01, Meta loss averaged over last 500 steps = 2.6999e-01, PNorm = 157.5560, GNorm = 0.2384
Took 109.63896894454956 seconds to complete one epoch of meta training
Took 116.08551716804504 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483755
Epoch 692
Meta loss on this task batch = 3.1831e-01, Meta loss averaged over last 500 steps = 2.7012e-01, PNorm = 157.5620, GNorm = 0.2554
Meta loss on this task batch = 2.7932e-01, Meta loss averaged over last 500 steps = 2.7012e-01, PNorm = 157.5679, GNorm = 0.2728
Meta loss on this task batch = 2.3312e-01, Meta loss averaged over last 500 steps = 2.6993e-01, PNorm = 157.5743, GNorm = 0.2106
Meta loss on this task batch = 2.4390e-01, Meta loss averaged over last 500 steps = 2.6987e-01, PNorm = 157.5800, GNorm = 0.2272
Meta loss on this task batch = 2.4145e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 157.5851, GNorm = 0.2211
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 2.6995e-01, PNorm = 157.5900, GNorm = 0.2263
Meta loss on this task batch = 2.5718e-01, Meta loss averaged over last 500 steps = 2.6996e-01, PNorm = 157.5948, GNorm = 0.2500
Meta loss on this task batch = 2.7961e-01, Meta loss averaged over last 500 steps = 2.7003e-01, PNorm = 157.5996, GNorm = 0.2572
Meta loss on this task batch = 2.6261e-01, Meta loss averaged over last 500 steps = 2.6995e-01, PNorm = 157.6043, GNorm = 0.2374
Meta loss on this task batch = 2.6128e-01, Meta loss averaged over last 500 steps = 2.6994e-01, PNorm = 157.6092, GNorm = 0.2237
Meta loss on this task batch = 2.7094e-01, Meta loss averaged over last 500 steps = 2.6994e-01, PNorm = 157.6148, GNorm = 0.2168
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.6987e-01, PNorm = 157.6202, GNorm = 0.2303
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.6988e-01, PNorm = 157.6250, GNorm = 0.2602
Meta loss on this task batch = 2.3966e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 157.6298, GNorm = 0.2186
Meta loss on this task batch = 3.0978e-01, Meta loss averaged over last 500 steps = 2.6997e-01, PNorm = 157.6339, GNorm = 0.2530
Meta loss on this task batch = 2.8616e-01, Meta loss averaged over last 500 steps = 2.6987e-01, PNorm = 157.6377, GNorm = 0.2664
Meta loss on this task batch = 3.3441e-01, Meta loss averaged over last 500 steps = 2.7001e-01, PNorm = 157.6401, GNorm = 0.3521
Meta loss on this task batch = 2.3472e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 157.6424, GNorm = 0.2255
Meta loss on this task batch = 2.3607e-01, Meta loss averaged over last 500 steps = 2.6992e-01, PNorm = 157.6454, GNorm = 0.2588
Took 110.68031525611877 seconds to complete one epoch of meta training
Took 118.59480595588684 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486897
Epoch 693
Meta loss on this task batch = 2.6024e-01, Meta loss averaged over last 500 steps = 2.6983e-01, PNorm = 157.6488, GNorm = 0.2421
Meta loss on this task batch = 2.8848e-01, Meta loss averaged over last 500 steps = 2.6993e-01, PNorm = 157.6522, GNorm = 0.2547
Meta loss on this task batch = 2.5027e-01, Meta loss averaged over last 500 steps = 2.6993e-01, PNorm = 157.6561, GNorm = 0.2019
Meta loss on this task batch = 2.1024e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 157.6609, GNorm = 0.2082
Meta loss on this task batch = 2.5003e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 157.6659, GNorm = 0.2431
Meta loss on this task batch = 2.7937e-01, Meta loss averaged over last 500 steps = 2.6974e-01, PNorm = 157.6706, GNorm = 0.2317
Meta loss on this task batch = 2.6061e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 157.6759, GNorm = 0.2263
Meta loss on this task batch = 2.7792e-01, Meta loss averaged over last 500 steps = 2.6967e-01, PNorm = 157.6822, GNorm = 0.2628
Meta loss on this task batch = 3.0889e-01, Meta loss averaged over last 500 steps = 2.6961e-01, PNorm = 157.6879, GNorm = 0.2261
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 157.6931, GNorm = 0.2457
Meta loss on this task batch = 2.3472e-01, Meta loss averaged over last 500 steps = 2.6960e-01, PNorm = 157.6991, GNorm = 0.2314
Meta loss on this task batch = 2.6469e-01, Meta loss averaged over last 500 steps = 2.6961e-01, PNorm = 157.7060, GNorm = 0.2303
Meta loss on this task batch = 2.6292e-01, Meta loss averaged over last 500 steps = 2.6956e-01, PNorm = 157.7129, GNorm = 0.2291
Meta loss on this task batch = 2.4956e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 157.7193, GNorm = 0.2308
Meta loss on this task batch = 2.5259e-01, Meta loss averaged over last 500 steps = 2.6950e-01, PNorm = 157.7254, GNorm = 0.2624
Meta loss on this task batch = 3.0693e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 157.7305, GNorm = 0.2651
Meta loss on this task batch = 2.8413e-01, Meta loss averaged over last 500 steps = 2.6964e-01, PNorm = 157.7352, GNorm = 0.2764
Meta loss on this task batch = 2.2197e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 157.7401, GNorm = 0.2110
Meta loss on this task batch = 2.2842e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 157.7457, GNorm = 0.2179
Took 109.08672404289246 seconds to complete one epoch of meta training
Took 117.11132216453552 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489161
Epoch 694
Meta loss on this task batch = 2.6804e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 157.7511, GNorm = 0.2589
Meta loss on this task batch = 2.9475e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 157.7562, GNorm = 0.2324
Meta loss on this task batch = 2.0151e-01, Meta loss averaged over last 500 steps = 2.6928e-01, PNorm = 157.7610, GNorm = 0.2053
Meta loss on this task batch = 2.7860e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 157.7653, GNorm = 0.2252
Meta loss on this task batch = 3.2220e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 157.7686, GNorm = 0.3079
Meta loss on this task batch = 2.5006e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 157.7722, GNorm = 0.2625
Meta loss on this task batch = 3.2419e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 157.7766, GNorm = 0.2451
Meta loss on this task batch = 2.6233e-01, Meta loss averaged over last 500 steps = 2.6951e-01, PNorm = 157.7809, GNorm = 0.2174
Meta loss on this task batch = 2.2105e-01, Meta loss averaged over last 500 steps = 2.6952e-01, PNorm = 157.7858, GNorm = 0.2142
Meta loss on this task batch = 2.6748e-01, Meta loss averaged over last 500 steps = 2.6952e-01, PNorm = 157.7909, GNorm = 0.2373
Meta loss on this task batch = 2.2990e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 157.7965, GNorm = 0.1991
Meta loss on this task batch = 2.9803e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 157.8023, GNorm = 0.2119
Meta loss on this task batch = 2.2460e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 157.8081, GNorm = 0.1865
Meta loss on this task batch = 3.2665e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 157.8136, GNorm = 0.3402
Meta loss on this task batch = 2.4154e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 157.8196, GNorm = 0.2266
Meta loss on this task batch = 2.8319e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 157.8256, GNorm = 0.2312
Meta loss on this task batch = 2.6999e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 157.8309, GNorm = 0.3094
Meta loss on this task batch = 2.5603e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 157.8364, GNorm = 0.2065
Meta loss on this task batch = 2.1655e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 157.8422, GNorm = 0.2652
Took 107.59780669212341 seconds to complete one epoch of meta training
Took 115.22822451591492 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467589
Epoch 695
Meta loss on this task batch = 3.1056e-01, Meta loss averaged over last 500 steps = 2.6939e-01, PNorm = 157.8477, GNorm = 0.2316
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.6929e-01, PNorm = 157.8528, GNorm = 0.2460
Meta loss on this task batch = 2.9671e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 157.8574, GNorm = 0.2230
Meta loss on this task batch = 2.9488e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 157.8619, GNorm = 0.2356
Meta loss on this task batch = 2.6911e-01, Meta loss averaged over last 500 steps = 2.6948e-01, PNorm = 157.8667, GNorm = 0.2139
Meta loss on this task batch = 2.8340e-01, Meta loss averaged over last 500 steps = 2.6953e-01, PNorm = 157.8704, GNorm = 0.2489
Meta loss on this task batch = 2.0681e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 157.8745, GNorm = 0.2126
Meta loss on this task batch = 2.5271e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 157.8777, GNorm = 0.2274
Meta loss on this task batch = 3.1257e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 157.8801, GNorm = 0.2569
Meta loss on this task batch = 2.7198e-01, Meta loss averaged over last 500 steps = 2.6924e-01, PNorm = 157.8826, GNorm = 0.2358
Meta loss on this task batch = 2.4145e-01, Meta loss averaged over last 500 steps = 2.6915e-01, PNorm = 157.8853, GNorm = 0.2165
Meta loss on this task batch = 3.1649e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 157.8881, GNorm = 0.2771
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 157.8918, GNorm = 0.2304
Meta loss on this task batch = 2.7723e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 157.8962, GNorm = 0.2176
Meta loss on this task batch = 2.9371e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 157.9006, GNorm = 0.2291
Meta loss on this task batch = 2.6137e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 157.9051, GNorm = 0.2389
Meta loss on this task batch = 3.2716e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 157.9095, GNorm = 0.2817
Meta loss on this task batch = 2.3423e-01, Meta loss averaged over last 500 steps = 2.6956e-01, PNorm = 157.9138, GNorm = 0.2063
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 157.9183, GNorm = 0.2600
Took 108.45963382720947 seconds to complete one epoch of meta training
Took 115.7355694770813 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469888
Epoch 696
Meta loss on this task batch = 2.7633e-01, Meta loss averaged over last 500 steps = 2.6953e-01, PNorm = 157.9232, GNorm = 0.1988
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 2.6960e-01, PNorm = 157.9278, GNorm = 0.2647
Meta loss on this task batch = 3.0564e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 157.9322, GNorm = 0.2263
Meta loss on this task batch = 2.7371e-01, Meta loss averaged over last 500 steps = 2.6963e-01, PNorm = 157.9366, GNorm = 0.2457
Meta loss on this task batch = 2.9996e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 157.9412, GNorm = 0.2539
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 2.6974e-01, PNorm = 157.9461, GNorm = 0.2216
Meta loss on this task batch = 2.3339e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 157.9518, GNorm = 0.2198
Meta loss on this task batch = 2.2223e-01, Meta loss averaged over last 500 steps = 2.6963e-01, PNorm = 157.9581, GNorm = 0.2020
Meta loss on this task batch = 3.1265e-01, Meta loss averaged over last 500 steps = 2.6975e-01, PNorm = 157.9646, GNorm = 0.2673
Meta loss on this task batch = 2.3550e-01, Meta loss averaged over last 500 steps = 2.6983e-01, PNorm = 157.9711, GNorm = 0.2298
Meta loss on this task batch = 3.1549e-01, Meta loss averaged over last 500 steps = 2.6989e-01, PNorm = 157.9766, GNorm = 0.2417
Meta loss on this task batch = 2.7309e-01, Meta loss averaged over last 500 steps = 2.6988e-01, PNorm = 157.9818, GNorm = 0.2216
Meta loss on this task batch = 2.4254e-01, Meta loss averaged over last 500 steps = 2.6971e-01, PNorm = 157.9875, GNorm = 0.2142
Meta loss on this task batch = 3.0022e-01, Meta loss averaged over last 500 steps = 2.6981e-01, PNorm = 157.9935, GNorm = 0.2725
Meta loss on this task batch = 1.9654e-01, Meta loss averaged over last 500 steps = 2.6969e-01, PNorm = 158.0000, GNorm = 0.1981
Meta loss on this task batch = 2.8106e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 158.0064, GNorm = 0.2577
Meta loss on this task batch = 2.7215e-01, Meta loss averaged over last 500 steps = 2.6963e-01, PNorm = 158.0124, GNorm = 0.2023
Meta loss on this task batch = 2.4393e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 158.0180, GNorm = 0.2261
Meta loss on this task batch = 2.0458e-01, Meta loss averaged over last 500 steps = 2.6943e-01, PNorm = 158.0239, GNorm = 0.2497
Took 110.58731412887573 seconds to complete one epoch of meta training
Took 117.09596753120422 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478732
Epoch 697
Meta loss on this task batch = 2.7943e-01, Meta loss averaged over last 500 steps = 2.6956e-01, PNorm = 158.0295, GNorm = 0.2544
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 2.6966e-01, PNorm = 158.0345, GNorm = 0.2633
Meta loss on this task batch = 2.7479e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 158.0394, GNorm = 0.2486
Meta loss on this task batch = 2.0014e-01, Meta loss averaged over last 500 steps = 2.6945e-01, PNorm = 158.0445, GNorm = 0.2124
Meta loss on this task batch = 2.6509e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 158.0498, GNorm = 0.2244
Meta loss on this task batch = 2.8668e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 158.0549, GNorm = 0.2473
Meta loss on this task batch = 2.7099e-01, Meta loss averaged over last 500 steps = 2.6933e-01, PNorm = 158.0594, GNorm = 0.2489
Meta loss on this task batch = 2.4825e-01, Meta loss averaged over last 500 steps = 2.6935e-01, PNorm = 158.0645, GNorm = 0.2265
Meta loss on this task batch = 1.9170e-01, Meta loss averaged over last 500 steps = 2.6912e-01, PNorm = 158.0692, GNorm = 0.1785
Meta loss on this task batch = 3.0111e-01, Meta loss averaged over last 500 steps = 2.6912e-01, PNorm = 158.0732, GNorm = 0.2452
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 2.6911e-01, PNorm = 158.0775, GNorm = 0.2233
Meta loss on this task batch = 3.3550e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 158.0821, GNorm = 0.3646
Meta loss on this task batch = 2.8766e-01, Meta loss averaged over last 500 steps = 2.6935e-01, PNorm = 158.0862, GNorm = 0.2540
Meta loss on this task batch = 2.3890e-01, Meta loss averaged over last 500 steps = 2.6931e-01, PNorm = 158.0907, GNorm = 0.1976
Meta loss on this task batch = 2.5411e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 158.0958, GNorm = 0.1926
Meta loss on this task batch = 2.2849e-01, Meta loss averaged over last 500 steps = 2.6935e-01, PNorm = 158.1015, GNorm = 0.2247
Meta loss on this task batch = 3.2113e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 158.1077, GNorm = 0.2336
Meta loss on this task batch = 3.1003e-01, Meta loss averaged over last 500 steps = 2.6971e-01, PNorm = 158.1141, GNorm = 0.2612
Meta loss on this task batch = 3.0286e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 158.1207, GNorm = 0.2920
Took 106.94245624542236 seconds to complete one epoch of meta training
Took 114.45008325576782 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.459593
Epoch 698
Meta loss on this task batch = 2.5441e-01, Meta loss averaged over last 500 steps = 2.6966e-01, PNorm = 158.1269, GNorm = 0.2339
Meta loss on this task batch = 2.6479e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 158.1324, GNorm = 0.2297
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 158.1377, GNorm = 0.1956
Meta loss on this task batch = 3.0330e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 158.1427, GNorm = 0.2535
Meta loss on this task batch = 3.1487e-01, Meta loss averaged over last 500 steps = 2.6946e-01, PNorm = 158.1472, GNorm = 0.2630
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 158.1519, GNorm = 0.2357
Meta loss on this task batch = 1.9750e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 158.1573, GNorm = 0.2162
Meta loss on this task batch = 2.8588e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 158.1624, GNorm = 0.2473
Meta loss on this task batch = 3.1695e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 158.1672, GNorm = 0.2233
Meta loss on this task batch = 3.2927e-01, Meta loss averaged over last 500 steps = 2.6959e-01, PNorm = 158.1712, GNorm = 0.2286
Meta loss on this task batch = 2.6600e-01, Meta loss averaged over last 500 steps = 2.6961e-01, PNorm = 158.1759, GNorm = 0.2285
Meta loss on this task batch = 3.2956e-01, Meta loss averaged over last 500 steps = 2.6968e-01, PNorm = 158.1803, GNorm = 0.2309
Meta loss on this task batch = 2.6595e-01, Meta loss averaged over last 500 steps = 2.6971e-01, PNorm = 158.1847, GNorm = 0.2285
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.6977e-01, PNorm = 158.1897, GNorm = 0.2473
Meta loss on this task batch = 2.3909e-01, Meta loss averaged over last 500 steps = 2.6973e-01, PNorm = 158.1947, GNorm = 0.2324
Meta loss on this task batch = 1.9990e-01, Meta loss averaged over last 500 steps = 2.6956e-01, PNorm = 158.1995, GNorm = 0.2603
Meta loss on this task batch = 2.6896e-01, Meta loss averaged over last 500 steps = 2.6958e-01, PNorm = 158.2053, GNorm = 0.2877
Meta loss on this task batch = 2.5508e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 158.2112, GNorm = 0.2105
Meta loss on this task batch = 3.0166e-01, Meta loss averaged over last 500 steps = 2.6948e-01, PNorm = 158.2153, GNorm = 0.3367
Took 109.31534934043884 seconds to complete one epoch of meta training
Took 116.73649883270264 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484975
Epoch 699
Meta loss on this task batch = 2.8000e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 158.2192, GNorm = 0.2251
Meta loss on this task batch = 2.1552e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 158.2243, GNorm = 0.2065
Meta loss on this task batch = 2.5038e-01, Meta loss averaged over last 500 steps = 2.6930e-01, PNorm = 158.2295, GNorm = 0.2195
Meta loss on this task batch = 3.1831e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 158.2349, GNorm = 0.2446
Meta loss on this task batch = 2.5579e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 158.2404, GNorm = 0.2135
Meta loss on this task batch = 2.4361e-01, Meta loss averaged over last 500 steps = 2.6932e-01, PNorm = 158.2462, GNorm = 0.2124
Meta loss on this task batch = 3.0361e-01, Meta loss averaged over last 500 steps = 2.6935e-01, PNorm = 158.2521, GNorm = 0.2374
Meta loss on this task batch = 2.5768e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.2585, GNorm = 0.2006
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.6959e-01, PNorm = 158.2647, GNorm = 0.2294
Meta loss on this task batch = 2.7222e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 158.2700, GNorm = 0.2131
Meta loss on this task batch = 2.4968e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 158.2758, GNorm = 0.2002
Meta loss on this task batch = 2.3740e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 158.2820, GNorm = 0.2188
Meta loss on this task batch = 2.2589e-01, Meta loss averaged over last 500 steps = 2.6935e-01, PNorm = 158.2885, GNorm = 0.2283
Meta loss on this task batch = 3.5482e-01, Meta loss averaged over last 500 steps = 2.6959e-01, PNorm = 158.2934, GNorm = 0.2730
Meta loss on this task batch = 2.4379e-01, Meta loss averaged over last 500 steps = 2.6944e-01, PNorm = 158.2985, GNorm = 0.2267
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 158.3039, GNorm = 0.2250
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.3092, GNorm = 0.2209
Meta loss on this task batch = 2.5937e-01, Meta loss averaged over last 500 steps = 2.6943e-01, PNorm = 158.3145, GNorm = 0.2242
Meta loss on this task batch = 3.1065e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.3204, GNorm = 0.2618
Took 105.94216322898865 seconds to complete one epoch of meta training
Took 113.53892731666565 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476260
Epoch 700
Meta loss on this task batch = 2.1839e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 158.3267, GNorm = 0.2014
Meta loss on this task batch = 2.4405e-01, Meta loss averaged over last 500 steps = 2.6922e-01, PNorm = 158.3325, GNorm = 0.2148
Meta loss on this task batch = 2.8100e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 158.3378, GNorm = 0.2808
Meta loss on this task batch = 2.4974e-01, Meta loss averaged over last 500 steps = 2.6929e-01, PNorm = 158.3431, GNorm = 0.2516
Meta loss on this task batch = 2.3372e-01, Meta loss averaged over last 500 steps = 2.6931e-01, PNorm = 158.3482, GNorm = 0.2008
Meta loss on this task batch = 2.9138e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 158.3538, GNorm = 0.2525
Meta loss on this task batch = 2.2047e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 158.3595, GNorm = 0.2300
Meta loss on this task batch = 3.2069e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 158.3658, GNorm = 0.2757
Meta loss on this task batch = 2.7379e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 158.3719, GNorm = 0.2048
Meta loss on this task batch = 2.6818e-01, Meta loss averaged over last 500 steps = 2.6916e-01, PNorm = 158.3779, GNorm = 0.1994
Meta loss on this task batch = 2.3099e-01, Meta loss averaged over last 500 steps = 2.6912e-01, PNorm = 158.3842, GNorm = 0.1975
Meta loss on this task batch = 2.5724e-01, Meta loss averaged over last 500 steps = 2.6915e-01, PNorm = 158.3898, GNorm = 0.2442
Meta loss on this task batch = 2.7330e-01, Meta loss averaged over last 500 steps = 2.6916e-01, PNorm = 158.3961, GNorm = 0.2077
Meta loss on this task batch = 2.3074e-01, Meta loss averaged over last 500 steps = 2.6908e-01, PNorm = 158.4023, GNorm = 0.2558
Meta loss on this task batch = 2.7905e-01, Meta loss averaged over last 500 steps = 2.6902e-01, PNorm = 158.4079, GNorm = 0.2406
Meta loss on this task batch = 2.9838e-01, Meta loss averaged over last 500 steps = 2.6909e-01, PNorm = 158.4132, GNorm = 0.2392
Meta loss on this task batch = 2.1039e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 158.4187, GNorm = 0.2104
Meta loss on this task batch = 3.2445e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 158.4236, GNorm = 0.2853
Meta loss on this task batch = 3.4810e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 158.4287, GNorm = 0.3057
Took 110.41690993309021 seconds to complete one epoch of meta training
Took 117.87280917167664 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471325
Epoch 701
Meta loss on this task batch = 2.8976e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 158.4336, GNorm = 0.2290
Meta loss on this task batch = 3.1048e-01, Meta loss averaged over last 500 steps = 2.6939e-01, PNorm = 158.4381, GNorm = 0.2746
Meta loss on this task batch = 3.1379e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 158.4427, GNorm = 0.2693
Meta loss on this task batch = 2.6104e-01, Meta loss averaged over last 500 steps = 2.6960e-01, PNorm = 158.4475, GNorm = 0.2260
Meta loss on this task batch = 2.5794e-01, Meta loss averaged over last 500 steps = 2.6952e-01, PNorm = 158.4521, GNorm = 0.2672
Meta loss on this task batch = 2.3937e-01, Meta loss averaged over last 500 steps = 2.6953e-01, PNorm = 158.4572, GNorm = 0.2010
Meta loss on this task batch = 2.6477e-01, Meta loss averaged over last 500 steps = 2.6951e-01, PNorm = 158.4631, GNorm = 0.2235
Meta loss on this task batch = 2.4950e-01, Meta loss averaged over last 500 steps = 2.6958e-01, PNorm = 158.4694, GNorm = 0.2501
Meta loss on this task batch = 2.5991e-01, Meta loss averaged over last 500 steps = 2.6967e-01, PNorm = 158.4761, GNorm = 0.2271
Meta loss on this task batch = 2.6277e-01, Meta loss averaged over last 500 steps = 2.6970e-01, PNorm = 158.4832, GNorm = 0.2333
Meta loss on this task batch = 2.2894e-01, Meta loss averaged over last 500 steps = 2.6962e-01, PNorm = 158.4897, GNorm = 0.2231
Meta loss on this task batch = 2.6849e-01, Meta loss averaged over last 500 steps = 2.6951e-01, PNorm = 158.4963, GNorm = 0.2344
Meta loss on this task batch = 2.1079e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.5019, GNorm = 0.2232
Meta loss on this task batch = 2.4956e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 158.5071, GNorm = 0.2355
Meta loss on this task batch = 2.4174e-01, Meta loss averaged over last 500 steps = 2.6924e-01, PNorm = 158.5125, GNorm = 0.2124
Meta loss on this task batch = 2.9756e-01, Meta loss averaged over last 500 steps = 2.6925e-01, PNorm = 158.5175, GNorm = 0.2226
Meta loss on this task batch = 2.6523e-01, Meta loss averaged over last 500 steps = 2.6926e-01, PNorm = 158.5217, GNorm = 0.2503
Meta loss on this task batch = 2.4829e-01, Meta loss averaged over last 500 steps = 2.6919e-01, PNorm = 158.5255, GNorm = 0.2193
Meta loss on this task batch = 3.5604e-01, Meta loss averaged over last 500 steps = 2.6929e-01, PNorm = 158.5289, GNorm = 0.3976
Took 107.2489058971405 seconds to complete one epoch of meta training
Took 115.23148894309998 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474290
Epoch 702
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.6918e-01, PNorm = 158.5327, GNorm = 0.2166
Meta loss on this task batch = 2.7526e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 158.5366, GNorm = 0.2347
Meta loss on this task batch = 2.7519e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 158.5406, GNorm = 0.2087
Meta loss on this task batch = 3.0450e-01, Meta loss averaged over last 500 steps = 2.6948e-01, PNorm = 158.5451, GNorm = 0.2332
Meta loss on this task batch = 2.5872e-01, Meta loss averaged over last 500 steps = 2.6941e-01, PNorm = 158.5504, GNorm = 0.2302
Meta loss on this task batch = 2.4737e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 158.5557, GNorm = 0.2177
Meta loss on this task batch = 3.2788e-01, Meta loss averaged over last 500 steps = 2.6943e-01, PNorm = 158.5610, GNorm = 0.2624
Meta loss on this task batch = 2.7063e-01, Meta loss averaged over last 500 steps = 2.6940e-01, PNorm = 158.5671, GNorm = 0.2078
Meta loss on this task batch = 2.4608e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 158.5732, GNorm = 0.1853
Meta loss on this task batch = 2.7354e-01, Meta loss averaged over last 500 steps = 2.6939e-01, PNorm = 158.5785, GNorm = 0.2125
Meta loss on this task batch = 2.4772e-01, Meta loss averaged over last 500 steps = 2.6935e-01, PNorm = 158.5829, GNorm = 0.2314
Meta loss on this task batch = 3.1487e-01, Meta loss averaged over last 500 steps = 2.6945e-01, PNorm = 158.5861, GNorm = 0.2118
Meta loss on this task batch = 2.3857e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.5899, GNorm = 0.2282
Meta loss on this task batch = 3.2555e-01, Meta loss averaged over last 500 steps = 2.6954e-01, PNorm = 158.5934, GNorm = 0.2567
Meta loss on this task batch = 2.5732e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 158.5970, GNorm = 0.1922
Meta loss on this task batch = 2.2506e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 158.6011, GNorm = 0.2032
Meta loss on this task batch = 2.6303e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 158.6044, GNorm = 0.2276
Meta loss on this task batch = 3.1515e-01, Meta loss averaged over last 500 steps = 2.6957e-01, PNorm = 158.6069, GNorm = 0.2371
Meta loss on this task batch = 2.7777e-01, Meta loss averaged over last 500 steps = 2.6959e-01, PNorm = 158.6098, GNorm = 0.2445
Took 114.10153412818909 seconds to complete one epoch of meta training
Took 121.6613073348999 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492502
Epoch 703
Meta loss on this task batch = 2.9114e-01, Meta loss averaged over last 500 steps = 2.6961e-01, PNorm = 158.6131, GNorm = 0.2363
Meta loss on this task batch = 2.3680e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.6165, GNorm = 0.2193
Meta loss on this task batch = 2.8942e-01, Meta loss averaged over last 500 steps = 2.6955e-01, PNorm = 158.6199, GNorm = 0.2429
Meta loss on this task batch = 2.2165e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 158.6242, GNorm = 0.2123
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.6949e-01, PNorm = 158.6290, GNorm = 0.2065
Meta loss on this task batch = 2.5442e-01, Meta loss averaged over last 500 steps = 2.6952e-01, PNorm = 158.6338, GNorm = 0.2303
Meta loss on this task batch = 2.5464e-01, Meta loss averaged over last 500 steps = 2.6947e-01, PNorm = 158.6384, GNorm = 0.2253
Meta loss on this task batch = 2.6771e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 158.6437, GNorm = 0.2553
Meta loss on this task batch = 2.2563e-01, Meta loss averaged over last 500 steps = 2.6934e-01, PNorm = 158.6493, GNorm = 0.1902
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 2.6942e-01, PNorm = 158.6541, GNorm = 0.2599
Meta loss on this task batch = 2.3867e-01, Meta loss averaged over last 500 steps = 2.6937e-01, PNorm = 158.6587, GNorm = 0.2191
Meta loss on this task batch = 2.8828e-01, Meta loss averaged over last 500 steps = 2.6936e-01, PNorm = 158.6634, GNorm = 0.2681
Meta loss on this task batch = 2.7140e-01, Meta loss averaged over last 500 steps = 2.6939e-01, PNorm = 158.6677, GNorm = 0.2316
Meta loss on this task batch = 2.7855e-01, Meta loss averaged over last 500 steps = 2.6938e-01, PNorm = 158.6718, GNorm = 0.2331
Meta loss on this task batch = 2.2883e-01, Meta loss averaged over last 500 steps = 2.6922e-01, PNorm = 158.6748, GNorm = 0.2250
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.6913e-01, PNorm = 158.6783, GNorm = 0.2355
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 2.6908e-01, PNorm = 158.6819, GNorm = 0.2263
Meta loss on this task batch = 3.5809e-01, Meta loss averaged over last 500 steps = 2.6929e-01, PNorm = 158.6853, GNorm = 0.2804
Meta loss on this task batch = 2.2690e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 158.6887, GNorm = 0.2542
Took 132.9159915447235 seconds to complete one epoch of meta training
Took 140.28824257850647 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496211
Epoch 704
Meta loss on this task batch = 2.3649e-01, Meta loss averaged over last 500 steps = 2.6923e-01, PNorm = 158.6940, GNorm = 0.2501
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.6917e-01, PNorm = 158.6989, GNorm = 0.2483
Meta loss on this task batch = 2.5166e-01, Meta loss averaged over last 500 steps = 2.6918e-01, PNorm = 158.7040, GNorm = 0.2082
Meta loss on this task batch = 2.5451e-01, Meta loss averaged over last 500 steps = 2.6921e-01, PNorm = 158.7093, GNorm = 0.2094
Meta loss on this task batch = 2.2346e-01, Meta loss averaged over last 500 steps = 2.6908e-01, PNorm = 158.7151, GNorm = 0.2182
Meta loss on this task batch = 2.7610e-01, Meta loss averaged over last 500 steps = 2.6904e-01, PNorm = 158.7200, GNorm = 0.2292
Meta loss on this task batch = 2.7462e-01, Meta loss averaged over last 500 steps = 2.6899e-01, PNorm = 158.7239, GNorm = 0.2786
Meta loss on this task batch = 2.5297e-01, Meta loss averaged over last 500 steps = 2.6898e-01, PNorm = 158.7287, GNorm = 0.2278
Meta loss on this task batch = 2.6377e-01, Meta loss averaged over last 500 steps = 2.6890e-01, PNorm = 158.7339, GNorm = 0.2262
Meta loss on this task batch = 2.2611e-01, Meta loss averaged over last 500 steps = 2.6903e-01, PNorm = 158.7399, GNorm = 0.2083
Meta loss on this task batch = 3.0507e-01, Meta loss averaged over last 500 steps = 2.6922e-01, PNorm = 158.7455, GNorm = 0.2246
Meta loss on this task batch = 2.5598e-01, Meta loss averaged over last 500 steps = 2.6925e-01, PNorm = 158.7512, GNorm = 0.2333
Meta loss on this task batch = 2.6227e-01, Meta loss averaged over last 500 steps = 2.6927e-01, PNorm = 158.7571, GNorm = 0.2547
Meta loss on this task batch = 2.8913e-01, Meta loss averaged over last 500 steps = 2.6931e-01, PNorm = 158.7619, GNorm = 0.2490
Meta loss on this task batch = 2.4044e-01, Meta loss averaged over last 500 steps = 2.6922e-01, PNorm = 158.7672, GNorm = 0.2383
Meta loss on this task batch = 2.5723e-01, Meta loss averaged over last 500 steps = 2.6917e-01, PNorm = 158.7725, GNorm = 0.3081
Meta loss on this task batch = 3.1683e-01, Meta loss averaged over last 500 steps = 2.6928e-01, PNorm = 158.7778, GNorm = 0.2567
Meta loss on this task batch = 2.1586e-01, Meta loss averaged over last 500 steps = 2.6917e-01, PNorm = 158.7831, GNorm = 0.2436
Meta loss on this task batch = 2.5267e-01, Meta loss averaged over last 500 steps = 2.6909e-01, PNorm = 158.7885, GNorm = 0.2757
Took 113.8248701095581 seconds to complete one epoch of meta training
Took 121.32575488090515 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472806
Epoch 705
Meta loss on this task batch = 3.1123e-01, Meta loss averaged over last 500 steps = 2.6911e-01, PNorm = 158.7946, GNorm = 0.2583
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.6908e-01, PNorm = 158.7996, GNorm = 0.2298
Meta loss on this task batch = 2.3514e-01, Meta loss averaged over last 500 steps = 2.6894e-01, PNorm = 158.8043, GNorm = 0.2259
Meta loss on this task batch = 2.6737e-01, Meta loss averaged over last 500 steps = 2.6891e-01, PNorm = 158.8082, GNorm = 0.2292
Meta loss on this task batch = 2.9696e-01, Meta loss averaged over last 500 steps = 2.6893e-01, PNorm = 158.8126, GNorm = 0.2493
Meta loss on this task batch = 2.6094e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 158.8163, GNorm = 0.2278
Meta loss on this task batch = 2.5779e-01, Meta loss averaged over last 500 steps = 2.6876e-01, PNorm = 158.8207, GNorm = 0.2343
Meta loss on this task batch = 2.3861e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 158.8256, GNorm = 0.2232
Meta loss on this task batch = 2.5159e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 158.8309, GNorm = 0.2546
Meta loss on this task batch = 2.8979e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 158.8364, GNorm = 0.2407
Meta loss on this task batch = 2.2713e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 158.8418, GNorm = 0.2741
Meta loss on this task batch = 2.6137e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 158.8475, GNorm = 0.2392
Meta loss on this task batch = 2.7168e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 158.8536, GNorm = 0.2135
Meta loss on this task batch = 3.0828e-01, Meta loss averaged over last 500 steps = 2.6873e-01, PNorm = 158.8596, GNorm = 0.2673
Meta loss on this task batch = 2.5208e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 158.8656, GNorm = 0.2286
Meta loss on this task batch = 2.8483e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 158.8713, GNorm = 0.2613
Meta loss on this task batch = 2.5408e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 158.8774, GNorm = 0.2165
Meta loss on this task batch = 2.8756e-01, Meta loss averaged over last 500 steps = 2.6883e-01, PNorm = 158.8824, GNorm = 0.2377
Meta loss on this task batch = 2.3939e-01, Meta loss averaged over last 500 steps = 2.6879e-01, PNorm = 158.8870, GNorm = 0.2439
Took 108.73884415626526 seconds to complete one epoch of meta training
Took 116.03556060791016 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.518490
Epoch 706
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 2.6878e-01, PNorm = 158.8911, GNorm = 0.2378
Meta loss on this task batch = 2.1704e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 158.8957, GNorm = 0.1897
Meta loss on this task batch = 2.8732e-01, Meta loss averaged over last 500 steps = 2.6865e-01, PNorm = 158.8981, GNorm = 0.3033
Meta loss on this task batch = 2.6750e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 158.9006, GNorm = 0.2161
Meta loss on this task batch = 2.4400e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 158.9039, GNorm = 0.2197
Meta loss on this task batch = 3.1396e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 158.9074, GNorm = 0.2887
Meta loss on this task batch = 2.9377e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 158.9114, GNorm = 0.2475
Meta loss on this task batch = 2.5266e-01, Meta loss averaged over last 500 steps = 2.6854e-01, PNorm = 158.9154, GNorm = 0.2246
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 158.9206, GNorm = 0.2528
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 158.9259, GNorm = 0.2269
Meta loss on this task batch = 2.6351e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 158.9313, GNorm = 0.1914
Meta loss on this task batch = 2.6960e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 158.9365, GNorm = 0.2316
Meta loss on this task batch = 3.2133e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 158.9418, GNorm = 0.2222
Meta loss on this task batch = 2.6682e-01, Meta loss averaged over last 500 steps = 2.6880e-01, PNorm = 158.9476, GNorm = 0.2404
Meta loss on this task batch = 2.2376e-01, Meta loss averaged over last 500 steps = 2.6870e-01, PNorm = 158.9536, GNorm = 0.2361
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 158.9602, GNorm = 0.2172
Meta loss on this task batch = 2.9726e-01, Meta loss averaged over last 500 steps = 2.6859e-01, PNorm = 158.9654, GNorm = 0.2391
Meta loss on this task batch = 2.2687e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 158.9702, GNorm = 0.2204
Meta loss on this task batch = 2.6817e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 158.9739, GNorm = 0.2806
Took 111.09991645812988 seconds to complete one epoch of meta training
Took 118.51688241958618 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488800
Epoch 707
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 2.6852e-01, PNorm = 158.9779, GNorm = 0.2228
Meta loss on this task batch = 2.7893e-01, Meta loss averaged over last 500 steps = 2.6855e-01, PNorm = 158.9819, GNorm = 0.2515
Meta loss on this task batch = 2.8870e-01, Meta loss averaged over last 500 steps = 2.6866e-01, PNorm = 158.9857, GNorm = 0.2594
Meta loss on this task batch = 3.0089e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 158.9894, GNorm = 0.2649
Meta loss on this task batch = 2.3225e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 158.9938, GNorm = 0.1899
Meta loss on this task batch = 2.8218e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 158.9984, GNorm = 0.2302
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 159.0027, GNorm = 0.2083
Meta loss on this task batch = 2.3905e-01, Meta loss averaged over last 500 steps = 2.6867e-01, PNorm = 159.0074, GNorm = 0.1972
Meta loss on this task batch = 2.3389e-01, Meta loss averaged over last 500 steps = 2.6861e-01, PNorm = 159.0133, GNorm = 0.1868
Meta loss on this task batch = 2.6669e-01, Meta loss averaged over last 500 steps = 2.6856e-01, PNorm = 159.0191, GNorm = 0.1966
Meta loss on this task batch = 2.3205e-01, Meta loss averaged over last 500 steps = 2.6850e-01, PNorm = 159.0238, GNorm = 0.2195
Meta loss on this task batch = 3.7185e-01, Meta loss averaged over last 500 steps = 2.6871e-01, PNorm = 159.0273, GNorm = 0.3469
Meta loss on this task batch = 2.1823e-01, Meta loss averaged over last 500 steps = 2.6860e-01, PNorm = 159.0312, GNorm = 0.1990
Meta loss on this task batch = 2.8257e-01, Meta loss averaged over last 500 steps = 2.6864e-01, PNorm = 159.0354, GNorm = 0.2685
Meta loss on this task batch = 2.4895e-01, Meta loss averaged over last 500 steps = 2.6864e-01, PNorm = 159.0399, GNorm = 0.2943
Meta loss on this task batch = 2.7139e-01, Meta loss averaged over last 500 steps = 2.6863e-01, PNorm = 159.0439, GNorm = 0.2195
Meta loss on this task batch = 2.3425e-01, Meta loss averaged over last 500 steps = 2.6849e-01, PNorm = 159.0484, GNorm = 0.2088
Meta loss on this task batch = 2.5575e-01, Meta loss averaged over last 500 steps = 2.6835e-01, PNorm = 159.0532, GNorm = 0.2290
Meta loss on this task batch = 2.3382e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 159.0574, GNorm = 0.3109
Took 110.7985827922821 seconds to complete one epoch of meta training
Took 117.41260027885437 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480945
Epoch 708
Meta loss on this task batch = 2.5158e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 159.0628, GNorm = 0.2292
Meta loss on this task batch = 2.4593e-01, Meta loss averaged over last 500 steps = 2.6822e-01, PNorm = 159.0690, GNorm = 0.2015
Meta loss on this task batch = 3.0539e-01, Meta loss averaged over last 500 steps = 2.6825e-01, PNorm = 159.0749, GNorm = 0.2410
Meta loss on this task batch = 2.2987e-01, Meta loss averaged over last 500 steps = 2.6820e-01, PNorm = 159.0802, GNorm = 0.2182
Meta loss on this task batch = 2.2497e-01, Meta loss averaged over last 500 steps = 2.6811e-01, PNorm = 159.0856, GNorm = 0.2031
Meta loss on this task batch = 2.3591e-01, Meta loss averaged over last 500 steps = 2.6812e-01, PNorm = 159.0906, GNorm = 0.2277
Meta loss on this task batch = 2.4351e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.0957, GNorm = 0.2521
Meta loss on this task batch = 2.6521e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.1002, GNorm = 0.2258
Meta loss on this task batch = 2.4761e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 159.1048, GNorm = 0.2562
Meta loss on this task batch = 3.4286e-01, Meta loss averaged over last 500 steps = 2.6842e-01, PNorm = 159.1098, GNorm = 0.2634
Meta loss on this task batch = 2.6609e-01, Meta loss averaged over last 500 steps = 2.6858e-01, PNorm = 159.1149, GNorm = 0.2437
Meta loss on this task batch = 3.1040e-01, Meta loss averaged over last 500 steps = 2.6877e-01, PNorm = 159.1196, GNorm = 0.2641
Meta loss on this task batch = 2.5726e-01, Meta loss averaged over last 500 steps = 2.6872e-01, PNorm = 159.1248, GNorm = 0.3166
Meta loss on this task batch = 2.9704e-01, Meta loss averaged over last 500 steps = 2.6874e-01, PNorm = 159.1312, GNorm = 0.3551
Meta loss on this task batch = 2.8910e-01, Meta loss averaged over last 500 steps = 2.6875e-01, PNorm = 159.1374, GNorm = 0.2414
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.6869e-01, PNorm = 159.1435, GNorm = 0.2421
Meta loss on this task batch = 2.1674e-01, Meta loss averaged over last 500 steps = 2.6847e-01, PNorm = 159.1501, GNorm = 0.2090
Meta loss on this task batch = 2.8172e-01, Meta loss averaged over last 500 steps = 2.6846e-01, PNorm = 159.1573, GNorm = 0.2191
Meta loss on this task batch = 2.6959e-01, Meta loss averaged over last 500 steps = 2.6845e-01, PNorm = 159.1640, GNorm = 0.2482
Took 109.77966332435608 seconds to complete one epoch of meta training
Took 117.22233080863953 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490842
Epoch 709
Meta loss on this task batch = 2.2276e-01, Meta loss averaged over last 500 steps = 2.6832e-01, PNorm = 159.1707, GNorm = 0.2086
Meta loss on this task batch = 3.0430e-01, Meta loss averaged over last 500 steps = 2.6825e-01, PNorm = 159.1772, GNorm = 0.2880
Meta loss on this task batch = 3.0595e-01, Meta loss averaged over last 500 steps = 2.6833e-01, PNorm = 159.1823, GNorm = 0.2955
Meta loss on this task batch = 1.8861e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 159.1876, GNorm = 0.1979
Meta loss on this task batch = 2.5281e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.1922, GNorm = 0.2691
Meta loss on this task batch = 2.5407e-01, Meta loss averaged over last 500 steps = 2.6802e-01, PNorm = 159.1966, GNorm = 0.2297
Meta loss on this task batch = 2.9517e-01, Meta loss averaged over last 500 steps = 2.6809e-01, PNorm = 159.2011, GNorm = 0.2125
Meta loss on this task batch = 2.8858e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 159.2062, GNorm = 0.2086
Meta loss on this task batch = 2.6948e-01, Meta loss averaged over last 500 steps = 2.6816e-01, PNorm = 159.2119, GNorm = 0.2236
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 159.2175, GNorm = 0.2157
Meta loss on this task batch = 2.5113e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.2233, GNorm = 0.3089
Meta loss on this task batch = 3.1766e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 159.2283, GNorm = 0.2179
Meta loss on this task batch = 2.5501e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 159.2334, GNorm = 0.2097
Meta loss on this task batch = 3.0010e-01, Meta loss averaged over last 500 steps = 2.6816e-01, PNorm = 159.2383, GNorm = 0.2288
Meta loss on this task batch = 2.6088e-01, Meta loss averaged over last 500 steps = 2.6812e-01, PNorm = 159.2435, GNorm = 0.2347
Meta loss on this task batch = 2.9654e-01, Meta loss averaged over last 500 steps = 2.6813e-01, PNorm = 159.2487, GNorm = 0.2373
Meta loss on this task batch = 3.3932e-01, Meta loss averaged over last 500 steps = 2.6824e-01, PNorm = 159.2541, GNorm = 0.2732
Meta loss on this task batch = 2.2141e-01, Meta loss averaged over last 500 steps = 2.6819e-01, PNorm = 159.2598, GNorm = 0.1902
Meta loss on this task batch = 2.4194e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.2658, GNorm = 0.2361
Took 106.58178305625916 seconds to complete one epoch of meta training
Took 114.55698561668396 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490436
Epoch 710
Meta loss on this task batch = 2.6139e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.2720, GNorm = 0.2275
Meta loss on this task batch = 2.5872e-01, Meta loss averaged over last 500 steps = 2.6800e-01, PNorm = 159.2776, GNorm = 0.2156
Meta loss on this task batch = 2.9882e-01, Meta loss averaged over last 500 steps = 2.6803e-01, PNorm = 159.2838, GNorm = 0.1982
Meta loss on this task batch = 2.8798e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.2898, GNorm = 0.2524
Meta loss on this task batch = 3.0674e-01, Meta loss averaged over last 500 steps = 2.6812e-01, PNorm = 159.2950, GNorm = 0.2537
Meta loss on this task batch = 3.0970e-01, Meta loss averaged over last 500 steps = 2.6822e-01, PNorm = 159.3007, GNorm = 0.2449
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 159.3065, GNorm = 0.2340
Meta loss on this task batch = 2.5305e-01, Meta loss averaged over last 500 steps = 2.6821e-01, PNorm = 159.3117, GNorm = 0.2400
Meta loss on this task batch = 2.3889e-01, Meta loss averaged over last 500 steps = 2.6819e-01, PNorm = 159.3171, GNorm = 0.1941
Meta loss on this task batch = 2.5987e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.3228, GNorm = 0.2455
Meta loss on this task batch = 2.6817e-01, Meta loss averaged over last 500 steps = 2.6809e-01, PNorm = 159.3289, GNorm = 0.2194
Meta loss on this task batch = 2.6268e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.3347, GNorm = 0.2303
Meta loss on this task batch = 2.8915e-01, Meta loss averaged over last 500 steps = 2.6811e-01, PNorm = 159.3396, GNorm = 0.2300
Meta loss on this task batch = 2.9279e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.3452, GNorm = 0.2333
Meta loss on this task batch = 2.6965e-01, Meta loss averaged over last 500 steps = 2.6796e-01, PNorm = 159.3507, GNorm = 0.1964
Meta loss on this task batch = 2.6166e-01, Meta loss averaged over last 500 steps = 2.6796e-01, PNorm = 159.3560, GNorm = 0.2184
Meta loss on this task batch = 2.5892e-01, Meta loss averaged over last 500 steps = 2.6795e-01, PNorm = 159.3614, GNorm = 0.1994
Meta loss on this task batch = 2.2073e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 159.3666, GNorm = 0.1887
Meta loss on this task batch = 2.4512e-01, Meta loss averaged over last 500 steps = 2.6781e-01, PNorm = 159.3709, GNorm = 0.2930
Took 111.35327053070068 seconds to complete one epoch of meta training
Took 117.71475100517273 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478968
Epoch 711
Meta loss on this task batch = 3.1375e-01, Meta loss averaged over last 500 steps = 2.6788e-01, PNorm = 159.3759, GNorm = 0.2423
Meta loss on this task batch = 2.2270e-01, Meta loss averaged over last 500 steps = 2.6781e-01, PNorm = 159.3793, GNorm = 0.2545
Meta loss on this task batch = 2.8385e-01, Meta loss averaged over last 500 steps = 2.6778e-01, PNorm = 159.3833, GNorm = 0.2163
Meta loss on this task batch = 3.4826e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 159.3873, GNorm = 0.2699
Meta loss on this task batch = 2.9420e-01, Meta loss averaged over last 500 steps = 2.6800e-01, PNorm = 159.3919, GNorm = 0.2674
Meta loss on this task batch = 3.4434e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 159.3954, GNorm = 0.2646
Meta loss on this task batch = 2.5368e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 159.3993, GNorm = 0.2214
Meta loss on this task batch = 2.6069e-01, Meta loss averaged over last 500 steps = 2.6825e-01, PNorm = 159.4031, GNorm = 0.2456
Meta loss on this task batch = 2.3686e-01, Meta loss averaged over last 500 steps = 2.6819e-01, PNorm = 159.4078, GNorm = 0.2108
Meta loss on this task batch = 2.4350e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 159.4131, GNorm = 0.2135
Meta loss on this task batch = 2.4865e-01, Meta loss averaged over last 500 steps = 2.6813e-01, PNorm = 159.4192, GNorm = 0.2467
Meta loss on this task batch = 2.4610e-01, Meta loss averaged over last 500 steps = 2.6818e-01, PNorm = 159.4250, GNorm = 0.2498
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 159.4307, GNorm = 0.2593
Meta loss on this task batch = 2.0836e-01, Meta loss averaged over last 500 steps = 2.6809e-01, PNorm = 159.4365, GNorm = 0.2035
Meta loss on this task batch = 2.7493e-01, Meta loss averaged over last 500 steps = 2.6804e-01, PNorm = 159.4406, GNorm = 0.2642
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 159.4443, GNorm = 0.2492
Meta loss on this task batch = 2.6057e-01, Meta loss averaged over last 500 steps = 2.6805e-01, PNorm = 159.4486, GNorm = 0.2217
Meta loss on this task batch = 2.6347e-01, Meta loss averaged over last 500 steps = 2.6804e-01, PNorm = 159.4529, GNorm = 0.2369
Meta loss on this task batch = 2.5329e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 159.4571, GNorm = 0.2736
Took 107.07268857955933 seconds to complete one epoch of meta training
Took 114.818279504776 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478619
Epoch 712
Meta loss on this task batch = 2.6955e-01, Meta loss averaged over last 500 steps = 2.6797e-01, PNorm = 159.4613, GNorm = 0.2199
Meta loss on this task batch = 3.1295e-01, Meta loss averaged over last 500 steps = 2.6807e-01, PNorm = 159.4648, GNorm = 0.2329
Meta loss on this task batch = 2.7851e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.4686, GNorm = 0.2462
Meta loss on this task batch = 2.4842e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 159.4731, GNorm = 0.2391
Meta loss on this task batch = 2.9804e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 159.4769, GNorm = 0.2673
Meta loss on this task batch = 3.0158e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.4810, GNorm = 0.2421
Meta loss on this task batch = 2.6218e-01, Meta loss averaged over last 500 steps = 2.6804e-01, PNorm = 159.4851, GNorm = 0.2230
Meta loss on this task batch = 2.4356e-01, Meta loss averaged over last 500 steps = 2.6803e-01, PNorm = 159.4900, GNorm = 0.1924
Meta loss on this task batch = 2.7178e-01, Meta loss averaged over last 500 steps = 2.6798e-01, PNorm = 159.4949, GNorm = 0.2241
Meta loss on this task batch = 2.8096e-01, Meta loss averaged over last 500 steps = 2.6806e-01, PNorm = 159.4999, GNorm = 0.2456
Meta loss on this task batch = 2.3785e-01, Meta loss averaged over last 500 steps = 2.6805e-01, PNorm = 159.5057, GNorm = 0.2206
Meta loss on this task batch = 2.8455e-01, Meta loss averaged over last 500 steps = 2.6814e-01, PNorm = 159.5112, GNorm = 0.2382
Meta loss on this task batch = 2.6135e-01, Meta loss averaged over last 500 steps = 2.6811e-01, PNorm = 159.5170, GNorm = 0.2061
Meta loss on this task batch = 2.8146e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 159.5223, GNorm = 0.2055
Meta loss on this task batch = 2.2561e-01, Meta loss averaged over last 500 steps = 2.6808e-01, PNorm = 159.5279, GNorm = 0.2281
Meta loss on this task batch = 2.4908e-01, Meta loss averaged over last 500 steps = 2.6810e-01, PNorm = 159.5332, GNorm = 0.2191
Meta loss on this task batch = 3.4136e-01, Meta loss averaged over last 500 steps = 2.6829e-01, PNorm = 159.5381, GNorm = 0.2421
Meta loss on this task batch = 2.2404e-01, Meta loss averaged over last 500 steps = 2.6828e-01, PNorm = 159.5426, GNorm = 0.2541
Meta loss on this task batch = 2.4152e-01, Meta loss averaged over last 500 steps = 2.6826e-01, PNorm = 159.5475, GNorm = 0.2678
Took 111.9367151260376 seconds to complete one epoch of meta training
Took 119.25140142440796 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496125
Epoch 713
Meta loss on this task batch = 2.5546e-01, Meta loss averaged over last 500 steps = 2.6823e-01, PNorm = 159.5521, GNorm = 0.2384
Meta loss on this task batch = 2.6401e-01, Meta loss averaged over last 500 steps = 2.6817e-01, PNorm = 159.5571, GNorm = 0.2288
Meta loss on this task batch = 2.3549e-01, Meta loss averaged over last 500 steps = 2.6807e-01, PNorm = 159.5620, GNorm = 0.1788
Meta loss on this task batch = 2.8566e-01, Meta loss averaged over last 500 steps = 2.6807e-01, PNorm = 159.5673, GNorm = 0.2273
Meta loss on this task batch = 2.4146e-01, Meta loss averaged over last 500 steps = 2.6799e-01, PNorm = 159.5720, GNorm = 0.2121
Meta loss on this task batch = 2.0713e-01, Meta loss averaged over last 500 steps = 2.6780e-01, PNorm = 159.5773, GNorm = 0.1958
Meta loss on this task batch = 2.4388e-01, Meta loss averaged over last 500 steps = 2.6766e-01, PNorm = 159.5821, GNorm = 0.2146
Meta loss on this task batch = 2.5184e-01, Meta loss averaged over last 500 steps = 2.6769e-01, PNorm = 159.5869, GNorm = 0.2267
Meta loss on this task batch = 3.3357e-01, Meta loss averaged over last 500 steps = 2.6782e-01, PNorm = 159.5922, GNorm = 0.2608
Meta loss on this task batch = 2.6566e-01, Meta loss averaged over last 500 steps = 2.6792e-01, PNorm = 159.5983, GNorm = 0.2567
Meta loss on this task batch = 3.0414e-01, Meta loss averaged over last 500 steps = 2.6789e-01, PNorm = 159.6045, GNorm = 0.2420
Meta loss on this task batch = 2.2489e-01, Meta loss averaged over last 500 steps = 2.6780e-01, PNorm = 159.6104, GNorm = 0.2207
Meta loss on this task batch = 2.5602e-01, Meta loss averaged over last 500 steps = 2.6781e-01, PNorm = 159.6172, GNorm = 0.2227
Meta loss on this task batch = 2.5230e-01, Meta loss averaged over last 500 steps = 2.6781e-01, PNorm = 159.6245, GNorm = 0.2439
Meta loss on this task batch = 2.3860e-01, Meta loss averaged over last 500 steps = 2.6776e-01, PNorm = 159.6317, GNorm = 0.2399
Meta loss on this task batch = 2.7030e-01, Meta loss averaged over last 500 steps = 2.6771e-01, PNorm = 159.6391, GNorm = 0.2369
Meta loss on this task batch = 2.6327e-01, Meta loss averaged over last 500 steps = 2.6774e-01, PNorm = 159.6465, GNorm = 0.2583
Meta loss on this task batch = 2.9505e-01, Meta loss averaged over last 500 steps = 2.6781e-01, PNorm = 159.6537, GNorm = 0.2411
Meta loss on this task batch = 3.2180e-01, Meta loss averaged over last 500 steps = 2.6801e-01, PNorm = 159.6597, GNorm = 0.3049
Took 112.98167681694031 seconds to complete one epoch of meta training
Took 120.75790333747864 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473285
Epoch 714
Meta loss on this task batch = 2.8152e-01, Meta loss averaged over last 500 steps = 2.6797e-01, PNorm = 159.6644, GNorm = 0.2547
Meta loss on this task batch = 2.6476e-01, Meta loss averaged over last 500 steps = 2.6799e-01, PNorm = 159.6680, GNorm = 0.2293
Meta loss on this task batch = 2.5721e-01, Meta loss averaged over last 500 steps = 2.6794e-01, PNorm = 159.6709, GNorm = 0.2400
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.6790e-01, PNorm = 159.6743, GNorm = 0.2114
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 2.6790e-01, PNorm = 159.6783, GNorm = 0.2072
Meta loss on this task batch = 2.4424e-01, Meta loss averaged over last 500 steps = 2.6781e-01, PNorm = 159.6822, GNorm = 0.1994
Meta loss on this task batch = 3.3915e-01, Meta loss averaged over last 500 steps = 2.6797e-01, PNorm = 159.6852, GNorm = 0.2587
Meta loss on this task batch = 2.6762e-01, Meta loss averaged over last 500 steps = 2.6802e-01, PNorm = 159.6890, GNorm = 0.2384
Meta loss on this task batch = 2.7662e-01, Meta loss averaged over last 500 steps = 2.6796e-01, PNorm = 159.6933, GNorm = 0.2185
Meta loss on this task batch = 2.4101e-01, Meta loss averaged over last 500 steps = 2.6794e-01, PNorm = 159.6990, GNorm = 0.2100
Meta loss on this task batch = 2.5417e-01, Meta loss averaged over last 500 steps = 2.6799e-01, PNorm = 159.7044, GNorm = 0.2148
Meta loss on this task batch = 2.2790e-01, Meta loss averaged over last 500 steps = 2.6786e-01, PNorm = 159.7103, GNorm = 0.1889
Meta loss on this task batch = 2.3964e-01, Meta loss averaged over last 500 steps = 2.6772e-01, PNorm = 159.7165, GNorm = 0.2195
Meta loss on this task batch = 2.4377e-01, Meta loss averaged over last 500 steps = 2.6765e-01, PNorm = 159.7232, GNorm = 0.2200
Meta loss on this task batch = 2.6484e-01, Meta loss averaged over last 500 steps = 2.6769e-01, PNorm = 159.7297, GNorm = 0.1967
Meta loss on this task batch = 2.9615e-01, Meta loss averaged over last 500 steps = 2.6778e-01, PNorm = 159.7351, GNorm = 0.2502
Meta loss on this task batch = 2.6994e-01, Meta loss averaged over last 500 steps = 2.6774e-01, PNorm = 159.7403, GNorm = 0.2669
Meta loss on this task batch = 2.5712e-01, Meta loss averaged over last 500 steps = 2.6773e-01, PNorm = 159.7450, GNorm = 0.2544
Meta loss on this task batch = 2.8142e-01, Meta loss averaged over last 500 steps = 2.6771e-01, PNorm = 159.7495, GNorm = 0.2681
Took 110.6534526348114 seconds to complete one epoch of meta training
Took 118.44623279571533 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471119
Epoch 715
Meta loss on this task batch = 2.5410e-01, Meta loss averaged over last 500 steps = 2.6764e-01, PNorm = 159.7539, GNorm = 0.2048
Meta loss on this task batch = 3.0029e-01, Meta loss averaged over last 500 steps = 2.6767e-01, PNorm = 159.7571, GNorm = 0.2415
Meta loss on this task batch = 2.5061e-01, Meta loss averaged over last 500 steps = 2.6768e-01, PNorm = 159.7605, GNorm = 0.2045
Meta loss on this task batch = 2.8778e-01, Meta loss averaged over last 500 steps = 2.6762e-01, PNorm = 159.7640, GNorm = 0.2478
Meta loss on this task batch = 2.4468e-01, Meta loss averaged over last 500 steps = 2.6763e-01, PNorm = 159.7669, GNorm = 0.2164
Meta loss on this task batch = 2.6803e-01, Meta loss averaged over last 500 steps = 2.6760e-01, PNorm = 159.7704, GNorm = 0.2116
Meta loss on this task batch = 2.3109e-01, Meta loss averaged over last 500 steps = 2.6749e-01, PNorm = 159.7746, GNorm = 0.1880
Meta loss on this task batch = 2.4874e-01, Meta loss averaged over last 500 steps = 2.6750e-01, PNorm = 159.7789, GNorm = 0.2274
Meta loss on this task batch = 2.0194e-01, Meta loss averaged over last 500 steps = 2.6746e-01, PNorm = 159.7840, GNorm = 0.2041
Meta loss on this task batch = 2.6890e-01, Meta loss averaged over last 500 steps = 2.6748e-01, PNorm = 159.7889, GNorm = 0.2039
Meta loss on this task batch = 2.8115e-01, Meta loss averaged over last 500 steps = 2.6746e-01, PNorm = 159.7943, GNorm = 0.2109
Meta loss on this task batch = 2.7968e-01, Meta loss averaged over last 500 steps = 2.6740e-01, PNorm = 159.7981, GNorm = 0.2792
Meta loss on this task batch = 2.5769e-01, Meta loss averaged over last 500 steps = 2.6740e-01, PNorm = 159.8025, GNorm = 0.2571
Meta loss on this task batch = 2.8350e-01, Meta loss averaged over last 500 steps = 2.6753e-01, PNorm = 159.8064, GNorm = 0.2401
Meta loss on this task batch = 2.5309e-01, Meta loss averaged over last 500 steps = 2.6746e-01, PNorm = 159.8102, GNorm = 0.2335
Meta loss on this task batch = 3.0293e-01, Meta loss averaged over last 500 steps = 2.6749e-01, PNorm = 159.8142, GNorm = 0.2705
Meta loss on this task batch = 2.6903e-01, Meta loss averaged over last 500 steps = 2.6745e-01, PNorm = 159.8183, GNorm = 0.2260
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.6738e-01, PNorm = 159.8227, GNorm = 0.2382
Meta loss on this task batch = 3.4720e-01, Meta loss averaged over last 500 steps = 2.6764e-01, PNorm = 159.8265, GNorm = 0.3284
Took 109.39169883728027 seconds to complete one epoch of meta training
Took 116.9761312007904 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475758
Epoch 716
Meta loss on this task batch = 2.3730e-01, Meta loss averaged over last 500 steps = 2.6756e-01, PNorm = 159.8313, GNorm = 0.2288
Meta loss on this task batch = 2.6560e-01, Meta loss averaged over last 500 steps = 2.6751e-01, PNorm = 159.8356, GNorm = 0.2600
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.6748e-01, PNorm = 159.8391, GNorm = 0.2214
Meta loss on this task batch = 2.3693e-01, Meta loss averaged over last 500 steps = 2.6754e-01, PNorm = 159.8436, GNorm = 0.2671
Meta loss on this task batch = 2.4209e-01, Meta loss averaged over last 500 steps = 2.6741e-01, PNorm = 159.8486, GNorm = 0.2006
Meta loss on this task batch = 2.6986e-01, Meta loss averaged over last 500 steps = 2.6734e-01, PNorm = 159.8532, GNorm = 0.2128
Meta loss on this task batch = 2.6108e-01, Meta loss averaged over last 500 steps = 2.6732e-01, PNorm = 159.8580, GNorm = 0.2137
Meta loss on this task batch = 2.4741e-01, Meta loss averaged over last 500 steps = 2.6727e-01, PNorm = 159.8622, GNorm = 0.2105
Meta loss on this task batch = 2.2582e-01, Meta loss averaged over last 500 steps = 2.6716e-01, PNorm = 159.8669, GNorm = 0.1844
Meta loss on this task batch = 3.1105e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 159.8712, GNorm = 0.2213
Meta loss on this task batch = 2.9603e-01, Meta loss averaged over last 500 steps = 2.6722e-01, PNorm = 159.8757, GNorm = 0.2199
Meta loss on this task batch = 2.2416e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 159.8801, GNorm = 0.2796
Meta loss on this task batch = 2.2826e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 159.8850, GNorm = 0.1938
Meta loss on this task batch = 2.5637e-01, Meta loss averaged over last 500 steps = 2.6695e-01, PNorm = 159.8902, GNorm = 0.2363
Meta loss on this task batch = 3.1066e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 159.8961, GNorm = 0.2390
Meta loss on this task batch = 2.6390e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 159.9022, GNorm = 0.2422
Meta loss on this task batch = 2.6637e-01, Meta loss averaged over last 500 steps = 2.6705e-01, PNorm = 159.9090, GNorm = 0.2386
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.6708e-01, PNorm = 159.9151, GNorm = 0.2342
Meta loss on this task batch = 3.1890e-01, Meta loss averaged over last 500 steps = 2.6717e-01, PNorm = 159.9196, GNorm = 0.4342
Took 113.40661096572876 seconds to complete one epoch of meta training
Took 121.22668647766113 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.467626
Epoch 717
Meta loss on this task batch = 2.1648e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 159.9249, GNorm = 0.2181
Meta loss on this task batch = 2.5833e-01, Meta loss averaged over last 500 steps = 2.6707e-01, PNorm = 159.9305, GNorm = 0.2527
Meta loss on this task batch = 2.4728e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 159.9357, GNorm = 0.2657
Meta loss on this task batch = 2.8832e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 159.9405, GNorm = 0.2318
Meta loss on this task batch = 2.0131e-01, Meta loss averaged over last 500 steps = 2.6692e-01, PNorm = 159.9460, GNorm = 0.1952
Meta loss on this task batch = 2.6649e-01, Meta loss averaged over last 500 steps = 2.6695e-01, PNorm = 159.9510, GNorm = 0.2141
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.6688e-01, PNorm = 159.9557, GNorm = 0.2274
Meta loss on this task batch = 3.2215e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 159.9605, GNorm = 0.2562
Meta loss on this task batch = 3.0273e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 159.9651, GNorm = 0.2133
Meta loss on this task batch = 1.9430e-01, Meta loss averaged over last 500 steps = 2.6688e-01, PNorm = 159.9701, GNorm = 0.1824
Meta loss on this task batch = 2.6013e-01, Meta loss averaged over last 500 steps = 2.6690e-01, PNorm = 159.9751, GNorm = 0.2343
Meta loss on this task batch = 3.0241e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 159.9797, GNorm = 0.2616
Meta loss on this task batch = 2.6553e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 159.9841, GNorm = 0.2192
Meta loss on this task batch = 2.5776e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 159.9889, GNorm = 0.2269
Meta loss on this task batch = 2.8525e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 159.9932, GNorm = 0.2450
Meta loss on this task batch = 3.1251e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 159.9976, GNorm = 0.2504
Meta loss on this task batch = 2.6945e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 160.0015, GNorm = 0.2336
Meta loss on this task batch = 2.4401e-01, Meta loss averaged over last 500 steps = 2.6708e-01, PNorm = 160.0059, GNorm = 0.2074
Meta loss on this task batch = 2.2249e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 160.0118, GNorm = 0.2854
Took 109.10464382171631 seconds to complete one epoch of meta training
Took 117.25487899780273 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496350
Epoch 718
Meta loss on this task batch = 2.3751e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 160.0171, GNorm = 0.2330
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.6684e-01, PNorm = 160.0228, GNorm = 0.2332
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 160.0280, GNorm = 0.2480
Meta loss on this task batch = 2.9516e-01, Meta loss averaged over last 500 steps = 2.6702e-01, PNorm = 160.0334, GNorm = 0.2602
Meta loss on this task batch = 2.5929e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 160.0379, GNorm = 0.2309
Meta loss on this task batch = 3.1160e-01, Meta loss averaged over last 500 steps = 2.6711e-01, PNorm = 160.0417, GNorm = 0.2520
Meta loss on this task batch = 2.7583e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 160.0436, GNorm = 0.2510
Meta loss on this task batch = 2.3639e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 160.0457, GNorm = 0.2190
Meta loss on this task batch = 2.9190e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 160.0484, GNorm = 0.2597
Meta loss on this task batch = 3.3104e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 160.0501, GNorm = 0.2782
Meta loss on this task batch = 2.4011e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 160.0521, GNorm = 0.2262
Meta loss on this task batch = 2.3805e-01, Meta loss averaged over last 500 steps = 2.6719e-01, PNorm = 160.0556, GNorm = 0.2073
Meta loss on this task batch = 2.4374e-01, Meta loss averaged over last 500 steps = 2.6716e-01, PNorm = 160.0597, GNorm = 0.2050
Meta loss on this task batch = 2.6955e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 160.0647, GNorm = 0.2187
Meta loss on this task batch = 2.5028e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 160.0703, GNorm = 0.2175
Meta loss on this task batch = 2.7983e-01, Meta loss averaged over last 500 steps = 2.6716e-01, PNorm = 160.0767, GNorm = 0.2581
Meta loss on this task batch = 2.5627e-01, Meta loss averaged over last 500 steps = 2.6713e-01, PNorm = 160.0837, GNorm = 0.2465
Meta loss on this task batch = 2.5925e-01, Meta loss averaged over last 500 steps = 2.6717e-01, PNorm = 160.0914, GNorm = 0.2016
Meta loss on this task batch = 2.6062e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 160.0987, GNorm = 0.2730
Took 107.02136492729187 seconds to complete one epoch of meta training
Took 113.99305820465088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475881
Epoch 719
Meta loss on this task batch = 2.4651e-01, Meta loss averaged over last 500 steps = 2.6710e-01, PNorm = 160.1063, GNorm = 0.2007
Meta loss on this task batch = 2.2283e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 160.1144, GNorm = 0.2080
Meta loss on this task batch = 2.7141e-01, Meta loss averaged over last 500 steps = 2.6690e-01, PNorm = 160.1225, GNorm = 0.2426
Meta loss on this task batch = 2.9456e-01, Meta loss averaged over last 500 steps = 2.6682e-01, PNorm = 160.1306, GNorm = 0.2119
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.6692e-01, PNorm = 160.1387, GNorm = 0.2291
Meta loss on this task batch = 3.2136e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 160.1447, GNorm = 0.3315
Meta loss on this task batch = 2.2809e-01, Meta loss averaged over last 500 steps = 2.6702e-01, PNorm = 160.1501, GNorm = 0.2256
Meta loss on this task batch = 2.6352e-01, Meta loss averaged over last 500 steps = 2.6697e-01, PNorm = 160.1551, GNorm = 0.2282
Meta loss on this task batch = 2.2873e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 160.1592, GNorm = 0.1919
Meta loss on this task batch = 2.4155e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 160.1639, GNorm = 0.2899
Meta loss on this task batch = 2.4549e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 160.1687, GNorm = 0.2318
Meta loss on this task batch = 2.5568e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 160.1737, GNorm = 0.2103
Meta loss on this task batch = 3.2132e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 160.1796, GNorm = 0.2727
Meta loss on this task batch = 2.3506e-01, Meta loss averaged over last 500 steps = 2.6697e-01, PNorm = 160.1857, GNorm = 0.2228
Meta loss on this task batch = 2.5091e-01, Meta loss averaged over last 500 steps = 2.6686e-01, PNorm = 160.1919, GNorm = 0.2177
Meta loss on this task batch = 2.6168e-01, Meta loss averaged over last 500 steps = 2.6686e-01, PNorm = 160.1983, GNorm = 0.2432
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 160.2047, GNorm = 0.2188
Meta loss on this task batch = 3.2982e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 160.2106, GNorm = 0.3336
Meta loss on this task batch = 2.9153e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 160.2158, GNorm = 0.3020
Took 107.29096984863281 seconds to complete one epoch of meta training
Took 114.63001585006714 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473638
Epoch 720
Meta loss on this task batch = 2.3718e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 160.2215, GNorm = 0.2229
Meta loss on this task batch = 3.0322e-01, Meta loss averaged over last 500 steps = 2.6719e-01, PNorm = 160.2263, GNorm = 0.2742
Meta loss on this task batch = 2.4824e-01, Meta loss averaged over last 500 steps = 2.6708e-01, PNorm = 160.2318, GNorm = 0.2572
Meta loss on this task batch = 2.3441e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 160.2371, GNorm = 0.1999
Meta loss on this task batch = 2.4068e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 160.2418, GNorm = 0.2095
Meta loss on this task batch = 2.2407e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 160.2468, GNorm = 0.2254
Meta loss on this task batch = 2.3188e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 160.2509, GNorm = 0.2553
Meta loss on this task batch = 2.7609e-01, Meta loss averaged over last 500 steps = 2.6690e-01, PNorm = 160.2548, GNorm = 0.2434
Meta loss on this task batch = 3.5315e-01, Meta loss averaged over last 500 steps = 2.6720e-01, PNorm = 160.2583, GNorm = 0.2805
Meta loss on this task batch = 2.3930e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 160.2623, GNorm = 0.1876
Meta loss on this task batch = 2.4989e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 160.2672, GNorm = 0.2188
Meta loss on this task batch = 2.5679e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 160.2728, GNorm = 0.2249
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 160.2783, GNorm = 0.2205
Meta loss on this task batch = 2.9917e-01, Meta loss averaged over last 500 steps = 2.6691e-01, PNorm = 160.2836, GNorm = 0.2128
Meta loss on this task batch = 2.3640e-01, Meta loss averaged over last 500 steps = 2.6694e-01, PNorm = 160.2892, GNorm = 0.2289
Meta loss on this task batch = 3.3143e-01, Meta loss averaged over last 500 steps = 2.6707e-01, PNorm = 160.2945, GNorm = 0.2665
Meta loss on this task batch = 3.1948e-01, Meta loss averaged over last 500 steps = 2.6725e-01, PNorm = 160.3004, GNorm = 0.2799
Meta loss on this task batch = 3.0847e-01, Meta loss averaged over last 500 steps = 2.6727e-01, PNorm = 160.3061, GNorm = 0.2194
Meta loss on this task batch = 2.8846e-01, Meta loss averaged over last 500 steps = 2.6739e-01, PNorm = 160.3103, GNorm = 0.3292
Took 112.01873016357422 seconds to complete one epoch of meta training
Took 120.53443813323975 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464128
Epoch 721
Meta loss on this task batch = 2.4203e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 160.3155, GNorm = 0.1909
Meta loss on this task batch = 2.5516e-01, Meta loss averaged over last 500 steps = 2.6725e-01, PNorm = 160.3205, GNorm = 0.2242
Meta loss on this task batch = 2.7709e-01, Meta loss averaged over last 500 steps = 2.6724e-01, PNorm = 160.3253, GNorm = 0.2502
Meta loss on this task batch = 2.7863e-01, Meta loss averaged over last 500 steps = 2.6726e-01, PNorm = 160.3298, GNorm = 0.2140
Meta loss on this task batch = 2.1550e-01, Meta loss averaged over last 500 steps = 2.6718e-01, PNorm = 160.3342, GNorm = 0.2317
Meta loss on this task batch = 2.8256e-01, Meta loss averaged over last 500 steps = 2.6731e-01, PNorm = 160.3388, GNorm = 0.2672
Meta loss on this task batch = 2.4037e-01, Meta loss averaged over last 500 steps = 2.6717e-01, PNorm = 160.3430, GNorm = 0.2080
Meta loss on this task batch = 2.2383e-01, Meta loss averaged over last 500 steps = 2.6713e-01, PNorm = 160.3463, GNorm = 0.2210
Meta loss on this task batch = 2.1431e-01, Meta loss averaged over last 500 steps = 2.6696e-01, PNorm = 160.3503, GNorm = 0.2489
Meta loss on this task batch = 2.5146e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 160.3534, GNorm = 0.2482
Meta loss on this task batch = 2.4939e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 160.3567, GNorm = 0.2357
Meta loss on this task batch = 2.3221e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 160.3606, GNorm = 0.2063
Meta loss on this task batch = 3.3964e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 160.3643, GNorm = 0.2650
Meta loss on this task batch = 3.2180e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 160.3682, GNorm = 0.2336
Meta loss on this task batch = 2.5701e-01, Meta loss averaged over last 500 steps = 2.6703e-01, PNorm = 160.3725, GNorm = 0.2108
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 160.3774, GNorm = 0.2770
Meta loss on this task batch = 2.9112e-01, Meta loss averaged over last 500 steps = 2.6711e-01, PNorm = 160.3822, GNorm = 0.2391
Meta loss on this task batch = 2.9672e-01, Meta loss averaged over last 500 steps = 2.6707e-01, PNorm = 160.3876, GNorm = 0.2515
Meta loss on this task batch = 2.6123e-01, Meta loss averaged over last 500 steps = 2.6706e-01, PNorm = 160.3930, GNorm = 0.9374
Took 116.6353542804718 seconds to complete one epoch of meta training
Took 124.7392680644989 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486968
Epoch 722
Meta loss on this task batch = 2.3384e-01, Meta loss averaged over last 500 steps = 2.6697e-01, PNorm = 160.3986, GNorm = 0.2343
Meta loss on this task batch = 2.3889e-01, Meta loss averaged over last 500 steps = 2.6686e-01, PNorm = 160.4059, GNorm = 0.2503
Meta loss on this task batch = 3.1733e-01, Meta loss averaged over last 500 steps = 2.6697e-01, PNorm = 160.4118, GNorm = 0.2692
Meta loss on this task batch = 2.4496e-01, Meta loss averaged over last 500 steps = 2.6681e-01, PNorm = 160.4186, GNorm = 0.2138
Meta loss on this task batch = 3.0572e-01, Meta loss averaged over last 500 steps = 2.6695e-01, PNorm = 160.4256, GNorm = 0.2529
Meta loss on this task batch = 2.6081e-01, Meta loss averaged over last 500 steps = 2.6692e-01, PNorm = 160.4331, GNorm = 0.2137
Meta loss on this task batch = 2.6077e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 160.4408, GNorm = 0.2195
Meta loss on this task batch = 3.0412e-01, Meta loss averaged over last 500 steps = 2.6685e-01, PNorm = 160.4470, GNorm = 0.2583
Meta loss on this task batch = 2.9400e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 160.4511, GNorm = 0.2814
Meta loss on this task batch = 2.9883e-01, Meta loss averaged over last 500 steps = 2.6688e-01, PNorm = 160.4546, GNorm = 0.2691
Meta loss on this task batch = 2.8347e-01, Meta loss averaged over last 500 steps = 2.6685e-01, PNorm = 160.4577, GNorm = 0.2537
Meta loss on this task batch = 2.7234e-01, Meta loss averaged over last 500 steps = 2.6683e-01, PNorm = 160.4605, GNorm = 0.2773
Meta loss on this task batch = 2.5435e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 160.4635, GNorm = 0.1764
Meta loss on this task batch = 2.4635e-01, Meta loss averaged over last 500 steps = 2.6692e-01, PNorm = 160.4670, GNorm = 0.2311
Meta loss on this task batch = 2.7906e-01, Meta loss averaged over last 500 steps = 2.6685e-01, PNorm = 160.4712, GNorm = 0.2256
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 2.6693e-01, PNorm = 160.4763, GNorm = 0.2113
Meta loss on this task batch = 2.9523e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 160.4811, GNorm = 0.2260
Meta loss on this task batch = 2.1765e-01, Meta loss averaged over last 500 steps = 2.6678e-01, PNorm = 160.4866, GNorm = 0.1828
Meta loss on this task batch = 2.3755e-01, Meta loss averaged over last 500 steps = 2.6677e-01, PNorm = 160.4928, GNorm = 0.2388
Took 106.65571904182434 seconds to complete one epoch of meta training
Took 114.35042524337769 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472710
Epoch 723
Meta loss on this task batch = 2.5662e-01, Meta loss averaged over last 500 steps = 2.6668e-01, PNorm = 160.4992, GNorm = 0.2337
Meta loss on this task batch = 3.4275e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 160.5050, GNorm = 0.2550
Meta loss on this task batch = 2.9926e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 160.5106, GNorm = 0.2301
Meta loss on this task batch = 2.7280e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 160.5159, GNorm = 0.2248
Meta loss on this task batch = 2.6086e-01, Meta loss averaged over last 500 steps = 2.6705e-01, PNorm = 160.5213, GNorm = 0.2295
Meta loss on this task batch = 2.4709e-01, Meta loss averaged over last 500 steps = 2.6713e-01, PNorm = 160.5268, GNorm = 0.2392
Meta loss on this task batch = 3.0208e-01, Meta loss averaged over last 500 steps = 2.6718e-01, PNorm = 160.5325, GNorm = 0.2926
Meta loss on this task batch = 2.7137e-01, Meta loss averaged over last 500 steps = 2.6719e-01, PNorm = 160.5378, GNorm = 0.2232
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.6716e-01, PNorm = 160.5428, GNorm = 0.2420
Meta loss on this task batch = 2.3891e-01, Meta loss averaged over last 500 steps = 2.6724e-01, PNorm = 160.5481, GNorm = 0.2665
Meta loss on this task batch = 3.4039e-01, Meta loss averaged over last 500 steps = 2.6739e-01, PNorm = 160.5522, GNorm = 0.3017
Meta loss on this task batch = 2.4736e-01, Meta loss averaged over last 500 steps = 2.6731e-01, PNorm = 160.5570, GNorm = 0.2111
Meta loss on this task batch = 2.6548e-01, Meta loss averaged over last 500 steps = 2.6730e-01, PNorm = 160.5624, GNorm = 0.2310
Meta loss on this task batch = 2.6761e-01, Meta loss averaged over last 500 steps = 2.6734e-01, PNorm = 160.5672, GNorm = 0.2552
Meta loss on this task batch = 2.8231e-01, Meta loss averaged over last 500 steps = 2.6752e-01, PNorm = 160.5710, GNorm = 0.2534
Meta loss on this task batch = 2.8374e-01, Meta loss averaged over last 500 steps = 2.6748e-01, PNorm = 160.5760, GNorm = 0.2422
Meta loss on this task batch = 2.6795e-01, Meta loss averaged over last 500 steps = 2.6749e-01, PNorm = 160.5812, GNorm = 0.2299
Meta loss on this task batch = 2.6625e-01, Meta loss averaged over last 500 steps = 2.6735e-01, PNorm = 160.5869, GNorm = 0.2031
Meta loss on this task batch = 1.8371e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 160.5929, GNorm = 0.1969
Took 116.16641974449158 seconds to complete one epoch of meta training
Took 123.77497911453247 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465209
Epoch 724
Meta loss on this task batch = 2.8390e-01, Meta loss averaged over last 500 steps = 2.6723e-01, PNorm = 160.5992, GNorm = 0.2202
Meta loss on this task batch = 2.8048e-01, Meta loss averaged over last 500 steps = 2.6729e-01, PNorm = 160.6052, GNorm = 0.2360
Meta loss on this task batch = 3.4632e-01, Meta loss averaged over last 500 steps = 2.6752e-01, PNorm = 160.6103, GNorm = 0.2797
Meta loss on this task batch = 2.9163e-01, Meta loss averaged over last 500 steps = 2.6746e-01, PNorm = 160.6140, GNorm = 0.2593
Meta loss on this task batch = 2.1417e-01, Meta loss averaged over last 500 steps = 2.6727e-01, PNorm = 160.6184, GNorm = 0.2085
Meta loss on this task batch = 2.4016e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 160.6220, GNorm = 0.2221
Meta loss on this task batch = 2.3966e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 160.6260, GNorm = 0.2055
Meta loss on this task batch = 2.8353e-01, Meta loss averaged over last 500 steps = 2.6715e-01, PNorm = 160.6294, GNorm = 0.2617
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 2.6719e-01, PNorm = 160.6326, GNorm = 0.2289
Meta loss on this task batch = 2.4664e-01, Meta loss averaged over last 500 steps = 2.6707e-01, PNorm = 160.6363, GNorm = 0.2248
Meta loss on this task batch = 2.2527e-01, Meta loss averaged over last 500 steps = 2.6689e-01, PNorm = 160.6405, GNorm = 0.2460
Meta loss on this task batch = 2.4439e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 160.6446, GNorm = 0.2392
Meta loss on this task batch = 2.8179e-01, Meta loss averaged over last 500 steps = 2.6704e-01, PNorm = 160.6494, GNorm = 0.2365
Meta loss on this task batch = 2.7339e-01, Meta loss averaged over last 500 steps = 2.6701e-01, PNorm = 160.6546, GNorm = 0.1998
Meta loss on this task batch = 2.4573e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 160.6595, GNorm = 0.2033
Meta loss on this task batch = 2.7355e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 160.6649, GNorm = 0.2185
Meta loss on this task batch = 2.5247e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 160.6702, GNorm = 0.2367
Meta loss on this task batch = 2.2012e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 160.6753, GNorm = 0.2063
Meta loss on this task batch = 3.4953e-01, Meta loss averaged over last 500 steps = 2.6668e-01, PNorm = 160.6790, GNorm = 0.3633
Took 109.36044907569885 seconds to complete one epoch of meta training
Took 116.16672396659851 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491220
Epoch 725
Meta loss on this task batch = 2.8506e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 160.6832, GNorm = 0.2407
Meta loss on this task batch = 2.9085e-01, Meta loss averaged over last 500 steps = 2.6684e-01, PNorm = 160.6879, GNorm = 0.2583
Meta loss on this task batch = 3.2378e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 160.6922, GNorm = 0.2805
Meta loss on this task batch = 2.4558e-01, Meta loss averaged over last 500 steps = 2.6704e-01, PNorm = 160.6969, GNorm = 0.1945
Meta loss on this task batch = 2.9874e-01, Meta loss averaged over last 500 steps = 2.6713e-01, PNorm = 160.7009, GNorm = 0.2796
Meta loss on this task batch = 3.0744e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 160.7048, GNorm = 0.2426
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.6709e-01, PNorm = 160.7090, GNorm = 0.2229
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.6715e-01, PNorm = 160.7136, GNorm = 0.1971
Meta loss on this task batch = 3.1163e-01, Meta loss averaged over last 500 steps = 2.6728e-01, PNorm = 160.7184, GNorm = 0.2109
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.6712e-01, PNorm = 160.7233, GNorm = 0.1974
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 2.6714e-01, PNorm = 160.7284, GNorm = 0.2227
Meta loss on this task batch = 2.5902e-01, Meta loss averaged over last 500 steps = 2.6717e-01, PNorm = 160.7327, GNorm = 0.2523
Meta loss on this task batch = 2.2240e-01, Meta loss averaged over last 500 steps = 2.6700e-01, PNorm = 160.7371, GNorm = 0.2328
Meta loss on this task batch = 2.5238e-01, Meta loss averaged over last 500 steps = 2.6699e-01, PNorm = 160.7420, GNorm = 0.2405
Meta loss on this task batch = 2.4693e-01, Meta loss averaged over last 500 steps = 2.6691e-01, PNorm = 160.7473, GNorm = 0.2630
Meta loss on this task batch = 2.4987e-01, Meta loss averaged over last 500 steps = 2.6686e-01, PNorm = 160.7532, GNorm = 0.2242
Meta loss on this task batch = 2.1892e-01, Meta loss averaged over last 500 steps = 2.6680e-01, PNorm = 160.7581, GNorm = 0.2353
Meta loss on this task batch = 1.9862e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 160.7626, GNorm = 0.2147
Meta loss on this task batch = 2.4871e-01, Meta loss averaged over last 500 steps = 2.6677e-01, PNorm = 160.7669, GNorm = 0.2733
Took 108.15134024620056 seconds to complete one epoch of meta training
Took 116.11262273788452 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493382
Epoch 726
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.6656e-01, PNorm = 160.7714, GNorm = 0.2322
Meta loss on this task batch = 2.4641e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 160.7755, GNorm = 0.2498
Meta loss on this task batch = 3.2290e-01, Meta loss averaged over last 500 steps = 2.6669e-01, PNorm = 160.7800, GNorm = 0.2524
Meta loss on this task batch = 3.0022e-01, Meta loss averaged over last 500 steps = 2.6674e-01, PNorm = 160.7847, GNorm = 0.2475
Meta loss on this task batch = 2.7529e-01, Meta loss averaged over last 500 steps = 2.6677e-01, PNorm = 160.7885, GNorm = 0.2560
Meta loss on this task batch = 2.3972e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 160.7922, GNorm = 0.2372
Meta loss on this task batch = 2.6266e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 160.7960, GNorm = 0.2176
Meta loss on this task batch = 2.1826e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 160.7997, GNorm = 0.2347
Meta loss on this task batch = 2.4948e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 160.8044, GNorm = 0.2357
Meta loss on this task batch = 2.7000e-01, Meta loss averaged over last 500 steps = 2.6664e-01, PNorm = 160.8086, GNorm = 0.2555
Meta loss on this task batch = 2.4852e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 160.8129, GNorm = 0.2374
Meta loss on this task batch = 2.9509e-01, Meta loss averaged over last 500 steps = 2.6668e-01, PNorm = 160.8165, GNorm = 0.2486
Meta loss on this task batch = 3.1240e-01, Meta loss averaged over last 500 steps = 2.6687e-01, PNorm = 160.8211, GNorm = 0.2812
Meta loss on this task batch = 2.3675e-01, Meta loss averaged over last 500 steps = 2.6670e-01, PNorm = 160.8260, GNorm = 0.2284
Meta loss on this task batch = 2.5277e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 160.8311, GNorm = 0.2384
Meta loss on this task batch = 2.3951e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 160.8371, GNorm = 0.2071
Meta loss on this task batch = 2.6763e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 160.8436, GNorm = 0.2310
Meta loss on this task batch = 2.4006e-01, Meta loss averaged over last 500 steps = 2.6664e-01, PNorm = 160.8505, GNorm = 0.2273
Meta loss on this task batch = 2.4271e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 160.8563, GNorm = 0.3210
Took 110.39732646942139 seconds to complete one epoch of meta training
Took 118.3471691608429 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475449
Epoch 727
Meta loss on this task batch = 3.0359e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 160.8613, GNorm = 0.2870
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 160.8655, GNorm = 0.2533
Meta loss on this task batch = 3.2539e-01, Meta loss averaged over last 500 steps = 2.6678e-01, PNorm = 160.8696, GNorm = 0.2446
Meta loss on this task batch = 3.1189e-01, Meta loss averaged over last 500 steps = 2.6698e-01, PNorm = 160.8737, GNorm = 0.2285
Meta loss on this task batch = 2.6484e-01, Meta loss averaged over last 500 steps = 2.6686e-01, PNorm = 160.8783, GNorm = 0.2385
Meta loss on this task batch = 2.2357e-01, Meta loss averaged over last 500 steps = 2.6661e-01, PNorm = 160.8838, GNorm = 0.2318
Meta loss on this task batch = 2.6526e-01, Meta loss averaged over last 500 steps = 2.6656e-01, PNorm = 160.8895, GNorm = 0.2155
Meta loss on this task batch = 2.6804e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 160.8960, GNorm = 0.2146
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 160.9024, GNorm = 0.2543
Meta loss on this task batch = 2.9264e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 160.9082, GNorm = 0.2308
Meta loss on this task batch = 2.3537e-01, Meta loss averaged over last 500 steps = 2.6647e-01, PNorm = 160.9142, GNorm = 0.2083
Meta loss on this task batch = 2.3025e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 160.9205, GNorm = 0.2241
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 160.9267, GNorm = 0.2705
Meta loss on this task batch = 2.4039e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 160.9322, GNorm = 0.2297
Meta loss on this task batch = 2.1024e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 160.9370, GNorm = 0.1825
Meta loss on this task batch = 2.6230e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 160.9411, GNorm = 0.2519
Meta loss on this task batch = 2.3105e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 160.9455, GNorm = 0.2061
Meta loss on this task batch = 2.8371e-01, Meta loss averaged over last 500 steps = 2.6641e-01, PNorm = 160.9496, GNorm = 0.2467
Meta loss on this task batch = 2.9614e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 160.9531, GNorm = 0.3005
Took 110.18948268890381 seconds to complete one epoch of meta training
Took 117.3896849155426 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481913
Epoch 728
Meta loss on this task batch = 2.1391e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 160.9570, GNorm = 0.2083
Meta loss on this task batch = 2.7308e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 160.9609, GNorm = 0.2243
Meta loss on this task batch = 2.8885e-01, Meta loss averaged over last 500 steps = 2.6655e-01, PNorm = 160.9639, GNorm = 0.2711
Meta loss on this task batch = 2.5097e-01, Meta loss averaged over last 500 steps = 2.6653e-01, PNorm = 160.9673, GNorm = 0.2239
Meta loss on this task batch = 2.4483e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 160.9716, GNorm = 0.2075
Meta loss on this task batch = 3.3609e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 160.9755, GNorm = 0.2801
Meta loss on this task batch = 3.2777e-01, Meta loss averaged over last 500 steps = 2.6662e-01, PNorm = 160.9797, GNorm = 0.2302
Meta loss on this task batch = 2.2716e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 160.9843, GNorm = 0.2285
Meta loss on this task batch = 2.8293e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 160.9891, GNorm = 0.3101
Meta loss on this task batch = 2.5828e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 160.9946, GNorm = 0.2522
Meta loss on this task batch = 2.9875e-01, Meta loss averaged over last 500 steps = 2.6653e-01, PNorm = 160.9999, GNorm = 0.2450
Meta loss on this task batch = 3.1380e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 161.0057, GNorm = 0.2372
Meta loss on this task batch = 2.5784e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 161.0123, GNorm = 0.2736
Meta loss on this task batch = 2.3491e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 161.0186, GNorm = 0.2193
Meta loss on this task batch = 2.2603e-01, Meta loss averaged over last 500 steps = 2.6641e-01, PNorm = 161.0246, GNorm = 0.2407
Meta loss on this task batch = 2.3102e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 161.0311, GNorm = 0.2192
Meta loss on this task batch = 2.4229e-01, Meta loss averaged over last 500 steps = 2.6631e-01, PNorm = 161.0373, GNorm = 0.1753
Meta loss on this task batch = 2.6670e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 161.0429, GNorm = 0.2096
Meta loss on this task batch = 2.1687e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 161.0489, GNorm = 0.2482
Took 115.51047420501709 seconds to complete one epoch of meta training
Took 123.19679260253906 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492172
Epoch 729
Meta loss on this task batch = 2.4076e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 161.0550, GNorm = 0.2071
Meta loss on this task batch = 2.2946e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 161.0610, GNorm = 0.2228
Meta loss on this task batch = 2.4010e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 161.0669, GNorm = 0.2279
Meta loss on this task batch = 2.2887e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 161.0727, GNorm = 0.2077
Meta loss on this task batch = 2.4602e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 161.0789, GNorm = 0.2736
Meta loss on this task batch = 2.9344e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 161.0847, GNorm = 0.2607
Meta loss on this task batch = 2.4538e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 161.0902, GNorm = 0.2572
Meta loss on this task batch = 2.9149e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 161.0954, GNorm = 0.2435
Meta loss on this task batch = 3.0468e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 161.0996, GNorm = 0.2664
Meta loss on this task batch = 2.9706e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 161.1036, GNorm = 0.2547
Meta loss on this task batch = 2.9867e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.1074, GNorm = 0.2361
Meta loss on this task batch = 2.6308e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 161.1111, GNorm = 0.2424
Meta loss on this task batch = 2.5638e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 161.1152, GNorm = 0.2160
Meta loss on this task batch = 2.5948e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 161.1194, GNorm = 0.2298
Meta loss on this task batch = 2.2407e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.1246, GNorm = 0.2336
Meta loss on this task batch = 2.4197e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 161.1299, GNorm = 0.2182
Meta loss on this task batch = 2.4659e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 161.1353, GNorm = 0.2580
Meta loss on this task batch = 2.9383e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 161.1404, GNorm = 0.2686
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.1465, GNorm = 0.2582
Took 111.83137464523315 seconds to complete one epoch of meta training
Took 120.2539005279541 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494079
Epoch 730
Meta loss on this task batch = 2.9168e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 161.1523, GNorm = 0.2408
Meta loss on this task batch = 2.9325e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 161.1585, GNorm = 0.2238
Meta loss on this task batch = 2.9792e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 161.1649, GNorm = 0.2490
Meta loss on this task batch = 3.3413e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 161.1708, GNorm = 0.2861
Meta loss on this task batch = 2.3660e-01, Meta loss averaged over last 500 steps = 2.6618e-01, PNorm = 161.1767, GNorm = 0.2211
Meta loss on this task batch = 2.6777e-01, Meta loss averaged over last 500 steps = 2.6626e-01, PNorm = 161.1830, GNorm = 0.2179
Meta loss on this task batch = 2.9324e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 161.1884, GNorm = 0.2331
Meta loss on this task batch = 2.3909e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 161.1928, GNorm = 0.2550
Meta loss on this task batch = 2.3639e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 161.1975, GNorm = 0.1950
Meta loss on this task batch = 2.6051e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 161.2022, GNorm = 0.2247
Meta loss on this task batch = 2.2774e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 161.2073, GNorm = 0.2299
Meta loss on this task batch = 2.4500e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 161.2119, GNorm = 0.2139
Meta loss on this task batch = 3.4238e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 161.2162, GNorm = 0.2419
Meta loss on this task batch = 2.1733e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 161.2213, GNorm = 0.2201
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 161.2277, GNorm = 0.2635
Meta loss on this task batch = 2.7552e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 161.2329, GNorm = 0.2105
Meta loss on this task batch = 2.2919e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 161.2382, GNorm = 0.2246
Meta loss on this task batch = 2.9011e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 161.2427, GNorm = 0.2296
Meta loss on this task batch = 2.6279e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 161.2472, GNorm = 0.3180
Took 113.52352404594421 seconds to complete one epoch of meta training
Took 120.53088760375977 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.513587
Epoch 731
Meta loss on this task batch = 2.7755e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 161.2509, GNorm = 0.2296
Meta loss on this task batch = 2.5329e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 161.2545, GNorm = 0.2337
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 161.2581, GNorm = 0.2172
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.6626e-01, PNorm = 161.2615, GNorm = 0.2047
Meta loss on this task batch = 2.7601e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 161.2648, GNorm = 0.2384
Meta loss on this task batch = 2.2774e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 161.2685, GNorm = 0.2034
Meta loss on this task batch = 2.5023e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 161.2721, GNorm = 0.2344
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 161.2753, GNorm = 0.2073
Meta loss on this task batch = 2.5004e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 161.2784, GNorm = 0.2125
Meta loss on this task batch = 2.8532e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 161.2820, GNorm = 0.2700
Meta loss on this task batch = 2.9126e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 161.2861, GNorm = 0.2432
Meta loss on this task batch = 2.5010e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 161.2905, GNorm = 0.2519
Meta loss on this task batch = 3.3391e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 161.2945, GNorm = 0.2775
Meta loss on this task batch = 2.2659e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 161.2988, GNorm = 0.2130
Meta loss on this task batch = 2.2033e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 161.3033, GNorm = 0.1945
Meta loss on this task batch = 2.4568e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 161.3085, GNorm = 0.2434
Meta loss on this task batch = 3.0118e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 161.3141, GNorm = 0.2536
Meta loss on this task batch = 2.5263e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 161.3203, GNorm = 0.2467
Meta loss on this task batch = 3.2268e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 161.3264, GNorm = 0.3212
Took 106.5062689781189 seconds to complete one epoch of meta training
Took 114.18704390525818 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508761
Epoch 732
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 161.3315, GNorm = 0.2343
Meta loss on this task batch = 2.8742e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 161.3363, GNorm = 0.2339
Meta loss on this task batch = 2.4629e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 161.3408, GNorm = 0.2219
Meta loss on this task batch = 2.6068e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 161.3448, GNorm = 0.2421
Meta loss on this task batch = 2.3391e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 161.3485, GNorm = 0.2092
Meta loss on this task batch = 2.9274e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 161.3519, GNorm = 0.2379
Meta loss on this task batch = 2.9679e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 161.3554, GNorm = 0.2705
Meta loss on this task batch = 2.3739e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 161.3596, GNorm = 0.1903
Meta loss on this task batch = 2.6119e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 161.3642, GNorm = 0.1982
Meta loss on this task batch = 2.2536e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 161.3686, GNorm = 0.1897
Meta loss on this task batch = 2.3187e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 161.3732, GNorm = 0.1968
Meta loss on this task batch = 2.7218e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 161.3767, GNorm = 0.2492
Meta loss on this task batch = 2.9621e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 161.3809, GNorm = 0.2406
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 161.3858, GNorm = 0.2802
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.6626e-01, PNorm = 161.3911, GNorm = 0.2715
Meta loss on this task batch = 2.7753e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 161.3970, GNorm = 0.2649
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 161.4020, GNorm = 0.2347
Meta loss on this task batch = 2.2009e-01, Meta loss averaged over last 500 steps = 2.6615e-01, PNorm = 161.4076, GNorm = 0.2263
Meta loss on this task batch = 3.0015e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 161.4134, GNorm = 0.2623
Took 104.6942925453186 seconds to complete one epoch of meta training
Took 112.10988116264343 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505478
Epoch 733
Meta loss on this task batch = 2.5907e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 161.4192, GNorm = 0.2257
Meta loss on this task batch = 2.4171e-01, Meta loss averaged over last 500 steps = 2.6612e-01, PNorm = 161.4254, GNorm = 0.2204
Meta loss on this task batch = 2.7461e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 161.4315, GNorm = 0.2323
Meta loss on this task batch = 2.5764e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 161.4375, GNorm = 0.2781
Meta loss on this task batch = 2.2062e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.4430, GNorm = 0.2041
Meta loss on this task batch = 2.3839e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 161.4490, GNorm = 0.1973
Meta loss on this task batch = 2.8364e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 161.4543, GNorm = 0.2301
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 161.4592, GNorm = 0.2335
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 161.4639, GNorm = 0.2723
Meta loss on this task batch = 2.4705e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 161.4690, GNorm = 0.1712
Meta loss on this task batch = 2.5796e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 161.4744, GNorm = 0.2131
Meta loss on this task batch = 2.5924e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 161.4792, GNorm = 0.2469
Meta loss on this task batch = 2.5536e-01, Meta loss averaged over last 500 steps = 2.6588e-01, PNorm = 161.4844, GNorm = 0.2071
Meta loss on this task batch = 2.5847e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 161.4901, GNorm = 0.2373
Meta loss on this task batch = 2.8046e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 161.4956, GNorm = 0.2350
Meta loss on this task batch = 2.9449e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 161.5023, GNorm = 0.2550
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 161.5081, GNorm = 0.2715
Meta loss on this task batch = 2.7351e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 161.5140, GNorm = 0.2567
Meta loss on this task batch = 2.2776e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 161.5197, GNorm = 0.2669
Took 107.44077730178833 seconds to complete one epoch of meta training
Took 114.74524784088135 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491652
Epoch 734
Meta loss on this task batch = 2.7837e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 161.5259, GNorm = 0.2225
Meta loss on this task batch = 2.1386e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 161.5320, GNorm = 0.1988
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 161.5376, GNorm = 0.2133
Meta loss on this task batch = 3.3450e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 161.5435, GNorm = 0.2700
Meta loss on this task batch = 2.2853e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 161.5498, GNorm = 0.2086
Meta loss on this task batch = 2.2126e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 161.5567, GNorm = 0.1801
Meta loss on this task batch = 2.7679e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.5632, GNorm = 0.2458
Meta loss on this task batch = 2.8669e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 161.5695, GNorm = 0.2330
Meta loss on this task batch = 2.1694e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 161.5757, GNorm = 0.2017
Meta loss on this task batch = 2.7875e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.5817, GNorm = 0.2411
Meta loss on this task batch = 2.0552e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 161.5872, GNorm = 0.1974
Meta loss on this task batch = 3.3023e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 161.5917, GNorm = 0.2675
Meta loss on this task batch = 2.3786e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 161.5957, GNorm = 0.2141
Meta loss on this task batch = 3.2144e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 161.5986, GNorm = 0.2655
Meta loss on this task batch = 3.1463e-01, Meta loss averaged over last 500 steps = 2.6647e-01, PNorm = 161.6011, GNorm = 0.2983
Meta loss on this task batch = 3.0847e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 161.6046, GNorm = 0.2487
Meta loss on this task batch = 2.6331e-01, Meta loss averaged over last 500 steps = 2.6639e-01, PNorm = 161.6084, GNorm = 0.2146
Meta loss on this task batch = 2.3871e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 161.6133, GNorm = 0.2702
Meta loss on this task batch = 3.0635e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 161.6181, GNorm = 0.2460
Took 104.77379298210144 seconds to complete one epoch of meta training
Took 112.02304339408875 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493258
Epoch 735
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 161.6233, GNorm = 0.2151
Meta loss on this task batch = 2.4570e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 161.6292, GNorm = 0.2085
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 161.6354, GNorm = 0.2288
Meta loss on this task batch = 2.2401e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 161.6410, GNorm = 0.2088
Meta loss on this task batch = 1.9847e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 161.6471, GNorm = 0.2070
Meta loss on this task batch = 2.6213e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 161.6545, GNorm = 0.2249
Meta loss on this task batch = 2.5782e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 161.6616, GNorm = 0.2856
Meta loss on this task batch = 3.2074e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 161.6676, GNorm = 0.3126
Meta loss on this task batch = 2.5350e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 161.6734, GNorm = 0.2038
Meta loss on this task batch = 2.5526e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 161.6793, GNorm = 0.2659
Meta loss on this task batch = 2.8678e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 161.6842, GNorm = 0.2690
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 161.6893, GNorm = 0.2165
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 161.6939, GNorm = 0.2169
Meta loss on this task batch = 2.5235e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 161.6983, GNorm = 0.2296
Meta loss on this task batch = 2.2704e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 161.7023, GNorm = 0.1931
Meta loss on this task batch = 2.6011e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 161.7059, GNorm = 0.2446
Meta loss on this task batch = 2.8526e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 161.7090, GNorm = 0.2480
Meta loss on this task batch = 2.6740e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 161.7122, GNorm = 0.2240
Meta loss on this task batch = 3.5305e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 161.7149, GNorm = 0.3596
Took 107.98276495933533 seconds to complete one epoch of meta training
Took 115.19452595710754 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488166
Epoch 736
Meta loss on this task batch = 2.2170e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 161.7183, GNorm = 0.2201
Meta loss on this task batch = 3.3154e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 161.7221, GNorm = 0.2509
Meta loss on this task batch = 1.9358e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 161.7271, GNorm = 0.2051
Meta loss on this task batch = 3.0755e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 161.7315, GNorm = 0.2186
Meta loss on this task batch = 3.1450e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 161.7360, GNorm = 0.2528
Meta loss on this task batch = 2.5488e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 161.7408, GNorm = 0.2148
Meta loss on this task batch = 2.4258e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.7452, GNorm = 0.2135
Meta loss on this task batch = 2.5740e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 161.7503, GNorm = 0.2340
Meta loss on this task batch = 2.5875e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 161.7558, GNorm = 0.2036
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 161.7615, GNorm = 0.2426
Meta loss on this task batch = 2.7791e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 161.7677, GNorm = 0.2200
Meta loss on this task batch = 2.4083e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 161.7737, GNorm = 0.2110
Meta loss on this task batch = 2.7279e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 161.7797, GNorm = 0.2357
Meta loss on this task batch = 3.0445e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 161.7858, GNorm = 0.2279
Meta loss on this task batch = 2.5656e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 161.7919, GNorm = 0.3199
Meta loss on this task batch = 3.1882e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 161.7965, GNorm = 0.3094
Meta loss on this task batch = 2.4563e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 161.8013, GNorm = 0.1874
Meta loss on this task batch = 2.0051e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 161.8060, GNorm = 0.1906
Meta loss on this task batch = 3.1930e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 161.8109, GNorm = 0.3128
Took 109.13714218139648 seconds to complete one epoch of meta training
Took 116.43534278869629 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479374
Epoch 737
Meta loss on this task batch = 2.9160e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 161.8150, GNorm = 0.2208
Meta loss on this task batch = 2.7407e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 161.8193, GNorm = 0.2418
Meta loss on this task batch = 2.2880e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 161.8237, GNorm = 0.2367
Meta loss on this task batch = 2.7496e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 161.8279, GNorm = 0.2359
Meta loss on this task batch = 2.8151e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 161.8322, GNorm = 0.2205
Meta loss on this task batch = 2.6830e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 161.8367, GNorm = 0.2117
Meta loss on this task batch = 2.4598e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 161.8416, GNorm = 0.2228
Meta loss on this task batch = 3.3052e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 161.8467, GNorm = 0.2296
Meta loss on this task batch = 3.0014e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 161.8512, GNorm = 0.2743
Meta loss on this task batch = 2.3125e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 161.8561, GNorm = 0.2214
Meta loss on this task batch = 2.8149e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 161.8611, GNorm = 0.2059
Meta loss on this task batch = 2.5804e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 161.8659, GNorm = 0.2325
Meta loss on this task batch = 2.5660e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 161.8699, GNorm = 0.2311
Meta loss on this task batch = 2.3876e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 161.8736, GNorm = 0.2106
Meta loss on this task batch = 3.3870e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 161.8772, GNorm = 0.2372
Meta loss on this task batch = 1.8954e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 161.8809, GNorm = 0.1903
Meta loss on this task batch = 2.3731e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 161.8849, GNorm = 0.2273
Meta loss on this task batch = 2.5604e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 161.8886, GNorm = 0.2388
Meta loss on this task batch = 2.0332e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 161.8934, GNorm = 0.2127
Took 106.42029619216919 seconds to complete one epoch of meta training
Took 113.39385151863098 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480721
Epoch 738
Meta loss on this task batch = 2.0595e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 161.8972, GNorm = 0.2261
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 161.9000, GNorm = 0.2408
Meta loss on this task batch = 2.8010e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 161.9023, GNorm = 0.2491
Meta loss on this task batch = 3.1488e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 161.9049, GNorm = 0.2493
Meta loss on this task batch = 2.7681e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 161.9071, GNorm = 0.2503
Meta loss on this task batch = 2.3207e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 161.9089, GNorm = 0.1983
Meta loss on this task batch = 2.5756e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 161.9117, GNorm = 0.2303
Meta loss on this task batch = 2.6142e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 161.9150, GNorm = 0.2320
Meta loss on this task batch = 2.6229e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 161.9190, GNorm = 0.2193
Meta loss on this task batch = 2.9714e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 161.9242, GNorm = 0.2743
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 161.9295, GNorm = 0.2755
Meta loss on this task batch = 3.0712e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 161.9351, GNorm = 0.2639
Meta loss on this task batch = 2.5112e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 161.9404, GNorm = 0.2098
Meta loss on this task batch = 2.7123e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 161.9463, GNorm = 0.2409
Meta loss on this task batch = 2.2289e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 161.9520, GNorm = 0.2007
Meta loss on this task batch = 3.1289e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 161.9574, GNorm = 0.2425
Meta loss on this task batch = 2.3146e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 161.9628, GNorm = 0.2351
Meta loss on this task batch = 2.8732e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 161.9680, GNorm = 0.2258
Meta loss on this task batch = 2.5872e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 161.9730, GNorm = 0.2966
Took 107.50266027450562 seconds to complete one epoch of meta training
Took 114.26238751411438 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491353
Epoch 739
Meta loss on this task batch = 2.2459e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 161.9780, GNorm = 0.2316
Meta loss on this task batch = 2.2526e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 161.9828, GNorm = 0.2326
Meta loss on this task batch = 2.6875e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 161.9872, GNorm = 0.2318
Meta loss on this task batch = 2.3092e-01, Meta loss averaged over last 500 steps = 2.6543e-01, PNorm = 161.9909, GNorm = 0.2358
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 161.9947, GNorm = 0.2272
Meta loss on this task batch = 2.5644e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 161.9985, GNorm = 0.2066
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 162.0020, GNorm = 0.2963
Meta loss on this task batch = 3.0348e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 162.0048, GNorm = 0.2360
Meta loss on this task batch = 3.1331e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 162.0077, GNorm = 0.2479
Meta loss on this task batch = 2.5955e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 162.0110, GNorm = 0.2342
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 162.0148, GNorm = 0.2357
Meta loss on this task batch = 3.0765e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 162.0188, GNorm = 0.2521
Meta loss on this task batch = 2.9519e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 162.0217, GNorm = 0.2419
Meta loss on this task batch = 2.3921e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 162.0250, GNorm = 0.2516
Meta loss on this task batch = 2.4136e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 162.0291, GNorm = 0.1916
Meta loss on this task batch = 3.2871e-01, Meta loss averaged over last 500 steps = 2.6612e-01, PNorm = 162.0331, GNorm = 0.2567
Meta loss on this task batch = 2.7377e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 162.0371, GNorm = 0.2387
Meta loss on this task batch = 2.8588e-01, Meta loss averaged over last 500 steps = 2.6618e-01, PNorm = 162.0417, GNorm = 0.2697
Meta loss on this task batch = 2.3836e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 162.0469, GNorm = 0.2579
Took 104.97094416618347 seconds to complete one epoch of meta training
Took 112.28827047348022 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.455178
Epoch 740
Meta loss on this task batch = 2.4794e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 162.0523, GNorm = 0.2227
Meta loss on this task batch = 2.4045e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 162.0579, GNorm = 0.2094
Meta loss on this task batch = 2.8793e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 162.0643, GNorm = 0.2128
Meta loss on this task batch = 2.6039e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 162.0712, GNorm = 0.2329
Meta loss on this task batch = 3.1221e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 162.0776, GNorm = 0.3127
Meta loss on this task batch = 2.3135e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 162.0831, GNorm = 0.2356
Meta loss on this task batch = 2.8900e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 162.0883, GNorm = 0.2264
Meta loss on this task batch = 2.3901e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 162.0922, GNorm = 0.2340
Meta loss on this task batch = 2.6992e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 162.0956, GNorm = 0.2248
Meta loss on this task batch = 3.0363e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 162.0988, GNorm = 0.2390
Meta loss on this task batch = 3.1669e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 162.1023, GNorm = 0.2567
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 162.1056, GNorm = 0.2385
Meta loss on this task batch = 2.5227e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 162.1094, GNorm = 0.2037
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 162.1138, GNorm = 0.2434
Meta loss on this task batch = 2.6806e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 162.1184, GNorm = 0.2782
Meta loss on this task batch = 2.7289e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 162.1234, GNorm = 0.2146
Meta loss on this task batch = 2.4008e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 162.1286, GNorm = 0.2237
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 162.1330, GNorm = 0.2365
Meta loss on this task batch = 3.2885e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 162.1360, GNorm = 0.3008
Took 107.09010720252991 seconds to complete one epoch of meta training
Took 113.31678438186646 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502208
Epoch 741
Meta loss on this task batch = 2.8206e-01, Meta loss averaged over last 500 steps = 2.6647e-01, PNorm = 162.1390, GNorm = 0.2056
Meta loss on this task batch = 2.6364e-01, Meta loss averaged over last 500 steps = 2.6647e-01, PNorm = 162.1415, GNorm = 0.2210
Meta loss on this task batch = 3.2124e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 162.1438, GNorm = 0.2705
Meta loss on this task batch = 3.2511e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 162.1457, GNorm = 0.2572
Meta loss on this task batch = 2.7884e-01, Meta loss averaged over last 500 steps = 2.6668e-01, PNorm = 162.1487, GNorm = 0.2255
Meta loss on this task batch = 2.7268e-01, Meta loss averaged over last 500 steps = 2.6666e-01, PNorm = 162.1521, GNorm = 0.2460
Meta loss on this task batch = 2.6011e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 162.1564, GNorm = 0.2073
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.6662e-01, PNorm = 162.1610, GNorm = 0.2300
Meta loss on this task batch = 2.3442e-01, Meta loss averaged over last 500 steps = 2.6659e-01, PNorm = 162.1662, GNorm = 0.1975
Meta loss on this task batch = 2.5015e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 162.1717, GNorm = 0.2322
Meta loss on this task batch = 2.4220e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 162.1770, GNorm = 0.2289
Meta loss on this task batch = 3.0191e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 162.1827, GNorm = 0.2661
Meta loss on this task batch = 2.4527e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 162.1886, GNorm = 0.2442
Meta loss on this task batch = 2.3787e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 162.1950, GNorm = 0.2248
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 2.6677e-01, PNorm = 162.2013, GNorm = 0.2662
Meta loss on this task batch = 2.4796e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 162.2074, GNorm = 0.2126
Meta loss on this task batch = 2.5189e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 162.2135, GNorm = 0.2163
Meta loss on this task batch = 2.3487e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 162.2197, GNorm = 0.1983
Meta loss on this task batch = 2.4775e-01, Meta loss averaged over last 500 steps = 2.6656e-01, PNorm = 162.2255, GNorm = 0.2474
Took 107.8250675201416 seconds to complete one epoch of meta training
Took 115.68305373191833 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517702
Epoch 742
Meta loss on this task batch = 2.2162e-01, Meta loss averaged over last 500 steps = 2.6644e-01, PNorm = 162.2308, GNorm = 0.1984
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 2.6650e-01, PNorm = 162.2360, GNorm = 0.2376
Meta loss on this task batch = 2.4767e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 162.2413, GNorm = 0.2350
Meta loss on this task batch = 2.1805e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 162.2463, GNorm = 0.2108
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 162.2501, GNorm = 0.2621
Meta loss on this task batch = 2.8636e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 162.2538, GNorm = 0.2667
Meta loss on this task batch = 2.4997e-01, Meta loss averaged over last 500 steps = 2.6626e-01, PNorm = 162.2573, GNorm = 0.2390
Meta loss on this task batch = 2.7147e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 162.2608, GNorm = 0.2281
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 162.2645, GNorm = 0.2396
Meta loss on this task batch = 2.7260e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 162.2684, GNorm = 0.2016
Meta loss on this task batch = 2.2663e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 162.2726, GNorm = 0.2008
Meta loss on this task batch = 2.8812e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 162.2766, GNorm = 0.2290
Meta loss on this task batch = 2.0033e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 162.2810, GNorm = 0.2050
Meta loss on this task batch = 2.7251e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 162.2865, GNorm = 0.2749
Meta loss on this task batch = 2.7835e-01, Meta loss averaged over last 500 steps = 2.6644e-01, PNorm = 162.2925, GNorm = 0.2403
Meta loss on this task batch = 3.4108e-01, Meta loss averaged over last 500 steps = 2.6650e-01, PNorm = 162.2980, GNorm = 0.2521
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 162.3035, GNorm = 0.1968
Meta loss on this task batch = 2.2183e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 162.3094, GNorm = 0.2060
Meta loss on this task batch = 3.0925e-01, Meta loss averaged over last 500 steps = 2.6662e-01, PNorm = 162.3162, GNorm = 0.3321
Took 110.12336850166321 seconds to complete one epoch of meta training
Took 117.62256693840027 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497661
Epoch 743
Meta loss on this task batch = 2.4715e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 162.3224, GNorm = 0.2335
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 162.3279, GNorm = 0.2477
Meta loss on this task batch = 3.4600e-01, Meta loss averaged over last 500 steps = 2.6674e-01, PNorm = 162.3321, GNorm = 0.2613
Meta loss on this task batch = 2.7581e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 162.3363, GNorm = 0.2083
Meta loss on this task batch = 2.5712e-01, Meta loss averaged over last 500 steps = 2.6676e-01, PNorm = 162.3389, GNorm = 0.2792
Meta loss on this task batch = 2.5670e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 162.3418, GNorm = 0.2305
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 2.6671e-01, PNorm = 162.3446, GNorm = 0.2262
Meta loss on this task batch = 2.4783e-01, Meta loss averaged over last 500 steps = 2.6669e-01, PNorm = 162.3477, GNorm = 0.2161
Meta loss on this task batch = 2.5936e-01, Meta loss averaged over last 500 steps = 2.6671e-01, PNorm = 162.3512, GNorm = 0.2295
Meta loss on this task batch = 2.7231e-01, Meta loss averaged over last 500 steps = 2.6668e-01, PNorm = 162.3552, GNorm = 0.2164
Meta loss on this task batch = 2.2732e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 162.3591, GNorm = 0.1962
Meta loss on this task batch = 2.6170e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 162.3631, GNorm = 0.2339
Meta loss on this task batch = 2.8670e-01, Meta loss averaged over last 500 steps = 2.6679e-01, PNorm = 162.3671, GNorm = 0.2405
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.6670e-01, PNorm = 162.3706, GNorm = 0.2226
Meta loss on this task batch = 2.6893e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 162.3739, GNorm = 0.1932
Meta loss on this task batch = 2.6245e-01, Meta loss averaged over last 500 steps = 2.6677e-01, PNorm = 162.3777, GNorm = 0.3123
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 2.6679e-01, PNorm = 162.3817, GNorm = 0.2673
Meta loss on this task batch = 2.6577e-01, Meta loss averaged over last 500 steps = 2.6672e-01, PNorm = 162.3856, GNorm = 0.2094
Meta loss on this task batch = 2.2783e-01, Meta loss averaged over last 500 steps = 2.6665e-01, PNorm = 162.3901, GNorm = 0.2482
Took 107.72868180274963 seconds to complete one epoch of meta training
Took 114.91899299621582 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492928
Epoch 744
Meta loss on this task batch = 2.8345e-01, Meta loss averaged over last 500 steps = 2.6670e-01, PNorm = 162.3948, GNorm = 0.2377
Meta loss on this task batch = 2.8610e-01, Meta loss averaged over last 500 steps = 2.6670e-01, PNorm = 162.4000, GNorm = 0.2429
Meta loss on this task batch = 2.5275e-01, Meta loss averaged over last 500 steps = 2.6658e-01, PNorm = 162.4053, GNorm = 0.2009
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 162.4100, GNorm = 0.2325
Meta loss on this task batch = 2.9147e-01, Meta loss averaged over last 500 steps = 2.6667e-01, PNorm = 162.4140, GNorm = 0.2426
Meta loss on this task batch = 2.7154e-01, Meta loss averaged over last 500 steps = 2.6677e-01, PNorm = 162.4180, GNorm = 0.2417
Meta loss on this task batch = 2.3063e-01, Meta loss averaged over last 500 steps = 2.6675e-01, PNorm = 162.4224, GNorm = 0.2293
Meta loss on this task batch = 2.8040e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 162.4263, GNorm = 0.2387
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.6673e-01, PNorm = 162.4303, GNorm = 0.2423
Meta loss on this task batch = 2.4042e-01, Meta loss averaged over last 500 steps = 2.6662e-01, PNorm = 162.4342, GNorm = 0.2246
Meta loss on this task batch = 2.4960e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 162.4379, GNorm = 0.2328
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.6650e-01, PNorm = 162.4422, GNorm = 0.2273
Meta loss on this task batch = 2.4565e-01, Meta loss averaged over last 500 steps = 2.6644e-01, PNorm = 162.4459, GNorm = 0.2260
Meta loss on this task batch = 2.5892e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 162.4498, GNorm = 0.2271
Meta loss on this task batch = 2.9807e-01, Meta loss averaged over last 500 steps = 2.6650e-01, PNorm = 162.4532, GNorm = 0.2084
Meta loss on this task batch = 3.0005e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 162.4566, GNorm = 0.2194
Meta loss on this task batch = 2.7565e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 162.4611, GNorm = 0.2313
Meta loss on this task batch = 2.3790e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 162.4661, GNorm = 0.2260
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 162.4710, GNorm = 0.2762
Took 107.45224046707153 seconds to complete one epoch of meta training
Took 115.1060106754303 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503892
Epoch 745
Meta loss on this task batch = 2.3241e-01, Meta loss averaged over last 500 steps = 2.6645e-01, PNorm = 162.4763, GNorm = 0.2037
Meta loss on this task batch = 2.7144e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 162.4818, GNorm = 0.2534
Meta loss on this task batch = 2.4092e-01, Meta loss averaged over last 500 steps = 2.6641e-01, PNorm = 162.4879, GNorm = 0.2118
Meta loss on this task batch = 2.6435e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 162.4934, GNorm = 0.2549
Meta loss on this task batch = 2.1635e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 162.4999, GNorm = 0.2207
Meta loss on this task batch = 2.3555e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 162.5061, GNorm = 0.1928
Meta loss on this task batch = 2.8521e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 162.5121, GNorm = 0.2539
Meta loss on this task batch = 3.1735e-01, Meta loss averaged over last 500 steps = 2.6656e-01, PNorm = 162.5178, GNorm = 0.2345
Meta loss on this task batch = 2.4133e-01, Meta loss averaged over last 500 steps = 2.6650e-01, PNorm = 162.5231, GNorm = 0.2352
Meta loss on this task batch = 2.8320e-01, Meta loss averaged over last 500 steps = 2.6647e-01, PNorm = 162.5279, GNorm = 0.2621
Meta loss on this task batch = 2.2618e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 162.5331, GNorm = 0.1979
Meta loss on this task batch = 2.9103e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 162.5381, GNorm = 0.2612
Meta loss on this task batch = 2.6103e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 162.5436, GNorm = 0.2083
Meta loss on this task batch = 2.2419e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 162.5493, GNorm = 0.2102
Meta loss on this task batch = 2.7552e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 162.5551, GNorm = 0.2332
Meta loss on this task batch = 2.1669e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 162.5607, GNorm = 0.2076
Meta loss on this task batch = 2.4630e-01, Meta loss averaged over last 500 steps = 2.6633e-01, PNorm = 162.5666, GNorm = 0.2024
Meta loss on this task batch = 3.2858e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 162.5717, GNorm = 0.2750
Meta loss on this task batch = 3.1106e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 162.5758, GNorm = 0.3666
Took 106.15520405769348 seconds to complete one epoch of meta training
Took 113.34501600265503 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.518410
Epoch 746
Meta loss on this task batch = 2.9026e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 162.5801, GNorm = 0.2631
Meta loss on this task batch = 2.5991e-01, Meta loss averaged over last 500 steps = 2.6659e-01, PNorm = 162.5846, GNorm = 0.2067
Meta loss on this task batch = 2.6315e-01, Meta loss averaged over last 500 steps = 2.6659e-01, PNorm = 162.5880, GNorm = 0.3045
Meta loss on this task batch = 2.6760e-01, Meta loss averaged over last 500 steps = 2.6659e-01, PNorm = 162.5919, GNorm = 0.2160
Meta loss on this task batch = 3.0668e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 162.5964, GNorm = 0.2758
Meta loss on this task batch = 2.4911e-01, Meta loss averaged over last 500 steps = 2.6646e-01, PNorm = 162.6010, GNorm = 0.2549
Meta loss on this task batch = 2.4698e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 162.6064, GNorm = 0.2227
Meta loss on this task batch = 2.4493e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 162.6121, GNorm = 0.2122
Meta loss on this task batch = 2.0661e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 162.6185, GNorm = 0.1976
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 162.6250, GNorm = 0.2137
Meta loss on this task batch = 2.5234e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 162.6314, GNorm = 0.1953
Meta loss on this task batch = 2.5184e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 162.6375, GNorm = 0.2328
Meta loss on this task batch = 3.2310e-01, Meta loss averaged over last 500 steps = 2.6660e-01, PNorm = 162.6430, GNorm = 0.2642
Meta loss on this task batch = 2.8623e-01, Meta loss averaged over last 500 steps = 2.6662e-01, PNorm = 162.6488, GNorm = 0.2529
Meta loss on this task batch = 2.8402e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 162.6538, GNorm = 0.2474
Meta loss on this task batch = 2.4329e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 162.6588, GNorm = 0.2126
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 162.6630, GNorm = 0.2485
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 2.6657e-01, PNorm = 162.6667, GNorm = 0.2481
Meta loss on this task batch = 2.3410e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 162.6697, GNorm = 0.2644
Took 108.57273292541504 seconds to complete one epoch of meta training
Took 115.18119287490845 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499895
Epoch 747
Meta loss on this task batch = 2.1712e-01, Meta loss averaged over last 500 steps = 2.6638e-01, PNorm = 162.6735, GNorm = 0.2100
Meta loss on this task batch = 2.8311e-01, Meta loss averaged over last 500 steps = 2.6647e-01, PNorm = 162.6774, GNorm = 0.2398
Meta loss on this task batch = 3.1042e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 162.6812, GNorm = 0.2946
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 162.6849, GNorm = 0.1901
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 162.6898, GNorm = 0.3201
Meta loss on this task batch = 2.2469e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 162.6952, GNorm = 0.2045
Meta loss on this task batch = 2.2690e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 162.7012, GNorm = 0.2080
Meta loss on this task batch = 2.5242e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 162.7074, GNorm = 0.2411
Meta loss on this task batch = 2.6762e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 162.7142, GNorm = 0.2106
Meta loss on this task batch = 2.8805e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 162.7211, GNorm = 0.2335
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 162.7278, GNorm = 0.2106
Meta loss on this task batch = 2.7849e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 162.7337, GNorm = 0.2763
Meta loss on this task batch = 2.4998e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 162.7392, GNorm = 0.2157
Meta loss on this task batch = 2.6252e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 162.7443, GNorm = 0.2231
Meta loss on this task batch = 2.3944e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 162.7495, GNorm = 0.2208
Meta loss on this task batch = 2.1600e-01, Meta loss averaged over last 500 steps = 2.6627e-01, PNorm = 162.7557, GNorm = 0.2024
Meta loss on this task batch = 2.7031e-01, Meta loss averaged over last 500 steps = 2.6631e-01, PNorm = 162.7607, GNorm = 0.2652
Meta loss on this task batch = 2.8933e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 162.7662, GNorm = 0.2505
Meta loss on this task batch = 1.9286e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 162.7720, GNorm = 0.2401
Took 108.61981749534607 seconds to complete one epoch of meta training
Took 116.1288628578186 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493018
Epoch 748
Meta loss on this task batch = 2.6170e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 162.7774, GNorm = 0.2138
Meta loss on this task batch = 2.5893e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 162.7825, GNorm = 0.2453
Meta loss on this task batch = 2.7853e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 162.7867, GNorm = 0.2299
Meta loss on this task batch = 2.5667e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 162.7917, GNorm = 0.1978
Meta loss on this task batch = 2.4954e-01, Meta loss averaged over last 500 steps = 2.6588e-01, PNorm = 162.7962, GNorm = 0.2241
Meta loss on this task batch = 2.4353e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 162.8006, GNorm = 0.2132
Meta loss on this task batch = 2.0195e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 162.8053, GNorm = 0.2046
Meta loss on this task batch = 2.4716e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 162.8100, GNorm = 0.2227
Meta loss on this task batch = 2.3547e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 162.8146, GNorm = 0.2223
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 162.8196, GNorm = 0.2822
Meta loss on this task batch = 2.6706e-01, Meta loss averaged over last 500 steps = 2.6562e-01, PNorm = 162.8250, GNorm = 0.2244
Meta loss on this task batch = 2.7110e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 162.8300, GNorm = 0.2149
Meta loss on this task batch = 3.1161e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 162.8342, GNorm = 0.2446
Meta loss on this task batch = 3.1281e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 162.8392, GNorm = 0.2885
Meta loss on this task batch = 3.1083e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 162.8433, GNorm = 0.2271
Meta loss on this task batch = 2.3024e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 162.8475, GNorm = 0.2159
Meta loss on this task batch = 3.4310e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 162.8514, GNorm = 0.2772
Meta loss on this task batch = 2.2783e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 162.8556, GNorm = 0.2119
Meta loss on this task batch = 2.8450e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 162.8601, GNorm = 0.2593
Took 109.37129330635071 seconds to complete one epoch of meta training
Took 116.91473650932312 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509893
Epoch 749
Meta loss on this task batch = 2.7276e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 162.8640, GNorm = 0.2562
Meta loss on this task batch = 2.4893e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 162.8687, GNorm = 0.2617
Meta loss on this task batch = 2.7336e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 162.8734, GNorm = 0.2319
Meta loss on this task batch = 3.0398e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 162.8778, GNorm = 0.2342
Meta loss on this task batch = 2.3395e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 162.8819, GNorm = 0.2136
Meta loss on this task batch = 2.6507e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 162.8857, GNorm = 0.2322
Meta loss on this task batch = 2.4849e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 162.8909, GNorm = 0.2370
Meta loss on this task batch = 2.2050e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 162.8963, GNorm = 0.2348
Meta loss on this task batch = 3.5530e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 162.9007, GNorm = 0.2775
Meta loss on this task batch = 2.8500e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 162.9048, GNorm = 0.2545
Meta loss on this task batch = 2.6312e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 162.9088, GNorm = 0.2402
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 162.9135, GNorm = 0.2593
Meta loss on this task batch = 3.0783e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 162.9183, GNorm = 0.2266
Meta loss on this task batch = 1.9401e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 162.9246, GNorm = 0.2182
Meta loss on this task batch = 2.6117e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 162.9306, GNorm = 0.2400
Meta loss on this task batch = 2.8484e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 162.9358, GNorm = 0.2140
Meta loss on this task batch = 2.1432e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 162.9418, GNorm = 0.2085
Meta loss on this task batch = 2.9200e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 162.9481, GNorm = 0.2186
Meta loss on this task batch = 2.9900e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 162.9533, GNorm = 0.2633
Took 110.50930309295654 seconds to complete one epoch of meta training
Took 118.20520257949829 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499670
Epoch 750
Meta loss on this task batch = 2.3696e-01, Meta loss averaged over last 500 steps = 2.6558e-01, PNorm = 162.9582, GNorm = 0.2568
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.6557e-01, PNorm = 162.9627, GNorm = 0.2376
Meta loss on this task batch = 3.0591e-01, Meta loss averaged over last 500 steps = 2.6561e-01, PNorm = 162.9666, GNorm = 0.2734
Meta loss on this task batch = 2.6431e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 162.9698, GNorm = 0.2269
Meta loss on this task batch = 2.8453e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 162.9725, GNorm = 0.2693
Meta loss on this task batch = 2.6079e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 162.9749, GNorm = 0.2095
Meta loss on this task batch = 2.9036e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 162.9764, GNorm = 0.2392
Meta loss on this task batch = 3.2244e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 162.9778, GNorm = 0.2430
Meta loss on this task batch = 2.6932e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 162.9791, GNorm = 0.2315
Meta loss on this task batch = 3.0656e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 162.9808, GNorm = 0.3221
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 162.9831, GNorm = 0.2196
Meta loss on this task batch = 2.8248e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 162.9860, GNorm = 0.2290
Meta loss on this task batch = 2.6184e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 162.9897, GNorm = 0.2309
Meta loss on this task batch = 2.5675e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 162.9936, GNorm = 0.2374
Meta loss on this task batch = 2.9706e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 162.9971, GNorm = 0.2146
Meta loss on this task batch = 2.7606e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 163.0006, GNorm = 0.2140
Meta loss on this task batch = 2.1786e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 163.0049, GNorm = 0.2204
Meta loss on this task batch = 2.6606e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 163.0098, GNorm = 0.2073
Meta loss on this task batch = 1.9798e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 163.0155, GNorm = 0.2595
Took 133.19788885116577 seconds to complete one epoch of meta training
Took 140.67652297019958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493148
Epoch 751
Meta loss on this task batch = 2.6558e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 163.0217, GNorm = 0.2347
Meta loss on this task batch = 2.9979e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 163.0273, GNorm = 0.2424
Meta loss on this task batch = 2.2972e-01, Meta loss averaged over last 500 steps = 2.6594e-01, PNorm = 163.0325, GNorm = 0.2087
Meta loss on this task batch = 2.9356e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.0367, GNorm = 0.2383
Meta loss on this task batch = 2.5219e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 163.0396, GNorm = 0.2658
Meta loss on this task batch = 2.8218e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.0420, GNorm = 0.2616
Meta loss on this task batch = 2.4500e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 163.0440, GNorm = 0.2335
Meta loss on this task batch = 2.5406e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 163.0466, GNorm = 0.2261
Meta loss on this task batch = 2.5168e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 163.0505, GNorm = 0.2239
Meta loss on this task batch = 2.7872e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 163.0542, GNorm = 0.2309
Meta loss on this task batch = 3.0367e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 163.0577, GNorm = 0.2600
Meta loss on this task batch = 3.4009e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 163.0607, GNorm = 0.2938
Meta loss on this task batch = 2.6823e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 163.0639, GNorm = 0.2182
Meta loss on this task batch = 2.3941e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 163.0681, GNorm = 0.2251
Meta loss on this task batch = 2.5477e-01, Meta loss averaged over last 500 steps = 2.6569e-01, PNorm = 163.0715, GNorm = 0.2048
Meta loss on this task batch = 2.7571e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 163.0754, GNorm = 0.2157
Meta loss on this task batch = 2.3394e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 163.0800, GNorm = 0.2397
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 163.0844, GNorm = 0.2236
Meta loss on this task batch = 2.3832e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 163.0885, GNorm = 0.2446
Took 108.76863646507263 seconds to complete one epoch of meta training
Took 115.91453766822815 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499912
Epoch 752
Meta loss on this task batch = 2.0531e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 163.0935, GNorm = 0.1894
Meta loss on this task batch = 2.9732e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 163.0983, GNorm = 0.2482
Meta loss on this task batch = 2.6397e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 163.1024, GNorm = 0.2208
Meta loss on this task batch = 2.4452e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 163.1072, GNorm = 0.2213
Meta loss on this task batch = 2.7596e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 163.1117, GNorm = 0.2067
Meta loss on this task batch = 2.5411e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 163.1165, GNorm = 0.2285
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 163.1211, GNorm = 0.2266
Meta loss on this task batch = 2.5912e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 163.1266, GNorm = 0.2259
Meta loss on this task batch = 2.9980e-01, Meta loss averaged over last 500 steps = 2.6612e-01, PNorm = 163.1320, GNorm = 0.2414
Meta loss on this task batch = 2.3234e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 163.1379, GNorm = 0.2022
Meta loss on this task batch = 1.9379e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 163.1443, GNorm = 0.2081
Meta loss on this task batch = 3.0028e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.1503, GNorm = 0.2457
Meta loss on this task batch = 2.3516e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 163.1565, GNorm = 0.2053
Meta loss on this task batch = 2.5386e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 163.1610, GNorm = 0.2733
Meta loss on this task batch = 2.4579e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.1660, GNorm = 0.2446
Meta loss on this task batch = 3.0196e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.1712, GNorm = 0.2705
Meta loss on this task batch = 2.7006e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 163.1766, GNorm = 0.2283
Meta loss on this task batch = 2.9394e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 163.1820, GNorm = 0.2288
Meta loss on this task batch = 2.4210e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 163.1878, GNorm = 0.2380
Took 107.76161599159241 seconds to complete one epoch of meta training
Took 115.4257001876831 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509736
Epoch 753
Meta loss on this task batch = 2.6851e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 163.1936, GNorm = 0.2224
Meta loss on this task batch = 2.6514e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 163.1995, GNorm = 0.2303
Meta loss on this task batch = 2.5884e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 163.2052, GNorm = 0.2060
Meta loss on this task batch = 3.1290e-01, Meta loss averaged over last 500 steps = 2.6614e-01, PNorm = 163.2108, GNorm = 0.2347
Meta loss on this task batch = 2.3411e-01, Meta loss averaged over last 500 steps = 2.6612e-01, PNorm = 163.2164, GNorm = 0.2135
Meta loss on this task batch = 2.5352e-01, Meta loss averaged over last 500 steps = 2.6615e-01, PNorm = 163.2226, GNorm = 0.2308
Meta loss on this task batch = 2.8466e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 163.2285, GNorm = 0.2430
Meta loss on this task batch = 2.4939e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 163.2341, GNorm = 0.2293
Meta loss on this task batch = 2.6513e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 163.2393, GNorm = 0.2374
Meta loss on this task batch = 2.3666e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 163.2442, GNorm = 0.2248
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 163.2481, GNorm = 0.2549
Meta loss on this task batch = 2.7915e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 163.2521, GNorm = 0.2235
Meta loss on this task batch = 2.3950e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 163.2561, GNorm = 0.2648
Meta loss on this task batch = 2.6354e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 163.2605, GNorm = 0.1947
Meta loss on this task batch = 2.5858e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 163.2647, GNorm = 0.2682
Meta loss on this task batch = 2.7604e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 163.2697, GNorm = 0.2199
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 163.2746, GNorm = 0.2270
Meta loss on this task batch = 2.4416e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 163.2797, GNorm = 0.2545
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 163.2861, GNorm = 0.2635
Took 108.08067321777344 seconds to complete one epoch of meta training
Took 114.34643411636353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478319
Epoch 754
Meta loss on this task batch = 2.6423e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 163.2916, GNorm = 0.2454
Meta loss on this task batch = 2.4920e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 163.2978, GNorm = 0.2483
Meta loss on this task batch = 2.3855e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 163.3046, GNorm = 0.2571
Meta loss on this task batch = 2.3202e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 163.3111, GNorm = 0.2120
Meta loss on this task batch = 2.9617e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 163.3168, GNorm = 0.2722
Meta loss on this task batch = 2.6994e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 163.3229, GNorm = 0.2801
Meta loss on this task batch = 2.9132e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 163.3282, GNorm = 0.2212
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 163.3341, GNorm = 0.2279
Meta loss on this task batch = 2.6418e-01, Meta loss averaged over last 500 steps = 2.6592e-01, PNorm = 163.3392, GNorm = 0.2884
Meta loss on this task batch = 2.8148e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 163.3441, GNorm = 0.2442
Meta loss on this task batch = 2.6936e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 163.3486, GNorm = 0.2686
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 163.3531, GNorm = 0.2445
Meta loss on this task batch = 2.6003e-01, Meta loss averaged over last 500 steps = 2.6580e-01, PNorm = 163.3579, GNorm = 0.2348
Meta loss on this task batch = 2.2577e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 163.3635, GNorm = 0.2065
Meta loss on this task batch = 2.9154e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 163.3691, GNorm = 0.2334
Meta loss on this task batch = 2.2511e-01, Meta loss averaged over last 500 steps = 2.6574e-01, PNorm = 163.3749, GNorm = 0.2194
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 163.3808, GNorm = 0.2399
Meta loss on this task batch = 3.0295e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 163.3867, GNorm = 0.2516
Meta loss on this task batch = 2.4611e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 163.3929, GNorm = 0.2092
Took 108.40699338912964 seconds to complete one epoch of meta training
Took 115.42656803131104 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498568
Epoch 755
Meta loss on this task batch = 2.8584e-01, Meta loss averaged over last 500 steps = 2.6576e-01, PNorm = 163.3982, GNorm = 0.2462
Meta loss on this task batch = 2.3369e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 163.4033, GNorm = 0.2150
Meta loss on this task batch = 2.7035e-01, Meta loss averaged over last 500 steps = 2.6586e-01, PNorm = 163.4085, GNorm = 0.2120
Meta loss on this task batch = 2.9113e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.4137, GNorm = 0.2368
Meta loss on this task batch = 2.7071e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 163.4183, GNorm = 0.2291
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 163.4233, GNorm = 0.2559
Meta loss on this task batch = 2.9223e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 163.4283, GNorm = 0.2364
Meta loss on this task batch = 2.9394e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 163.4340, GNorm = 0.2512
Meta loss on this task batch = 3.0442e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 163.4384, GNorm = 0.2593
Meta loss on this task batch = 2.2855e-01, Meta loss averaged over last 500 steps = 2.6642e-01, PNorm = 163.4425, GNorm = 0.2112
Meta loss on this task batch = 2.7382e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 163.4469, GNorm = 0.2360
Meta loss on this task batch = 2.5620e-01, Meta loss averaged over last 500 steps = 2.6640e-01, PNorm = 163.4518, GNorm = 0.2021
Meta loss on this task batch = 2.4026e-01, Meta loss averaged over last 500 steps = 2.6639e-01, PNorm = 163.4567, GNorm = 0.2007
Meta loss on this task batch = 2.6747e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 163.4616, GNorm = 0.2201
Meta loss on this task batch = 2.5860e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 163.4659, GNorm = 0.2167
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 163.4703, GNorm = 0.2465
Meta loss on this task batch = 2.1226e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 163.4744, GNorm = 0.1921
Meta loss on this task batch = 2.6174e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 163.4795, GNorm = 0.2069
Meta loss on this task batch = 2.4833e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 163.4855, GNorm = 0.2233
Took 104.46604371070862 seconds to complete one epoch of meta training
Took 111.76167821884155 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493263
Epoch 756
Meta loss on this task batch = 2.4931e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 163.4916, GNorm = 0.2140
Meta loss on this task batch = 2.9301e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 163.4977, GNorm = 0.2284
Meta loss on this task batch = 2.8528e-01, Meta loss averaged over last 500 steps = 2.6626e-01, PNorm = 163.5036, GNorm = 0.2580
Meta loss on this task batch = 2.4192e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 163.5094, GNorm = 0.2202
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.6618e-01, PNorm = 163.5155, GNorm = 0.3022
Meta loss on this task batch = 2.4733e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 163.5212, GNorm = 0.2358
Meta loss on this task batch = 2.3912e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.5270, GNorm = 0.2020
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.5334, GNorm = 0.2213
Meta loss on this task batch = 3.1203e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 163.5391, GNorm = 0.2567
Meta loss on this task batch = 2.5451e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 163.5442, GNorm = 0.2359
Meta loss on this task batch = 2.6137e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 163.5498, GNorm = 0.2400
Meta loss on this task batch = 2.7736e-01, Meta loss averaged over last 500 steps = 2.6589e-01, PNorm = 163.5546, GNorm = 0.2403
Meta loss on this task batch = 2.5677e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 163.5590, GNorm = 0.2766
Meta loss on this task batch = 2.6818e-01, Meta loss averaged over last 500 steps = 2.6588e-01, PNorm = 163.5636, GNorm = 0.2503
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 163.5675, GNorm = 0.2439
Meta loss on this task batch = 2.6586e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 163.5719, GNorm = 0.2571
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 163.5766, GNorm = 0.2266
Meta loss on this task batch = 3.1016e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 163.5802, GNorm = 0.2528
Meta loss on this task batch = 2.3656e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.5841, GNorm = 0.2708
Took 110.43020606040955 seconds to complete one epoch of meta training
Took 117.65848755836487 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491422
Epoch 757
Meta loss on this task batch = 2.1592e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.5885, GNorm = 0.1911
Meta loss on this task batch = 3.2238e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 163.5927, GNorm = 0.2303
Meta loss on this task batch = 2.7135e-01, Meta loss averaged over last 500 steps = 2.6612e-01, PNorm = 163.5976, GNorm = 0.2617
Meta loss on this task batch = 2.6297e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 163.6016, GNorm = 0.2028
Meta loss on this task batch = 2.3864e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 163.6066, GNorm = 0.2060
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.6607e-01, PNorm = 163.6124, GNorm = 0.2044
Meta loss on this task batch = 2.2868e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 163.6186, GNorm = 0.1936
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 163.6244, GNorm = 0.2479
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 163.6304, GNorm = 0.2246
Meta loss on this task batch = 2.7718e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 163.6367, GNorm = 0.2164
Meta loss on this task batch = 2.5912e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 163.6425, GNorm = 0.2318
Meta loss on this task batch = 3.1259e-01, Meta loss averaged over last 500 steps = 2.6613e-01, PNorm = 163.6456, GNorm = 0.3172
Meta loss on this task batch = 2.6410e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 163.6494, GNorm = 0.2355
Meta loss on this task batch = 2.8402e-01, Meta loss averaged over last 500 steps = 2.6619e-01, PNorm = 163.6539, GNorm = 0.2086
Meta loss on this task batch = 2.5307e-01, Meta loss averaged over last 500 steps = 2.6620e-01, PNorm = 163.6587, GNorm = 0.2432
Meta loss on this task batch = 3.2653e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 163.6629, GNorm = 0.2551
Meta loss on this task batch = 2.6622e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 163.6677, GNorm = 0.2374
Meta loss on this task batch = 2.7569e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 163.6719, GNorm = 0.2308
Meta loss on this task batch = 2.9898e-01, Meta loss averaged over last 500 steps = 2.6621e-01, PNorm = 163.6761, GNorm = 0.3125
Took 108.25064373016357 seconds to complete one epoch of meta training
Took 115.35742783546448 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485398
Epoch 758
Meta loss on this task batch = 2.6309e-01, Meta loss averaged over last 500 steps = 2.6628e-01, PNorm = 163.6810, GNorm = 0.1878
Meta loss on this task batch = 2.5832e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 163.6861, GNorm = 0.2416
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 163.6910, GNorm = 0.2366
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.6639e-01, PNorm = 163.6962, GNorm = 0.2065
Meta loss on this task batch = 2.4208e-01, Meta loss averaged over last 500 steps = 2.6637e-01, PNorm = 163.7011, GNorm = 0.2462
Meta loss on this task batch = 2.8451e-01, Meta loss averaged over last 500 steps = 2.6629e-01, PNorm = 163.7053, GNorm = 0.2657
Meta loss on this task batch = 2.2546e-01, Meta loss averaged over last 500 steps = 2.6618e-01, PNorm = 163.7104, GNorm = 0.1918
Meta loss on this task batch = 3.1466e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 163.7151, GNorm = 0.2868
Meta loss on this task batch = 2.5395e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 163.7203, GNorm = 0.2098
Meta loss on this task batch = 2.9623e-01, Meta loss averaged over last 500 steps = 2.6632e-01, PNorm = 163.7259, GNorm = 0.2717
Meta loss on this task batch = 2.5121e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 163.7317, GNorm = 0.2645
Meta loss on this task batch = 2.4807e-01, Meta loss averaged over last 500 steps = 2.6626e-01, PNorm = 163.7383, GNorm = 0.2563
Meta loss on this task batch = 2.2221e-01, Meta loss averaged over last 500 steps = 2.6611e-01, PNorm = 163.7446, GNorm = 0.2004
Meta loss on this task batch = 2.2860e-01, Meta loss averaged over last 500 steps = 2.6610e-01, PNorm = 163.7511, GNorm = 0.1971
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 163.7574, GNorm = 0.2412
Meta loss on this task batch = 2.6635e-01, Meta loss averaged over last 500 steps = 2.6624e-01, PNorm = 163.7634, GNorm = 0.2115
Meta loss on this task batch = 3.2994e-01, Meta loss averaged over last 500 steps = 2.6644e-01, PNorm = 163.7664, GNorm = 0.3439
Meta loss on this task batch = 2.4692e-01, Meta loss averaged over last 500 steps = 2.6639e-01, PNorm = 163.7695, GNorm = 0.2249
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 163.7732, GNorm = 0.2402
Took 109.10095453262329 seconds to complete one epoch of meta training
Took 115.77069425582886 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495010
Epoch 759
Meta loss on this task batch = 2.8750e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 163.7767, GNorm = 0.2397
Meta loss on this task batch = 2.7424e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 163.7807, GNorm = 0.2203
Meta loss on this task batch = 3.2436e-01, Meta loss averaged over last 500 steps = 2.6643e-01, PNorm = 163.7855, GNorm = 0.2358
Meta loss on this task batch = 2.3979e-01, Meta loss averaged over last 500 steps = 2.6641e-01, PNorm = 163.7907, GNorm = 0.2068
Meta loss on this task batch = 1.9503e-01, Meta loss averaged over last 500 steps = 2.6636e-01, PNorm = 163.7966, GNorm = 0.1846
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 2.6631e-01, PNorm = 163.8023, GNorm = 0.2157
Meta loss on this task batch = 2.7739e-01, Meta loss averaged over last 500 steps = 2.6635e-01, PNorm = 163.8080, GNorm = 0.2327
Meta loss on this task batch = 3.3064e-01, Meta loss averaged over last 500 steps = 2.6652e-01, PNorm = 163.8138, GNorm = 0.2723
Meta loss on this task batch = 2.5165e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 163.8193, GNorm = 0.2421
Meta loss on this task batch = 2.8682e-01, Meta loss averaged over last 500 steps = 2.6654e-01, PNorm = 163.8252, GNorm = 0.2282
Meta loss on this task batch = 2.6565e-01, Meta loss averaged over last 500 steps = 2.6663e-01, PNorm = 163.8312, GNorm = 0.2399
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.6669e-01, PNorm = 163.8370, GNorm = 0.2104
Meta loss on this task batch = 1.9476e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 163.8430, GNorm = 0.1916
Meta loss on this task batch = 2.7565e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 163.8484, GNorm = 0.2206
Meta loss on this task batch = 2.4096e-01, Meta loss averaged over last 500 steps = 2.6641e-01, PNorm = 163.8535, GNorm = 0.2109
Meta loss on this task batch = 2.8410e-01, Meta loss averaged over last 500 steps = 2.6648e-01, PNorm = 163.8580, GNorm = 0.2802
Meta loss on this task batch = 2.7206e-01, Meta loss averaged over last 500 steps = 2.6651e-01, PNorm = 163.8624, GNorm = 0.2166
Meta loss on this task batch = 2.4751e-01, Meta loss averaged over last 500 steps = 2.6649e-01, PNorm = 163.8668, GNorm = 0.2193
Meta loss on this task batch = 1.8017e-01, Meta loss averaged over last 500 steps = 2.6634e-01, PNorm = 163.8710, GNorm = 0.2265
Took 109.37715435028076 seconds to complete one epoch of meta training
Took 116.77322959899902 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488199
Epoch 760
Meta loss on this task batch = 1.7817e-01, Meta loss averaged over last 500 steps = 2.6618e-01, PNorm = 163.8758, GNorm = 0.1947
Meta loss on this task batch = 2.1964e-01, Meta loss averaged over last 500 steps = 2.6605e-01, PNorm = 163.8806, GNorm = 0.1833
Meta loss on this task batch = 2.7719e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.8857, GNorm = 0.2171
Meta loss on this task batch = 2.5934e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.8905, GNorm = 0.2426
Meta loss on this task batch = 2.1591e-01, Meta loss averaged over last 500 steps = 2.6584e-01, PNorm = 163.8954, GNorm = 0.1918
Meta loss on this task batch = 2.4359e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 163.9002, GNorm = 0.2164
Meta loss on this task batch = 2.9605e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 163.9051, GNorm = 0.2515
Meta loss on this task batch = 2.5480e-01, Meta loss averaged over last 500 steps = 2.6599e-01, PNorm = 163.9098, GNorm = 0.2312
Meta loss on this task batch = 2.4085e-01, Meta loss averaged over last 500 steps = 2.6595e-01, PNorm = 163.9145, GNorm = 0.2194
Meta loss on this task batch = 2.2734e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 163.9188, GNorm = 0.2235
Meta loss on this task batch = 3.1667e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 163.9229, GNorm = 0.2488
Meta loss on this task batch = 2.7104e-01, Meta loss averaged over last 500 steps = 2.6601e-01, PNorm = 163.9265, GNorm = 0.2353
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 163.9308, GNorm = 0.2368
Meta loss on this task batch = 2.5759e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 163.9338, GNorm = 0.2332
Meta loss on this task batch = 2.7048e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.9378, GNorm = 0.2273
Meta loss on this task batch = 2.9570e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 163.9413, GNorm = 0.3309
Meta loss on this task batch = 2.6305e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 163.9450, GNorm = 0.2461
Meta loss on this task batch = 2.6161e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 163.9486, GNorm = 0.2469
Meta loss on this task batch = 2.5115e-01, Meta loss averaged over last 500 steps = 2.6606e-01, PNorm = 163.9521, GNorm = 0.2467
Took 109.51937055587769 seconds to complete one epoch of meta training
Took 116.6950831413269 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504971
Epoch 761
Meta loss on this task batch = 2.4279e-01, Meta loss averaged over last 500 steps = 2.6590e-01, PNorm = 163.9560, GNorm = 0.2420
Meta loss on this task batch = 2.8588e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 163.9599, GNorm = 0.2705
Meta loss on this task batch = 2.3952e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 163.9639, GNorm = 0.2209
Meta loss on this task batch = 2.4443e-01, Meta loss averaged over last 500 steps = 2.6567e-01, PNorm = 163.9683, GNorm = 0.2267
Meta loss on this task batch = 3.1751e-01, Meta loss averaged over last 500 steps = 2.6583e-01, PNorm = 163.9731, GNorm = 0.2779
Meta loss on this task batch = 2.5107e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 163.9773, GNorm = 0.2449
Meta loss on this task batch = 2.7618e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 163.9806, GNorm = 0.2117
Meta loss on this task batch = 2.7137e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 163.9843, GNorm = 0.2107
Meta loss on this task batch = 2.5601e-01, Meta loss averaged over last 500 steps = 2.6572e-01, PNorm = 163.9881, GNorm = 0.2116
Meta loss on this task batch = 2.1668e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 163.9921, GNorm = 0.2260
Meta loss on this task batch = 3.4580e-01, Meta loss averaged over last 500 steps = 2.6600e-01, PNorm = 163.9948, GNorm = 0.2990
Meta loss on this task batch = 3.0632e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 163.9968, GNorm = 0.2364
Meta loss on this task batch = 2.2352e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 163.9994, GNorm = 0.2294
Meta loss on this task batch = 3.2874e-01, Meta loss averaged over last 500 steps = 2.6604e-01, PNorm = 164.0027, GNorm = 0.2437
Meta loss on this task batch = 2.5211e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 164.0069, GNorm = 0.2223
Meta loss on this task batch = 2.2581e-01, Meta loss averaged over last 500 steps = 2.6597e-01, PNorm = 164.0115, GNorm = 0.2138
Meta loss on this task batch = 2.3882e-01, Meta loss averaged over last 500 steps = 2.6588e-01, PNorm = 164.0162, GNorm = 0.2421
Meta loss on this task batch = 2.6170e-01, Meta loss averaged over last 500 steps = 2.6585e-01, PNorm = 164.0215, GNorm = 0.2310
Meta loss on this task batch = 3.1781e-01, Meta loss averaged over last 500 steps = 2.6598e-01, PNorm = 164.0263, GNorm = 0.2806
Took 107.83767223358154 seconds to complete one epoch of meta training
Took 115.64053773880005 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479030
Epoch 762
Meta loss on this task batch = 3.0738e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 164.0317, GNorm = 0.2520
Meta loss on this task batch = 2.6534e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 164.0370, GNorm = 0.2289
Meta loss on this task batch = 2.9228e-01, Meta loss averaged over last 500 steps = 2.6623e-01, PNorm = 164.0409, GNorm = 0.2759
Meta loss on this task batch = 2.9473e-01, Meta loss averaged over last 500 steps = 2.6625e-01, PNorm = 164.0453, GNorm = 0.2284
Meta loss on this task batch = 2.5275e-01, Meta loss averaged over last 500 steps = 2.6622e-01, PNorm = 164.0502, GNorm = 0.2007
Meta loss on this task batch = 3.2402e-01, Meta loss averaged over last 500 steps = 2.6616e-01, PNorm = 164.0550, GNorm = 0.2562
Meta loss on this task batch = 2.9169e-01, Meta loss averaged over last 500 steps = 2.6630e-01, PNorm = 164.0584, GNorm = 0.2626
Meta loss on this task batch = 2.4103e-01, Meta loss averaged over last 500 steps = 2.6612e-01, PNorm = 164.0613, GNorm = 0.2181
Meta loss on this task batch = 2.1510e-01, Meta loss averaged over last 500 steps = 2.6617e-01, PNorm = 164.0646, GNorm = 0.2070
Meta loss on this task batch = 2.3680e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 164.0676, GNorm = 0.2328
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 164.0705, GNorm = 0.2258
Meta loss on this task batch = 2.6515e-01, Meta loss averaged over last 500 steps = 2.6593e-01, PNorm = 164.0735, GNorm = 0.2415
Meta loss on this task batch = 2.9424e-01, Meta loss averaged over last 500 steps = 2.6603e-01, PNorm = 164.0770, GNorm = 0.2510
Meta loss on this task batch = 2.8727e-01, Meta loss averaged over last 500 steps = 2.6609e-01, PNorm = 164.0808, GNorm = 0.2445
Meta loss on this task batch = 2.5150e-01, Meta loss averaged over last 500 steps = 2.6608e-01, PNorm = 164.0851, GNorm = 0.2172
Meta loss on this task batch = 2.5112e-01, Meta loss averaged over last 500 steps = 2.6602e-01, PNorm = 164.0892, GNorm = 0.2503
Meta loss on this task batch = 2.5150e-01, Meta loss averaged over last 500 steps = 2.6596e-01, PNorm = 164.0931, GNorm = 0.2379
Meta loss on this task batch = 2.1217e-01, Meta loss averaged over last 500 steps = 2.6591e-01, PNorm = 164.0977, GNorm = 0.2238
Meta loss on this task batch = 2.2197e-01, Meta loss averaged over last 500 steps = 2.6581e-01, PNorm = 164.1024, GNorm = 0.3067
Took 110.7100477218628 seconds to complete one epoch of meta training
Took 117.94230461120605 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505801
Epoch 763
Meta loss on this task batch = 2.7662e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 164.1078, GNorm = 0.2301
Meta loss on this task batch = 2.3000e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 164.1134, GNorm = 0.2087
Meta loss on this task batch = 2.8447e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 164.1188, GNorm = 0.2221
Meta loss on this task batch = 2.8303e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 164.1243, GNorm = 0.2873
Meta loss on this task batch = 2.3681e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 164.1300, GNorm = 0.2086
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 164.1352, GNorm = 0.2416
Meta loss on this task batch = 2.8421e-01, Meta loss averaged over last 500 steps = 2.6570e-01, PNorm = 164.1397, GNorm = 0.2604
Meta loss on this task batch = 2.6626e-01, Meta loss averaged over last 500 steps = 2.6568e-01, PNorm = 164.1442, GNorm = 0.1970
Meta loss on this task batch = 2.8310e-01, Meta loss averaged over last 500 steps = 2.6579e-01, PNorm = 164.1484, GNorm = 0.2457
Meta loss on this task batch = 2.5582e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 164.1528, GNorm = 0.2152
Meta loss on this task batch = 2.8961e-01, Meta loss averaged over last 500 steps = 2.6577e-01, PNorm = 164.1563, GNorm = 0.2428
Meta loss on this task batch = 2.5893e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 164.1601, GNorm = 0.2302
Meta loss on this task batch = 2.3443e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 164.1638, GNorm = 0.2057
Meta loss on this task batch = 2.7979e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 164.1676, GNorm = 0.2796
Meta loss on this task batch = 2.5231e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 164.1717, GNorm = 0.2077
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 2.6566e-01, PNorm = 164.1755, GNorm = 0.2595
Meta loss on this task batch = 2.1113e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 164.1793, GNorm = 0.2121
Meta loss on this task batch = 2.5727e-01, Meta loss averaged over last 500 steps = 2.6552e-01, PNorm = 164.1831, GNorm = 0.2212
Meta loss on this task batch = 2.3111e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 164.1871, GNorm = 0.2417
Took 107.75481581687927 seconds to complete one epoch of meta training
Took 115.34946346282959 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507591
Epoch 764
Meta loss on this task batch = 2.7874e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 164.1914, GNorm = 0.2357
Meta loss on this task batch = 2.6168e-01, Meta loss averaged over last 500 steps = 2.6539e-01, PNorm = 164.1962, GNorm = 0.2566
Meta loss on this task batch = 2.6591e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 164.2010, GNorm = 0.2206
Meta loss on this task batch = 2.4049e-01, Meta loss averaged over last 500 steps = 2.6555e-01, PNorm = 164.2058, GNorm = 0.2187
Meta loss on this task batch = 2.2034e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 164.2114, GNorm = 0.2154
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.6563e-01, PNorm = 164.2177, GNorm = 0.2558
Meta loss on this task batch = 2.4347e-01, Meta loss averaged over last 500 steps = 2.6571e-01, PNorm = 164.2228, GNorm = 0.2425
Meta loss on this task batch = 2.9394e-01, Meta loss averaged over last 500 steps = 2.6573e-01, PNorm = 164.2280, GNorm = 0.2353
Meta loss on this task batch = 2.4361e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 164.2333, GNorm = 0.2212
Meta loss on this task batch = 2.5132e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 164.2381, GNorm = 0.2051
Meta loss on this task batch = 2.7082e-01, Meta loss averaged over last 500 steps = 2.6551e-01, PNorm = 164.2423, GNorm = 0.2400
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 2.6560e-01, PNorm = 164.2463, GNorm = 0.2397
Meta loss on this task batch = 3.3215e-01, Meta loss averaged over last 500 steps = 2.6575e-01, PNorm = 164.2502, GNorm = 0.3553
Meta loss on this task batch = 2.9577e-01, Meta loss averaged over last 500 steps = 2.6582e-01, PNorm = 164.2543, GNorm = 0.2512
Meta loss on this task batch = 2.4256e-01, Meta loss averaged over last 500 steps = 2.6578e-01, PNorm = 164.2582, GNorm = 0.2438
Meta loss on this task batch = 3.4054e-01, Meta loss averaged over last 500 steps = 2.6587e-01, PNorm = 164.2621, GNorm = 0.2677
Meta loss on this task batch = 2.0112e-01, Meta loss averaged over last 500 steps = 2.6565e-01, PNorm = 164.2661, GNorm = 0.2131
Meta loss on this task batch = 2.3351e-01, Meta loss averaged over last 500 steps = 2.6551e-01, PNorm = 164.2698, GNorm = 0.2271
Meta loss on this task batch = 2.4754e-01, Meta loss averaged over last 500 steps = 2.6550e-01, PNorm = 164.2741, GNorm = 0.2630
Took 107.88943076133728 seconds to complete one epoch of meta training
Took 114.98507499694824 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.512008
Epoch 765
Meta loss on this task batch = 2.3147e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 164.2785, GNorm = 0.1928
Meta loss on this task batch = 2.7677e-01, Meta loss averaged over last 500 steps = 2.6553e-01, PNorm = 164.2831, GNorm = 0.2163
Meta loss on this task batch = 2.4770e-01, Meta loss averaged over last 500 steps = 2.6540e-01, PNorm = 164.2878, GNorm = 0.2038
Meta loss on this task batch = 2.6644e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 164.2923, GNorm = 0.2206
Meta loss on this task batch = 2.0266e-01, Meta loss averaged over last 500 steps = 2.6530e-01, PNorm = 164.2974, GNorm = 0.1993
Meta loss on this task batch = 2.2582e-01, Meta loss averaged over last 500 steps = 2.6523e-01, PNorm = 164.3029, GNorm = 0.2175
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 2.6535e-01, PNorm = 164.3079, GNorm = 0.2124
Meta loss on this task batch = 2.8823e-01, Meta loss averaged over last 500 steps = 2.6548e-01, PNorm = 164.3131, GNorm = 0.2173
Meta loss on this task batch = 3.0871e-01, Meta loss averaged over last 500 steps = 2.6556e-01, PNorm = 164.3185, GNorm = 0.2146
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 2.6564e-01, PNorm = 164.3238, GNorm = 0.2513
Meta loss on this task batch = 2.3852e-01, Meta loss averaged over last 500 steps = 2.6554e-01, PNorm = 164.3294, GNorm = 0.2038
Meta loss on this task batch = 2.6572e-01, Meta loss averaged over last 500 steps = 2.6556e-01, PNorm = 164.3356, GNorm = 0.2158
Meta loss on this task batch = 2.6258e-01, Meta loss averaged over last 500 steps = 2.6547e-01, PNorm = 164.3415, GNorm = 0.2461
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.6542e-01, PNorm = 164.3480, GNorm = 0.2309
Meta loss on this task batch = 2.4028e-01, Meta loss averaged over last 500 steps = 2.6527e-01, PNorm = 164.3544, GNorm = 0.2546
Meta loss on this task batch = 2.6738e-01, Meta loss averaged over last 500 steps = 2.6529e-01, PNorm = 164.3601, GNorm = 0.2216
Meta loss on this task batch = 2.4655e-01, Meta loss averaged over last 500 steps = 2.6527e-01, PNorm = 164.3660, GNorm = 0.2186
Meta loss on this task batch = 2.4914e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 164.3720, GNorm = 0.2234
Meta loss on this task batch = 2.9103e-01, Meta loss averaged over last 500 steps = 2.6515e-01, PNorm = 164.3776, GNorm = 0.2913
Took 110.18957090377808 seconds to complete one epoch of meta training
Took 117.59478068351746 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515915
Epoch 766
Meta loss on this task batch = 2.4582e-01, Meta loss averaged over last 500 steps = 2.6516e-01, PNorm = 164.3825, GNorm = 0.2325
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 2.6522e-01, PNorm = 164.3869, GNorm = 0.2536
Meta loss on this task batch = 2.2414e-01, Meta loss averaged over last 500 steps = 2.6501e-01, PNorm = 164.3918, GNorm = 0.2258
Meta loss on this task batch = 2.1172e-01, Meta loss averaged over last 500 steps = 2.6488e-01, PNorm = 164.3973, GNorm = 0.2522
Meta loss on this task batch = 3.5227e-01, Meta loss averaged over last 500 steps = 2.6502e-01, PNorm = 164.4028, GNorm = 0.2844
Meta loss on this task batch = 2.6616e-01, Meta loss averaged over last 500 steps = 2.6507e-01, PNorm = 164.4081, GNorm = 0.2486
Meta loss on this task batch = 3.0203e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 164.4132, GNorm = 0.2663
Meta loss on this task batch = 2.2105e-01, Meta loss averaged over last 500 steps = 2.6514e-01, PNorm = 164.4181, GNorm = 0.2066
Meta loss on this task batch = 2.6173e-01, Meta loss averaged over last 500 steps = 2.6509e-01, PNorm = 164.4218, GNorm = 0.2755
Meta loss on this task batch = 3.0547e-01, Meta loss averaged over last 500 steps = 2.6518e-01, PNorm = 164.4250, GNorm = 0.2429
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 2.6505e-01, PNorm = 164.4281, GNorm = 0.2401
Meta loss on this task batch = 2.9148e-01, Meta loss averaged over last 500 steps = 2.6517e-01, PNorm = 164.4312, GNorm = 0.2120
Meta loss on this task batch = 2.1334e-01, Meta loss averaged over last 500 steps = 2.6502e-01, PNorm = 164.4345, GNorm = 0.2503
Meta loss on this task batch = 2.3085e-01, Meta loss averaged over last 500 steps = 2.6500e-01, PNorm = 164.4388, GNorm = 0.2033
Meta loss on this task batch = 2.4016e-01, Meta loss averaged over last 500 steps = 2.6494e-01, PNorm = 164.4428, GNorm = 0.2104
Meta loss on this task batch = 2.7673e-01, Meta loss averaged over last 500 steps = 2.6489e-01, PNorm = 164.4478, GNorm = 0.2713
Meta loss on this task batch = 2.3973e-01, Meta loss averaged over last 500 steps = 2.6474e-01, PNorm = 164.4522, GNorm = 0.2421
Meta loss on this task batch = 2.4805e-01, Meta loss averaged over last 500 steps = 2.6471e-01, PNorm = 164.4571, GNorm = 0.2078
Meta loss on this task batch = 3.2348e-01, Meta loss averaged over last 500 steps = 2.6485e-01, PNorm = 164.4627, GNorm = 0.5080
Took 108.42171287536621 seconds to complete one epoch of meta training
Took 115.97758293151855 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495526
Epoch 767
Meta loss on this task batch = 2.6881e-01, Meta loss averaged over last 500 steps = 2.6482e-01, PNorm = 164.4682, GNorm = 0.2395
Meta loss on this task batch = 2.3776e-01, Meta loss averaged over last 500 steps = 2.6476e-01, PNorm = 164.4736, GNorm = 0.2146
Meta loss on this task batch = 2.5694e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 164.4787, GNorm = 0.2292
Meta loss on this task batch = 2.9218e-01, Meta loss averaged over last 500 steps = 2.6483e-01, PNorm = 164.4835, GNorm = 0.2542
Meta loss on this task batch = 2.3619e-01, Meta loss averaged over last 500 steps = 2.6470e-01, PNorm = 164.4880, GNorm = 0.2206
Meta loss on this task batch = 2.6158e-01, Meta loss averaged over last 500 steps = 2.6457e-01, PNorm = 164.4922, GNorm = 0.2693
Meta loss on this task batch = 2.9943e-01, Meta loss averaged over last 500 steps = 2.6460e-01, PNorm = 164.4949, GNorm = 0.2913
Meta loss on this task batch = 3.1367e-01, Meta loss averaged over last 500 steps = 2.6470e-01, PNorm = 164.4969, GNorm = 0.3062
Meta loss on this task batch = 2.9571e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 164.4981, GNorm = 0.2633
Meta loss on this task batch = 2.4199e-01, Meta loss averaged over last 500 steps = 2.6449e-01, PNorm = 164.4996, GNorm = 0.2246
Meta loss on this task batch = 2.4537e-01, Meta loss averaged over last 500 steps = 2.6442e-01, PNorm = 164.5019, GNorm = 0.2205
Meta loss on this task batch = 2.8216e-01, Meta loss averaged over last 500 steps = 2.6444e-01, PNorm = 164.5047, GNorm = 0.2455
Meta loss on this task batch = 2.2802e-01, Meta loss averaged over last 500 steps = 2.6437e-01, PNorm = 164.5082, GNorm = 0.2214
Meta loss on this task batch = 2.8084e-01, Meta loss averaged over last 500 steps = 2.6439e-01, PNorm = 164.5126, GNorm = 0.2364
Meta loss on this task batch = 2.4288e-01, Meta loss averaged over last 500 steps = 2.6440e-01, PNorm = 164.5176, GNorm = 0.2005
Meta loss on this task batch = 2.8652e-01, Meta loss averaged over last 500 steps = 2.6448e-01, PNorm = 164.5230, GNorm = 0.2415
Meta loss on this task batch = 2.1577e-01, Meta loss averaged over last 500 steps = 2.6442e-01, PNorm = 164.5290, GNorm = 0.2428
Meta loss on this task batch = 2.8289e-01, Meta loss averaged over last 500 steps = 2.6439e-01, PNorm = 164.5348, GNorm = 0.2226
Meta loss on this task batch = 1.8174e-01, Meta loss averaged over last 500 steps = 2.6426e-01, PNorm = 164.5410, GNorm = 0.2280
Took 107.65270256996155 seconds to complete one epoch of meta training
Took 116.20601081848145 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490034
Epoch 768
Meta loss on this task batch = 2.5396e-01, Meta loss averaged over last 500 steps = 2.6429e-01, PNorm = 164.5473, GNorm = 0.2137
Meta loss on this task batch = 2.6061e-01, Meta loss averaged over last 500 steps = 2.6422e-01, PNorm = 164.5536, GNorm = 0.2224
Meta loss on this task batch = 2.8886e-01, Meta loss averaged over last 500 steps = 2.6430e-01, PNorm = 164.5597, GNorm = 0.2651
Meta loss on this task batch = 2.9632e-01, Meta loss averaged over last 500 steps = 2.6439e-01, PNorm = 164.5644, GNorm = 0.2598
Meta loss on this task batch = 2.8431e-01, Meta loss averaged over last 500 steps = 2.6449e-01, PNorm = 164.5690, GNorm = 0.2144
Meta loss on this task batch = 2.4809e-01, Meta loss averaged over last 500 steps = 2.6449e-01, PNorm = 164.5728, GNorm = 0.2528
Meta loss on this task batch = 3.0113e-01, Meta loss averaged over last 500 steps = 2.6465e-01, PNorm = 164.5764, GNorm = 0.2532
Meta loss on this task batch = 2.3665e-01, Meta loss averaged over last 500 steps = 2.6456e-01, PNorm = 164.5803, GNorm = 0.2099
Meta loss on this task batch = 3.0728e-01, Meta loss averaged over last 500 steps = 2.6468e-01, PNorm = 164.5838, GNorm = 0.2186
Meta loss on this task batch = 2.3809e-01, Meta loss averaged over last 500 steps = 2.6472e-01, PNorm = 164.5888, GNorm = 0.2175
Meta loss on this task batch = 3.2321e-01, Meta loss averaged over last 500 steps = 2.6478e-01, PNorm = 164.5930, GNorm = 0.2680
Meta loss on this task batch = 2.0793e-01, Meta loss averaged over last 500 steps = 2.6462e-01, PNorm = 164.5973, GNorm = 0.1823
Meta loss on this task batch = 2.9280e-01, Meta loss averaged over last 500 steps = 2.6471e-01, PNorm = 164.6023, GNorm = 0.2235
Meta loss on this task batch = 2.2170e-01, Meta loss averaged over last 500 steps = 2.6461e-01, PNorm = 164.6067, GNorm = 0.2136
Meta loss on this task batch = 2.6124e-01, Meta loss averaged over last 500 steps = 2.6457e-01, PNorm = 164.6115, GNorm = 0.2079
Meta loss on this task batch = 2.4990e-01, Meta loss averaged over last 500 steps = 2.6453e-01, PNorm = 164.6170, GNorm = 0.1963
Meta loss on this task batch = 2.2288e-01, Meta loss averaged over last 500 steps = 2.6452e-01, PNorm = 164.6234, GNorm = 0.2018
Meta loss on this task batch = 2.1929e-01, Meta loss averaged over last 500 steps = 2.6438e-01, PNorm = 164.6300, GNorm = 0.2125
Meta loss on this task batch = 2.2533e-01, Meta loss averaged over last 500 steps = 2.6443e-01, PNorm = 164.6369, GNorm = 0.2612
Took 110.27364158630371 seconds to complete one epoch of meta training
Took 118.27332282066345 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494274
Epoch 769
Meta loss on this task batch = 2.2070e-01, Meta loss averaged over last 500 steps = 2.6433e-01, PNorm = 164.6442, GNorm = 0.1883
Meta loss on this task batch = 2.5719e-01, Meta loss averaged over last 500 steps = 2.6428e-01, PNorm = 164.6515, GNorm = 0.2537
Meta loss on this task batch = 2.5443e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 164.6583, GNorm = 0.2321
Meta loss on this task batch = 2.1740e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 164.6645, GNorm = 0.2004
Meta loss on this task batch = 2.9726e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 164.6705, GNorm = 0.2488
Meta loss on this task batch = 2.7104e-01, Meta loss averaged over last 500 steps = 2.6406e-01, PNorm = 164.6751, GNorm = 0.2657
Meta loss on this task batch = 2.5319e-01, Meta loss averaged over last 500 steps = 2.6407e-01, PNorm = 164.6791, GNorm = 0.2373
Meta loss on this task batch = 2.6550e-01, Meta loss averaged over last 500 steps = 2.6401e-01, PNorm = 164.6820, GNorm = 0.2666
Meta loss on this task batch = 2.9230e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 164.6850, GNorm = 0.2962
Meta loss on this task batch = 2.7282e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 164.6886, GNorm = 0.2225
Meta loss on this task batch = 2.5953e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 164.6926, GNorm = 0.2977
Meta loss on this task batch = 2.8526e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 164.6971, GNorm = 0.2750
Meta loss on this task batch = 2.8950e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 164.7020, GNorm = 0.3188
Meta loss on this task batch = 2.8821e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 164.7076, GNorm = 0.2310
Meta loss on this task batch = 2.7716e-01, Meta loss averaged over last 500 steps = 2.6414e-01, PNorm = 164.7134, GNorm = 0.2564
Meta loss on this task batch = 2.2973e-01, Meta loss averaged over last 500 steps = 2.6406e-01, PNorm = 164.7205, GNorm = 0.2447
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 2.6413e-01, PNorm = 164.7268, GNorm = 0.2436
Meta loss on this task batch = 2.7905e-01, Meta loss averaged over last 500 steps = 2.6417e-01, PNorm = 164.7326, GNorm = 0.2581
Meta loss on this task batch = 2.1243e-01, Meta loss averaged over last 500 steps = 2.6402e-01, PNorm = 164.7385, GNorm = 0.2667
Took 112.81818437576294 seconds to complete one epoch of meta training
Took 120.75207138061523 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483319
Epoch 770
Meta loss on this task batch = 2.2617e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 164.7445, GNorm = 0.2610
Meta loss on this task batch = 2.6636e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 164.7494, GNorm = 0.2413
Meta loss on this task batch = 2.5333e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 164.7536, GNorm = 0.2104
Meta loss on this task batch = 2.5560e-01, Meta loss averaged over last 500 steps = 2.6385e-01, PNorm = 164.7588, GNorm = 0.2185
Meta loss on this task batch = 2.2983e-01, Meta loss averaged over last 500 steps = 2.6378e-01, PNorm = 164.7641, GNorm = 0.2161
Meta loss on this task batch = 2.6723e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 164.7699, GNorm = 0.2373
Meta loss on this task batch = 2.0827e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 164.7760, GNorm = 0.2132
Meta loss on this task batch = 3.0521e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 164.7826, GNorm = 0.2356
Meta loss on this task batch = 2.8647e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 164.7892, GNorm = 0.2052
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 164.7957, GNorm = 0.2106
Meta loss on this task batch = 2.9307e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 164.8026, GNorm = 0.2674
Meta loss on this task batch = 2.1043e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 164.8095, GNorm = 0.1890
Meta loss on this task batch = 2.3630e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 164.8156, GNorm = 0.2356
Meta loss on this task batch = 2.9376e-01, Meta loss averaged over last 500 steps = 2.6368e-01, PNorm = 164.8210, GNorm = 0.2392
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 164.8257, GNorm = 0.2998
Meta loss on this task batch = 2.6987e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 164.8302, GNorm = 0.2274
Meta loss on this task batch = 2.6414e-01, Meta loss averaged over last 500 steps = 2.6383e-01, PNorm = 164.8338, GNorm = 0.2708
Meta loss on this task batch = 2.7167e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 164.8372, GNorm = 0.2814
Meta loss on this task batch = 2.8851e-01, Meta loss averaged over last 500 steps = 2.6394e-01, PNorm = 164.8417, GNorm = 0.3113
Took 106.53595662117004 seconds to complete one epoch of meta training
Took 114.12722992897034 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489487
Epoch 771
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.6393e-01, PNorm = 164.8464, GNorm = 0.2205
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 164.8518, GNorm = 0.2329
Meta loss on this task batch = 2.4365e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 164.8573, GNorm = 0.2120
Meta loss on this task batch = 2.8813e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 164.8627, GNorm = 0.2459
Meta loss on this task batch = 2.1062e-01, Meta loss averaged over last 500 steps = 2.6377e-01, PNorm = 164.8685, GNorm = 0.1773
Meta loss on this task batch = 2.5537e-01, Meta loss averaged over last 500 steps = 2.6378e-01, PNorm = 164.8748, GNorm = 0.2882
Meta loss on this task batch = 2.8126e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 164.8803, GNorm = 0.2444
Meta loss on this task batch = 2.4001e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 164.8860, GNorm = 0.2180
Meta loss on this task batch = 2.6014e-01, Meta loss averaged over last 500 steps = 2.6385e-01, PNorm = 164.8914, GNorm = 0.2259
Meta loss on this task batch = 2.7468e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 164.8970, GNorm = 0.2456
Meta loss on this task batch = 3.1297e-01, Meta loss averaged over last 500 steps = 2.6406e-01, PNorm = 164.9019, GNorm = 0.2982
Meta loss on this task batch = 3.0204e-01, Meta loss averaged over last 500 steps = 2.6420e-01, PNorm = 164.9064, GNorm = 0.2957
Meta loss on this task batch = 2.4299e-01, Meta loss averaged over last 500 steps = 2.6411e-01, PNorm = 164.9103, GNorm = 0.1966
Meta loss on this task batch = 2.1646e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 164.9149, GNorm = 0.2036
Meta loss on this task batch = 2.3845e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 164.9196, GNorm = 0.2044
Meta loss on this task batch = 2.4135e-01, Meta loss averaged over last 500 steps = 2.6382e-01, PNorm = 164.9242, GNorm = 0.2269
Meta loss on this task batch = 2.4884e-01, Meta loss averaged over last 500 steps = 2.6387e-01, PNorm = 164.9287, GNorm = 0.2291
Meta loss on this task batch = 2.8244e-01, Meta loss averaged over last 500 steps = 2.6385e-01, PNorm = 164.9324, GNorm = 0.2530
Meta loss on this task batch = 2.6753e-01, Meta loss averaged over last 500 steps = 2.6386e-01, PNorm = 164.9356, GNorm = 0.2806
Took 107.7543077468872 seconds to complete one epoch of meta training
Took 114.97661900520325 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498854
Epoch 772
Meta loss on this task batch = 2.7189e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 164.9383, GNorm = 0.2114
Meta loss on this task batch = 2.4195e-01, Meta loss averaged over last 500 steps = 2.6389e-01, PNorm = 164.9411, GNorm = 0.2414
Meta loss on this task batch = 2.7008e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 164.9446, GNorm = 0.2228
Meta loss on this task batch = 2.6980e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 164.9481, GNorm = 0.2679
Meta loss on this task batch = 2.2826e-01, Meta loss averaged over last 500 steps = 2.6384e-01, PNorm = 164.9527, GNorm = 0.2272
Meta loss on this task batch = 2.6631e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 164.9573, GNorm = 0.1995
Meta loss on this task batch = 2.7608e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 164.9617, GNorm = 0.1940
Meta loss on this task batch = 2.2859e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 164.9666, GNorm = 0.1859
Meta loss on this task batch = 2.6106e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 164.9719, GNorm = 0.2067
Meta loss on this task batch = 3.1122e-01, Meta loss averaged over last 500 steps = 2.6375e-01, PNorm = 164.9776, GNorm = 0.3401
Meta loss on this task batch = 2.3883e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 164.9839, GNorm = 0.2271
Meta loss on this task batch = 2.7064e-01, Meta loss averaged over last 500 steps = 2.6365e-01, PNorm = 164.9896, GNorm = 0.2307
Meta loss on this task batch = 2.1743e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 164.9955, GNorm = 0.2190
Meta loss on this task batch = 3.0535e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 165.0009, GNorm = 0.2657
Meta loss on this task batch = 2.3161e-01, Meta loss averaged over last 500 steps = 2.6376e-01, PNorm = 165.0063, GNorm = 0.2543
Meta loss on this task batch = 2.4860e-01, Meta loss averaged over last 500 steps = 2.6373e-01, PNorm = 165.0108, GNorm = 0.2492
Meta loss on this task batch = 2.4567e-01, Meta loss averaged over last 500 steps = 2.6372e-01, PNorm = 165.0153, GNorm = 0.2223
Meta loss on this task batch = 2.4883e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 165.0198, GNorm = 0.2215
Meta loss on this task batch = 2.6911e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 165.0236, GNorm = 0.3038
Took 109.70124220848083 seconds to complete one epoch of meta training
Took 117.44951295852661 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506925
Epoch 773
Meta loss on this task batch = 2.5060e-01, Meta loss averaged over last 500 steps = 2.6353e-01, PNorm = 165.0279, GNorm = 0.2532
Meta loss on this task batch = 2.7035e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 165.0319, GNorm = 0.2373
Meta loss on this task batch = 2.5092e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 165.0360, GNorm = 0.2153
Meta loss on this task batch = 2.8863e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 165.0404, GNorm = 0.2675
Meta loss on this task batch = 2.1072e-01, Meta loss averaged over last 500 steps = 2.6343e-01, PNorm = 165.0454, GNorm = 0.2483
Meta loss on this task batch = 2.3945e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 165.0502, GNorm = 0.2457
Meta loss on this task batch = 2.4913e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 165.0545, GNorm = 0.2981
Meta loss on this task batch = 2.6274e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 165.0588, GNorm = 0.2475
Meta loss on this task batch = 2.2643e-01, Meta loss averaged over last 500 steps = 2.6330e-01, PNorm = 165.0628, GNorm = 0.2184
Meta loss on this task batch = 2.6078e-01, Meta loss averaged over last 500 steps = 2.6331e-01, PNorm = 165.0662, GNorm = 0.2486
Meta loss on this task batch = 3.2189e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 165.0693, GNorm = 0.2798
Meta loss on this task batch = 2.7425e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 165.0725, GNorm = 0.2089
Meta loss on this task batch = 2.1422e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 165.0761, GNorm = 0.2250
Meta loss on this task batch = 2.7411e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 165.0802, GNorm = 0.2347
Meta loss on this task batch = 3.0916e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 165.0843, GNorm = 0.2995
Meta loss on this task batch = 2.6829e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 165.0886, GNorm = 0.2658
Meta loss on this task batch = 2.3857e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 165.0926, GNorm = 0.2292
Meta loss on this task batch = 2.7355e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 165.0971, GNorm = 0.2379
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.6350e-01, PNorm = 165.1014, GNorm = 0.2438
Took 111.97749161720276 seconds to complete one epoch of meta training
Took 119.55723190307617 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510392
Epoch 774
Meta loss on this task batch = 2.8752e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 165.1061, GNorm = 0.2498
Meta loss on this task batch = 2.5029e-01, Meta loss averaged over last 500 steps = 2.6357e-01, PNorm = 165.1116, GNorm = 0.2185
Meta loss on this task batch = 2.4551e-01, Meta loss averaged over last 500 steps = 2.6363e-01, PNorm = 165.1169, GNorm = 0.2286
Meta loss on this task batch = 2.6056e-01, Meta loss averaged over last 500 steps = 2.6361e-01, PNorm = 165.1218, GNorm = 0.2339
Meta loss on this task batch = 2.4284e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 165.1265, GNorm = 0.2099
Meta loss on this task batch = 3.3350e-01, Meta loss averaged over last 500 steps = 2.6380e-01, PNorm = 165.1306, GNorm = 0.2753
Meta loss on this task batch = 3.1593e-01, Meta loss averaged over last 500 steps = 2.6391e-01, PNorm = 165.1355, GNorm = 0.2612
Meta loss on this task batch = 3.0060e-01, Meta loss averaged over last 500 steps = 2.6399e-01, PNorm = 165.1397, GNorm = 0.2398
Meta loss on this task batch = 2.6402e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 165.1430, GNorm = 0.2323
Meta loss on this task batch = 2.2546e-01, Meta loss averaged over last 500 steps = 2.6390e-01, PNorm = 165.1468, GNorm = 0.2168
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.6396e-01, PNorm = 165.1495, GNorm = 0.3007
Meta loss on this task batch = 2.8912e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 165.1526, GNorm = 0.2314
Meta loss on this task batch = 1.9721e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 165.1559, GNorm = 0.2358
Meta loss on this task batch = 2.4390e-01, Meta loss averaged over last 500 steps = 2.6404e-01, PNorm = 165.1584, GNorm = 0.2048
Meta loss on this task batch = 2.3381e-01, Meta loss averaged over last 500 steps = 2.6403e-01, PNorm = 165.1611, GNorm = 0.2593
Meta loss on this task batch = 2.8840e-01, Meta loss averaged over last 500 steps = 2.6405e-01, PNorm = 165.1642, GNorm = 0.2359
Meta loss on this task batch = 2.4264e-01, Meta loss averaged over last 500 steps = 2.6400e-01, PNorm = 165.1678, GNorm = 0.2252
Meta loss on this task batch = 2.5465e-01, Meta loss averaged over last 500 steps = 2.6397e-01, PNorm = 165.1723, GNorm = 0.2482
Meta loss on this task batch = 2.4612e-01, Meta loss averaged over last 500 steps = 2.6384e-01, PNorm = 165.1772, GNorm = 0.2555
Took 108.91716575622559 seconds to complete one epoch of meta training
Took 116.72007513046265 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510657
Epoch 775
Meta loss on this task batch = 2.9990e-01, Meta loss averaged over last 500 steps = 2.6381e-01, PNorm = 165.1820, GNorm = 0.2281
Meta loss on this task batch = 2.4027e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 165.1867, GNorm = 0.2166
Meta loss on this task batch = 2.2598e-01, Meta loss averaged over last 500 steps = 2.6366e-01, PNorm = 165.1909, GNorm = 0.2159
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 2.6347e-01, PNorm = 165.1949, GNorm = 0.2327
Meta loss on this task batch = 2.7582e-01, Meta loss averaged over last 500 steps = 2.6357e-01, PNorm = 165.1991, GNorm = 0.2530
Meta loss on this task batch = 2.5758e-01, Meta loss averaged over last 500 steps = 2.6352e-01, PNorm = 165.2032, GNorm = 0.2293
Meta loss on this task batch = 2.5579e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 165.2078, GNorm = 0.2340
Meta loss on this task batch = 2.7944e-01, Meta loss averaged over last 500 steps = 2.6354e-01, PNorm = 165.2128, GNorm = 0.2917
Meta loss on this task batch = 2.4698e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 165.2182, GNorm = 0.2330
Meta loss on this task batch = 2.7935e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 165.2238, GNorm = 0.2483
Meta loss on this task batch = 2.5360e-01, Meta loss averaged over last 500 steps = 2.6348e-01, PNorm = 165.2295, GNorm = 0.2373
Meta loss on this task batch = 2.8231e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 165.2347, GNorm = 0.2342
Meta loss on this task batch = 2.6879e-01, Meta loss averaged over last 500 steps = 2.6356e-01, PNorm = 165.2397, GNorm = 0.2413
Meta loss on this task batch = 2.8566e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 165.2439, GNorm = 0.2356
Meta loss on this task batch = 2.8823e-01, Meta loss averaged over last 500 steps = 2.6355e-01, PNorm = 165.2480, GNorm = 0.2234
Meta loss on this task batch = 2.6454e-01, Meta loss averaged over last 500 steps = 2.6351e-01, PNorm = 165.2521, GNorm = 0.2315
Meta loss on this task batch = 2.7184e-01, Meta loss averaged over last 500 steps = 2.6353e-01, PNorm = 165.2561, GNorm = 0.2446
Meta loss on this task batch = 2.4337e-01, Meta loss averaged over last 500 steps = 2.6346e-01, PNorm = 165.2600, GNorm = 0.2494
Meta loss on this task batch = 2.6925e-01, Meta loss averaged over last 500 steps = 2.6339e-01, PNorm = 165.2646, GNorm = 0.2704
Took 109.48926758766174 seconds to complete one epoch of meta training
Took 116.78973317146301 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473258
Epoch 776
Meta loss on this task batch = 2.4466e-01, Meta loss averaged over last 500 steps = 2.6349e-01, PNorm = 165.2695, GNorm = 0.2003
Meta loss on this task batch = 2.4435e-01, Meta loss averaged over last 500 steps = 2.6345e-01, PNorm = 165.2749, GNorm = 0.2492
Meta loss on this task batch = 3.4524e-01, Meta loss averaged over last 500 steps = 2.6357e-01, PNorm = 165.2789, GNorm = 0.3427
Meta loss on this task batch = 2.8150e-01, Meta loss averaged over last 500 steps = 2.6371e-01, PNorm = 165.2838, GNorm = 0.2406
Meta loss on this task batch = 2.8164e-01, Meta loss averaged over last 500 steps = 2.6369e-01, PNorm = 165.2892, GNorm = 0.2774
Meta loss on this task batch = 2.4905e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 165.2948, GNorm = 0.2166
Meta loss on this task batch = 2.7639e-01, Meta loss averaged over last 500 steps = 2.6367e-01, PNorm = 165.3003, GNorm = 0.2316
Meta loss on this task batch = 2.3744e-01, Meta loss averaged over last 500 steps = 2.6359e-01, PNorm = 165.3060, GNorm = 0.2006
Meta loss on this task batch = 2.6259e-01, Meta loss averaged over last 500 steps = 2.6350e-01, PNorm = 165.3119, GNorm = 0.2157
Meta loss on this task batch = 2.9994e-01, Meta loss averaged over last 500 steps = 2.6357e-01, PNorm = 165.3184, GNorm = 0.2719
Meta loss on this task batch = 2.1616e-01, Meta loss averaged over last 500 steps = 2.6344e-01, PNorm = 165.3246, GNorm = 0.2169
Meta loss on this task batch = 2.4404e-01, Meta loss averaged over last 500 steps = 2.6340e-01, PNorm = 165.3311, GNorm = 0.2570
Meta loss on this task batch = 2.7390e-01, Meta loss averaged over last 500 steps = 2.6337e-01, PNorm = 165.3371, GNorm = 0.2406
Meta loss on this task batch = 2.2002e-01, Meta loss averaged over last 500 steps = 2.6317e-01, PNorm = 165.3425, GNorm = 0.2140
Meta loss on this task batch = 2.3869e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 165.3477, GNorm = 0.2421
Meta loss on this task batch = 2.0975e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 165.3530, GNorm = 0.2089
Meta loss on this task batch = 2.4685e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 165.3582, GNorm = 0.2201
Meta loss on this task batch = 3.0698e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 165.3625, GNorm = 0.2508
Meta loss on this task batch = 1.9306e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 165.3670, GNorm = 0.2351
Took 112.04337215423584 seconds to complete one epoch of meta training
Took 119.81588196754456 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472118
Epoch 777
Meta loss on this task batch = 2.5555e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 165.3719, GNorm = 0.2255
Meta loss on this task batch = 3.2603e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 165.3761, GNorm = 0.2774
Meta loss on this task batch = 2.6934e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 165.3804, GNorm = 0.2300
Meta loss on this task batch = 2.9653e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 165.3843, GNorm = 0.2544
Meta loss on this task batch = 2.4470e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 165.3886, GNorm = 0.2431
Meta loss on this task batch = 2.6171e-01, Meta loss averaged over last 500 steps = 2.6305e-01, PNorm = 165.3927, GNorm = 0.2243
Meta loss on this task batch = 2.4290e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 165.3970, GNorm = 0.2332
Meta loss on this task batch = 2.5758e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 165.4017, GNorm = 0.2288
Meta loss on this task batch = 2.6910e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 165.4061, GNorm = 0.2449
Meta loss on this task batch = 2.4456e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 165.4107, GNorm = 0.2518
Meta loss on this task batch = 2.5128e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 165.4152, GNorm = 0.2433
Meta loss on this task batch = 2.5699e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 165.4194, GNorm = 0.2308
Meta loss on this task batch = 2.5478e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 165.4233, GNorm = 0.2144
Meta loss on this task batch = 2.5732e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 165.4275, GNorm = 0.2396
Meta loss on this task batch = 2.4305e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 165.4308, GNorm = 0.2149
Meta loss on this task batch = 2.5211e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 165.4342, GNorm = 0.2352
Meta loss on this task batch = 2.1834e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 165.4373, GNorm = 0.2436
Meta loss on this task batch = 2.6783e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 165.4410, GNorm = 0.2540
Meta loss on this task batch = 2.6815e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 165.4455, GNorm = 0.2876
Took 107.95233392715454 seconds to complete one epoch of meta training
Took 115.43405985832214 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485899
Epoch 778
Meta loss on this task batch = 2.7729e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 165.4504, GNorm = 0.2405
Meta loss on this task batch = 2.3644e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 165.4559, GNorm = 0.2444
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 165.4613, GNorm = 0.2931
Meta loss on this task batch = 2.4931e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 165.4658, GNorm = 0.2125
Meta loss on this task batch = 2.5976e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 165.4703, GNorm = 0.2336
Meta loss on this task batch = 2.3374e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 165.4757, GNorm = 0.2061
Meta loss on this task batch = 3.0290e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 165.4797, GNorm = 0.2928
Meta loss on this task batch = 1.7339e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 165.4845, GNorm = 0.1948
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 165.4895, GNorm = 0.2359
Meta loss on this task batch = 2.2079e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 165.4949, GNorm = 0.2014
Meta loss on this task batch = 2.6326e-01, Meta loss averaged over last 500 steps = 2.6245e-01, PNorm = 165.5003, GNorm = 0.2124
Meta loss on this task batch = 2.8789e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 165.5055, GNorm = 0.2458
Meta loss on this task batch = 3.0009e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 165.5109, GNorm = 0.2623
Meta loss on this task batch = 2.6764e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 165.5160, GNorm = 0.2145
Meta loss on this task batch = 2.8704e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 165.5209, GNorm = 0.2114
Meta loss on this task batch = 2.6443e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 165.5254, GNorm = 0.2480
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 165.5296, GNorm = 0.2054
Meta loss on this task batch = 2.5721e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 165.5334, GNorm = 0.2489
Meta loss on this task batch = 2.2563e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 165.5378, GNorm = 0.2419
Took 107.87772870063782 seconds to complete one epoch of meta training
Took 115.15840601921082 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502706
Epoch 779
Meta loss on this task batch = 2.6305e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 165.5419, GNorm = 0.2327
Meta loss on this task batch = 3.0113e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 165.5456, GNorm = 0.2505
Meta loss on this task batch = 2.8474e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 165.5502, GNorm = 0.1922
Meta loss on this task batch = 2.6225e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 165.5548, GNorm = 0.2409
Meta loss on this task batch = 2.1514e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 165.5593, GNorm = 0.2147
Meta loss on this task batch = 2.7713e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 165.5649, GNorm = 0.2542
Meta loss on this task batch = 2.8300e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 165.5702, GNorm = 0.2433
Meta loss on this task batch = 2.6910e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 165.5748, GNorm = 0.2286
Meta loss on this task batch = 1.8400e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 165.5793, GNorm = 0.2156
Meta loss on this task batch = 2.3893e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 165.5833, GNorm = 0.2253
Meta loss on this task batch = 2.9537e-01, Meta loss averaged over last 500 steps = 2.6247e-01, PNorm = 165.5861, GNorm = 0.2766
Meta loss on this task batch = 2.2449e-01, Meta loss averaged over last 500 steps = 2.6241e-01, PNorm = 165.5883, GNorm = 0.2289
Meta loss on this task batch = 3.1375e-01, Meta loss averaged over last 500 steps = 2.6247e-01, PNorm = 165.5897, GNorm = 0.2577
Meta loss on this task batch = 2.3028e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 165.5919, GNorm = 0.2110
Meta loss on this task batch = 2.7963e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 165.5945, GNorm = 0.2157
Meta loss on this task batch = 2.7115e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 165.5978, GNorm = 0.2060
Meta loss on this task batch = 2.5164e-01, Meta loss averaged over last 500 steps = 2.6250e-01, PNorm = 165.6015, GNorm = 0.2022
Meta loss on this task batch = 2.8198e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 165.6059, GNorm = 0.2313
Meta loss on this task batch = 3.0879e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 165.6106, GNorm = 0.2565
Took 111.5589005947113 seconds to complete one epoch of meta training
Took 118.24169135093689 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501838
Epoch 780
Meta loss on this task batch = 2.6850e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 165.6153, GNorm = 0.2175
Meta loss on this task batch = 2.6232e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 165.6205, GNorm = 0.2297
Meta loss on this task batch = 2.2932e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 165.6264, GNorm = 0.2036
Meta loss on this task batch = 2.6550e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 165.6330, GNorm = 0.2558
Meta loss on this task batch = 3.4500e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 165.6394, GNorm = 0.3096
Meta loss on this task batch = 2.4922e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 165.6445, GNorm = 0.2248
Meta loss on this task batch = 3.2066e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 165.6490, GNorm = 0.2668
Meta loss on this task batch = 2.3596e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 165.6541, GNorm = 0.2222
Meta loss on this task batch = 2.4506e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 165.6583, GNorm = 0.2405
Meta loss on this task batch = 2.4964e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 165.6624, GNorm = 0.2249
Meta loss on this task batch = 2.8353e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 165.6667, GNorm = 0.2352
Meta loss on this task batch = 2.5619e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 165.6708, GNorm = 0.2333
Meta loss on this task batch = 2.9229e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 165.6740, GNorm = 0.2813
Meta loss on this task batch = 2.4508e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 165.6774, GNorm = 0.2015
Meta loss on this task batch = 2.2754e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 165.6814, GNorm = 0.2044
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 165.6864, GNorm = 0.2489
Meta loss on this task batch = 2.5585e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 165.6920, GNorm = 0.2440
Meta loss on this task batch = 2.3741e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 165.6975, GNorm = 0.2043
Meta loss on this task batch = 2.9593e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 165.7026, GNorm = 0.2827
Took 107.8206160068512 seconds to complete one epoch of meta training
Took 114.10755228996277 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501294
Epoch 781
Meta loss on this task batch = 2.8345e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 165.7073, GNorm = 0.2354
Meta loss on this task batch = 2.7623e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 165.7120, GNorm = 0.2452
Meta loss on this task batch = 2.8902e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 165.7169, GNorm = 0.2223
Meta loss on this task batch = 3.2378e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 165.7216, GNorm = 0.2882
Meta loss on this task batch = 1.8737e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 165.7271, GNorm = 0.1967
Meta loss on this task batch = 2.2178e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 165.7330, GNorm = 0.2080
Meta loss on this task batch = 2.6333e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 165.7383, GNorm = 0.3091
Meta loss on this task batch = 1.8178e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 165.7438, GNorm = 0.2072
Meta loss on this task batch = 2.1998e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 165.7504, GNorm = 0.2401
Meta loss on this task batch = 2.6116e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 165.7563, GNorm = 0.2402
Meta loss on this task batch = 2.6705e-01, Meta loss averaged over last 500 steps = 2.6225e-01, PNorm = 165.7617, GNorm = 0.2293
Meta loss on this task batch = 2.3951e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 165.7670, GNorm = 0.2269
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 165.7712, GNorm = 0.2396
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 2.6213e-01, PNorm = 165.7744, GNorm = 0.2386
Meta loss on this task batch = 2.3952e-01, Meta loss averaged over last 500 steps = 2.6200e-01, PNorm = 165.7770, GNorm = 0.2408
Meta loss on this task batch = 2.4821e-01, Meta loss averaged over last 500 steps = 2.6204e-01, PNorm = 165.7805, GNorm = 0.2744
Meta loss on this task batch = 3.0149e-01, Meta loss averaged over last 500 steps = 2.6210e-01, PNorm = 165.7842, GNorm = 0.2770
Meta loss on this task batch = 3.0978e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 165.7869, GNorm = 0.2865
Meta loss on this task batch = 3.0055e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 165.7898, GNorm = 0.3550
Took 111.31923913955688 seconds to complete one epoch of meta training
Took 118.52617168426514 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492830
Epoch 782
Meta loss on this task batch = 2.7088e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 165.7927, GNorm = 0.2569
Meta loss on this task batch = 2.9398e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 165.7961, GNorm = 0.2251
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 165.8004, GNorm = 0.2656
Meta loss on this task batch = 3.0730e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 165.8048, GNorm = 0.2407
Meta loss on this task batch = 2.5158e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 165.8100, GNorm = 0.2324
Meta loss on this task batch = 2.7304e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 165.8152, GNorm = 0.2183
Meta loss on this task batch = 2.6940e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 165.8212, GNorm = 0.2157
Meta loss on this task batch = 2.7133e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 165.8272, GNorm = 0.2072
Meta loss on this task batch = 2.6920e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 165.8329, GNorm = 0.2451
Meta loss on this task batch = 2.4107e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 165.8386, GNorm = 0.2088
Meta loss on this task batch = 2.9947e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 165.8437, GNorm = 0.2250
Meta loss on this task batch = 1.9014e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 165.8489, GNorm = 0.1953
Meta loss on this task batch = 2.5449e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 165.8538, GNorm = 0.2396
Meta loss on this task batch = 2.3137e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 165.8592, GNorm = 0.2101
Meta loss on this task batch = 2.2954e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 165.8648, GNorm = 0.1898
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 165.8702, GNorm = 0.2291
Meta loss on this task batch = 2.3816e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 165.8749, GNorm = 0.2018
Meta loss on this task batch = 2.5799e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 165.8791, GNorm = 0.2074
Meta loss on this task batch = 3.2889e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 165.8825, GNorm = 0.3059
Took 109.64469075202942 seconds to complete one epoch of meta training
Took 117.73876595497131 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476843
Epoch 783
Meta loss on this task batch = 1.9432e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 165.8861, GNorm = 0.2689
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 165.8898, GNorm = 0.1922
Meta loss on this task batch = 2.2497e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 165.8941, GNorm = 0.2527
Meta loss on this task batch = 2.4365e-01, Meta loss averaged over last 500 steps = 2.6198e-01, PNorm = 165.8982, GNorm = 0.1977
Meta loss on this task batch = 3.3952e-01, Meta loss averaged over last 500 steps = 2.6204e-01, PNorm = 165.9024, GNorm = 0.2625
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 2.6210e-01, PNorm = 165.9066, GNorm = 0.2499
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 165.9108, GNorm = 0.2463
Meta loss on this task batch = 2.9190e-01, Meta loss averaged over last 500 steps = 2.6212e-01, PNorm = 165.9147, GNorm = 0.2767
Meta loss on this task batch = 2.6136e-01, Meta loss averaged over last 500 steps = 2.6210e-01, PNorm = 165.9191, GNorm = 0.2554
Meta loss on this task batch = 2.5219e-01, Meta loss averaged over last 500 steps = 2.6208e-01, PNorm = 165.9234, GNorm = 0.2148
Meta loss on this task batch = 2.6080e-01, Meta loss averaged over last 500 steps = 2.6212e-01, PNorm = 165.9271, GNorm = 0.2373
Meta loss on this task batch = 2.8896e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 165.9301, GNorm = 0.2505
Meta loss on this task batch = 2.8836e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 165.9339, GNorm = 0.2735
Meta loss on this task batch = 2.5047e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 165.9379, GNorm = 0.2239
Meta loss on this task batch = 2.5420e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 165.9415, GNorm = 0.2212
Meta loss on this task batch = 2.9946e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 165.9457, GNorm = 0.2987
Meta loss on this task batch = 2.7474e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 165.9494, GNorm = 0.2459
Meta loss on this task batch = 2.7059e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 165.9520, GNorm = 0.2761
Meta loss on this task batch = 2.7906e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 165.9548, GNorm = 0.2607
Took 113.26100468635559 seconds to complete one epoch of meta training
Took 120.20675897598267 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487553
Epoch 784
Meta loss on this task batch = 2.3590e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 165.9578, GNorm = 0.2573
Meta loss on this task batch = 2.5268e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 165.9612, GNorm = 0.2144
Meta loss on this task batch = 3.1963e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 165.9654, GNorm = 0.2938
Meta loss on this task batch = 2.2893e-01, Meta loss averaged over last 500 steps = 2.6210e-01, PNorm = 165.9700, GNorm = 0.2302
Meta loss on this task batch = 2.3723e-01, Meta loss averaged over last 500 steps = 2.6202e-01, PNorm = 165.9744, GNorm = 0.2010
Meta loss on this task batch = 2.6586e-01, Meta loss averaged over last 500 steps = 2.6196e-01, PNorm = 165.9790, GNorm = 0.2228
Meta loss on this task batch = 2.4495e-01, Meta loss averaged over last 500 steps = 2.6192e-01, PNorm = 165.9842, GNorm = 0.2224
Meta loss on this task batch = 2.5556e-01, Meta loss averaged over last 500 steps = 2.6192e-01, PNorm = 165.9897, GNorm = 0.2419
Meta loss on this task batch = 2.2582e-01, Meta loss averaged over last 500 steps = 2.6180e-01, PNorm = 165.9955, GNorm = 0.2071
Meta loss on this task batch = 2.3408e-01, Meta loss averaged over last 500 steps = 2.6171e-01, PNorm = 166.0012, GNorm = 0.2052
Meta loss on this task batch = 3.2816e-01, Meta loss averaged over last 500 steps = 2.6189e-01, PNorm = 166.0067, GNorm = 0.2695
Meta loss on this task batch = 2.5785e-01, Meta loss averaged over last 500 steps = 2.6183e-01, PNorm = 166.0118, GNorm = 0.2535
Meta loss on this task batch = 3.0842e-01, Meta loss averaged over last 500 steps = 2.6200e-01, PNorm = 166.0162, GNorm = 0.2659
Meta loss on this task batch = 2.9856e-01, Meta loss averaged over last 500 steps = 2.6197e-01, PNorm = 166.0201, GNorm = 0.3187
Meta loss on this task batch = 3.1078e-01, Meta loss averaged over last 500 steps = 2.6208e-01, PNorm = 166.0230, GNorm = 0.2881
Meta loss on this task batch = 2.4415e-01, Meta loss averaged over last 500 steps = 2.6198e-01, PNorm = 166.0264, GNorm = 0.2548
Meta loss on this task batch = 2.9759e-01, Meta loss averaged over last 500 steps = 2.6207e-01, PNorm = 166.0295, GNorm = 0.2785
Meta loss on this task batch = 2.0502e-01, Meta loss averaged over last 500 steps = 2.6198e-01, PNorm = 166.0330, GNorm = 0.1943
Meta loss on this task batch = 3.0492e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 166.0370, GNorm = 0.3125
Took 114.06887650489807 seconds to complete one epoch of meta training
Took 120.80411124229431 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514325
Epoch 785
Meta loss on this task batch = 2.7773e-01, Meta loss averaged over last 500 steps = 2.6225e-01, PNorm = 166.0410, GNorm = 0.2423
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 166.0449, GNorm = 0.2451
Meta loss on this task batch = 3.0938e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 166.0481, GNorm = 0.2588
Meta loss on this task batch = 2.6535e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 166.0516, GNorm = 0.2355
Meta loss on this task batch = 2.4976e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 166.0552, GNorm = 0.2397
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.6213e-01, PNorm = 166.0596, GNorm = 0.2137
Meta loss on this task batch = 2.7150e-01, Meta loss averaged over last 500 steps = 2.6210e-01, PNorm = 166.0642, GNorm = 0.2535
Meta loss on this task batch = 2.1661e-01, Meta loss averaged over last 500 steps = 2.6198e-01, PNorm = 166.0695, GNorm = 0.2004
Meta loss on this task batch = 2.3213e-01, Meta loss averaged over last 500 steps = 2.6180e-01, PNorm = 166.0749, GNorm = 0.2246
Meta loss on this task batch = 2.6213e-01, Meta loss averaged over last 500 steps = 2.6184e-01, PNorm = 166.0808, GNorm = 0.2155
Meta loss on this task batch = 2.3506e-01, Meta loss averaged over last 500 steps = 2.6192e-01, PNorm = 166.0866, GNorm = 0.2048
Meta loss on this task batch = 2.4783e-01, Meta loss averaged over last 500 steps = 2.6187e-01, PNorm = 166.0923, GNorm = 0.2179
Meta loss on this task batch = 2.6174e-01, Meta loss averaged over last 500 steps = 2.6184e-01, PNorm = 166.0985, GNorm = 0.1944
Meta loss on this task batch = 1.9489e-01, Meta loss averaged over last 500 steps = 2.6157e-01, PNorm = 166.1049, GNorm = 0.2046
Meta loss on this task batch = 2.9835e-01, Meta loss averaged over last 500 steps = 2.6166e-01, PNorm = 166.1109, GNorm = 0.2788
Meta loss on this task batch = 2.3479e-01, Meta loss averaged over last 500 steps = 2.6156e-01, PNorm = 166.1169, GNorm = 0.2149
Meta loss on this task batch = 2.9707e-01, Meta loss averaged over last 500 steps = 2.6162e-01, PNorm = 166.1216, GNorm = 0.2694
Meta loss on this task batch = 3.1380e-01, Meta loss averaged over last 500 steps = 2.6170e-01, PNorm = 166.1252, GNorm = 0.2939
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.6190e-01, PNorm = 166.1282, GNorm = 0.2990
Took 109.23701357841492 seconds to complete one epoch of meta training
Took 116.90129566192627 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505597
Epoch 786
Meta loss on this task batch = 2.3979e-01, Meta loss averaged over last 500 steps = 2.6183e-01, PNorm = 166.1317, GNorm = 0.2269
Meta loss on this task batch = 2.5191e-01, Meta loss averaged over last 500 steps = 2.6185e-01, PNorm = 166.1357, GNorm = 0.2325
Meta loss on this task batch = 2.7599e-01, Meta loss averaged over last 500 steps = 2.6184e-01, PNorm = 166.1396, GNorm = 0.2286
Meta loss on this task batch = 2.5169e-01, Meta loss averaged over last 500 steps = 2.6180e-01, PNorm = 166.1438, GNorm = 0.2170
Meta loss on this task batch = 2.3729e-01, Meta loss averaged over last 500 steps = 2.6178e-01, PNorm = 166.1480, GNorm = 0.2153
Meta loss on this task batch = 2.9415e-01, Meta loss averaged over last 500 steps = 2.6200e-01, PNorm = 166.1523, GNorm = 0.2170
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 166.1568, GNorm = 0.2205
Meta loss on this task batch = 2.1371e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 166.1618, GNorm = 0.2084
Meta loss on this task batch = 2.9522e-01, Meta loss averaged over last 500 steps = 2.6222e-01, PNorm = 166.1671, GNorm = 0.2475
Meta loss on this task batch = 2.7244e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 166.1725, GNorm = 0.2584
Meta loss on this task batch = 3.0706e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 166.1775, GNorm = 0.2428
Meta loss on this task batch = 2.8452e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 166.1827, GNorm = 0.2350
Meta loss on this task batch = 2.8003e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 166.1873, GNorm = 0.2828
Meta loss on this task batch = 3.2436e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.1919, GNorm = 0.2383
Meta loss on this task batch = 1.9320e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 166.1967, GNorm = 0.2271
Meta loss on this task batch = 2.9637e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 166.2008, GNorm = 0.2590
Meta loss on this task batch = 2.5674e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 166.2056, GNorm = 0.2391
Meta loss on this task batch = 2.1388e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 166.2104, GNorm = 0.2055
Meta loss on this task batch = 2.4882e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 166.2150, GNorm = 0.2900
Took 113.98650074005127 seconds to complete one epoch of meta training
Took 121.01831197738647 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508130
Epoch 787
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 166.2195, GNorm = 0.2301
Meta loss on this task batch = 2.8384e-01, Meta loss averaged over last 500 steps = 2.6245e-01, PNorm = 166.2244, GNorm = 0.2263
Meta loss on this task batch = 2.9070e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 166.2291, GNorm = 0.2498
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 166.2343, GNorm = 0.2139
Meta loss on this task batch = 3.0960e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 166.2385, GNorm = 0.2353
Meta loss on this task batch = 2.1817e-01, Meta loss averaged over last 500 steps = 2.6245e-01, PNorm = 166.2429, GNorm = 0.2117
Meta loss on this task batch = 2.3266e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 166.2479, GNorm = 0.2313
Meta loss on this task batch = 2.8352e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 166.2527, GNorm = 0.2472
Meta loss on this task batch = 3.1529e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 166.2559, GNorm = 0.2699
Meta loss on this task batch = 2.7741e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 166.2586, GNorm = 0.2629
Meta loss on this task batch = 2.0646e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 166.2620, GNorm = 0.1823
Meta loss on this task batch = 3.4143e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 166.2641, GNorm = 0.2789
Meta loss on this task batch = 2.2892e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 166.2661, GNorm = 0.1993
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 166.2684, GNorm = 0.2464
Meta loss on this task batch = 2.6613e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 166.2707, GNorm = 0.2557
Meta loss on this task batch = 3.1308e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 166.2741, GNorm = 0.2845
Meta loss on this task batch = 2.7302e-01, Meta loss averaged over last 500 steps = 2.6259e-01, PNorm = 166.2776, GNorm = 0.2562
Meta loss on this task batch = 2.0524e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 166.2822, GNorm = 0.1793
Meta loss on this task batch = 2.7460e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 166.2862, GNorm = 0.2922
Took 111.00938034057617 seconds to complete one epoch of meta training
Took 118.5800952911377 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502897
Epoch 788
Meta loss on this task batch = 3.3395e-01, Meta loss averaged over last 500 steps = 2.6250e-01, PNorm = 166.2912, GNorm = 0.2408
Meta loss on this task batch = 2.8163e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 166.2963, GNorm = 0.2213
Meta loss on this task batch = 2.5888e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.3017, GNorm = 0.2217
Meta loss on this task batch = 2.5006e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 166.3070, GNorm = 0.2278
Meta loss on this task batch = 2.2063e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 166.3125, GNorm = 0.2149
Meta loss on this task batch = 2.1329e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 166.3177, GNorm = 0.1938
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 166.3226, GNorm = 0.2314
Meta loss on this task batch = 2.4631e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.3276, GNorm = 0.1809
Meta loss on this task batch = 2.6588e-01, Meta loss averaged over last 500 steps = 2.6222e-01, PNorm = 166.3336, GNorm = 0.2518
Meta loss on this task batch = 2.6320e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 166.3390, GNorm = 0.2165
Meta loss on this task batch = 2.9129e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 166.3444, GNorm = 0.2214
Meta loss on this task batch = 2.2549e-01, Meta loss averaged over last 500 steps = 2.6204e-01, PNorm = 166.3497, GNorm = 0.2191
Meta loss on this task batch = 2.8968e-01, Meta loss averaged over last 500 steps = 2.6204e-01, PNorm = 166.3553, GNorm = 0.2390
Meta loss on this task batch = 1.9719e-01, Meta loss averaged over last 500 steps = 2.6195e-01, PNorm = 166.3599, GNorm = 0.2174
Meta loss on this task batch = 3.0397e-01, Meta loss averaged over last 500 steps = 2.6213e-01, PNorm = 166.3643, GNorm = 0.2570
Meta loss on this task batch = 2.7142e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 166.3682, GNorm = 0.2420
Meta loss on this task batch = 2.5459e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 166.3716, GNorm = 0.2746
Meta loss on this task batch = 2.6391e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 166.3743, GNorm = 0.2457
Meta loss on this task batch = 2.8886e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 166.3770, GNorm = 0.2712
Took 110.72052717208862 seconds to complete one epoch of meta training
Took 118.14432525634766 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508964
Epoch 789
Meta loss on this task batch = 2.5241e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 166.3802, GNorm = 0.1904
Meta loss on this task batch = 3.3407e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.3834, GNorm = 0.2599
Meta loss on this task batch = 2.1662e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 166.3870, GNorm = 0.2377
Meta loss on this task batch = 2.4559e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 166.3915, GNorm = 0.2164
Meta loss on this task batch = 2.4820e-01, Meta loss averaged over last 500 steps = 2.6227e-01, PNorm = 166.3963, GNorm = 0.2039
Meta loss on this task batch = 2.4924e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 166.4015, GNorm = 0.2136
Meta loss on this task batch = 2.5736e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.4059, GNorm = 0.2327
Meta loss on this task batch = 2.6129e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 166.4104, GNorm = 0.2125
Meta loss on this task batch = 2.8762e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 166.4154, GNorm = 0.2538
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 166.4206, GNorm = 0.2162
Meta loss on this task batch = 2.6971e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 166.4258, GNorm = 0.2202
Meta loss on this task batch = 2.4436e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.4312, GNorm = 0.2168
Meta loss on this task batch = 2.2469e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 166.4360, GNorm = 0.1907
Meta loss on this task batch = 2.6141e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 166.4409, GNorm = 0.2617
Meta loss on this task batch = 2.3616e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 166.4458, GNorm = 0.1960
Meta loss on this task batch = 2.8086e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 166.4500, GNorm = 0.2410
Meta loss on this task batch = 2.9239e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 166.4541, GNorm = 0.2478
Meta loss on this task batch = 2.3414e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 166.4590, GNorm = 0.2245
Meta loss on this task batch = 2.8781e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 166.4643, GNorm = 0.2770
Took 109.66374087333679 seconds to complete one epoch of meta training
Took 117.40037775039673 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521600
Epoch 790
Meta loss on this task batch = 2.3192e-01, Meta loss averaged over last 500 steps = 2.6207e-01, PNorm = 166.4698, GNorm = 0.2071
Meta loss on this task batch = 2.0397e-01, Meta loss averaged over last 500 steps = 2.6198e-01, PNorm = 166.4757, GNorm = 0.1776
Meta loss on this task batch = 2.8377e-01, Meta loss averaged over last 500 steps = 2.6195e-01, PNorm = 166.4816, GNorm = 0.3211
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.6208e-01, PNorm = 166.4865, GNorm = 0.2328
Meta loss on this task batch = 2.3923e-01, Meta loss averaged over last 500 steps = 2.6204e-01, PNorm = 166.4922, GNorm = 0.2230
Meta loss on this task batch = 3.4765e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.4973, GNorm = 0.2966
Meta loss on this task batch = 2.8369e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 166.5024, GNorm = 0.2294
Meta loss on this task batch = 2.3888e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 166.5077, GNorm = 0.2216
Meta loss on this task batch = 2.7640e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 166.5129, GNorm = 0.2208
Meta loss on this task batch = 2.5700e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 166.5181, GNorm = 0.2177
Meta loss on this task batch = 2.7376e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 166.5224, GNorm = 0.2597
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 166.5272, GNorm = 0.2569
Meta loss on this task batch = 2.3301e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 166.5312, GNorm = 0.2357
Meta loss on this task batch = 2.6974e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 166.5361, GNorm = 0.2448
Meta loss on this task batch = 2.6015e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 166.5421, GNorm = 0.2407
Meta loss on this task batch = 2.2182e-01, Meta loss averaged over last 500 steps = 2.6227e-01, PNorm = 166.5489, GNorm = 0.2252
Meta loss on this task batch = 2.7526e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.5557, GNorm = 0.2585
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 166.5614, GNorm = 0.2443
Meta loss on this task batch = 2.8745e-01, Meta loss averaged over last 500 steps = 2.6220e-01, PNorm = 166.5665, GNorm = 0.3575
Took 107.84102249145508 seconds to complete one epoch of meta training
Took 115.9932656288147 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.520242
Epoch 791
Meta loss on this task batch = 3.0009e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 166.5711, GNorm = 0.2775
Meta loss on this task batch = 2.2453e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 166.5759, GNorm = 0.2390
Meta loss on this task batch = 2.9134e-01, Meta loss averaged over last 500 steps = 2.6207e-01, PNorm = 166.5804, GNorm = 0.2341
Meta loss on this task batch = 2.9453e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 166.5844, GNorm = 0.2625
Meta loss on this task batch = 2.4478e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 166.5887, GNorm = 0.2054
Meta loss on this task batch = 2.6530e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 166.5934, GNorm = 0.2394
Meta loss on this task batch = 2.7214e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 166.5985, GNorm = 0.2358
Meta loss on this task batch = 2.9159e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 166.6031, GNorm = 0.3003
Meta loss on this task batch = 2.4713e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 166.6078, GNorm = 0.2443
Meta loss on this task batch = 2.3780e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 166.6124, GNorm = 0.2481
Meta loss on this task batch = 3.2920e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.6163, GNorm = 0.2657
Meta loss on this task batch = 2.5153e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 166.6204, GNorm = 0.2162
Meta loss on this task batch = 2.1679e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 166.6252, GNorm = 0.2085
Meta loss on this task batch = 2.7185e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 166.6298, GNorm = 0.2210
Meta loss on this task batch = 2.6454e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 166.6340, GNorm = 0.2293
Meta loss on this task batch = 2.4848e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 166.6388, GNorm = 0.2254
Meta loss on this task batch = 2.3589e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 166.6437, GNorm = 0.2171
Meta loss on this task batch = 2.5906e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 166.6491, GNorm = 0.2251
Meta loss on this task batch = 2.6716e-01, Meta loss averaged over last 500 steps = 2.6236e-01, PNorm = 166.6554, GNorm = 0.2533
Took 111.11144471168518 seconds to complete one epoch of meta training
Took 119.01514077186584 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.524341
Epoch 792
Meta loss on this task batch = 3.1838e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 166.6603, GNorm = 0.2626
Meta loss on this task batch = 2.8563e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 166.6650, GNorm = 0.2602
Meta loss on this task batch = 2.7659e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 166.6700, GNorm = 0.2283
Meta loss on this task batch = 2.8146e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.6748, GNorm = 0.2294
Meta loss on this task batch = 3.2899e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 166.6791, GNorm = 0.2724
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 166.6830, GNorm = 0.2643
Meta loss on this task batch = 3.0300e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 166.6865, GNorm = 0.2677
Meta loss on this task batch = 2.6558e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 166.6900, GNorm = 0.2364
Meta loss on this task batch = 2.8315e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 166.6932, GNorm = 0.2476
Meta loss on this task batch = 2.3183e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 166.6971, GNorm = 0.2079
Meta loss on this task batch = 2.5957e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 166.7013, GNorm = 0.2222
Meta loss on this task batch = 2.2197e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 166.7063, GNorm = 0.2067
Meta loss on this task batch = 3.0195e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 166.7113, GNorm = 0.2771
Meta loss on this task batch = 2.0952e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 166.7165, GNorm = 0.2058
Meta loss on this task batch = 2.4582e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 166.7211, GNorm = 0.2381
Meta loss on this task batch = 2.4868e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 166.7253, GNorm = 0.2374
Meta loss on this task batch = 2.2141e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 166.7298, GNorm = 0.2475
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 166.7335, GNorm = 0.2433
Meta loss on this task batch = 2.4149e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 166.7369, GNorm = 0.2546
Took 112.40039110183716 seconds to complete one epoch of meta training
Took 120.09386968612671 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.526036
Epoch 793
Meta loss on this task batch = 3.1935e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 166.7395, GNorm = 0.2528
Meta loss on this task batch = 2.4195e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 166.7425, GNorm = 0.2344
Meta loss on this task batch = 2.3513e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 166.7455, GNorm = 0.2352
Meta loss on this task batch = 2.7933e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 166.7484, GNorm = 0.2591
Meta loss on this task batch = 2.8663e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 166.7514, GNorm = 0.2324
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 166.7537, GNorm = 0.2271
Meta loss on this task batch = 2.3916e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.7563, GNorm = 0.2272
Meta loss on this task batch = 2.5070e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 166.7596, GNorm = 0.1999
Meta loss on this task batch = 2.4722e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.7631, GNorm = 0.2148
Meta loss on this task batch = 2.9843e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 166.7673, GNorm = 0.3123
Meta loss on this task batch = 2.4198e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 166.7723, GNorm = 0.2110
Meta loss on this task batch = 2.4010e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 166.7775, GNorm = 0.2139
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 166.7827, GNorm = 0.2176
Meta loss on this task batch = 2.7425e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 166.7878, GNorm = 0.2252
Meta loss on this task batch = 2.6186e-01, Meta loss averaged over last 500 steps = 2.6239e-01, PNorm = 166.7926, GNorm = 0.2259
Meta loss on this task batch = 3.4491e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 166.7971, GNorm = 0.2468
Meta loss on this task batch = 2.8787e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 166.8014, GNorm = 0.2426
Meta loss on this task batch = 2.9844e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 166.8056, GNorm = 0.3171
Meta loss on this task batch = 2.2956e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 166.8105, GNorm = 0.2716
Took 109.54499983787537 seconds to complete one epoch of meta training
Took 116.31993317604065 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.526434
Epoch 794
Meta loss on this task batch = 2.2269e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 166.8162, GNorm = 0.2240
Meta loss on this task batch = 2.6310e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 166.8214, GNorm = 0.2436
Meta loss on this task batch = 2.3870e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 166.8267, GNorm = 0.2052
Meta loss on this task batch = 2.8782e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 166.8323, GNorm = 0.2264
Meta loss on this task batch = 2.5766e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 166.8379, GNorm = 0.2337
Meta loss on this task batch = 2.7087e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 166.8439, GNorm = 0.2157
Meta loss on this task batch = 2.8330e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 166.8495, GNorm = 0.2263
Meta loss on this task batch = 2.4054e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 166.8552, GNorm = 0.2101
Meta loss on this task batch = 1.9730e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 166.8607, GNorm = 0.2201
Meta loss on this task batch = 2.7877e-01, Meta loss averaged over last 500 steps = 2.6262e-01, PNorm = 166.8645, GNorm = 0.2899
Meta loss on this task batch = 2.5722e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 166.8689, GNorm = 0.2162
Meta loss on this task batch = 2.0269e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 166.8735, GNorm = 0.1986
Meta loss on this task batch = 3.4907e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 166.8776, GNorm = 0.2729
Meta loss on this task batch = 3.3767e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 166.8812, GNorm = 0.2722
Meta loss on this task batch = 2.9737e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 166.8851, GNorm = 0.2921
Meta loss on this task batch = 2.5071e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 166.8898, GNorm = 0.2186
Meta loss on this task batch = 2.1737e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 166.8945, GNorm = 0.2256
Meta loss on this task batch = 2.6665e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 166.8990, GNorm = 0.2311
Meta loss on this task batch = 2.1271e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 166.9040, GNorm = 0.2297
Took 108.22642970085144 seconds to complete one epoch of meta training
Took 115.28498482704163 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.535010
Found better MAML checkpoint after meta validation, saving now
Epoch 795
Meta loss on this task batch = 2.7334e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 166.9091, GNorm = 0.2079
Meta loss on this task batch = 2.3278e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 166.9144, GNorm = 0.2517
Meta loss on this task batch = 2.8076e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 166.9190, GNorm = 0.2401
Meta loss on this task batch = 2.4403e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 166.9237, GNorm = 0.2119
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 166.9282, GNorm = 0.2418
Meta loss on this task batch = 2.0435e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 166.9335, GNorm = 0.1989
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 166.9384, GNorm = 0.2233
Meta loss on this task batch = 2.4212e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 166.9432, GNorm = 0.2125
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 166.9479, GNorm = 0.2134
Meta loss on this task batch = 2.7781e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 166.9531, GNorm = 0.2344
Meta loss on this task batch = 2.1598e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 166.9582, GNorm = 0.1892
Meta loss on this task batch = 2.3677e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 166.9636, GNorm = 0.1904
Meta loss on this task batch = 2.3315e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 166.9691, GNorm = 0.2138
Meta loss on this task batch = 2.9481e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 166.9739, GNorm = 0.2593
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 166.9781, GNorm = 0.1911
Meta loss on this task batch = 2.5899e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 166.9823, GNorm = 0.2122
Meta loss on this task batch = 3.2389e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 166.9853, GNorm = 0.3125
Meta loss on this task batch = 2.5386e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 166.9880, GNorm = 0.2377
Meta loss on this task batch = 2.6203e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 166.9912, GNorm = 0.2681
Took 106.45093894004822 seconds to complete one epoch of meta training
Took 113.85825943946838 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497694
Epoch 796
Meta loss on this task batch = 3.0274e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 166.9950, GNorm = 0.2269
Meta loss on this task batch = 2.4469e-01, Meta loss averaged over last 500 steps = 2.6265e-01, PNorm = 166.9997, GNorm = 0.2358
Meta loss on this task batch = 2.2746e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 167.0050, GNorm = 0.1885
Meta loss on this task batch = 2.1239e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 167.0102, GNorm = 0.2075
Meta loss on this task batch = 2.8550e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 167.0158, GNorm = 0.2321
Meta loss on this task batch = 2.6523e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 167.0217, GNorm = 0.2250
Meta loss on this task batch = 2.8855e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 167.0279, GNorm = 0.2137
Meta loss on this task batch = 2.2689e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 167.0339, GNorm = 0.2052
Meta loss on this task batch = 3.3471e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 167.0394, GNorm = 0.2388
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 167.0445, GNorm = 0.2303
Meta loss on this task batch = 2.1246e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 167.0501, GNorm = 0.2207
Meta loss on this task batch = 2.5194e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 167.0552, GNorm = 0.2387
Meta loss on this task batch = 2.6041e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 167.0600, GNorm = 0.2401
Meta loss on this task batch = 2.2698e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 167.0647, GNorm = 0.2078
Meta loss on this task batch = 2.7190e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 167.0699, GNorm = 0.2327
Meta loss on this task batch = 2.7673e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 167.0748, GNorm = 0.2736
Meta loss on this task batch = 2.3512e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 167.0799, GNorm = 0.2219
Meta loss on this task batch = 2.4472e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 167.0846, GNorm = 0.2371
Meta loss on this task batch = 2.7467e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 167.0899, GNorm = 0.2736
Took 109.76766753196716 seconds to complete one epoch of meta training
Took 117.99086856842041 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507905
Epoch 797
Meta loss on this task batch = 2.7484e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 167.0947, GNorm = 0.2594
Meta loss on this task batch = 2.7022e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 167.0997, GNorm = 0.2201
Meta loss on this task batch = 2.6271e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 167.1043, GNorm = 0.2314
Meta loss on this task batch = 3.0281e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 167.1090, GNorm = 0.2478
Meta loss on this task batch = 3.0147e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 167.1135, GNorm = 0.2359
Meta loss on this task batch = 2.4000e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 167.1178, GNorm = 0.1979
Meta loss on this task batch = 2.5447e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 167.1228, GNorm = 0.2602
Meta loss on this task batch = 2.7302e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 167.1282, GNorm = 0.1988
Meta loss on this task batch = 2.3546e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 167.1355, GNorm = 0.2365
Meta loss on this task batch = 2.5820e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 167.1421, GNorm = 0.2206
Meta loss on this task batch = 2.4369e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 167.1484, GNorm = 0.2071
Meta loss on this task batch = 2.5227e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 167.1554, GNorm = 0.2282
Meta loss on this task batch = 2.5285e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 167.1625, GNorm = 0.2328
Meta loss on this task batch = 2.4245e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 167.1693, GNorm = 0.2090
Meta loss on this task batch = 2.1960e-01, Meta loss averaged over last 500 steps = 2.6259e-01, PNorm = 167.1761, GNorm = 0.2173
Meta loss on this task batch = 2.3558e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 167.1833, GNorm = 0.1914
Meta loss on this task batch = 2.7587e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 167.1899, GNorm = 0.2527
Meta loss on this task batch = 3.1421e-01, Meta loss averaged over last 500 steps = 2.6247e-01, PNorm = 167.1947, GNorm = 0.2876
Meta loss on this task batch = 2.9204e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 167.1987, GNorm = 0.2705
Took 110.36576676368713 seconds to complete one epoch of meta training
Took 118.75450015068054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493312
Epoch 798
Meta loss on this task batch = 3.1048e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 167.2023, GNorm = 0.2900
Meta loss on this task batch = 2.5780e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 167.2060, GNorm = 0.2491
Meta loss on this task batch = 2.9330e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 167.2095, GNorm = 0.2516
Meta loss on this task batch = 2.4633e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 167.2129, GNorm = 0.2223
Meta loss on this task batch = 2.7911e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 167.2168, GNorm = 0.2453
Meta loss on this task batch = 2.4083e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 167.2207, GNorm = 0.2396
Meta loss on this task batch = 2.6353e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 167.2252, GNorm = 0.1977
Meta loss on this task batch = 2.5557e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 167.2305, GNorm = 0.2065
Meta loss on this task batch = 2.7815e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 167.2359, GNorm = 0.1978
Meta loss on this task batch = 2.5888e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 167.2417, GNorm = 0.2117
Meta loss on this task batch = 2.6709e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 167.2481, GNorm = 0.2500
Meta loss on this task batch = 2.0890e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 167.2554, GNorm = 0.2138
Meta loss on this task batch = 2.6002e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 167.2628, GNorm = 0.2436
Meta loss on this task batch = 2.1867e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 167.2703, GNorm = 0.2152
Meta loss on this task batch = 3.0844e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 167.2776, GNorm = 0.2761
Meta loss on this task batch = 2.5261e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 167.2846, GNorm = 0.2347
Meta loss on this task batch = 2.6584e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 167.2924, GNorm = 0.2279
Meta loss on this task batch = 2.6246e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 167.3001, GNorm = 0.2452
Meta loss on this task batch = 2.6103e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 167.3066, GNorm = 0.3167
Took 111.89506483078003 seconds to complete one epoch of meta training
Took 119.09224462509155 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517275
Epoch 799
Meta loss on this task batch = 2.5650e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 167.3128, GNorm = 0.2207
Meta loss on this task batch = 2.9824e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 167.3187, GNorm = 0.2502
Meta loss on this task batch = 2.6232e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 167.3242, GNorm = 0.2079
Meta loss on this task batch = 2.6929e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 167.3297, GNorm = 0.2453
Meta loss on this task batch = 2.3105e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 167.3355, GNorm = 0.2282
Meta loss on this task batch = 2.5430e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 167.3412, GNorm = 0.2209
Meta loss on this task batch = 3.0063e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 167.3460, GNorm = 0.2321
Meta loss on this task batch = 2.0268e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 167.3519, GNorm = 0.2295
Meta loss on this task batch = 2.2874e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 167.3578, GNorm = 0.2254
Meta loss on this task batch = 2.7650e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 167.3636, GNorm = 0.2209
Meta loss on this task batch = 2.9806e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 167.3686, GNorm = 0.2474
Meta loss on this task batch = 2.5116e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 167.3742, GNorm = 0.2049
Meta loss on this task batch = 2.6887e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 167.3801, GNorm = 0.1899
Meta loss on this task batch = 2.5215e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 167.3865, GNorm = 0.2071
Meta loss on this task batch = 3.0341e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 167.3926, GNorm = 0.2662
Meta loss on this task batch = 2.0131e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 167.3992, GNorm = 0.1915
Meta loss on this task batch = 2.2017e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 167.4058, GNorm = 0.1734
Meta loss on this task batch = 2.7146e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 167.4126, GNorm = 0.2202
Meta loss on this task batch = 2.4331e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 167.4193, GNorm = 0.2487
Took 106.1337616443634 seconds to complete one epoch of meta training
Took 113.32174777984619 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495168
Epoch 800
Meta loss on this task batch = 2.7882e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 167.4255, GNorm = 0.2416
Meta loss on this task batch = 2.3243e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 167.4317, GNorm = 0.2263
Meta loss on this task batch = 2.2715e-01, Meta loss averaged over last 500 steps = 2.6266e-01, PNorm = 167.4382, GNorm = 0.2372
Meta loss on this task batch = 2.7995e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 167.4445, GNorm = 0.2470
Meta loss on this task batch = 2.7456e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 167.4504, GNorm = 0.2717
Meta loss on this task batch = 2.6794e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 167.4560, GNorm = 0.2256
Meta loss on this task batch = 2.6871e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 167.4611, GNorm = 0.2572
Meta loss on this task batch = 2.3344e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 167.4663, GNorm = 0.2655
Meta loss on this task batch = 2.6641e-01, Meta loss averaged over last 500 steps = 2.6272e-01, PNorm = 167.4712, GNorm = 0.2494
Meta loss on this task batch = 2.6401e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 167.4769, GNorm = 0.2828
Meta loss on this task batch = 2.1322e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 167.4825, GNorm = 0.1992
Meta loss on this task batch = 2.2460e-01, Meta loss averaged over last 500 steps = 2.6245e-01, PNorm = 167.4882, GNorm = 0.2315
Meta loss on this task batch = 3.1204e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 167.4934, GNorm = 0.2524
Meta loss on this task batch = 2.3702e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 167.4983, GNorm = 0.2258
Meta loss on this task batch = 3.0882e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 167.5037, GNorm = 0.2592
Meta loss on this task batch = 2.3862e-01, Meta loss averaged over last 500 steps = 2.6243e-01, PNorm = 167.5096, GNorm = 0.2344
Meta loss on this task batch = 2.4439e-01, Meta loss averaged over last 500 steps = 2.6236e-01, PNorm = 167.5155, GNorm = 0.2199
Meta loss on this task batch = 2.4271e-01, Meta loss averaged over last 500 steps = 2.6227e-01, PNorm = 167.5214, GNorm = 0.2080
Meta loss on this task batch = 2.6294e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 167.5264, GNorm = 0.2278
Took 108.1884126663208 seconds to complete one epoch of meta training
Took 115.68190455436707 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490210
Epoch 801
Meta loss on this task batch = 2.0052e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 167.5318, GNorm = 0.1943
Meta loss on this task batch = 3.0880e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 167.5370, GNorm = 0.2826
Meta loss on this task batch = 2.7745e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 167.5429, GNorm = 0.2139
Meta loss on this task batch = 2.7970e-01, Meta loss averaged over last 500 steps = 2.6251e-01, PNorm = 167.5481, GNorm = 0.2310
Meta loss on this task batch = 3.0139e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 167.5519, GNorm = 0.2620
Meta loss on this task batch = 2.4942e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 167.5555, GNorm = 0.2137
Meta loss on this task batch = 2.5565e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 167.5596, GNorm = 0.2421
Meta loss on this task batch = 2.4886e-01, Meta loss averaged over last 500 steps = 2.6254e-01, PNorm = 167.5644, GNorm = 0.2197
Meta loss on this task batch = 2.3527e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 167.5695, GNorm = 0.2155
Meta loss on this task batch = 2.3445e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 167.5744, GNorm = 0.2422
Meta loss on this task batch = 2.5300e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 167.5795, GNorm = 0.2993
Meta loss on this task batch = 2.7928e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 167.5850, GNorm = 0.2338
Meta loss on this task batch = 2.3281e-01, Meta loss averaged over last 500 steps = 2.6248e-01, PNorm = 167.5907, GNorm = 0.2074
Meta loss on this task batch = 2.9920e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 167.5950, GNorm = 0.3015
Meta loss on this task batch = 2.7383e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 167.5970, GNorm = 0.2884
Meta loss on this task batch = 2.4148e-01, Meta loss averaged over last 500 steps = 2.6250e-01, PNorm = 167.5986, GNorm = 0.2239
Meta loss on this task batch = 2.0138e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 167.6002, GNorm = 0.2091
Meta loss on this task batch = 2.5999e-01, Meta loss averaged over last 500 steps = 2.6235e-01, PNorm = 167.6023, GNorm = 0.2354
Meta loss on this task batch = 3.1383e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 167.6042, GNorm = 0.2894
Took 109.30551981925964 seconds to complete one epoch of meta training
Took 116.90151906013489 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.516660
Epoch 802
Meta loss on this task batch = 2.8250e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 167.6063, GNorm = 0.2059
Meta loss on this task batch = 2.8131e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 167.6082, GNorm = 0.2605
Meta loss on this task batch = 2.4121e-01, Meta loss averaged over last 500 steps = 2.6238e-01, PNorm = 167.6103, GNorm = 0.2565
Meta loss on this task batch = 2.6633e-01, Meta loss averaged over last 500 steps = 2.6236e-01, PNorm = 167.6127, GNorm = 0.2880
Meta loss on this task batch = 2.2429e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 167.6167, GNorm = 0.2087
Meta loss on this task batch = 2.5476e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 167.6212, GNorm = 0.2088
Meta loss on this task batch = 2.9530e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 167.6260, GNorm = 0.2387
Meta loss on this task batch = 2.9657e-01, Meta loss averaged over last 500 steps = 2.6250e-01, PNorm = 167.6307, GNorm = 0.2380
Meta loss on this task batch = 2.3814e-01, Meta loss averaged over last 500 steps = 2.6229e-01, PNorm = 167.6357, GNorm = 0.2126
Meta loss on this task batch = 2.2280e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 167.6411, GNorm = 0.1989
Meta loss on this task batch = 2.2073e-01, Meta loss averaged over last 500 steps = 2.6205e-01, PNorm = 167.6463, GNorm = 0.1977
Meta loss on this task batch = 2.5315e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 167.6522, GNorm = 0.2298
Meta loss on this task batch = 2.4700e-01, Meta loss averaged over last 500 steps = 2.6200e-01, PNorm = 167.6587, GNorm = 0.2214
Meta loss on this task batch = 3.1397e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 167.6652, GNorm = 0.2452
Meta loss on this task batch = 2.6799e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 167.6717, GNorm = 0.2355
Meta loss on this task batch = 2.2321e-01, Meta loss averaged over last 500 steps = 2.6201e-01, PNorm = 167.6778, GNorm = 0.2172
Meta loss on this task batch = 2.4423e-01, Meta loss averaged over last 500 steps = 2.6207e-01, PNorm = 167.6834, GNorm = 0.2376
Meta loss on this task batch = 1.9572e-01, Meta loss averaged over last 500 steps = 2.6197e-01, PNorm = 167.6893, GNorm = 0.2060
Meta loss on this task batch = 3.1894e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 167.6935, GNorm = 0.3952
Took 116.68982744216919 seconds to complete one epoch of meta training
Took 123.69946336746216 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514180
Epoch 803
Meta loss on this task batch = 2.6585e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 167.6976, GNorm = 0.2188
Meta loss on this task batch = 2.3696e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 167.7011, GNorm = 0.2697
Meta loss on this task batch = 2.0244e-01, Meta loss averaged over last 500 steps = 2.6213e-01, PNorm = 167.7059, GNorm = 0.2546
Meta loss on this task batch = 2.6255e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 167.7106, GNorm = 0.2278
Meta loss on this task batch = 3.0023e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 167.7152, GNorm = 0.2505
Meta loss on this task batch = 2.8237e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 167.7203, GNorm = 0.2159
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 167.7252, GNorm = 0.2198
Meta loss on this task batch = 2.8138e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 167.7300, GNorm = 0.2542
Meta loss on this task batch = 2.8965e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 167.7346, GNorm = 0.2080
Meta loss on this task batch = 2.3889e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 167.7393, GNorm = 0.2116
Meta loss on this task batch = 2.5759e-01, Meta loss averaged over last 500 steps = 2.6219e-01, PNorm = 167.7442, GNorm = 0.2234
Meta loss on this task batch = 2.4604e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 167.7492, GNorm = 0.2376
Meta loss on this task batch = 2.3795e-01, Meta loss averaged over last 500 steps = 2.6215e-01, PNorm = 167.7544, GNorm = 0.2412
Meta loss on this task batch = 2.9563e-01, Meta loss averaged over last 500 steps = 2.6223e-01, PNorm = 167.7585, GNorm = 0.2553
Meta loss on this task batch = 2.7036e-01, Meta loss averaged over last 500 steps = 2.6223e-01, PNorm = 167.7625, GNorm = 0.2359
Meta loss on this task batch = 3.1760e-01, Meta loss averaged over last 500 steps = 2.6238e-01, PNorm = 167.7654, GNorm = 0.2948
Meta loss on this task batch = 2.4889e-01, Meta loss averaged over last 500 steps = 2.6237e-01, PNorm = 167.7687, GNorm = 0.2289
Meta loss on this task batch = 2.9117e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 167.7723, GNorm = 0.2538
Meta loss on this task batch = 2.3756e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 167.7765, GNorm = 0.3057
Took 110.23403978347778 seconds to complete one epoch of meta training
Took 117.35817098617554 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499171
Epoch 804
Meta loss on this task batch = 2.6718e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 167.7811, GNorm = 0.2074
Meta loss on this task batch = 2.7526e-01, Meta loss averaged over last 500 steps = 2.6249e-01, PNorm = 167.7857, GNorm = 0.2337
Meta loss on this task batch = 2.3616e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 167.7912, GNorm = 0.2180
Meta loss on this task batch = 2.6494e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 167.7953, GNorm = 0.3037
Meta loss on this task batch = 3.0761e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 167.7989, GNorm = 0.2599
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 167.8027, GNorm = 0.2345
Meta loss on this task batch = 2.7993e-01, Meta loss averaged over last 500 steps = 2.6261e-01, PNorm = 167.8078, GNorm = 0.3069
Meta loss on this task batch = 2.7455e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 167.8130, GNorm = 0.2139
Meta loss on this task batch = 2.9120e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 167.8187, GNorm = 0.3177
Meta loss on this task batch = 2.4836e-01, Meta loss averaged over last 500 steps = 2.6263e-01, PNorm = 167.8243, GNorm = 0.2001
Meta loss on this task batch = 2.8899e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 167.8297, GNorm = 0.2288
Meta loss on this task batch = 2.6645e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 167.8356, GNorm = 0.2310
Meta loss on this task batch = 2.7549e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 167.8409, GNorm = 0.2655
Meta loss on this task batch = 2.6709e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 167.8454, GNorm = 0.3113
Meta loss on this task batch = 2.7094e-01, Meta loss averaged over last 500 steps = 2.6287e-01, PNorm = 167.8497, GNorm = 0.2129
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 167.8548, GNorm = 0.2770
Meta loss on this task batch = 2.2286e-01, Meta loss averaged over last 500 steps = 2.6285e-01, PNorm = 167.8596, GNorm = 0.1959
Meta loss on this task batch = 2.7401e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 167.8640, GNorm = 0.2396
Meta loss on this task batch = 3.3311e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 167.8674, GNorm = 0.2678
Took 112.94301152229309 seconds to complete one epoch of meta training
Took 120.08474564552307 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504136
Epoch 805
Meta loss on this task batch = 2.8188e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 167.8710, GNorm = 0.2383
Meta loss on this task batch = 2.5190e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 167.8747, GNorm = 0.2495
Meta loss on this task batch = 2.5959e-01, Meta loss averaged over last 500 steps = 2.6283e-01, PNorm = 167.8790, GNorm = 0.2033
Meta loss on this task batch = 2.5853e-01, Meta loss averaged over last 500 steps = 2.6286e-01, PNorm = 167.8837, GNorm = 0.2060
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 167.8882, GNorm = 0.2702
Meta loss on this task batch = 2.3332e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 167.8933, GNorm = 0.2333
Meta loss on this task batch = 2.9797e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 167.8992, GNorm = 0.3037
Meta loss on this task batch = 2.8298e-01, Meta loss averaged over last 500 steps = 2.6297e-01, PNorm = 167.9055, GNorm = 0.2432
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 167.9114, GNorm = 0.2559
Meta loss on this task batch = 2.8112e-01, Meta loss averaged over last 500 steps = 2.6295e-01, PNorm = 167.9176, GNorm = 0.2285
Meta loss on this task batch = 2.8044e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 167.9227, GNorm = 0.2305
Meta loss on this task batch = 2.7497e-01, Meta loss averaged over last 500 steps = 2.6308e-01, PNorm = 167.9276, GNorm = 0.2325
Meta loss on this task batch = 2.4522e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 167.9332, GNorm = 0.2158
Meta loss on this task batch = 2.4508e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 167.9388, GNorm = 0.2147
Meta loss on this task batch = 2.6579e-01, Meta loss averaged over last 500 steps = 2.6312e-01, PNorm = 167.9442, GNorm = 0.2443
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 2.6316e-01, PNorm = 167.9494, GNorm = 0.2277
Meta loss on this task batch = 2.2844e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 167.9541, GNorm = 0.2271
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 167.9582, GNorm = 0.2356
Meta loss on this task batch = 2.3828e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 167.9620, GNorm = 0.2543
Took 106.30967664718628 seconds to complete one epoch of meta training
Took 114.60927987098694 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494809
Epoch 806
Meta loss on this task batch = 2.5666e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 167.9660, GNorm = 0.2111
Meta loss on this task batch = 2.8632e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 167.9707, GNorm = 0.2457
Meta loss on this task batch = 2.4781e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 167.9752, GNorm = 0.2329
Meta loss on this task batch = 2.4116e-01, Meta loss averaged over last 500 steps = 2.6299e-01, PNorm = 167.9800, GNorm = 0.2102
Meta loss on this task batch = 2.9246e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 167.9845, GNorm = 0.2319
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 167.9891, GNorm = 0.2538
Meta loss on this task batch = 2.4447e-01, Meta loss averaged over last 500 steps = 2.6296e-01, PNorm = 167.9940, GNorm = 0.2138
Meta loss on this task batch = 2.9151e-01, Meta loss averaged over last 500 steps = 2.6302e-01, PNorm = 167.9996, GNorm = 0.2273
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 2.6315e-01, PNorm = 168.0051, GNorm = 0.2076
Meta loss on this task batch = 2.5891e-01, Meta loss averaged over last 500 steps = 2.6314e-01, PNorm = 168.0111, GNorm = 0.2045
Meta loss on this task batch = 2.9312e-01, Meta loss averaged over last 500 steps = 2.6304e-01, PNorm = 168.0167, GNorm = 0.2408
Meta loss on this task batch = 2.3326e-01, Meta loss averaged over last 500 steps = 2.6301e-01, PNorm = 168.0227, GNorm = 0.2127
Meta loss on this task batch = 2.8291e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 168.0285, GNorm = 0.2003
Meta loss on this task batch = 2.3856e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 168.0343, GNorm = 0.1978
Meta loss on this task batch = 2.5292e-01, Meta loss averaged over last 500 steps = 2.6295e-01, PNorm = 168.0399, GNorm = 0.2399
Meta loss on this task batch = 2.1245e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 168.0451, GNorm = 0.2109
Meta loss on this task batch = 2.4760e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 168.0500, GNorm = 0.2476
Meta loss on this task batch = 2.4520e-01, Meta loss averaged over last 500 steps = 2.6278e-01, PNorm = 168.0546, GNorm = 0.2002
Meta loss on this task batch = 2.5056e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 168.0599, GNorm = 0.2982
Took 112.57558226585388 seconds to complete one epoch of meta training
Took 120.48415946960449 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498675
Epoch 807
Meta loss on this task batch = 2.4462e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 168.0657, GNorm = 0.2313
Meta loss on this task batch = 2.6129e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 168.0712, GNorm = 0.2360
Meta loss on this task batch = 2.2479e-01, Meta loss averaged over last 500 steps = 2.6274e-01, PNorm = 168.0765, GNorm = 0.2045
Meta loss on this task batch = 2.5448e-01, Meta loss averaged over last 500 steps = 2.6273e-01, PNorm = 168.0823, GNorm = 0.2440
Meta loss on this task batch = 3.1125e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 168.0873, GNorm = 0.2405
Meta loss on this task batch = 2.7641e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 168.0910, GNorm = 0.2399
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.6281e-01, PNorm = 168.0948, GNorm = 0.2280
Meta loss on this task batch = 2.1496e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 168.0982, GNorm = 0.2602
Meta loss on this task batch = 2.2422e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 168.1014, GNorm = 0.2101
Meta loss on this task batch = 2.5592e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 168.1045, GNorm = 0.2287
Meta loss on this task batch = 2.5047e-01, Meta loss averaged over last 500 steps = 2.6255e-01, PNorm = 168.1082, GNorm = 0.2448
Meta loss on this task batch = 3.0416e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 168.1115, GNorm = 0.2513
Meta loss on this task batch = 2.4338e-01, Meta loss averaged over last 500 steps = 2.6268e-01, PNorm = 168.1151, GNorm = 0.2173
Meta loss on this task batch = 2.8914e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 168.1183, GNorm = 0.2341
Meta loss on this task batch = 2.7494e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 168.1217, GNorm = 0.2025
Meta loss on this task batch = 2.7440e-01, Meta loss averaged over last 500 steps = 2.6303e-01, PNorm = 168.1255, GNorm = 0.2474
Meta loss on this task batch = 2.1044e-01, Meta loss averaged over last 500 steps = 2.6291e-01, PNorm = 168.1289, GNorm = 0.2196
Meta loss on this task batch = 2.5617e-01, Meta loss averaged over last 500 steps = 2.6295e-01, PNorm = 168.1327, GNorm = 0.2456
Meta loss on this task batch = 3.2857e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 168.1358, GNorm = 0.4425
Took 107.0351014137268 seconds to complete one epoch of meta training
Took 114.89714741706848 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508355
Epoch 808
Meta loss on this task batch = 2.7984e-01, Meta loss averaged over last 500 steps = 2.6306e-01, PNorm = 168.1399, GNorm = 0.2562
Meta loss on this task batch = 2.5994e-01, Meta loss averaged over last 500 steps = 2.6310e-01, PNorm = 168.1446, GNorm = 0.2301
Meta loss on this task batch = 2.4317e-01, Meta loss averaged over last 500 steps = 2.6309e-01, PNorm = 168.1500, GNorm = 0.2182
Meta loss on this task batch = 2.5777e-01, Meta loss averaged over last 500 steps = 2.6300e-01, PNorm = 168.1555, GNorm = 0.2370
Meta loss on this task batch = 2.7625e-01, Meta loss averaged over last 500 steps = 2.6293e-01, PNorm = 168.1596, GNorm = 0.2931
Meta loss on this task batch = 2.7948e-01, Meta loss averaged over last 500 steps = 2.6289e-01, PNorm = 168.1648, GNorm = 0.2253
Meta loss on this task batch = 2.6607e-01, Meta loss averaged over last 500 steps = 2.6288e-01, PNorm = 168.1704, GNorm = 0.2343
Meta loss on this task batch = 2.3567e-01, Meta loss averaged over last 500 steps = 2.6276e-01, PNorm = 168.1752, GNorm = 0.2177
Meta loss on this task batch = 2.8754e-01, Meta loss averaged over last 500 steps = 2.6282e-01, PNorm = 168.1796, GNorm = 0.2630
Meta loss on this task batch = 2.3054e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 168.1832, GNorm = 0.2035
Meta loss on this task batch = 2.6940e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 168.1869, GNorm = 0.2388
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 168.1902, GNorm = 0.2289
Meta loss on this task batch = 2.8051e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 168.1939, GNorm = 0.2330
Meta loss on this task batch = 2.3523e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 168.1975, GNorm = 0.2121
Meta loss on this task batch = 2.4018e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 168.2014, GNorm = 0.2124
Meta loss on this task batch = 2.9531e-01, Meta loss averaged over last 500 steps = 2.6269e-01, PNorm = 168.2052, GNorm = 0.2298
Meta loss on this task batch = 2.3649e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 168.2096, GNorm = 0.2228
Meta loss on this task batch = 2.6402e-01, Meta loss averaged over last 500 steps = 2.6271e-01, PNorm = 168.2138, GNorm = 0.2395
Meta loss on this task batch = 2.7412e-01, Meta loss averaged over last 500 steps = 2.6275e-01, PNorm = 168.2169, GNorm = 0.2958
Took 111.47880864143372 seconds to complete one epoch of meta training
Took 119.0356912612915 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479966
Epoch 809
Meta loss on this task batch = 2.4317e-01, Meta loss averaged over last 500 steps = 2.6277e-01, PNorm = 168.2202, GNorm = 0.2165
Meta loss on this task batch = 2.4232e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 168.2236, GNorm = 0.1945
Meta loss on this task batch = 2.7628e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 168.2274, GNorm = 0.2583
Meta loss on this task batch = 2.6660e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 168.2317, GNorm = 0.2475
Meta loss on this task batch = 2.2703e-01, Meta loss averaged over last 500 steps = 2.6284e-01, PNorm = 168.2360, GNorm = 0.2192
Meta loss on this task batch = 2.4251e-01, Meta loss averaged over last 500 steps = 2.6267e-01, PNorm = 168.2406, GNorm = 0.1925
Meta loss on this task batch = 2.6055e-01, Meta loss averaged over last 500 steps = 2.6280e-01, PNorm = 168.2450, GNorm = 0.2163
Meta loss on this task batch = 2.5235e-01, Meta loss averaged over last 500 steps = 2.6279e-01, PNorm = 168.2497, GNorm = 0.2119
Meta loss on this task batch = 2.8382e-01, Meta loss averaged over last 500 steps = 2.6290e-01, PNorm = 168.2554, GNorm = 0.2730
Meta loss on this task batch = 2.5183e-01, Meta loss averaged over last 500 steps = 2.6292e-01, PNorm = 168.2614, GNorm = 0.2652
Meta loss on this task batch = 2.2793e-01, Meta loss averaged over last 500 steps = 2.6270e-01, PNorm = 168.2680, GNorm = 0.2193
Meta loss on this task batch = 1.8349e-01, Meta loss averaged over last 500 steps = 2.6253e-01, PNorm = 168.2747, GNorm = 0.1989
Meta loss on this task batch = 2.8253e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 168.2810, GNorm = 0.2448
Meta loss on this task batch = 2.5866e-01, Meta loss averaged over last 500 steps = 2.6252e-01, PNorm = 168.2870, GNorm = 0.1873
Meta loss on this task batch = 2.9126e-01, Meta loss averaged over last 500 steps = 2.6258e-01, PNorm = 168.2929, GNorm = 0.2804
Meta loss on this task batch = 2.8373e-01, Meta loss averaged over last 500 steps = 2.6264e-01, PNorm = 168.2985, GNorm = 0.2428
Meta loss on this task batch = 2.4246e-01, Meta loss averaged over last 500 steps = 2.6260e-01, PNorm = 168.3033, GNorm = 0.2479
Meta loss on this task batch = 2.7274e-01, Meta loss averaged over last 500 steps = 2.6257e-01, PNorm = 168.3084, GNorm = 0.2247
Meta loss on this task batch = 2.3385e-01, Meta loss averaged over last 500 steps = 2.6246e-01, PNorm = 168.3131, GNorm = 0.3155
Took 110.29516339302063 seconds to complete one epoch of meta training
Took 117.86695337295532 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491984
Epoch 810
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 2.6247e-01, PNorm = 168.3171, GNorm = 0.2443
Meta loss on this task batch = 2.9748e-01, Meta loss averaged over last 500 steps = 2.6256e-01, PNorm = 168.3204, GNorm = 0.2675
Meta loss on this task batch = 2.2216e-01, Meta loss averaged over last 500 steps = 2.6240e-01, PNorm = 168.3235, GNorm = 0.2056
Meta loss on this task batch = 3.0640e-01, Meta loss averaged over last 500 steps = 2.6247e-01, PNorm = 168.3261, GNorm = 0.2478
Meta loss on this task batch = 2.4574e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 168.3289, GNorm = 0.2231
Meta loss on this task batch = 2.3168e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 168.3316, GNorm = 0.2275
Meta loss on this task batch = 2.2604e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 168.3344, GNorm = 0.1860
Meta loss on this task batch = 2.4097e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 168.3382, GNorm = 0.2357
Meta loss on this task batch = 2.5972e-01, Meta loss averaged over last 500 steps = 2.6216e-01, PNorm = 168.3421, GNorm = 0.2290
Meta loss on this task batch = 2.8050e-01, Meta loss averaged over last 500 steps = 2.6226e-01, PNorm = 168.3462, GNorm = 0.2457
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 168.3500, GNorm = 0.2568
Meta loss on this task batch = 2.7966e-01, Meta loss averaged over last 500 steps = 2.6234e-01, PNorm = 168.3543, GNorm = 0.2299
Meta loss on this task batch = 2.1400e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 168.3596, GNorm = 0.2279
Meta loss on this task batch = 2.6459e-01, Meta loss averaged over last 500 steps = 2.6230e-01, PNorm = 168.3650, GNorm = 0.2370
Meta loss on this task batch = 2.3046e-01, Meta loss averaged over last 500 steps = 2.6231e-01, PNorm = 168.3713, GNorm = 0.2140
Meta loss on this task batch = 3.0004e-01, Meta loss averaged over last 500 steps = 2.6244e-01, PNorm = 168.3778, GNorm = 0.2419
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.6232e-01, PNorm = 168.3839, GNorm = 0.2368
Meta loss on this task batch = 3.0760e-01, Meta loss averaged over last 500 steps = 2.6242e-01, PNorm = 168.3901, GNorm = 0.2253
Meta loss on this task batch = 2.6210e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 168.3961, GNorm = 0.3143
Took 113.35237741470337 seconds to complete one epoch of meta training
Took 121.21044039726257 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486799
Epoch 811
Meta loss on this task batch = 2.9832e-01, Meta loss averaged over last 500 steps = 2.6233e-01, PNorm = 168.4013, GNorm = 0.3057
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.6223e-01, PNorm = 168.4060, GNorm = 0.2388
Meta loss on this task batch = 2.0109e-01, Meta loss averaged over last 500 steps = 2.6214e-01, PNorm = 168.4108, GNorm = 0.2132
Meta loss on this task batch = 2.9675e-01, Meta loss averaged over last 500 steps = 2.6214e-01, PNorm = 168.4157, GNorm = 0.2333
Meta loss on this task batch = 2.6791e-01, Meta loss averaged over last 500 steps = 2.6227e-01, PNorm = 168.4201, GNorm = 0.2214
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 168.4247, GNorm = 0.2696
Meta loss on this task batch = 2.6097e-01, Meta loss averaged over last 500 steps = 2.6218e-01, PNorm = 168.4288, GNorm = 0.2529
Meta loss on this task batch = 2.1825e-01, Meta loss averaged over last 500 steps = 2.6207e-01, PNorm = 168.4333, GNorm = 0.1922
Meta loss on this task batch = 2.6114e-01, Meta loss averaged over last 500 steps = 2.6197e-01, PNorm = 168.4381, GNorm = 0.2276
Meta loss on this task batch = 2.5568e-01, Meta loss averaged over last 500 steps = 2.6195e-01, PNorm = 168.4433, GNorm = 0.2115
Meta loss on this task batch = 2.5959e-01, Meta loss averaged over last 500 steps = 2.6197e-01, PNorm = 168.4487, GNorm = 0.1851
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.6203e-01, PNorm = 168.4540, GNorm = 0.3343
Meta loss on this task batch = 2.8813e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 168.4584, GNorm = 0.2496
Meta loss on this task batch = 2.5689e-01, Meta loss averaged over last 500 steps = 2.6214e-01, PNorm = 168.4633, GNorm = 0.1967
Meta loss on this task batch = 2.0391e-01, Meta loss averaged over last 500 steps = 2.6209e-01, PNorm = 168.4676, GNorm = 0.2384
Meta loss on this task batch = 3.3366e-01, Meta loss averaged over last 500 steps = 2.6223e-01, PNorm = 168.4718, GNorm = 0.2549
Meta loss on this task batch = 2.2676e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 168.4763, GNorm = 0.1924
Meta loss on this task batch = 2.5181e-01, Meta loss averaged over last 500 steps = 2.6222e-01, PNorm = 168.4810, GNorm = 0.2250
Meta loss on this task batch = 2.6895e-01, Meta loss averaged over last 500 steps = 2.6224e-01, PNorm = 168.4853, GNorm = 0.2662
Took 147.72754573822021 seconds to complete one epoch of meta training
Took 155.6528308391571 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479851
Epoch 812
Meta loss on this task batch = 2.1749e-01, Meta loss averaged over last 500 steps = 2.6228e-01, PNorm = 168.4893, GNorm = 0.2125
Meta loss on this task batch = 2.4364e-01, Meta loss averaged over last 500 steps = 2.6217e-01, PNorm = 168.4942, GNorm = 0.2287
Meta loss on this task batch = 2.7576e-01, Meta loss averaged over last 500 steps = 2.6225e-01, PNorm = 168.4997, GNorm = 0.2346
Meta loss on this task batch = 2.7350e-01, Meta loss averaged over last 500 steps = 2.6221e-01, PNorm = 168.5055, GNorm = 0.2320
Meta loss on this task batch = 2.6455e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 168.5109, GNorm = 0.2246
Meta loss on this task batch = 2.6350e-01, Meta loss averaged over last 500 steps = 2.6205e-01, PNorm = 168.5164, GNorm = 0.2210
Meta loss on this task batch = 2.2029e-01, Meta loss averaged over last 500 steps = 2.6201e-01, PNorm = 168.5215, GNorm = 0.1992
Meta loss on this task batch = 2.5470e-01, Meta loss averaged over last 500 steps = 2.6201e-01, PNorm = 168.5268, GNorm = 0.2223
Meta loss on this task batch = 2.1168e-01, Meta loss averaged over last 500 steps = 2.6189e-01, PNorm = 168.5321, GNorm = 0.2036
Meta loss on this task batch = 2.6526e-01, Meta loss averaged over last 500 steps = 2.6191e-01, PNorm = 168.5379, GNorm = 0.2052
Meta loss on this task batch = 2.7121e-01, Meta loss averaged over last 500 steps = 2.6198e-01, PNorm = 168.5435, GNorm = 0.2344
Meta loss on this task batch = 2.7258e-01, Meta loss averaged over last 500 steps = 2.6194e-01, PNorm = 168.5487, GNorm = 0.2469
Meta loss on this task batch = 2.8218e-01, Meta loss averaged over last 500 steps = 2.6196e-01, PNorm = 168.5542, GNorm = 0.2516
Meta loss on this task batch = 2.8938e-01, Meta loss averaged over last 500 steps = 2.6211e-01, PNorm = 168.5599, GNorm = 0.2492
Meta loss on this task batch = 2.7288e-01, Meta loss averaged over last 500 steps = 2.6206e-01, PNorm = 168.5659, GNorm = 0.2435
Meta loss on this task batch = 2.6474e-01, Meta loss averaged over last 500 steps = 2.6205e-01, PNorm = 168.5712, GNorm = 0.2744
Meta loss on this task batch = 2.2318e-01, Meta loss averaged over last 500 steps = 2.6188e-01, PNorm = 168.5766, GNorm = 0.2232
Meta loss on this task batch = 2.8550e-01, Meta loss averaged over last 500 steps = 2.6188e-01, PNorm = 168.5810, GNorm = 0.2871
Meta loss on this task batch = 2.3691e-01, Meta loss averaged over last 500 steps = 2.6180e-01, PNorm = 168.5851, GNorm = 0.2941
Took 109.47874879837036 seconds to complete one epoch of meta training
Took 116.50271463394165 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489960
Epoch 813
Meta loss on this task batch = 2.7040e-01, Meta loss averaged over last 500 steps = 2.6169e-01, PNorm = 168.5890, GNorm = 0.2153
Meta loss on this task batch = 2.5430e-01, Meta loss averaged over last 500 steps = 2.6181e-01, PNorm = 168.5929, GNorm = 0.2263
Meta loss on this task batch = 3.0919e-01, Meta loss averaged over last 500 steps = 2.6184e-01, PNorm = 168.5964, GNorm = 0.2563
Meta loss on this task batch = 2.5409e-01, Meta loss averaged over last 500 steps = 2.6183e-01, PNorm = 168.6009, GNorm = 0.2230
Meta loss on this task batch = 1.9039e-01, Meta loss averaged over last 500 steps = 2.6178e-01, PNorm = 168.6060, GNorm = 0.1995
Meta loss on this task batch = 2.7013e-01, Meta loss averaged over last 500 steps = 2.6183e-01, PNorm = 168.6111, GNorm = 0.2447
Meta loss on this task batch = 2.5338e-01, Meta loss averaged over last 500 steps = 2.6180e-01, PNorm = 168.6163, GNorm = 0.2250
Meta loss on this task batch = 2.4574e-01, Meta loss averaged over last 500 steps = 2.6172e-01, PNorm = 168.6220, GNorm = 0.2726
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.6169e-01, PNorm = 168.6275, GNorm = 0.2473
Meta loss on this task batch = 2.4296e-01, Meta loss averaged over last 500 steps = 2.6167e-01, PNorm = 168.6330, GNorm = 0.2548
Meta loss on this task batch = 2.9196e-01, Meta loss averaged over last 500 steps = 2.6164e-01, PNorm = 168.6375, GNorm = 0.2376
Meta loss on this task batch = 2.6286e-01, Meta loss averaged over last 500 steps = 2.6172e-01, PNorm = 168.6418, GNorm = 0.2259
Meta loss on this task batch = 2.3035e-01, Meta loss averaged over last 500 steps = 2.6172e-01, PNorm = 168.6459, GNorm = 0.2110
Meta loss on this task batch = 2.4080e-01, Meta loss averaged over last 500 steps = 2.6163e-01, PNorm = 168.6488, GNorm = 0.2817
Meta loss on this task batch = 2.5471e-01, Meta loss averaged over last 500 steps = 2.6151e-01, PNorm = 168.6523, GNorm = 0.2236
Meta loss on this task batch = 2.8549e-01, Meta loss averaged over last 500 steps = 2.6153e-01, PNorm = 168.6553, GNorm = 0.2268
Meta loss on this task batch = 2.3118e-01, Meta loss averaged over last 500 steps = 2.6158e-01, PNorm = 168.6574, GNorm = 0.2631
Meta loss on this task batch = 2.1997e-01, Meta loss averaged over last 500 steps = 2.6134e-01, PNorm = 168.6601, GNorm = 0.2459
Meta loss on this task batch = 3.2104e-01, Meta loss averaged over last 500 steps = 2.6152e-01, PNorm = 168.6629, GNorm = 0.3120
Took 112.4403920173645 seconds to complete one epoch of meta training
Took 119.9827868938446 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511289
Epoch 814
Meta loss on this task batch = 2.5755e-01, Meta loss averaged over last 500 steps = 2.6148e-01, PNorm = 168.6655, GNorm = 0.2155
Meta loss on this task batch = 2.9719e-01, Meta loss averaged over last 500 steps = 2.6154e-01, PNorm = 168.6680, GNorm = 0.2767
Meta loss on this task batch = 2.8222e-01, Meta loss averaged over last 500 steps = 2.6148e-01, PNorm = 168.6709, GNorm = 0.2762
Meta loss on this task batch = 2.2730e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 168.6738, GNorm = 0.2200
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.6151e-01, PNorm = 168.6777, GNorm = 0.2095
Meta loss on this task batch = 2.4729e-01, Meta loss averaged over last 500 steps = 2.6146e-01, PNorm = 168.6815, GNorm = 0.2473
Meta loss on this task batch = 2.7743e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 168.6856, GNorm = 0.2302
Meta loss on this task batch = 2.4748e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 168.6898, GNorm = 0.2305
Meta loss on this task batch = 2.6649e-01, Meta loss averaged over last 500 steps = 2.6129e-01, PNorm = 168.6942, GNorm = 0.2389
Meta loss on this task batch = 2.3926e-01, Meta loss averaged over last 500 steps = 2.6127e-01, PNorm = 168.6991, GNorm = 0.2081
Meta loss on this task batch = 2.2430e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 168.7047, GNorm = 0.2229
Meta loss on this task batch = 2.4024e-01, Meta loss averaged over last 500 steps = 2.6133e-01, PNorm = 168.7116, GNorm = 0.2390
Meta loss on this task batch = 2.3379e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 168.7186, GNorm = 0.2396
Meta loss on this task batch = 2.8041e-01, Meta loss averaged over last 500 steps = 2.6129e-01, PNorm = 168.7252, GNorm = 0.2380
Meta loss on this task batch = 3.1990e-01, Meta loss averaged over last 500 steps = 2.6140e-01, PNorm = 168.7307, GNorm = 0.2929
Meta loss on this task batch = 2.7803e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 168.7359, GNorm = 0.2217
Meta loss on this task batch = 2.0019e-01, Meta loss averaged over last 500 steps = 2.6125e-01, PNorm = 168.7410, GNorm = 0.2123
Meta loss on this task batch = 2.7695e-01, Meta loss averaged over last 500 steps = 2.6135e-01, PNorm = 168.7448, GNorm = 0.2824
Meta loss on this task batch = 2.8197e-01, Meta loss averaged over last 500 steps = 2.6134e-01, PNorm = 168.7485, GNorm = 0.3011
Took 111.7335205078125 seconds to complete one epoch of meta training
Took 119.24902844429016 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499134
Epoch 815
Meta loss on this task batch = 2.5242e-01, Meta loss averaged over last 500 steps = 2.6145e-01, PNorm = 168.7524, GNorm = 0.2205
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.6138e-01, PNorm = 168.7563, GNorm = 0.2015
Meta loss on this task batch = 2.6305e-01, Meta loss averaged over last 500 steps = 2.6136e-01, PNorm = 168.7603, GNorm = 0.2290
Meta loss on this task batch = 2.7413e-01, Meta loss averaged over last 500 steps = 2.6140e-01, PNorm = 168.7642, GNorm = 0.2140
Meta loss on this task batch = 2.7901e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 168.7687, GNorm = 0.2335
Meta loss on this task batch = 2.5657e-01, Meta loss averaged over last 500 steps = 2.6137e-01, PNorm = 168.7729, GNorm = 0.1930
Meta loss on this task batch = 2.6591e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 168.7777, GNorm = 0.2163
Meta loss on this task batch = 2.5713e-01, Meta loss averaged over last 500 steps = 2.6124e-01, PNorm = 168.7821, GNorm = 0.2127
Meta loss on this task batch = 2.5331e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 168.7866, GNorm = 0.2141
Meta loss on this task batch = 2.5341e-01, Meta loss averaged over last 500 steps = 2.6133e-01, PNorm = 168.7918, GNorm = 0.2341
Meta loss on this task batch = 2.9212e-01, Meta loss averaged over last 500 steps = 2.6142e-01, PNorm = 168.7974, GNorm = 0.2274
Meta loss on this task batch = 2.6237e-01, Meta loss averaged over last 500 steps = 2.6144e-01, PNorm = 168.8032, GNorm = 0.2387
Meta loss on this task batch = 2.7588e-01, Meta loss averaged over last 500 steps = 2.6148e-01, PNorm = 168.8088, GNorm = 0.2450
Meta loss on this task batch = 2.2339e-01, Meta loss averaged over last 500 steps = 2.6140e-01, PNorm = 168.8150, GNorm = 0.2450
Meta loss on this task batch = 2.6493e-01, Meta loss averaged over last 500 steps = 2.6136e-01, PNorm = 168.8214, GNorm = 0.2132
Meta loss on this task batch = 2.4058e-01, Meta loss averaged over last 500 steps = 2.6133e-01, PNorm = 168.8272, GNorm = 0.2434
Meta loss on this task batch = 2.6093e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 168.8321, GNorm = 0.2315
Meta loss on this task batch = 1.9818e-01, Meta loss averaged over last 500 steps = 2.6122e-01, PNorm = 168.8373, GNorm = 0.2070
Meta loss on this task batch = 2.3311e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 168.8427, GNorm = 0.2929
Took 107.7258849143982 seconds to complete one epoch of meta training
Took 115.01051092147827 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500196
Epoch 816
Meta loss on this task batch = 2.5949e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 168.8477, GNorm = 0.2470
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.6137e-01, PNorm = 168.8517, GNorm = 0.2977
Meta loss on this task batch = 3.1217e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 168.8554, GNorm = 0.2797
Meta loss on this task batch = 2.7343e-01, Meta loss averaged over last 500 steps = 2.6139e-01, PNorm = 168.8573, GNorm = 0.3195
Meta loss on this task batch = 2.4946e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 168.8595, GNorm = 0.2472
Meta loss on this task batch = 2.1561e-01, Meta loss averaged over last 500 steps = 2.6128e-01, PNorm = 168.8626, GNorm = 0.1988
Meta loss on this task batch = 2.5381e-01, Meta loss averaged over last 500 steps = 2.6132e-01, PNorm = 168.8661, GNorm = 0.2210
Meta loss on this task batch = 2.4524e-01, Meta loss averaged over last 500 steps = 2.6141e-01, PNorm = 168.8702, GNorm = 0.2543
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 2.6143e-01, PNorm = 168.8750, GNorm = 0.2776
Meta loss on this task batch = 2.8825e-01, Meta loss averaged over last 500 steps = 2.6145e-01, PNorm = 168.8798, GNorm = 0.2460
Meta loss on this task batch = 2.5538e-01, Meta loss averaged over last 500 steps = 2.6148e-01, PNorm = 168.8849, GNorm = 0.2205
Meta loss on this task batch = 2.5924e-01, Meta loss averaged over last 500 steps = 2.6131e-01, PNorm = 168.8896, GNorm = 0.2681
Meta loss on this task batch = 2.4525e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 168.8950, GNorm = 0.2235
Meta loss on this task batch = 2.3975e-01, Meta loss averaged over last 500 steps = 2.6123e-01, PNorm = 168.9009, GNorm = 0.2204
Meta loss on this task batch = 2.8982e-01, Meta loss averaged over last 500 steps = 2.6126e-01, PNorm = 168.9072, GNorm = 0.2257
Meta loss on this task batch = 2.0672e-01, Meta loss averaged over last 500 steps = 2.6116e-01, PNorm = 168.9134, GNorm = 0.2079
Meta loss on this task batch = 2.3917e-01, Meta loss averaged over last 500 steps = 2.6109e-01, PNorm = 168.9198, GNorm = 0.2027
Meta loss on this task batch = 2.8451e-01, Meta loss averaged over last 500 steps = 2.6114e-01, PNorm = 168.9260, GNorm = 0.2651
Meta loss on this task batch = 2.5189e-01, Meta loss averaged over last 500 steps = 2.6117e-01, PNorm = 168.9326, GNorm = 0.2635
Took 111.25915026664734 seconds to complete one epoch of meta training
Took 118.96358513832092 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492180
Epoch 817
Meta loss on this task batch = 2.7380e-01, Meta loss averaged over last 500 steps = 2.6118e-01, PNorm = 168.9380, GNorm = 0.2833
Meta loss on this task batch = 2.2679e-01, Meta loss averaged over last 500 steps = 2.6112e-01, PNorm = 168.9438, GNorm = 0.2068
Meta loss on this task batch = 2.8282e-01, Meta loss averaged over last 500 steps = 2.6124e-01, PNorm = 168.9493, GNorm = 0.2459
Meta loss on this task batch = 2.4822e-01, Meta loss averaged over last 500 steps = 2.6118e-01, PNorm = 168.9548, GNorm = 0.2158
Meta loss on this task batch = 2.4498e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 168.9603, GNorm = 0.2328
Meta loss on this task batch = 2.8608e-01, Meta loss averaged over last 500 steps = 2.6111e-01, PNorm = 168.9652, GNorm = 0.2417
Meta loss on this task batch = 1.6048e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 168.9704, GNorm = 0.1778
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.6098e-01, PNorm = 168.9752, GNorm = 0.2242
Meta loss on this task batch = 2.2877e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 168.9795, GNorm = 0.2357
Meta loss on this task batch = 3.1527e-01, Meta loss averaged over last 500 steps = 2.6090e-01, PNorm = 168.9833, GNorm = 0.2470
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.6085e-01, PNorm = 168.9870, GNorm = 0.2226
Meta loss on this task batch = 2.3854e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 168.9905, GNorm = 0.2028
Meta loss on this task batch = 2.3350e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 168.9945, GNorm = 0.2159
Meta loss on this task batch = 2.7852e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 168.9987, GNorm = 0.2313
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 169.0037, GNorm = 0.2372
Meta loss on this task batch = 2.2895e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 169.0089, GNorm = 0.2348
Meta loss on this task batch = 2.6685e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 169.0132, GNorm = 0.2270
Meta loss on this task batch = 3.0351e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 169.0171, GNorm = 0.2311
Meta loss on this task batch = 2.4830e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 169.0217, GNorm = 0.2927
Took 110.44476652145386 seconds to complete one epoch of meta training
Took 117.73927021026611 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491126
Epoch 818
Meta loss on this task batch = 2.7345e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 169.0263, GNorm = 0.2451
Meta loss on this task batch = 2.9304e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 169.0307, GNorm = 0.2696
Meta loss on this task batch = 2.1926e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 169.0351, GNorm = 0.2325
Meta loss on this task batch = 2.3621e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 169.0392, GNorm = 0.2029
Meta loss on this task batch = 2.4026e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 169.0432, GNorm = 0.2194
Meta loss on this task batch = 2.6007e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 169.0472, GNorm = 0.2462
Meta loss on this task batch = 2.9146e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 169.0505, GNorm = 0.2486
Meta loss on this task batch = 2.2783e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 169.0536, GNorm = 0.2322
Meta loss on this task batch = 2.1132e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.0574, GNorm = 0.1800
Meta loss on this task batch = 2.4035e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 169.0613, GNorm = 0.2014
Meta loss on this task batch = 3.0631e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 169.0653, GNorm = 0.2642
Meta loss on this task batch = 2.3415e-01, Meta loss averaged over last 500 steps = 2.6017e-01, PNorm = 169.0695, GNorm = 0.2015
Meta loss on this task batch = 3.0133e-01, Meta loss averaged over last 500 steps = 2.6017e-01, PNorm = 169.0732, GNorm = 0.2427
Meta loss on this task batch = 3.1059e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 169.0771, GNorm = 0.2558
Meta loss on this task batch = 2.9655e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 169.0812, GNorm = 0.2177
Meta loss on this task batch = 2.8919e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 169.0844, GNorm = 0.2621
Meta loss on this task batch = 2.4093e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 169.0885, GNorm = 0.2427
Meta loss on this task batch = 2.9630e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 169.0926, GNorm = 0.2329
Meta loss on this task batch = 2.6798e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 169.0970, GNorm = 0.2661
Took 107.00142192840576 seconds to complete one epoch of meta training
Took 114.64716744422913 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487234
Epoch 819
Meta loss on this task batch = 2.9854e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 169.1013, GNorm = 0.2254
Meta loss on this task batch = 2.9572e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 169.1054, GNorm = 0.2461
Meta loss on this task batch = 2.2073e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 169.1099, GNorm = 0.1805
Meta loss on this task batch = 2.6414e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 169.1145, GNorm = 0.2061
Meta loss on this task batch = 1.8730e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 169.1192, GNorm = 0.1930
Meta loss on this task batch = 2.3155e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 169.1242, GNorm = 0.2047
Meta loss on this task batch = 1.9319e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 169.1293, GNorm = 0.2033
Meta loss on this task batch = 2.4248e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 169.1349, GNorm = 0.1993
Meta loss on this task batch = 2.9976e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 169.1395, GNorm = 0.2613
Meta loss on this task batch = 2.5976e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 169.1438, GNorm = 0.2235
Meta loss on this task batch = 2.8456e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 169.1483, GNorm = 0.2295
Meta loss on this task batch = 2.5940e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.1523, GNorm = 0.2448
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 169.1563, GNorm = 0.2339
Meta loss on this task batch = 2.4514e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 169.1603, GNorm = 0.2327
Meta loss on this task batch = 2.4520e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 169.1646, GNorm = 0.2297
Meta loss on this task batch = 2.7701e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 169.1696, GNorm = 0.2745
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 169.1745, GNorm = 0.2707
Meta loss on this task batch = 2.1269e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 169.1793, GNorm = 0.2057
Meta loss on this task batch = 2.5796e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 169.1843, GNorm = 0.2773
Took 110.7447144985199 seconds to complete one epoch of meta training
Took 117.09596276283264 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479653
Epoch 820
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 169.1891, GNorm = 0.2261
Meta loss on this task batch = 2.0959e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 169.1945, GNorm = 0.2220
Meta loss on this task batch = 3.3298e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 169.1991, GNorm = 0.2735
Meta loss on this task batch = 2.4690e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 169.2046, GNorm = 0.2215
Meta loss on this task batch = 2.2962e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 169.2101, GNorm = 0.2270
Meta loss on this task batch = 2.7155e-01, Meta loss averaged over last 500 steps = 2.6018e-01, PNorm = 169.2153, GNorm = 0.2148
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 169.2203, GNorm = 0.2781
Meta loss on this task batch = 2.5647e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 169.2246, GNorm = 0.3028
Meta loss on this task batch = 2.9991e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 169.2287, GNorm = 0.2274
Meta loss on this task batch = 2.3693e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 169.2321, GNorm = 0.2526
Meta loss on this task batch = 2.1535e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 169.2360, GNorm = 0.2208
Meta loss on this task batch = 2.6833e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 169.2392, GNorm = 0.2373
Meta loss on this task batch = 2.7827e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 169.2424, GNorm = 0.2844
Meta loss on this task batch = 2.5336e-01, Meta loss averaged over last 500 steps = 2.6022e-01, PNorm = 169.2458, GNorm = 0.2124
Meta loss on this task batch = 2.9820e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 169.2489, GNorm = 0.2788
Meta loss on this task batch = 2.6113e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 169.2519, GNorm = 0.2046
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 169.2549, GNorm = 0.2195
Meta loss on this task batch = 2.2752e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 169.2581, GNorm = 0.2132
Meta loss on this task batch = 2.3565e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 169.2613, GNorm = 0.2512
Took 109.73767924308777 seconds to complete one epoch of meta training
Took 117.25677633285522 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514124
Epoch 821
Meta loss on this task batch = 2.2948e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 169.2649, GNorm = 0.1980
Meta loss on this task batch = 2.5638e-01, Meta loss averaged over last 500 steps = 2.5997e-01, PNorm = 169.2677, GNorm = 0.2178
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.6004e-01, PNorm = 169.2703, GNorm = 0.2185
Meta loss on this task batch = 2.7865e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 169.2740, GNorm = 0.2202
Meta loss on this task batch = 2.5709e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 169.2774, GNorm = 0.2208
Meta loss on this task batch = 2.4586e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 169.2816, GNorm = 0.2545
Meta loss on this task batch = 2.3330e-01, Meta loss averaged over last 500 steps = 2.6013e-01, PNorm = 169.2857, GNorm = 0.2254
Meta loss on this task batch = 2.5752e-01, Meta loss averaged over last 500 steps = 2.6018e-01, PNorm = 169.2895, GNorm = 0.2399
Meta loss on this task batch = 2.7218e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 169.2933, GNorm = 0.2511
Meta loss on this task batch = 3.2617e-01, Meta loss averaged over last 500 steps = 2.6033e-01, PNorm = 169.2969, GNorm = 0.2872
Meta loss on this task batch = 2.8952e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 169.2997, GNorm = 0.2569
Meta loss on this task batch = 3.2202e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 169.3033, GNorm = 0.2826
Meta loss on this task batch = 2.6321e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 169.3079, GNorm = 0.2029
Meta loss on this task batch = 2.6721e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 169.3124, GNorm = 0.2384
Meta loss on this task batch = 2.6337e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 169.3164, GNorm = 0.2565
Meta loss on this task batch = 2.2778e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 169.3205, GNorm = 0.2300
Meta loss on this task batch = 2.6530e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 169.3248, GNorm = 0.2720
Meta loss on this task batch = 2.4258e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.3291, GNorm = 0.2357
Meta loss on this task batch = 3.2267e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 169.3322, GNorm = 0.3157
Took 108.94711375236511 seconds to complete one epoch of meta training
Took 116.34147620201111 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498507
Epoch 822
Meta loss on this task batch = 2.6872e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 169.3360, GNorm = 0.2412
Meta loss on this task batch = 2.3756e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 169.3399, GNorm = 0.2096
Meta loss on this task batch = 3.0062e-01, Meta loss averaged over last 500 steps = 2.6078e-01, PNorm = 169.3445, GNorm = 0.2463
Meta loss on this task batch = 2.7949e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 169.3490, GNorm = 0.2195
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 169.3531, GNorm = 0.2018
Meta loss on this task batch = 2.7949e-01, Meta loss averaged over last 500 steps = 2.6077e-01, PNorm = 169.3574, GNorm = 0.2232
Meta loss on this task batch = 2.9359e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 169.3620, GNorm = 0.2313
Meta loss on this task batch = 2.3716e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 169.3666, GNorm = 0.2154
Meta loss on this task batch = 2.6838e-01, Meta loss averaged over last 500 steps = 2.6082e-01, PNorm = 169.3712, GNorm = 0.2595
Meta loss on this task batch = 2.6744e-01, Meta loss averaged over last 500 steps = 2.6093e-01, PNorm = 169.3765, GNorm = 0.2269
Meta loss on this task batch = 2.2841e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 169.3820, GNorm = 0.2272
Meta loss on this task batch = 2.2811e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 169.3877, GNorm = 0.2169
Meta loss on this task batch = 2.2100e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.3942, GNorm = 0.1939
Meta loss on this task batch = 2.4840e-01, Meta loss averaged over last 500 steps = 2.6065e-01, PNorm = 169.3995, GNorm = 0.2340
Meta loss on this task batch = 2.6783e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 169.4046, GNorm = 0.2228
Meta loss on this task batch = 2.0727e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 169.4092, GNorm = 0.2121
Meta loss on this task batch = 2.7861e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 169.4134, GNorm = 0.2379
Meta loss on this task batch = 2.1820e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 169.4185, GNorm = 0.2309
Meta loss on this task batch = 2.7839e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 169.4236, GNorm = 0.2968
Took 107.00123190879822 seconds to complete one epoch of meta training
Took 114.39114427566528 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481996
Epoch 823
Meta loss on this task batch = 2.8224e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 169.4280, GNorm = 0.2381
Meta loss on this task batch = 2.0668e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 169.4321, GNorm = 0.2060
Meta loss on this task batch = 2.2512e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 169.4363, GNorm = 0.2006
Meta loss on this task batch = 3.2098e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 169.4405, GNorm = 0.2461
Meta loss on this task batch = 2.8779e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 169.4441, GNorm = 0.2582
Meta loss on this task batch = 2.3576e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.4485, GNorm = 0.2439
Meta loss on this task batch = 2.0028e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.4529, GNorm = 0.2127
Meta loss on this task batch = 2.5520e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 169.4582, GNorm = 0.2625
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 169.4635, GNorm = 0.2465
Meta loss on this task batch = 2.3882e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 169.4686, GNorm = 0.2151
Meta loss on this task batch = 2.4258e-01, Meta loss averaged over last 500 steps = 2.6012e-01, PNorm = 169.4729, GNorm = 0.2029
Meta loss on this task batch = 3.6149e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 169.4767, GNorm = 0.3168
Meta loss on this task batch = 2.2312e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 169.4812, GNorm = 0.1939
Meta loss on this task batch = 2.5494e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 169.4854, GNorm = 0.2366
Meta loss on this task batch = 2.8197e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 169.4891, GNorm = 0.2743
Meta loss on this task batch = 2.7520e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 169.4930, GNorm = 0.2399
Meta loss on this task batch = 2.9396e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 169.4971, GNorm = 0.2449
Meta loss on this task batch = 2.8455e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.5018, GNorm = 0.2213
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 169.5059, GNorm = 0.2971
Took 107.42344355583191 seconds to complete one epoch of meta training
Took 114.75865507125854 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474533
Epoch 824
Meta loss on this task batch = 2.2904e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 169.5103, GNorm = 0.1873
Meta loss on this task batch = 2.2956e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 169.5150, GNorm = 0.2257
Meta loss on this task batch = 2.5245e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 169.5198, GNorm = 0.2165
Meta loss on this task batch = 2.8073e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 169.5239, GNorm = 0.2377
Meta loss on this task batch = 2.4908e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 169.5282, GNorm = 0.2066
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 169.5326, GNorm = 0.2224
Meta loss on this task batch = 2.2372e-01, Meta loss averaged over last 500 steps = 2.6033e-01, PNorm = 169.5369, GNorm = 0.1841
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 169.5413, GNorm = 0.2130
Meta loss on this task batch = 2.6298e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 169.5451, GNorm = 0.2179
Meta loss on this task batch = 2.6389e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 169.5488, GNorm = 0.2260
Meta loss on this task batch = 2.6155e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 169.5527, GNorm = 0.2167
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 169.5566, GNorm = 0.2204
Meta loss on this task batch = 2.6932e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 169.5605, GNorm = 0.2475
Meta loss on this task batch = 3.2163e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 169.5634, GNorm = 0.2901
Meta loss on this task batch = 1.9802e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.5674, GNorm = 0.1932
Meta loss on this task batch = 2.6301e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 169.5716, GNorm = 0.2332
Meta loss on this task batch = 2.7667e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 169.5753, GNorm = 0.2343
Meta loss on this task batch = 2.5903e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 169.5791, GNorm = 0.2571
Meta loss on this task batch = 2.3538e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 169.5845, GNorm = 0.2812
Took 112.29925107955933 seconds to complete one epoch of meta training
Took 119.46098899841309 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487169
Epoch 825
Meta loss on this task batch = 2.1856e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 169.5903, GNorm = 0.2285
Meta loss on this task batch = 2.4774e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 169.5949, GNorm = 0.2384
Meta loss on this task batch = 2.0490e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 169.5997, GNorm = 0.2034
Meta loss on this task batch = 2.9873e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 169.6034, GNorm = 0.2728
Meta loss on this task batch = 2.2746e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 169.6074, GNorm = 0.1906
Meta loss on this task batch = 2.7290e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 169.6099, GNorm = 0.2514
Meta loss on this task batch = 2.8873e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 169.6122, GNorm = 0.2732
Meta loss on this task batch = 2.3065e-01, Meta loss averaged over last 500 steps = 2.6022e-01, PNorm = 169.6147, GNorm = 0.2213
Meta loss on this task batch = 2.3467e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 169.6172, GNorm = 0.2301
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 169.6198, GNorm = 0.2466
Meta loss on this task batch = 2.4242e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 169.6232, GNorm = 0.2476
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 169.6267, GNorm = 0.2314
Meta loss on this task batch = 2.6995e-01, Meta loss averaged over last 500 steps = 2.6022e-01, PNorm = 169.6309, GNorm = 0.2534
Meta loss on this task batch = 2.4735e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 169.6357, GNorm = 0.2189
Meta loss on this task batch = 2.8112e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.6409, GNorm = 0.2714
Meta loss on this task batch = 2.8656e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 169.6458, GNorm = 0.2492
Meta loss on this task batch = 2.5687e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 169.6514, GNorm = 0.2474
Meta loss on this task batch = 2.9235e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 169.6561, GNorm = 0.2352
Meta loss on this task batch = 3.1723e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 169.6616, GNorm = 0.3577
Took 114.12408375740051 seconds to complete one epoch of meta training
Took 122.53308486938477 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500037
Epoch 826
Meta loss on this task batch = 3.1687e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 169.6658, GNorm = 0.2719
Meta loss on this task batch = 2.5339e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.6704, GNorm = 0.2512
Meta loss on this task batch = 2.0154e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.6756, GNorm = 0.2003
Meta loss on this task batch = 2.4142e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.6805, GNorm = 0.2518
Meta loss on this task batch = 2.2365e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 169.6849, GNorm = 0.1966
Meta loss on this task batch = 2.7251e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.6885, GNorm = 0.2219
Meta loss on this task batch = 2.3813e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 169.6921, GNorm = 0.2031
Meta loss on this task batch = 2.5947e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 169.6956, GNorm = 0.1940
Meta loss on this task batch = 2.6925e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 169.6992, GNorm = 0.2210
Meta loss on this task batch = 2.9237e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 169.7027, GNorm = 0.2464
Meta loss on this task batch = 2.5059e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.7060, GNorm = 0.2044
Meta loss on this task batch = 2.4563e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 169.7094, GNorm = 0.2151
Meta loss on this task batch = 1.9965e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.7134, GNorm = 0.2230
Meta loss on this task batch = 2.4897e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 169.7171, GNorm = 0.2481
Meta loss on this task batch = 3.0782e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 169.7209, GNorm = 0.2433
Meta loss on this task batch = 2.4007e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 169.7251, GNorm = 0.2268
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.7295, GNorm = 0.2246
Meta loss on this task batch = 2.3293e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 169.7347, GNorm = 0.2437
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 169.7401, GNorm = 0.2617
Took 111.28149914741516 seconds to complete one epoch of meta training
Took 117.46485090255737 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486543
Epoch 827
Meta loss on this task batch = 2.4241e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 169.7447, GNorm = 0.2402
Meta loss on this task batch = 2.4827e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 169.7495, GNorm = 0.2127
Meta loss on this task batch = 2.7863e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 169.7540, GNorm = 0.2368
Meta loss on this task batch = 2.7196e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 169.7583, GNorm = 0.2584
Meta loss on this task batch = 2.5796e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 169.7625, GNorm = 0.2414
Meta loss on this task batch = 1.9514e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 169.7661, GNorm = 0.2108
Meta loss on this task batch = 1.8164e-01, Meta loss averaged over last 500 steps = 2.6038e-01, PNorm = 169.7707, GNorm = 0.2123
Meta loss on this task batch = 2.9377e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 169.7745, GNorm = 0.2337
Meta loss on this task batch = 2.8549e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 169.7782, GNorm = 0.2152
Meta loss on this task batch = 2.4898e-01, Meta loss averaged over last 500 steps = 2.6031e-01, PNorm = 169.7815, GNorm = 0.2233
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 169.7845, GNorm = 0.2756
Meta loss on this task batch = 2.4811e-01, Meta loss averaged over last 500 steps = 2.6029e-01, PNorm = 169.7870, GNorm = 0.2429
Meta loss on this task batch = 2.8108e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 169.7897, GNorm = 0.2987
Meta loss on this task batch = 2.8710e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 169.7919, GNorm = 0.2599
Meta loss on this task batch = 2.6176e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 169.7950, GNorm = 0.2291
Meta loss on this task batch = 2.6752e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 169.7988, GNorm = 0.2470
Meta loss on this task batch = 2.1914e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 169.8033, GNorm = 0.1756
Meta loss on this task batch = 3.0070e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 169.8074, GNorm = 0.2394
Meta loss on this task batch = 2.9638e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 169.8113, GNorm = 0.2609
Took 111.30296969413757 seconds to complete one epoch of meta training
Took 118.8791720867157 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485939
Epoch 828
Meta loss on this task batch = 2.9651e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 169.8150, GNorm = 0.2565
Meta loss on this task batch = 2.5221e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 169.8191, GNorm = 0.2263
Meta loss on this task batch = 2.2370e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 169.8232, GNorm = 0.2051
Meta loss on this task batch = 2.9607e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 169.8278, GNorm = 0.2354
Meta loss on this task batch = 3.0484e-01, Meta loss averaged over last 500 steps = 2.6083e-01, PNorm = 169.8328, GNorm = 0.2278
Meta loss on this task batch = 2.4930e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 169.8377, GNorm = 0.2126
Meta loss on this task batch = 2.3109e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.8429, GNorm = 0.2121
Meta loss on this task batch = 2.3061e-01, Meta loss averaged over last 500 steps = 2.6050e-01, PNorm = 169.8481, GNorm = 0.1984
Meta loss on this task batch = 2.0170e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 169.8534, GNorm = 0.1895
Meta loss on this task batch = 2.4992e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 169.8578, GNorm = 0.2476
Meta loss on this task batch = 3.2230e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 169.8628, GNorm = 0.2481
Meta loss on this task batch = 2.0499e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 169.8674, GNorm = 0.2255
Meta loss on this task batch = 2.3119e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 169.8720, GNorm = 0.2139
Meta loss on this task batch = 2.8288e-01, Meta loss averaged over last 500 steps = 2.6033e-01, PNorm = 169.8767, GNorm = 0.2398
Meta loss on this task batch = 2.9780e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 169.8808, GNorm = 0.2848
Meta loss on this task batch = 2.0093e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 169.8851, GNorm = 0.2223
Meta loss on this task batch = 3.1257e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 169.8882, GNorm = 0.2364
Meta loss on this task batch = 2.4959e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 169.8910, GNorm = 0.2349
Meta loss on this task batch = 2.3992e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 169.8935, GNorm = 0.2842
Took 107.21717023849487 seconds to complete one epoch of meta training
Took 114.46492409706116 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498217
Epoch 829
Meta loss on this task batch = 3.2649e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 169.8963, GNorm = 0.3386
Meta loss on this task batch = 2.3266e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 169.8997, GNorm = 0.2334
Meta loss on this task batch = 2.6184e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.9035, GNorm = 0.2492
Meta loss on this task batch = 2.5938e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 169.9078, GNorm = 0.2045
Meta loss on this task batch = 2.8086e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 169.9122, GNorm = 0.2667
Meta loss on this task batch = 2.4014e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 169.9168, GNorm = 0.2161
Meta loss on this task batch = 3.0087e-01, Meta loss averaged over last 500 steps = 2.6071e-01, PNorm = 169.9212, GNorm = 0.2756
Meta loss on this task batch = 2.5243e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 169.9258, GNorm = 0.2168
Meta loss on this task batch = 2.8860e-01, Meta loss averaged over last 500 steps = 2.6092e-01, PNorm = 169.9294, GNorm = 0.2602
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 2.6096e-01, PNorm = 169.9333, GNorm = 0.2356
Meta loss on this task batch = 2.6448e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 169.9372, GNorm = 0.2445
Meta loss on this task batch = 2.7565e-01, Meta loss averaged over last 500 steps = 2.6088e-01, PNorm = 169.9411, GNorm = 0.2301
Meta loss on this task batch = 2.6126e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 169.9447, GNorm = 0.2336
Meta loss on this task batch = 3.1153e-01, Meta loss averaged over last 500 steps = 2.6095e-01, PNorm = 169.9481, GNorm = 0.2929
Meta loss on this task batch = 2.6176e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 169.9512, GNorm = 0.2979
Meta loss on this task batch = 2.3681e-01, Meta loss averaged over last 500 steps = 2.6089e-01, PNorm = 169.9548, GNorm = 0.2051
Meta loss on this task batch = 1.8394e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 169.9589, GNorm = 0.2046
Meta loss on this task batch = 2.5202e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 169.9633, GNorm = 0.2194
Meta loss on this task batch = 2.1963e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 169.9672, GNorm = 0.2694
Took 109.60060548782349 seconds to complete one epoch of meta training
Took 117.05815815925598 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493227
Epoch 830
Meta loss on this task batch = 2.4374e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 169.9713, GNorm = 0.2261
Meta loss on this task batch = 3.3053e-01, Meta loss averaged over last 500 steps = 2.6073e-01, PNorm = 169.9752, GNorm = 0.2216
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 2.6067e-01, PNorm = 169.9803, GNorm = 0.2607
Meta loss on this task batch = 2.3440e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 169.9851, GNorm = 0.2452
Meta loss on this task batch = 2.3983e-01, Meta loss averaged over last 500 steps = 2.6054e-01, PNorm = 169.9903, GNorm = 0.1941
Meta loss on this task batch = 2.7092e-01, Meta loss averaged over last 500 steps = 2.6060e-01, PNorm = 169.9957, GNorm = 0.2499
Meta loss on this task batch = 2.4211e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 170.0011, GNorm = 0.2700
Meta loss on this task batch = 2.2367e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 170.0070, GNorm = 0.2031
Meta loss on this task batch = 2.8969e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 170.0123, GNorm = 0.2574
Meta loss on this task batch = 2.6928e-01, Meta loss averaged over last 500 steps = 2.6057e-01, PNorm = 170.0173, GNorm = 0.2340
Meta loss on this task batch = 2.0827e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 170.0220, GNorm = 0.2014
Meta loss on this task batch = 2.9258e-01, Meta loss averaged over last 500 steps = 2.6044e-01, PNorm = 170.0262, GNorm = 0.2805
Meta loss on this task batch = 2.3803e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 170.0307, GNorm = 0.2224
Meta loss on this task batch = 2.9997e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 170.0352, GNorm = 0.2786
Meta loss on this task batch = 2.4279e-01, Meta loss averaged over last 500 steps = 2.6031e-01, PNorm = 170.0399, GNorm = 0.2270
Meta loss on this task batch = 2.7847e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 170.0458, GNorm = 0.2310
Meta loss on this task batch = 3.3898e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 170.0518, GNorm = 0.2734
Meta loss on this task batch = 2.3066e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 170.0575, GNorm = 0.2515
Meta loss on this task batch = 1.7471e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 170.0631, GNorm = 0.2696
Took 111.12584257125854 seconds to complete one epoch of meta training
Took 118.31971144676208 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.497828
Epoch 831
Meta loss on this task batch = 2.7721e-01, Meta loss averaged over last 500 steps = 2.6022e-01, PNorm = 170.0688, GNorm = 0.2116
Meta loss on this task batch = 2.3831e-01, Meta loss averaged over last 500 steps = 2.6015e-01, PNorm = 170.0744, GNorm = 0.2145
Meta loss on this task batch = 2.1610e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 170.0802, GNorm = 0.2075
Meta loss on this task batch = 2.5761e-01, Meta loss averaged over last 500 steps = 2.6015e-01, PNorm = 170.0847, GNorm = 0.2213
Meta loss on this task batch = 2.2195e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 170.0897, GNorm = 0.1919
Meta loss on this task batch = 2.6928e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.0942, GNorm = 0.2757
Meta loss on this task batch = 2.4052e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 170.0986, GNorm = 0.2120
Meta loss on this task batch = 3.1418e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.1015, GNorm = 0.2699
Meta loss on this task batch = 2.5087e-01, Meta loss averaged over last 500 steps = 2.5994e-01, PNorm = 170.1038, GNorm = 0.2086
Meta loss on this task batch = 2.0777e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 170.1066, GNorm = 0.2192
Meta loss on this task batch = 3.4804e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.1092, GNorm = 0.2488
Meta loss on this task batch = 3.0734e-01, Meta loss averaged over last 500 steps = 2.6011e-01, PNorm = 170.1110, GNorm = 0.2638
Meta loss on this task batch = 2.3229e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 170.1133, GNorm = 0.2405
Meta loss on this task batch = 2.3831e-01, Meta loss averaged over last 500 steps = 2.5989e-01, PNorm = 170.1162, GNorm = 0.2280
Meta loss on this task batch = 2.4736e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.1200, GNorm = 0.2117
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.1241, GNorm = 0.2220
Meta loss on this task batch = 2.6906e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.1285, GNorm = 0.2230
Meta loss on this task batch = 2.4944e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 170.1332, GNorm = 0.2268
Meta loss on this task batch = 2.8186e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.1371, GNorm = 0.3076
Took 104.35359025001526 seconds to complete one epoch of meta training
Took 111.39861607551575 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471005
Epoch 832
Meta loss on this task batch = 2.2425e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.1405, GNorm = 0.2260
Meta loss on this task batch = 3.0088e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 170.1447, GNorm = 0.2615
Meta loss on this task batch = 2.6342e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.1488, GNorm = 0.2502
Meta loss on this task batch = 2.7615e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.1530, GNorm = 0.2271
Meta loss on this task batch = 2.4048e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.1579, GNorm = 0.2422
Meta loss on this task batch = 2.5903e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 170.1625, GNorm = 0.2465
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.1671, GNorm = 0.2542
Meta loss on this task batch = 2.9661e-01, Meta loss averaged over last 500 steps = 2.5999e-01, PNorm = 170.1719, GNorm = 0.2845
Meta loss on this task batch = 2.3359e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.1765, GNorm = 0.2679
Meta loss on this task batch = 2.5188e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 170.1808, GNorm = 0.2020
Meta loss on this task batch = 2.9656e-01, Meta loss averaged over last 500 steps = 2.5999e-01, PNorm = 170.1855, GNorm = 0.2429
Meta loss on this task batch = 2.8503e-01, Meta loss averaged over last 500 steps = 2.5994e-01, PNorm = 170.1901, GNorm = 0.2351
Meta loss on this task batch = 2.6960e-01, Meta loss averaged over last 500 steps = 2.5999e-01, PNorm = 170.1954, GNorm = 0.2756
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.5994e-01, PNorm = 170.2008, GNorm = 0.2197
Meta loss on this task batch = 2.6131e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.2064, GNorm = 0.2389
Meta loss on this task batch = 2.0558e-01, Meta loss averaged over last 500 steps = 2.5976e-01, PNorm = 170.2124, GNorm = 0.2241
Meta loss on this task batch = 2.4632e-01, Meta loss averaged over last 500 steps = 2.5966e-01, PNorm = 170.2184, GNorm = 0.2246
Meta loss on this task batch = 3.3916e-01, Meta loss averaged over last 500 steps = 2.5988e-01, PNorm = 170.2234, GNorm = 0.2669
Meta loss on this task batch = 2.3889e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 170.2289, GNorm = 0.2403
Took 109.31345653533936 seconds to complete one epoch of meta training
Took 115.66745495796204 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484926
Epoch 833
Meta loss on this task batch = 2.5357e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.2346, GNorm = 0.1985
Meta loss on this task batch = 2.3601e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.2400, GNorm = 0.2486
Meta loss on this task batch = 2.2446e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.2454, GNorm = 0.2033
Meta loss on this task batch = 2.5038e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.2507, GNorm = 0.2418
Meta loss on this task batch = 2.3858e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.2554, GNorm = 0.2063
Meta loss on this task batch = 2.1644e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 170.2608, GNorm = 0.2436
Meta loss on this task batch = 3.0059e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 170.2663, GNorm = 0.2445
Meta loss on this task batch = 2.2178e-01, Meta loss averaged over last 500 steps = 2.5976e-01, PNorm = 170.2714, GNorm = 0.2138
Meta loss on this task batch = 3.0168e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.2761, GNorm = 0.2450
Meta loss on this task batch = 2.3757e-01, Meta loss averaged over last 500 steps = 2.5988e-01, PNorm = 170.2806, GNorm = 0.2168
Meta loss on this task batch = 2.6590e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 170.2851, GNorm = 0.2782
Meta loss on this task batch = 3.1319e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.2891, GNorm = 0.2645
Meta loss on this task batch = 2.7650e-01, Meta loss averaged over last 500 steps = 2.5988e-01, PNorm = 170.2934, GNorm = 0.2235
Meta loss on this task batch = 2.4425e-01, Meta loss averaged over last 500 steps = 2.5994e-01, PNorm = 170.2976, GNorm = 0.2403
Meta loss on this task batch = 2.8340e-01, Meta loss averaged over last 500 steps = 2.6006e-01, PNorm = 170.3023, GNorm = 0.2207
Meta loss on this task batch = 2.4146e-01, Meta loss averaged over last 500 steps = 2.6003e-01, PNorm = 170.3073, GNorm = 0.2108
Meta loss on this task batch = 2.5948e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 170.3128, GNorm = 0.2104
Meta loss on this task batch = 2.8170e-01, Meta loss averaged over last 500 steps = 2.6000e-01, PNorm = 170.3177, GNorm = 0.2118
Meta loss on this task batch = 2.9731e-01, Meta loss averaged over last 500 steps = 2.6011e-01, PNorm = 170.3229, GNorm = 0.3543
Took 112.20235061645508 seconds to complete one epoch of meta training
Took 119.62226581573486 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500131
Epoch 834
Meta loss on this task batch = 2.3419e-01, Meta loss averaged over last 500 steps = 2.6000e-01, PNorm = 170.3272, GNorm = 0.2158
Meta loss on this task batch = 2.6287e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 170.3320, GNorm = 0.1886
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.5995e-01, PNorm = 170.3372, GNorm = 0.2328
Meta loss on this task batch = 2.3622e-01, Meta loss averaged over last 500 steps = 2.6000e-01, PNorm = 170.3430, GNorm = 0.1907
Meta loss on this task batch = 2.4704e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 170.3488, GNorm = 0.1968
Meta loss on this task batch = 2.2257e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 170.3545, GNorm = 0.2220
Meta loss on this task batch = 2.6139e-01, Meta loss averaged over last 500 steps = 2.5974e-01, PNorm = 170.3595, GNorm = 0.2382
Meta loss on this task batch = 2.3877e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.3645, GNorm = 0.2097
Meta loss on this task batch = 2.4585e-01, Meta loss averaged over last 500 steps = 2.5970e-01, PNorm = 170.3687, GNorm = 0.2550
Meta loss on this task batch = 2.5237e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.3729, GNorm = 0.2144
Meta loss on this task batch = 2.2727e-01, Meta loss averaged over last 500 steps = 2.5959e-01, PNorm = 170.3776, GNorm = 0.2189
Meta loss on this task batch = 3.0214e-01, Meta loss averaged over last 500 steps = 2.5964e-01, PNorm = 170.3819, GNorm = 0.3097
Meta loss on this task batch = 2.2462e-01, Meta loss averaged over last 500 steps = 2.5955e-01, PNorm = 170.3864, GNorm = 0.2037
Meta loss on this task batch = 2.7241e-01, Meta loss averaged over last 500 steps = 2.5963e-01, PNorm = 170.3906, GNorm = 0.2643
Meta loss on this task batch = 2.2864e-01, Meta loss averaged over last 500 steps = 2.5951e-01, PNorm = 170.3954, GNorm = 0.2158
Meta loss on this task batch = 2.8635e-01, Meta loss averaged over last 500 steps = 2.5962e-01, PNorm = 170.4005, GNorm = 0.2549
Meta loss on this task batch = 3.1197e-01, Meta loss averaged over last 500 steps = 2.5970e-01, PNorm = 170.4057, GNorm = 0.2741
Meta loss on this task batch = 3.0743e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 170.4112, GNorm = 0.2683
Meta loss on this task batch = 2.2298e-01, Meta loss averaged over last 500 steps = 2.5967e-01, PNorm = 170.4166, GNorm = 0.2595
Took 108.23999166488647 seconds to complete one epoch of meta training
Took 115.86972403526306 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503241
Epoch 835
Meta loss on this task batch = 2.0772e-01, Meta loss averaged over last 500 steps = 2.5962e-01, PNorm = 170.4220, GNorm = 0.2235
Meta loss on this task batch = 2.9628e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 170.4268, GNorm = 0.2496
Meta loss on this task batch = 2.5113e-01, Meta loss averaged over last 500 steps = 2.5964e-01, PNorm = 170.4321, GNorm = 0.2251
Meta loss on this task batch = 2.5046e-01, Meta loss averaged over last 500 steps = 2.5967e-01, PNorm = 170.4372, GNorm = 0.2115
Meta loss on this task batch = 3.0100e-01, Meta loss averaged over last 500 steps = 2.5974e-01, PNorm = 170.4418, GNorm = 0.2566
Meta loss on this task batch = 2.3254e-01, Meta loss averaged over last 500 steps = 2.5966e-01, PNorm = 170.4467, GNorm = 0.2201
Meta loss on this task batch = 2.7752e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 170.4520, GNorm = 0.2477
Meta loss on this task batch = 2.8338e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.4578, GNorm = 0.2488
Meta loss on this task batch = 2.4117e-01, Meta loss averaged over last 500 steps = 2.5974e-01, PNorm = 170.4633, GNorm = 0.2357
Meta loss on this task batch = 2.2700e-01, Meta loss averaged over last 500 steps = 2.5966e-01, PNorm = 170.4690, GNorm = 0.2168
Meta loss on this task batch = 2.5318e-01, Meta loss averaged over last 500 steps = 2.5971e-01, PNorm = 170.4753, GNorm = 0.2064
Meta loss on this task batch = 2.7331e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.4810, GNorm = 0.1869
Meta loss on this task batch = 2.1313e-01, Meta loss averaged over last 500 steps = 2.5968e-01, PNorm = 170.4864, GNorm = 0.2365
Meta loss on this task batch = 2.2745e-01, Meta loss averaged over last 500 steps = 2.5963e-01, PNorm = 170.4921, GNorm = 0.2392
Meta loss on this task batch = 3.2792e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 170.4973, GNorm = 0.2646
Meta loss on this task batch = 2.9837e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.5017, GNorm = 0.2612
Meta loss on this task batch = 2.3583e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 170.5057, GNorm = 0.2074
Meta loss on this task batch = 3.1052e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 170.5091, GNorm = 0.2470
Meta loss on this task batch = 1.7867e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.5129, GNorm = 0.2889
Took 111.71577906608582 seconds to complete one epoch of meta training
Took 119.11606478691101 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496634
Epoch 836
Meta loss on this task batch = 2.1308e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.5165, GNorm = 0.2697
Meta loss on this task batch = 2.6829e-01, Meta loss averaged over last 500 steps = 2.5974e-01, PNorm = 170.5189, GNorm = 0.2623
Meta loss on this task batch = 2.5879e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.5214, GNorm = 0.2215
Meta loss on this task batch = 2.3887e-01, Meta loss averaged over last 500 steps = 2.5968e-01, PNorm = 170.5245, GNorm = 0.2154
Meta loss on this task batch = 2.7836e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.5280, GNorm = 0.2372
Meta loss on this task batch = 2.7097e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 170.5315, GNorm = 0.2332
Meta loss on this task batch = 2.6385e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.5351, GNorm = 0.2343
Meta loss on this task batch = 3.0590e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.5393, GNorm = 0.2515
Meta loss on this task batch = 2.5471e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.5435, GNorm = 0.1955
Meta loss on this task batch = 2.7963e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.5478, GNorm = 0.2424
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.5531, GNorm = 0.2145
Meta loss on this task batch = 2.8638e-01, Meta loss averaged over last 500 steps = 2.5997e-01, PNorm = 170.5576, GNorm = 0.2359
Meta loss on this task batch = 2.4065e-01, Meta loss averaged over last 500 steps = 2.6000e-01, PNorm = 170.5619, GNorm = 0.2319
Meta loss on this task batch = 2.5703e-01, Meta loss averaged over last 500 steps = 2.6003e-01, PNorm = 170.5662, GNorm = 0.2367
Meta loss on this task batch = 2.5472e-01, Meta loss averaged over last 500 steps = 2.6002e-01, PNorm = 170.5702, GNorm = 0.2218
Meta loss on this task batch = 2.4429e-01, Meta loss averaged over last 500 steps = 2.5995e-01, PNorm = 170.5749, GNorm = 0.2480
Meta loss on this task batch = 2.5653e-01, Meta loss averaged over last 500 steps = 2.5994e-01, PNorm = 170.5793, GNorm = 0.2298
Meta loss on this task batch = 2.7131e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.5832, GNorm = 0.2186
Meta loss on this task batch = 2.1325e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.5880, GNorm = 0.2109
Took 106.49747776985168 seconds to complete one epoch of meta training
Took 113.6446053981781 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487695
Epoch 837
Meta loss on this task batch = 2.1284e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.5935, GNorm = 0.1888
Meta loss on this task batch = 2.9986e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.5988, GNorm = 0.2376
Meta loss on this task batch = 2.5870e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.6038, GNorm = 0.2648
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 170.6092, GNorm = 0.3194
Meta loss on this task batch = 2.4275e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 170.6137, GNorm = 0.2016
Meta loss on this task batch = 2.6457e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.6184, GNorm = 0.2236
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.6233, GNorm = 0.2111
Meta loss on this task batch = 3.3256e-01, Meta loss averaged over last 500 steps = 2.5995e-01, PNorm = 170.6274, GNorm = 0.2566
Meta loss on this task batch = 2.2006e-01, Meta loss averaged over last 500 steps = 2.5999e-01, PNorm = 170.6315, GNorm = 0.1917
Meta loss on this task batch = 2.6419e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.6357, GNorm = 0.2386
Meta loss on this task batch = 2.3616e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.6404, GNorm = 0.1873
Meta loss on this task batch = 2.2701e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 170.6451, GNorm = 0.1833
Meta loss on this task batch = 2.7726e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 170.6501, GNorm = 0.2272
Meta loss on this task batch = 2.4202e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 170.6544, GNorm = 0.1917
Meta loss on this task batch = 2.4162e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.6590, GNorm = 0.2031
Meta loss on this task batch = 2.6341e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.6638, GNorm = 0.2345
Meta loss on this task batch = 2.6223e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.6684, GNorm = 0.2087
Meta loss on this task batch = 2.1286e-01, Meta loss averaged over last 500 steps = 2.5968e-01, PNorm = 170.6739, GNorm = 0.2449
Meta loss on this task batch = 2.6561e-01, Meta loss averaged over last 500 steps = 2.5963e-01, PNorm = 170.6787, GNorm = 0.2702
Took 109.57692193984985 seconds to complete one epoch of meta training
Took 116.90969347953796 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492484
Epoch 838
Meta loss on this task batch = 2.5099e-01, Meta loss averaged over last 500 steps = 2.5962e-01, PNorm = 170.6833, GNorm = 0.2252
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 170.6878, GNorm = 0.2782
Meta loss on this task batch = 2.7538e-01, Meta loss averaged over last 500 steps = 2.5964e-01, PNorm = 170.6920, GNorm = 0.2523
Meta loss on this task batch = 2.4722e-01, Meta loss averaged over last 500 steps = 2.5968e-01, PNorm = 170.6960, GNorm = 0.2229
Meta loss on this task batch = 2.7140e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 170.6990, GNorm = 0.2396
Meta loss on this task batch = 2.7696e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 170.7019, GNorm = 0.3189
Meta loss on this task batch = 2.7220e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 170.7046, GNorm = 0.2449
Meta loss on this task batch = 2.3156e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.7080, GNorm = 0.2286
Meta loss on this task batch = 2.9445e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.7118, GNorm = 0.2413
Meta loss on this task batch = 2.7763e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.7158, GNorm = 0.2505
Meta loss on this task batch = 2.6337e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.7197, GNorm = 0.2296
Meta loss on this task batch = 2.7317e-01, Meta loss averaged over last 500 steps = 2.5988e-01, PNorm = 170.7234, GNorm = 0.2551
Meta loss on this task batch = 2.0694e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.7280, GNorm = 0.2091
Meta loss on this task batch = 2.5347e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 170.7319, GNorm = 0.2354
Meta loss on this task batch = 2.4053e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 170.7362, GNorm = 0.2180
Meta loss on this task batch = 2.3587e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 170.7403, GNorm = 0.2453
Meta loss on this task batch = 2.6567e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 170.7451, GNorm = 0.2529
Meta loss on this task batch = 2.2731e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 170.7503, GNorm = 0.2334
Meta loss on this task batch = 2.5084e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.7555, GNorm = 0.2564
Took 106.50093245506287 seconds to complete one epoch of meta training
Took 113.7101321220398 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491942
Epoch 839
Meta loss on this task batch = 3.0751e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 170.7603, GNorm = 0.2251
Meta loss on this task batch = 2.6144e-01, Meta loss averaged over last 500 steps = 2.5970e-01, PNorm = 170.7653, GNorm = 0.2356
Meta loss on this task batch = 2.5927e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.7699, GNorm = 0.2216
Meta loss on this task batch = 2.2411e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.7748, GNorm = 0.1921
Meta loss on this task batch = 2.8620e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.7790, GNorm = 0.2310
Meta loss on this task batch = 3.2782e-01, Meta loss averaged over last 500 steps = 2.5988e-01, PNorm = 170.7816, GNorm = 0.2772
Meta loss on this task batch = 3.0342e-01, Meta loss averaged over last 500 steps = 2.5994e-01, PNorm = 170.7840, GNorm = 0.2509
Meta loss on this task batch = 2.6140e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.7868, GNorm = 0.1933
Meta loss on this task batch = 2.4591e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 170.7894, GNorm = 0.2084
Meta loss on this task batch = 2.3800e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.7928, GNorm = 0.2101
Meta loss on this task batch = 2.5084e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.7969, GNorm = 0.2031
Meta loss on this task batch = 2.4430e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.8018, GNorm = 0.2502
Meta loss on this task batch = 2.5467e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.8071, GNorm = 0.2426
Meta loss on this task batch = 2.6263e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 170.8127, GNorm = 0.2245
Meta loss on this task batch = 2.5985e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.8181, GNorm = 0.2409
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 170.8230, GNorm = 0.2138
Meta loss on this task batch = 2.5731e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 170.8281, GNorm = 0.2146
Meta loss on this task batch = 2.5118e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 170.8336, GNorm = 0.2312
Meta loss on this task batch = 2.3577e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 170.8394, GNorm = 0.3079
Took 104.97389221191406 seconds to complete one epoch of meta training
Took 112.09986281394958 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515910
Epoch 840
Meta loss on this task batch = 2.0621e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 170.8455, GNorm = 0.2262
Meta loss on this task batch = 2.6505e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 170.8514, GNorm = 0.2593
Meta loss on this task batch = 2.7143e-01, Meta loss averaged over last 500 steps = 2.5976e-01, PNorm = 170.8568, GNorm = 0.2384
Meta loss on this task batch = 2.0744e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 170.8621, GNorm = 0.2036
Meta loss on this task batch = 2.8590e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 170.8665, GNorm = 0.2833
Meta loss on this task batch = 2.3182e-01, Meta loss averaged over last 500 steps = 2.5967e-01, PNorm = 170.8707, GNorm = 0.2062
Meta loss on this task batch = 2.6406e-01, Meta loss averaged over last 500 steps = 2.5968e-01, PNorm = 170.8745, GNorm = 0.2264
Meta loss on this task batch = 2.8018e-01, Meta loss averaged over last 500 steps = 2.5965e-01, PNorm = 170.8776, GNorm = 0.2225
Meta loss on this task batch = 2.7708e-01, Meta loss averaged over last 500 steps = 2.5964e-01, PNorm = 170.8808, GNorm = 0.2472
Meta loss on this task batch = 3.0898e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 170.8834, GNorm = 0.2507
Meta loss on this task batch = 2.4079e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 170.8862, GNorm = 0.2221
Meta loss on this task batch = 2.8403e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.8892, GNorm = 0.2697
Meta loss on this task batch = 2.5804e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.8915, GNorm = 0.2427
Meta loss on this task batch = 2.6282e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 170.8937, GNorm = 0.2268
Meta loss on this task batch = 2.4877e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.8972, GNorm = 0.2244
Meta loss on this task batch = 2.4629e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 170.9012, GNorm = 0.2012
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 170.9056, GNorm = 0.2374
Meta loss on this task batch = 3.3586e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 170.9098, GNorm = 0.2451
Meta loss on this task batch = 2.3198e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 170.9146, GNorm = 0.2417
Took 108.24228096008301 seconds to complete one epoch of meta training
Took 115.7256510257721 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480481
Epoch 841
Meta loss on this task batch = 2.4630e-01, Meta loss averaged over last 500 steps = 2.6003e-01, PNorm = 170.9195, GNorm = 0.2516
Meta loss on this task batch = 2.5096e-01, Meta loss averaged over last 500 steps = 2.5989e-01, PNorm = 170.9240, GNorm = 0.2242
Meta loss on this task batch = 2.8726e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 170.9283, GNorm = 0.2242
Meta loss on this task batch = 2.2422e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 170.9326, GNorm = 0.2112
Meta loss on this task batch = 2.1326e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 170.9373, GNorm = 0.2091
Meta loss on this task batch = 2.5617e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 170.9420, GNorm = 0.2509
Meta loss on this task batch = 3.0528e-01, Meta loss averaged over last 500 steps = 2.5988e-01, PNorm = 170.9462, GNorm = 0.2407
Meta loss on this task batch = 1.9490e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 170.9512, GNorm = 0.2219
Meta loss on this task batch = 2.4380e-01, Meta loss averaged over last 500 steps = 2.5970e-01, PNorm = 170.9565, GNorm = 0.2119
Meta loss on this task batch = 2.3211e-01, Meta loss averaged over last 500 steps = 2.5961e-01, PNorm = 170.9614, GNorm = 0.2160
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.5963e-01, PNorm = 170.9656, GNorm = 0.2870
Meta loss on this task batch = 2.8687e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 170.9688, GNorm = 0.2708
Meta loss on this task batch = 2.8843e-01, Meta loss averaged over last 500 steps = 2.5973e-01, PNorm = 170.9722, GNorm = 0.2112
Meta loss on this task batch = 3.3631e-01, Meta loss averaged over last 500 steps = 2.5989e-01, PNorm = 170.9756, GNorm = 0.2826
Meta loss on this task batch = 2.7219e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 170.9791, GNorm = 0.2457
Meta loss on this task batch = 2.4504e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 170.9825, GNorm = 0.2296
Meta loss on this task batch = 2.8366e-01, Meta loss averaged over last 500 steps = 2.5989e-01, PNorm = 170.9854, GNorm = 0.2421
Meta loss on this task batch = 2.5100e-01, Meta loss averaged over last 500 steps = 2.5987e-01, PNorm = 170.9888, GNorm = 0.2635
Meta loss on this task batch = 2.5132e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 170.9924, GNorm = 0.2658
Took 108.33892774581909 seconds to complete one epoch of meta training
Took 116.0378098487854 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493443
Epoch 842
Meta loss on this task batch = 2.7642e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 170.9957, GNorm = 0.2320
Meta loss on this task batch = 2.3279e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 170.9991, GNorm = 0.2051
Meta loss on this task batch = 2.9978e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 171.0022, GNorm = 0.2304
Meta loss on this task batch = 2.9395e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 171.0049, GNorm = 0.2901
Meta loss on this task batch = 2.7048e-01, Meta loss averaged over last 500 steps = 2.6019e-01, PNorm = 171.0074, GNorm = 0.2225
Meta loss on this task batch = 2.5424e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 171.0101, GNorm = 0.2397
Meta loss on this task batch = 2.3083e-01, Meta loss averaged over last 500 steps = 2.6018e-01, PNorm = 171.0132, GNorm = 0.1936
Meta loss on this task batch = 2.6228e-01, Meta loss averaged over last 500 steps = 2.6009e-01, PNorm = 171.0169, GNorm = 0.2177
Meta loss on this task batch = 2.8289e-01, Meta loss averaged over last 500 steps = 2.6003e-01, PNorm = 171.0208, GNorm = 0.2449
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 2.6001e-01, PNorm = 171.0250, GNorm = 0.2505
Meta loss on this task batch = 2.4191e-01, Meta loss averaged over last 500 steps = 2.6000e-01, PNorm = 171.0298, GNorm = 0.2343
Meta loss on this task batch = 2.5834e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 171.0347, GNorm = 0.2699
Meta loss on this task batch = 2.5682e-01, Meta loss averaged over last 500 steps = 2.6009e-01, PNorm = 171.0403, GNorm = 0.2413
Meta loss on this task batch = 2.4464e-01, Meta loss averaged over last 500 steps = 2.6009e-01, PNorm = 171.0458, GNorm = 0.2243
Meta loss on this task batch = 2.8089e-01, Meta loss averaged over last 500 steps = 2.6006e-01, PNorm = 171.0508, GNorm = 0.2356
Meta loss on this task batch = 2.8593e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 171.0558, GNorm = 0.2378
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.6004e-01, PNorm = 171.0610, GNorm = 0.2015
Meta loss on this task batch = 1.9753e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 171.0664, GNorm = 0.2011
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 2.5996e-01, PNorm = 171.0711, GNorm = 0.2671
Took 109.6493124961853 seconds to complete one epoch of meta training
Took 117.85313534736633 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504723
Epoch 843
Meta loss on this task batch = 2.2052e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 171.0754, GNorm = 0.1973
Meta loss on this task batch = 2.9409e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 171.0794, GNorm = 0.2612
Meta loss on this task batch = 2.9935e-01, Meta loss averaged over last 500 steps = 2.6012e-01, PNorm = 171.0833, GNorm = 0.2310
Meta loss on this task batch = 2.8411e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 171.0867, GNorm = 0.2931
Meta loss on this task batch = 2.4991e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 171.0902, GNorm = 0.2749
Meta loss on this task batch = 2.8865e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 171.0938, GNorm = 0.2699
Meta loss on this task batch = 2.4462e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 171.0976, GNorm = 0.2327
Meta loss on this task batch = 2.7667e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 171.1018, GNorm = 0.2481
Meta loss on this task batch = 2.6470e-01, Meta loss averaged over last 500 steps = 2.6022e-01, PNorm = 171.1063, GNorm = 0.2381
Meta loss on this task batch = 2.5467e-01, Meta loss averaged over last 500 steps = 2.6023e-01, PNorm = 171.1109, GNorm = 0.2379
Meta loss on this task batch = 2.7965e-01, Meta loss averaged over last 500 steps = 2.6030e-01, PNorm = 171.1163, GNorm = 0.2546
Meta loss on this task batch = 2.5337e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 171.1213, GNorm = 0.2448
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.6042e-01, PNorm = 171.1262, GNorm = 0.2890
Meta loss on this task batch = 2.7173e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 171.1315, GNorm = 0.2205
Meta loss on this task batch = 2.4315e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 171.1365, GNorm = 0.2163
Meta loss on this task batch = 2.5571e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 171.1418, GNorm = 0.2499
Meta loss on this task batch = 2.9713e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 171.1471, GNorm = 0.2502
Meta loss on this task batch = 2.7005e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 171.1523, GNorm = 0.2378
Meta loss on this task batch = 2.1643e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 171.1573, GNorm = 0.2276
Took 108.15066981315613 seconds to complete one epoch of meta training
Took 115.39515948295593 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496545
Epoch 844
Meta loss on this task batch = 2.6514e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 171.1626, GNorm = 0.2529
Meta loss on this task batch = 2.7573e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 171.1680, GNorm = 0.2128
Meta loss on this task batch = 2.4951e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 171.1728, GNorm = 0.1863
Meta loss on this task batch = 2.0294e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 171.1782, GNorm = 0.1854
Meta loss on this task batch = 2.6510e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 171.1832, GNorm = 0.2132
Meta loss on this task batch = 2.9077e-01, Meta loss averaged over last 500 steps = 2.6037e-01, PNorm = 171.1882, GNorm = 0.2590
Meta loss on this task batch = 2.6125e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 171.1937, GNorm = 0.2129
Meta loss on this task batch = 2.0796e-01, Meta loss averaged over last 500 steps = 2.6017e-01, PNorm = 171.1994, GNorm = 0.2313
Meta loss on this task batch = 3.0258e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 171.2041, GNorm = 0.2596
Meta loss on this task batch = 2.6504e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 171.2090, GNorm = 0.3003
Meta loss on this task batch = 2.1741e-01, Meta loss averaged over last 500 steps = 2.6035e-01, PNorm = 171.2146, GNorm = 0.1840
Meta loss on this task batch = 2.8936e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 171.2204, GNorm = 0.2076
Meta loss on this task batch = 2.5314e-01, Meta loss averaged over last 500 steps = 2.6034e-01, PNorm = 171.2263, GNorm = 0.2769
Meta loss on this task batch = 3.2708e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 171.2314, GNorm = 0.3530
Meta loss on this task batch = 2.6503e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 171.2369, GNorm = 0.2628
Meta loss on this task batch = 2.9162e-01, Meta loss averaged over last 500 steps = 2.6074e-01, PNorm = 171.2410, GNorm = 0.2884
Meta loss on this task batch = 2.5112e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 171.2452, GNorm = 0.2063
Meta loss on this task batch = 2.2758e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 171.2489, GNorm = 0.2385
Meta loss on this task batch = 2.6984e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 171.2517, GNorm = 0.3149
Took 107.85357522964478 seconds to complete one epoch of meta training
Took 115.14071393013 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495375
Epoch 845
Meta loss on this task batch = 2.6357e-01, Meta loss averaged over last 500 steps = 2.6046e-01, PNorm = 171.2549, GNorm = 0.2174
Meta loss on this task batch = 2.7091e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 171.2586, GNorm = 0.2151
Meta loss on this task batch = 2.2232e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 171.2623, GNorm = 0.2674
Meta loss on this task batch = 2.3640e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 171.2662, GNorm = 0.2301
Meta loss on this task batch = 2.7787e-01, Meta loss averaged over last 500 steps = 2.6023e-01, PNorm = 171.2706, GNorm = 0.2315
Meta loss on this task batch = 2.5573e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 171.2753, GNorm = 0.2611
Meta loss on this task batch = 2.2496e-01, Meta loss averaged over last 500 steps = 2.6006e-01, PNorm = 171.2810, GNorm = 0.2203
Meta loss on this task batch = 3.1946e-01, Meta loss averaged over last 500 steps = 2.6011e-01, PNorm = 171.2862, GNorm = 0.3054
Meta loss on this task batch = 2.9633e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 171.2914, GNorm = 0.2456
Meta loss on this task batch = 2.3983e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 171.2964, GNorm = 0.2043
Meta loss on this task batch = 2.4146e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 171.3013, GNorm = 0.2257
Meta loss on this task batch = 2.6728e-01, Meta loss averaged over last 500 steps = 2.6039e-01, PNorm = 171.3054, GNorm = 0.2032
Meta loss on this task batch = 2.9423e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 171.3086, GNorm = 0.3196
Meta loss on this task batch = 2.7736e-01, Meta loss averaged over last 500 steps = 2.6066e-01, PNorm = 171.3117, GNorm = 0.2319
Meta loss on this task batch = 2.4745e-01, Meta loss averaged over last 500 steps = 2.6056e-01, PNorm = 171.3144, GNorm = 0.2588
Meta loss on this task batch = 2.2411e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 171.3170, GNorm = 0.2255
Meta loss on this task batch = 3.0799e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 171.3201, GNorm = 0.2684
Meta loss on this task batch = 2.8626e-01, Meta loss averaged over last 500 steps = 2.6059e-01, PNorm = 171.3244, GNorm = 0.2701
Meta loss on this task batch = 2.9726e-01, Meta loss averaged over last 500 steps = 2.6058e-01, PNorm = 171.3276, GNorm = 0.2779
Took 105.41885232925415 seconds to complete one epoch of meta training
Took 112.52950096130371 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488062
Epoch 846
Meta loss on this task batch = 2.5831e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 171.3320, GNorm = 0.2468
Meta loss on this task batch = 2.8229e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 171.3359, GNorm = 0.2621
Meta loss on this task batch = 2.4867e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 171.3402, GNorm = 0.2365
Meta loss on this task batch = 2.8782e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 171.3448, GNorm = 0.2165
Meta loss on this task batch = 2.4674e-01, Meta loss averaged over last 500 steps = 2.6070e-01, PNorm = 171.3494, GNorm = 0.2300
Meta loss on this task batch = 2.8611e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 171.3539, GNorm = 0.2348
Meta loss on this task batch = 2.3938e-01, Meta loss averaged over last 500 steps = 2.6072e-01, PNorm = 171.3589, GNorm = 0.2133
Meta loss on this task batch = 2.2378e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 171.3638, GNorm = 0.2025
Meta loss on this task batch = 2.6985e-01, Meta loss averaged over last 500 steps = 2.6062e-01, PNorm = 171.3687, GNorm = 0.2295
Meta loss on this task batch = 3.1842e-01, Meta loss averaged over last 500 steps = 2.6076e-01, PNorm = 171.3729, GNorm = 0.2357
Meta loss on this task batch = 2.6710e-01, Meta loss averaged over last 500 steps = 2.6084e-01, PNorm = 171.3774, GNorm = 0.2110
Meta loss on this task batch = 2.8396e-01, Meta loss averaged over last 500 steps = 2.6086e-01, PNorm = 171.3818, GNorm = 0.2809
Meta loss on this task batch = 2.2030e-01, Meta loss averaged over last 500 steps = 2.6075e-01, PNorm = 171.3858, GNorm = 0.2415
Meta loss on this task batch = 2.8267e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 171.3899, GNorm = 0.2322
Meta loss on this task batch = 2.3563e-01, Meta loss averaged over last 500 steps = 2.6068e-01, PNorm = 171.3948, GNorm = 0.2419
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 2.6080e-01, PNorm = 171.3984, GNorm = 0.2701
Meta loss on this task batch = 2.2431e-01, Meta loss averaged over last 500 steps = 2.6081e-01, PNorm = 171.4024, GNorm = 0.2046
Meta loss on this task batch = 1.8046e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 171.4068, GNorm = 0.2067
Meta loss on this task batch = 2.8144e-01, Meta loss averaged over last 500 steps = 2.6064e-01, PNorm = 171.4114, GNorm = 0.2989
Took 106.78752088546753 seconds to complete one epoch of meta training
Took 113.9957582950592 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488526
Epoch 847
Meta loss on this task batch = 2.7402e-01, Meta loss averaged over last 500 steps = 2.6069e-01, PNorm = 171.4165, GNorm = 0.2558
Meta loss on this task batch = 2.2057e-01, Meta loss averaged over last 500 steps = 2.6053e-01, PNorm = 171.4212, GNorm = 0.2141
Meta loss on this task batch = 2.5481e-01, Meta loss averaged over last 500 steps = 2.6052e-01, PNorm = 171.4261, GNorm = 0.2234
Meta loss on this task batch = 2.2167e-01, Meta loss averaged over last 500 steps = 2.6040e-01, PNorm = 171.4307, GNorm = 0.2011
Meta loss on this task batch = 2.4479e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 171.4355, GNorm = 0.2112
Meta loss on this task batch = 2.4685e-01, Meta loss averaged over last 500 steps = 2.6045e-01, PNorm = 171.4405, GNorm = 0.2537
Meta loss on this task batch = 2.7744e-01, Meta loss averaged over last 500 steps = 2.6055e-01, PNorm = 171.4454, GNorm = 0.2180
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 2.6063e-01, PNorm = 171.4510, GNorm = 0.2242
Meta loss on this task batch = 2.7817e-01, Meta loss averaged over last 500 steps = 2.6061e-01, PNorm = 171.4565, GNorm = 0.2697
Meta loss on this task batch = 2.3104e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 171.4625, GNorm = 0.2160
Meta loss on this task batch = 2.1534e-01, Meta loss averaged over last 500 steps = 2.6043e-01, PNorm = 171.4684, GNorm = 0.2015
Meta loss on this task batch = 2.7681e-01, Meta loss averaged over last 500 steps = 2.6049e-01, PNorm = 171.4744, GNorm = 0.1981
Meta loss on this task batch = 2.2377e-01, Meta loss averaged over last 500 steps = 2.6047e-01, PNorm = 171.4802, GNorm = 0.2184
Meta loss on this task batch = 2.7741e-01, Meta loss averaged over last 500 steps = 2.6051e-01, PNorm = 171.4852, GNorm = 0.3068
Meta loss on this task batch = 2.5916e-01, Meta loss averaged over last 500 steps = 2.6048e-01, PNorm = 171.4905, GNorm = 0.2620
Meta loss on this task batch = 2.6561e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 171.4957, GNorm = 0.2331
Meta loss on this task batch = 2.0849e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 171.5016, GNorm = 0.2189
Meta loss on this task batch = 2.7249e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 171.5071, GNorm = 0.2273
Meta loss on this task batch = 2.9344e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 171.5123, GNorm = 0.2648
Took 110.20733666419983 seconds to complete one epoch of meta training
Took 117.80815124511719 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478771
Epoch 848
Meta loss on this task batch = 2.2639e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 171.5178, GNorm = 0.2197
Meta loss on this task batch = 2.4501e-01, Meta loss averaged over last 500 steps = 2.6004e-01, PNorm = 171.5234, GNorm = 0.2584
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 171.5288, GNorm = 0.2396
Meta loss on this task batch = 2.6713e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 171.5342, GNorm = 0.2735
Meta loss on this task batch = 2.7483e-01, Meta loss averaged over last 500 steps = 2.6021e-01, PNorm = 171.5397, GNorm = 0.2432
Meta loss on this task batch = 2.5770e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 171.5454, GNorm = 0.2225
Meta loss on this task batch = 3.0994e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 171.5504, GNorm = 0.2687
Meta loss on this task batch = 2.5650e-01, Meta loss averaged over last 500 steps = 2.6020e-01, PNorm = 171.5544, GNorm = 0.2709
Meta loss on this task batch = 2.8216e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 171.5579, GNorm = 0.3029
Meta loss on this task batch = 2.4613e-01, Meta loss averaged over last 500 steps = 2.6009e-01, PNorm = 171.5618, GNorm = 0.2249
Meta loss on this task batch = 2.5417e-01, Meta loss averaged over last 500 steps = 2.6005e-01, PNorm = 171.5657, GNorm = 0.2580
Meta loss on this task batch = 2.1795e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 171.5700, GNorm = 0.2040
Meta loss on this task batch = 2.4117e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 171.5741, GNorm = 0.2301
Meta loss on this task batch = 2.5001e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 171.5790, GNorm = 0.2565
Meta loss on this task batch = 2.4180e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 171.5833, GNorm = 0.2627
Meta loss on this task batch = 2.5443e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 171.5875, GNorm = 0.2019
Meta loss on this task batch = 2.9597e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 171.5924, GNorm = 0.2799
Meta loss on this task batch = 2.6298e-01, Meta loss averaged over last 500 steps = 2.5997e-01, PNorm = 171.5972, GNorm = 0.2599
Meta loss on this task batch = 2.7159e-01, Meta loss averaged over last 500 steps = 2.6007e-01, PNorm = 171.6023, GNorm = 0.2488
Took 110.38314390182495 seconds to complete one epoch of meta training
Took 117.2416398525238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454764
Epoch 849
Meta loss on this task batch = 2.4431e-01, Meta loss averaged over last 500 steps = 2.6007e-01, PNorm = 171.6074, GNorm = 0.2221
Meta loss on this task batch = 2.2433e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 171.6130, GNorm = 0.2063
Meta loss on this task batch = 2.9379e-01, Meta loss averaged over last 500 steps = 2.6015e-01, PNorm = 171.6177, GNorm = 0.2395
Meta loss on this task batch = 2.7808e-01, Meta loss averaged over last 500 steps = 2.6015e-01, PNorm = 171.6226, GNorm = 0.2569
Meta loss on this task batch = 2.3224e-01, Meta loss averaged over last 500 steps = 2.6018e-01, PNorm = 171.6280, GNorm = 0.2311
Meta loss on this task batch = 2.6613e-01, Meta loss averaged over last 500 steps = 2.6015e-01, PNorm = 171.6336, GNorm = 0.2857
Meta loss on this task batch = 3.2947e-01, Meta loss averaged over last 500 steps = 2.6025e-01, PNorm = 171.6378, GNorm = 0.2833
Meta loss on this task batch = 2.4684e-01, Meta loss averaged over last 500 steps = 2.6033e-01, PNorm = 171.6422, GNorm = 0.2382
Meta loss on this task batch = 2.6619e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 171.6460, GNorm = 0.2616
Meta loss on this task batch = 2.5391e-01, Meta loss averaged over last 500 steps = 2.6028e-01, PNorm = 171.6503, GNorm = 0.2187
Meta loss on this task batch = 2.6956e-01, Meta loss averaged over last 500 steps = 2.6024e-01, PNorm = 171.6544, GNorm = 0.2168
Meta loss on this task batch = 2.9339e-01, Meta loss averaged over last 500 steps = 2.6036e-01, PNorm = 171.6581, GNorm = 0.2315
Meta loss on this task batch = 2.2912e-01, Meta loss averaged over last 500 steps = 2.6041e-01, PNorm = 171.6626, GNorm = 0.2397
Meta loss on this task batch = 2.0617e-01, Meta loss averaged over last 500 steps = 2.6032e-01, PNorm = 171.6676, GNorm = 0.2084
Meta loss on this task batch = 2.5502e-01, Meta loss averaged over last 500 steps = 2.6031e-01, PNorm = 171.6729, GNorm = 0.2236
Meta loss on this task batch = 2.1300e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 171.6782, GNorm = 0.2128
Meta loss on this task batch = 2.4239e-01, Meta loss averaged over last 500 steps = 2.6026e-01, PNorm = 171.6833, GNorm = 0.2464
Meta loss on this task batch = 2.8269e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 171.6880, GNorm = 0.2478
Meta loss on this task batch = 3.0754e-01, Meta loss averaged over last 500 steps = 2.6027e-01, PNorm = 171.6921, GNorm = 0.3618
Took 109.31507325172424 seconds to complete one epoch of meta training
Took 116.56448316574097 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475527
Epoch 850
Meta loss on this task batch = 2.3122e-01, Meta loss averaged over last 500 steps = 2.6022e-01, PNorm = 171.6967, GNorm = 0.2139
Meta loss on this task batch = 2.3487e-01, Meta loss averaged over last 500 steps = 2.6013e-01, PNorm = 171.7015, GNorm = 0.1863
Meta loss on this task batch = 2.9371e-01, Meta loss averaged over last 500 steps = 2.6017e-01, PNorm = 171.7072, GNorm = 0.2397
Meta loss on this task batch = 3.2756e-01, Meta loss averaged over last 500 steps = 2.6023e-01, PNorm = 171.7126, GNorm = 0.2578
Meta loss on this task batch = 2.5517e-01, Meta loss averaged over last 500 steps = 2.6017e-01, PNorm = 171.7180, GNorm = 0.2144
Meta loss on this task batch = 2.1810e-01, Meta loss averaged over last 500 steps = 2.6003e-01, PNorm = 171.7236, GNorm = 0.1896
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 2.6008e-01, PNorm = 171.7294, GNorm = 0.2110
Meta loss on this task batch = 2.4633e-01, Meta loss averaged over last 500 steps = 2.6011e-01, PNorm = 171.7358, GNorm = 0.2507
Meta loss on this task batch = 2.9242e-01, Meta loss averaged over last 500 steps = 2.6019e-01, PNorm = 171.7416, GNorm = 0.2796
Meta loss on this task batch = 3.0155e-01, Meta loss averaged over last 500 steps = 2.6023e-01, PNorm = 171.7468, GNorm = 0.2456
Meta loss on this task batch = 2.1135e-01, Meta loss averaged over last 500 steps = 2.6016e-01, PNorm = 171.7524, GNorm = 0.2109
Meta loss on this task batch = 2.4164e-01, Meta loss averaged over last 500 steps = 2.6010e-01, PNorm = 171.7582, GNorm = 0.2238
Meta loss on this task batch = 2.2891e-01, Meta loss averaged over last 500 steps = 2.6011e-01, PNorm = 171.7643, GNorm = 0.2076
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.6014e-01, PNorm = 171.7698, GNorm = 0.2533
Meta loss on this task batch = 2.1347e-01, Meta loss averaged over last 500 steps = 2.6004e-01, PNorm = 171.7747, GNorm = 0.2383
Meta loss on this task batch = 2.3205e-01, Meta loss averaged over last 500 steps = 2.5997e-01, PNorm = 171.7800, GNorm = 0.1965
Meta loss on this task batch = 2.2518e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 171.7855, GNorm = 0.2109
Meta loss on this task batch = 3.2142e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 171.7904, GNorm = 0.2577
Meta loss on this task batch = 2.3975e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 171.7948, GNorm = 0.2557
Took 112.20599865913391 seconds to complete one epoch of meta training
Took 118.8709123134613 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505621
Epoch 851
Meta loss on this task batch = 2.0762e-01, Meta loss averaged over last 500 steps = 2.5969e-01, PNorm = 171.7999, GNorm = 0.1889
Meta loss on this task batch = 2.4768e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 171.8053, GNorm = 0.2352
Meta loss on this task batch = 2.6300e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 171.8103, GNorm = 0.2268
Meta loss on this task batch = 2.5154e-01, Meta loss averaged over last 500 steps = 2.5974e-01, PNorm = 171.8151, GNorm = 0.2063
Meta loss on this task batch = 2.4870e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 171.8196, GNorm = 0.2050
Meta loss on this task batch = 2.5956e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 171.8246, GNorm = 0.1996
Meta loss on this task batch = 2.0935e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 171.8294, GNorm = 0.1980
Meta loss on this task batch = 2.4477e-01, Meta loss averaged over last 500 steps = 2.5974e-01, PNorm = 171.8335, GNorm = 0.2526
Meta loss on this task batch = 2.5608e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 171.8383, GNorm = 0.3040
Meta loss on this task batch = 2.8132e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 171.8425, GNorm = 0.2428
Meta loss on this task batch = 2.5317e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 171.8471, GNorm = 0.2080
Meta loss on this task batch = 2.5172e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 171.8510, GNorm = 0.2335
Meta loss on this task batch = 3.0942e-01, Meta loss averaged over last 500 steps = 2.5986e-01, PNorm = 171.8546, GNorm = 0.2123
Meta loss on this task batch = 2.7445e-01, Meta loss averaged over last 500 steps = 2.5995e-01, PNorm = 171.8581, GNorm = 0.2746
Meta loss on this task batch = 2.2823e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 171.8618, GNorm = 0.2174
Meta loss on this task batch = 2.4629e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 171.8653, GNorm = 0.2336
Meta loss on this task batch = 2.5242e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 171.8695, GNorm = 0.2349
Meta loss on this task batch = 2.3593e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 171.8744, GNorm = 0.2275
Meta loss on this task batch = 3.3508e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 171.8795, GNorm = 0.3596
Took 105.05506181716919 seconds to complete one epoch of meta training
Took 113.09363341331482 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.466524
Epoch 852
Meta loss on this task batch = 2.5209e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 171.8846, GNorm = 0.2174
Meta loss on this task batch = 2.6921e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 171.8897, GNorm = 0.2316
Meta loss on this task batch = 2.5716e-01, Meta loss averaged over last 500 steps = 2.5984e-01, PNorm = 171.8952, GNorm = 0.2074
Meta loss on this task batch = 2.8872e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 171.9013, GNorm = 0.2590
Meta loss on this task batch = 2.4543e-01, Meta loss averaged over last 500 steps = 2.5981e-01, PNorm = 171.9071, GNorm = 0.2354
Meta loss on this task batch = 2.1420e-01, Meta loss averaged over last 500 steps = 2.5960e-01, PNorm = 171.9135, GNorm = 0.2078
Meta loss on this task batch = 2.2071e-01, Meta loss averaged over last 500 steps = 2.5941e-01, PNorm = 171.9191, GNorm = 0.2532
Meta loss on this task batch = 2.6293e-01, Meta loss averaged over last 500 steps = 2.5943e-01, PNorm = 171.9248, GNorm = 0.2368
Meta loss on this task batch = 2.5006e-01, Meta loss averaged over last 500 steps = 2.5953e-01, PNorm = 171.9303, GNorm = 0.2400
Meta loss on this task batch = 2.5023e-01, Meta loss averaged over last 500 steps = 2.5954e-01, PNorm = 171.9353, GNorm = 0.2446
Meta loss on this task batch = 2.6795e-01, Meta loss averaged over last 500 steps = 2.5963e-01, PNorm = 171.9388, GNorm = 0.2925
Meta loss on this task batch = 2.1038e-01, Meta loss averaged over last 500 steps = 2.5951e-01, PNorm = 171.9419, GNorm = 0.2467
Meta loss on this task batch = 2.9307e-01, Meta loss averaged over last 500 steps = 2.5962e-01, PNorm = 171.9451, GNorm = 0.2695
Meta loss on this task batch = 2.4631e-01, Meta loss averaged over last 500 steps = 2.5959e-01, PNorm = 171.9479, GNorm = 0.2009
Meta loss on this task batch = 2.4078e-01, Meta loss averaged over last 500 steps = 2.5953e-01, PNorm = 171.9508, GNorm = 0.2209
Meta loss on this task batch = 2.0352e-01, Meta loss averaged over last 500 steps = 2.5936e-01, PNorm = 171.9545, GNorm = 0.2117
Meta loss on this task batch = 2.8299e-01, Meta loss averaged over last 500 steps = 2.5942e-01, PNorm = 171.9583, GNorm = 0.2826
Meta loss on this task batch = 3.1617e-01, Meta loss averaged over last 500 steps = 2.5956e-01, PNorm = 171.9627, GNorm = 0.2659
Meta loss on this task batch = 3.1208e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 171.9679, GNorm = 0.3242
Took 110.626535654068 seconds to complete one epoch of meta training
Took 117.71509885787964 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491156
Epoch 853
Meta loss on this task batch = 2.4176e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 171.9738, GNorm = 0.2111
Meta loss on this task batch = 2.5604e-01, Meta loss averaged over last 500 steps = 2.5967e-01, PNorm = 171.9790, GNorm = 0.2537
Meta loss on this task batch = 2.6747e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 171.9843, GNorm = 0.2446
Meta loss on this task batch = 2.1987e-01, Meta loss averaged over last 500 steps = 2.5966e-01, PNorm = 171.9896, GNorm = 0.1962
Meta loss on this task batch = 2.4214e-01, Meta loss averaged over last 500 steps = 2.5968e-01, PNorm = 171.9944, GNorm = 0.2393
Meta loss on this task batch = 3.2532e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 171.9985, GNorm = 0.2574
Meta loss on this task batch = 2.4284e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 172.0034, GNorm = 0.2391
Meta loss on this task batch = 3.0792e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 172.0072, GNorm = 0.2537
Meta loss on this task batch = 2.8661e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 172.0116, GNorm = 0.2438
Meta loss on this task batch = 2.7014e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 172.0163, GNorm = 0.2207
Meta loss on this task batch = 2.5697e-01, Meta loss averaged over last 500 steps = 2.5991e-01, PNorm = 172.0212, GNorm = 0.2145
Meta loss on this task batch = 1.8962e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 172.0261, GNorm = 0.1777
Meta loss on this task batch = 2.2259e-01, Meta loss averaged over last 500 steps = 2.5998e-01, PNorm = 172.0314, GNorm = 0.2189
Meta loss on this task batch = 2.6254e-01, Meta loss averaged over last 500 steps = 2.5992e-01, PNorm = 172.0372, GNorm = 0.2406
Meta loss on this task batch = 2.5134e-01, Meta loss averaged over last 500 steps = 2.5985e-01, PNorm = 172.0429, GNorm = 0.2360
Meta loss on this task batch = 2.9127e-01, Meta loss averaged over last 500 steps = 2.5993e-01, PNorm = 172.0487, GNorm = 0.2426
Meta loss on this task batch = 2.2756e-01, Meta loss averaged over last 500 steps = 2.5980e-01, PNorm = 172.0544, GNorm = 0.2115
Meta loss on this task batch = 2.5917e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 172.0595, GNorm = 0.2609
Meta loss on this task batch = 2.8226e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 172.0646, GNorm = 0.2979
Took 110.04585981369019 seconds to complete one epoch of meta training
Took 117.64371299743652 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482776
Epoch 854
Meta loss on this task batch = 2.7171e-01, Meta loss averaged over last 500 steps = 2.5979e-01, PNorm = 172.0698, GNorm = 0.2117
Meta loss on this task batch = 2.4593e-01, Meta loss averaged over last 500 steps = 2.5976e-01, PNorm = 172.0750, GNorm = 0.2060
Meta loss on this task batch = 2.7450e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 172.0798, GNorm = 0.2457
Meta loss on this task batch = 2.8250e-01, Meta loss averaged over last 500 steps = 2.5990e-01, PNorm = 172.0837, GNorm = 0.2676
Meta loss on this task batch = 2.6010e-01, Meta loss averaged over last 500 steps = 2.5982e-01, PNorm = 172.0868, GNorm = 0.2512
Meta loss on this task batch = 2.5881e-01, Meta loss averaged over last 500 steps = 2.5975e-01, PNorm = 172.0891, GNorm = 0.2363
Meta loss on this task batch = 1.8312e-01, Meta loss averaged over last 500 steps = 2.5952e-01, PNorm = 172.0915, GNorm = 0.1895
Meta loss on this task batch = 2.3511e-01, Meta loss averaged over last 500 steps = 2.5948e-01, PNorm = 172.0935, GNorm = 0.2312
Meta loss on this task batch = 2.0748e-01, Meta loss averaged over last 500 steps = 2.5945e-01, PNorm = 172.0956, GNorm = 0.2208
Meta loss on this task batch = 3.0812e-01, Meta loss averaged over last 500 steps = 2.5948e-01, PNorm = 172.0973, GNorm = 0.2685
Meta loss on this task batch = 2.3658e-01, Meta loss averaged over last 500 steps = 2.5934e-01, PNorm = 172.0999, GNorm = 0.2240
Meta loss on this task batch = 2.8120e-01, Meta loss averaged over last 500 steps = 2.5940e-01, PNorm = 172.1029, GNorm = 0.2260
Meta loss on this task batch = 2.2545e-01, Meta loss averaged over last 500 steps = 2.5939e-01, PNorm = 172.1055, GNorm = 0.2204
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.5943e-01, PNorm = 172.1084, GNorm = 0.2283
Meta loss on this task batch = 3.1020e-01, Meta loss averaged over last 500 steps = 2.5965e-01, PNorm = 172.1112, GNorm = 0.2749
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 172.1146, GNorm = 0.2998
Meta loss on this task batch = 2.5567e-01, Meta loss averaged over last 500 steps = 2.5959e-01, PNorm = 172.1179, GNorm = 0.2169
Meta loss on this task batch = 2.4437e-01, Meta loss averaged over last 500 steps = 2.5967e-01, PNorm = 172.1218, GNorm = 0.2319
Meta loss on this task batch = 3.0999e-01, Meta loss averaged over last 500 steps = 2.5983e-01, PNorm = 172.1252, GNorm = 0.3496
Took 105.64910316467285 seconds to complete one epoch of meta training
Took 112.81679487228394 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477089
Epoch 855
Meta loss on this task batch = 2.5848e-01, Meta loss averaged over last 500 steps = 2.5978e-01, PNorm = 172.1289, GNorm = 0.2263
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.5977e-01, PNorm = 172.1325, GNorm = 0.2669
Meta loss on this task batch = 2.6033e-01, Meta loss averaged over last 500 steps = 2.5989e-01, PNorm = 172.1365, GNorm = 0.2090
Meta loss on this task batch = 2.4544e-01, Meta loss averaged over last 500 steps = 2.5976e-01, PNorm = 172.1408, GNorm = 0.2242
Meta loss on this task batch = 2.2592e-01, Meta loss averaged over last 500 steps = 2.5971e-01, PNorm = 172.1453, GNorm = 0.2040
Meta loss on this task batch = 2.4497e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 172.1500, GNorm = 0.2475
Meta loss on this task batch = 2.4043e-01, Meta loss averaged over last 500 steps = 2.5955e-01, PNorm = 172.1555, GNorm = 0.2216
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 2.5959e-01, PNorm = 172.1609, GNorm = 0.2347
Meta loss on this task batch = 2.1340e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.1662, GNorm = 0.2433
Meta loss on this task batch = 2.1659e-01, Meta loss averaged over last 500 steps = 2.5941e-01, PNorm = 172.1707, GNorm = 0.2105
Meta loss on this task batch = 2.2974e-01, Meta loss averaged over last 500 steps = 2.5931e-01, PNorm = 172.1751, GNorm = 0.2337
Meta loss on this task batch = 2.8574e-01, Meta loss averaged over last 500 steps = 2.5940e-01, PNorm = 172.1790, GNorm = 0.2551
Meta loss on this task batch = 2.6541e-01, Meta loss averaged over last 500 steps = 2.5933e-01, PNorm = 172.1825, GNorm = 0.2314
Meta loss on this task batch = 2.5521e-01, Meta loss averaged over last 500 steps = 2.5933e-01, PNorm = 172.1846, GNorm = 0.2599
Meta loss on this task batch = 2.7293e-01, Meta loss averaged over last 500 steps = 2.5930e-01, PNorm = 172.1864, GNorm = 0.2774
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 2.5925e-01, PNorm = 172.1883, GNorm = 0.2398
Meta loss on this task batch = 2.7429e-01, Meta loss averaged over last 500 steps = 2.5927e-01, PNorm = 172.1907, GNorm = 0.2215
Meta loss on this task batch = 2.6286e-01, Meta loss averaged over last 500 steps = 2.5925e-01, PNorm = 172.1930, GNorm = 0.2376
Meta loss on this task batch = 2.9836e-01, Meta loss averaged over last 500 steps = 2.5932e-01, PNorm = 172.1956, GNorm = 0.2948
Took 108.9548568725586 seconds to complete one epoch of meta training
Took 116.05701351165771 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479367
Epoch 856
Meta loss on this task batch = 2.6563e-01, Meta loss averaged over last 500 steps = 2.5923e-01, PNorm = 172.1988, GNorm = 0.2550
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.5921e-01, PNorm = 172.2029, GNorm = 0.2030
Meta loss on this task batch = 2.7233e-01, Meta loss averaged over last 500 steps = 2.5928e-01, PNorm = 172.2066, GNorm = 0.2349
Meta loss on this task batch = 2.6815e-01, Meta loss averaged over last 500 steps = 2.5945e-01, PNorm = 172.2110, GNorm = 0.2825
Meta loss on this task batch = 2.4018e-01, Meta loss averaged over last 500 steps = 2.5943e-01, PNorm = 172.2155, GNorm = 0.1848
Meta loss on this task batch = 3.0428e-01, Meta loss averaged over last 500 steps = 2.5960e-01, PNorm = 172.2197, GNorm = 0.2646
Meta loss on this task batch = 3.0694e-01, Meta loss averaged over last 500 steps = 2.5972e-01, PNorm = 172.2231, GNorm = 0.2405
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.5959e-01, PNorm = 172.2263, GNorm = 0.2418
Meta loss on this task batch = 2.4336e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.2298, GNorm = 0.1809
Meta loss on this task batch = 1.8889e-01, Meta loss averaged over last 500 steps = 2.5941e-01, PNorm = 172.2342, GNorm = 0.1817
Meta loss on this task batch = 2.3943e-01, Meta loss averaged over last 500 steps = 2.5941e-01, PNorm = 172.2389, GNorm = 0.1892
Meta loss on this task batch = 2.3084e-01, Meta loss averaged over last 500 steps = 2.5933e-01, PNorm = 172.2434, GNorm = 0.1988
Meta loss on this task batch = 2.8699e-01, Meta loss averaged over last 500 steps = 2.5942e-01, PNorm = 172.2484, GNorm = 0.2388
Meta loss on this task batch = 2.5212e-01, Meta loss averaged over last 500 steps = 2.5948e-01, PNorm = 172.2537, GNorm = 0.2839
Meta loss on this task batch = 3.1957e-01, Meta loss averaged over last 500 steps = 2.5954e-01, PNorm = 172.2588, GNorm = 0.2492
Meta loss on this task batch = 2.4246e-01, Meta loss averaged over last 500 steps = 2.5949e-01, PNorm = 172.2639, GNorm = 0.2004
Meta loss on this task batch = 1.8787e-01, Meta loss averaged over last 500 steps = 2.5944e-01, PNorm = 172.2690, GNorm = 0.1981
Meta loss on this task batch = 2.6997e-01, Meta loss averaged over last 500 steps = 2.5940e-01, PNorm = 172.2731, GNorm = 0.2449
Meta loss on this task batch = 2.9343e-01, Meta loss averaged over last 500 steps = 2.5951e-01, PNorm = 172.2765, GNorm = 0.3072
Took 107.50543260574341 seconds to complete one epoch of meta training
Took 115.00533294677734 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469848
Epoch 857
Meta loss on this task batch = 2.7501e-01, Meta loss averaged over last 500 steps = 2.5946e-01, PNorm = 172.2793, GNorm = 0.2467
Meta loss on this task batch = 2.6438e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.2817, GNorm = 0.2778
Meta loss on this task batch = 2.4976e-01, Meta loss averaged over last 500 steps = 2.5945e-01, PNorm = 172.2841, GNorm = 0.2040
Meta loss on this task batch = 2.1304e-01, Meta loss averaged over last 500 steps = 2.5919e-01, PNorm = 172.2868, GNorm = 0.2132
Meta loss on this task batch = 2.3510e-01, Meta loss averaged over last 500 steps = 2.5920e-01, PNorm = 172.2897, GNorm = 0.2081
Meta loss on this task batch = 2.6978e-01, Meta loss averaged over last 500 steps = 2.5939e-01, PNorm = 172.2931, GNorm = 0.2156
Meta loss on this task batch = 2.8217e-01, Meta loss averaged over last 500 steps = 2.5940e-01, PNorm = 172.2963, GNorm = 0.2967
Meta loss on this task batch = 3.1264e-01, Meta loss averaged over last 500 steps = 2.5955e-01, PNorm = 172.2995, GNorm = 0.2497
Meta loss on this task batch = 2.0924e-01, Meta loss averaged over last 500 steps = 2.5954e-01, PNorm = 172.3030, GNorm = 0.2328
Meta loss on this task batch = 2.3642e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.3073, GNorm = 0.1919
Meta loss on this task batch = 2.6205e-01, Meta loss averaged over last 500 steps = 2.5958e-01, PNorm = 172.3120, GNorm = 0.1922
Meta loss on this task batch = 2.2905e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.3174, GNorm = 0.2042
Meta loss on this task batch = 2.4481e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.3233, GNorm = 0.2133
Meta loss on this task batch = 2.5433e-01, Meta loss averaged over last 500 steps = 2.5938e-01, PNorm = 172.3291, GNorm = 0.2087
Meta loss on this task batch = 2.6828e-01, Meta loss averaged over last 500 steps = 2.5942e-01, PNorm = 172.3350, GNorm = 0.2580
Meta loss on this task batch = 2.4710e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.3414, GNorm = 0.2301
Meta loss on this task batch = 2.9248e-01, Meta loss averaged over last 500 steps = 2.5939e-01, PNorm = 172.3468, GNorm = 0.2590
Meta loss on this task batch = 2.5813e-01, Meta loss averaged over last 500 steps = 2.5929e-01, PNorm = 172.3513, GNorm = 0.2431
Meta loss on this task batch = 2.6968e-01, Meta loss averaged over last 500 steps = 2.5936e-01, PNorm = 172.3572, GNorm = 0.2982
Took 110.29991936683655 seconds to complete one epoch of meta training
Took 117.69018769264221 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494927
Epoch 858
Meta loss on this task batch = 2.9945e-01, Meta loss averaged over last 500 steps = 2.5949e-01, PNorm = 172.3625, GNorm = 0.3317
Meta loss on this task batch = 2.3156e-01, Meta loss averaged over last 500 steps = 2.5945e-01, PNorm = 172.3674, GNorm = 0.2213
Meta loss on this task batch = 2.7818e-01, Meta loss averaged over last 500 steps = 2.5949e-01, PNorm = 172.3720, GNorm = 0.2736
Meta loss on this task batch = 2.5488e-01, Meta loss averaged over last 500 steps = 2.5946e-01, PNorm = 172.3760, GNorm = 0.2583
Meta loss on this task batch = 2.2876e-01, Meta loss averaged over last 500 steps = 2.5942e-01, PNorm = 172.3805, GNorm = 0.2335
Meta loss on this task batch = 2.8142e-01, Meta loss averaged over last 500 steps = 2.5942e-01, PNorm = 172.3837, GNorm = 0.2889
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.5950e-01, PNorm = 172.3872, GNorm = 0.2152
Meta loss on this task batch = 2.3277e-01, Meta loss averaged over last 500 steps = 2.5937e-01, PNorm = 172.3912, GNorm = 0.2236
Meta loss on this task batch = 2.5684e-01, Meta loss averaged over last 500 steps = 2.5936e-01, PNorm = 172.3956, GNorm = 0.2161
Meta loss on this task batch = 3.0753e-01, Meta loss averaged over last 500 steps = 2.5942e-01, PNorm = 172.4004, GNorm = 0.2359
Meta loss on this task batch = 2.2678e-01, Meta loss averaged over last 500 steps = 2.5939e-01, PNorm = 172.4055, GNorm = 0.2322
Meta loss on this task batch = 2.4794e-01, Meta loss averaged over last 500 steps = 2.5937e-01, PNorm = 172.4111, GNorm = 0.2450
Meta loss on this task batch = 2.4772e-01, Meta loss averaged over last 500 steps = 2.5930e-01, PNorm = 172.4169, GNorm = 0.2434
Meta loss on this task batch = 2.3788e-01, Meta loss averaged over last 500 steps = 2.5918e-01, PNorm = 172.4231, GNorm = 0.2457
Meta loss on this task batch = 2.6121e-01, Meta loss averaged over last 500 steps = 2.5924e-01, PNorm = 172.4283, GNorm = 0.2454
Meta loss on this task batch = 2.3552e-01, Meta loss averaged over last 500 steps = 2.5921e-01, PNorm = 172.4336, GNorm = 0.2187
Meta loss on this task batch = 2.2144e-01, Meta loss averaged over last 500 steps = 2.5906e-01, PNorm = 172.4381, GNorm = 0.2192
Meta loss on this task batch = 2.5544e-01, Meta loss averaged over last 500 steps = 2.5900e-01, PNorm = 172.4427, GNorm = 0.2097
Meta loss on this task batch = 2.6452e-01, Meta loss averaged over last 500 steps = 2.5899e-01, PNorm = 172.4469, GNorm = 0.2613
Took 109.27413249015808 seconds to complete one epoch of meta training
Took 116.50993132591248 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502039
Epoch 859
Meta loss on this task batch = 1.8482e-01, Meta loss averaged over last 500 steps = 2.5883e-01, PNorm = 172.4520, GNorm = 0.1965
Meta loss on this task batch = 2.5457e-01, Meta loss averaged over last 500 steps = 2.5881e-01, PNorm = 172.4575, GNorm = 0.2249
Meta loss on this task batch = 2.9718e-01, Meta loss averaged over last 500 steps = 2.5900e-01, PNorm = 172.4629, GNorm = 0.2465
Meta loss on this task batch = 3.0564e-01, Meta loss averaged over last 500 steps = 2.5912e-01, PNorm = 172.4680, GNorm = 0.2707
Meta loss on this task batch = 1.8416e-01, Meta loss averaged over last 500 steps = 2.5881e-01, PNorm = 172.4736, GNorm = 0.1732
Meta loss on this task batch = 2.2183e-01, Meta loss averaged over last 500 steps = 2.5877e-01, PNorm = 172.4792, GNorm = 0.2378
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 2.5883e-01, PNorm = 172.4843, GNorm = 0.2655
Meta loss on this task batch = 2.5018e-01, Meta loss averaged over last 500 steps = 2.5886e-01, PNorm = 172.4895, GNorm = 0.2313
Meta loss on this task batch = 2.3070e-01, Meta loss averaged over last 500 steps = 2.5887e-01, PNorm = 172.4946, GNorm = 0.2182
Meta loss on this task batch = 2.9681e-01, Meta loss averaged over last 500 steps = 2.5896e-01, PNorm = 172.4998, GNorm = 0.2561
Meta loss on this task batch = 2.9341e-01, Meta loss averaged over last 500 steps = 2.5907e-01, PNorm = 172.5048, GNorm = 0.2594
Meta loss on this task batch = 2.4408e-01, Meta loss averaged over last 500 steps = 2.5913e-01, PNorm = 172.5103, GNorm = 0.2214
Meta loss on this task batch = 2.3519e-01, Meta loss averaged over last 500 steps = 2.5900e-01, PNorm = 172.5162, GNorm = 0.2392
Meta loss on this task batch = 2.8635e-01, Meta loss averaged over last 500 steps = 2.5913e-01, PNorm = 172.5216, GNorm = 0.2455
Meta loss on this task batch = 2.5547e-01, Meta loss averaged over last 500 steps = 2.5903e-01, PNorm = 172.5265, GNorm = 0.2488
Meta loss on this task batch = 2.9096e-01, Meta loss averaged over last 500 steps = 2.5914e-01, PNorm = 172.5308, GNorm = 0.2704
Meta loss on this task batch = 2.5858e-01, Meta loss averaged over last 500 steps = 2.5913e-01, PNorm = 172.5356, GNorm = 0.2732
Meta loss on this task batch = 2.7130e-01, Meta loss averaged over last 500 steps = 2.5904e-01, PNorm = 172.5412, GNorm = 0.2452
Meta loss on this task batch = 3.1880e-01, Meta loss averaged over last 500 steps = 2.5913e-01, PNorm = 172.5469, GNorm = 0.3269
Took 105.23641705513 seconds to complete one epoch of meta training
Took 112.69370222091675 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479530
Epoch 860
Meta loss on this task batch = 2.5300e-01, Meta loss averaged over last 500 steps = 2.5914e-01, PNorm = 172.5524, GNorm = 0.2302
Meta loss on this task batch = 2.2618e-01, Meta loss averaged over last 500 steps = 2.5903e-01, PNorm = 172.5584, GNorm = 0.2240
Meta loss on this task batch = 2.7757e-01, Meta loss averaged over last 500 steps = 2.5910e-01, PNorm = 172.5643, GNorm = 0.2426
Meta loss on this task batch = 2.4315e-01, Meta loss averaged over last 500 steps = 2.5907e-01, PNorm = 172.5704, GNorm = 0.2524
Meta loss on this task batch = 1.9984e-01, Meta loss averaged over last 500 steps = 2.5890e-01, PNorm = 172.5759, GNorm = 0.2165
Meta loss on this task batch = 2.4525e-01, Meta loss averaged over last 500 steps = 2.5880e-01, PNorm = 172.5811, GNorm = 0.2107
Meta loss on this task batch = 2.7408e-01, Meta loss averaged over last 500 steps = 2.5888e-01, PNorm = 172.5858, GNorm = 0.2189
Meta loss on this task batch = 2.8553e-01, Meta loss averaged over last 500 steps = 2.5893e-01, PNorm = 172.5907, GNorm = 0.2584
Meta loss on this task batch = 2.5663e-01, Meta loss averaged over last 500 steps = 2.5892e-01, PNorm = 172.5956, GNorm = 0.2257
Meta loss on this task batch = 2.2025e-01, Meta loss averaged over last 500 steps = 2.5889e-01, PNorm = 172.6008, GNorm = 0.1995
Meta loss on this task batch = 2.8571e-01, Meta loss averaged over last 500 steps = 2.5896e-01, PNorm = 172.6056, GNorm = 0.2574
Meta loss on this task batch = 3.2564e-01, Meta loss averaged over last 500 steps = 2.5917e-01, PNorm = 172.6098, GNorm = 0.3278
Meta loss on this task batch = 2.2946e-01, Meta loss averaged over last 500 steps = 2.5911e-01, PNorm = 172.6141, GNorm = 0.2303
Meta loss on this task batch = 2.8996e-01, Meta loss averaged over last 500 steps = 2.5921e-01, PNorm = 172.6182, GNorm = 0.2185
Meta loss on this task batch = 2.0495e-01, Meta loss averaged over last 500 steps = 2.5913e-01, PNorm = 172.6227, GNorm = 0.2307
Meta loss on this task batch = 2.7017e-01, Meta loss averaged over last 500 steps = 2.5916e-01, PNorm = 172.6275, GNorm = 0.2479
Meta loss on this task batch = 2.4681e-01, Meta loss averaged over last 500 steps = 2.5920e-01, PNorm = 172.6326, GNorm = 0.2243
Meta loss on this task batch = 2.3799e-01, Meta loss averaged over last 500 steps = 2.5907e-01, PNorm = 172.6382, GNorm = 0.2096
Meta loss on this task batch = 1.8558e-01, Meta loss averaged over last 500 steps = 2.5899e-01, PNorm = 172.6444, GNorm = 0.2360
Took 106.10865759849548 seconds to complete one epoch of meta training
Took 114.0433886051178 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510629
Epoch 861
Meta loss on this task batch = 2.8619e-01, Meta loss averaged over last 500 steps = 2.5902e-01, PNorm = 172.6505, GNorm = 0.2214
Meta loss on this task batch = 2.5076e-01, Meta loss averaged over last 500 steps = 2.5907e-01, PNorm = 172.6570, GNorm = 0.2209
Meta loss on this task batch = 2.5958e-01, Meta loss averaged over last 500 steps = 2.5901e-01, PNorm = 172.6635, GNorm = 0.2201
Meta loss on this task batch = 2.6847e-01, Meta loss averaged over last 500 steps = 2.5893e-01, PNorm = 172.6702, GNorm = 0.2614
Meta loss on this task batch = 2.4906e-01, Meta loss averaged over last 500 steps = 2.5881e-01, PNorm = 172.6758, GNorm = 0.2527
Meta loss on this task batch = 2.7890e-01, Meta loss averaged over last 500 steps = 2.5892e-01, PNorm = 172.6811, GNorm = 0.2331
Meta loss on this task batch = 2.3182e-01, Meta loss averaged over last 500 steps = 2.5897e-01, PNorm = 172.6864, GNorm = 0.2151
Meta loss on this task batch = 2.6192e-01, Meta loss averaged over last 500 steps = 2.5890e-01, PNorm = 172.6918, GNorm = 0.2464
Meta loss on this task batch = 2.4038e-01, Meta loss averaged over last 500 steps = 2.5888e-01, PNorm = 172.6970, GNorm = 0.2131
Meta loss on this task batch = 2.2409e-01, Meta loss averaged over last 500 steps = 2.5883e-01, PNorm = 172.7022, GNorm = 0.2171
Meta loss on this task batch = 2.3772e-01, Meta loss averaged over last 500 steps = 2.5870e-01, PNorm = 172.7074, GNorm = 0.2305
Meta loss on this task batch = 2.4814e-01, Meta loss averaged over last 500 steps = 2.5873e-01, PNorm = 172.7131, GNorm = 0.2189
Meta loss on this task batch = 2.1302e-01, Meta loss averaged over last 500 steps = 2.5860e-01, PNorm = 172.7189, GNorm = 0.2046
Meta loss on this task batch = 3.0391e-01, Meta loss averaged over last 500 steps = 2.5864e-01, PNorm = 172.7248, GNorm = 0.2917
Meta loss on this task batch = 3.0806e-01, Meta loss averaged over last 500 steps = 2.5878e-01, PNorm = 172.7305, GNorm = 0.2605
Meta loss on this task batch = 2.5791e-01, Meta loss averaged over last 500 steps = 2.5884e-01, PNorm = 172.7365, GNorm = 0.2643
Meta loss on this task batch = 2.2964e-01, Meta loss averaged over last 500 steps = 2.5879e-01, PNorm = 172.7422, GNorm = 0.2446
Meta loss on this task batch = 2.2962e-01, Meta loss averaged over last 500 steps = 2.5870e-01, PNorm = 172.7476, GNorm = 0.2156
Meta loss on this task batch = 1.7729e-01, Meta loss averaged over last 500 steps = 2.5863e-01, PNorm = 172.7539, GNorm = 0.2133
Took 111.0422670841217 seconds to complete one epoch of meta training
Took 117.46635842323303 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502688
Epoch 862
Meta loss on this task batch = 2.6971e-01, Meta loss averaged over last 500 steps = 2.5872e-01, PNorm = 172.7599, GNorm = 0.2835
Meta loss on this task batch = 2.9474e-01, Meta loss averaged over last 500 steps = 2.5865e-01, PNorm = 172.7658, GNorm = 0.2643
Meta loss on this task batch = 2.9513e-01, Meta loss averaged over last 500 steps = 2.5864e-01, PNorm = 172.7718, GNorm = 0.2452
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.5870e-01, PNorm = 172.7781, GNorm = 0.2173
Meta loss on this task batch = 2.4927e-01, Meta loss averaged over last 500 steps = 2.5858e-01, PNorm = 172.7840, GNorm = 0.2388
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.5874e-01, PNorm = 172.7902, GNorm = 0.2000
Meta loss on this task batch = 2.2957e-01, Meta loss averaged over last 500 steps = 2.5877e-01, PNorm = 172.7963, GNorm = 0.2481
Meta loss on this task batch = 2.6881e-01, Meta loss averaged over last 500 steps = 2.5877e-01, PNorm = 172.8013, GNorm = 0.2716
Meta loss on this task batch = 2.4189e-01, Meta loss averaged over last 500 steps = 2.5874e-01, PNorm = 172.8063, GNorm = 0.2385
Meta loss on this task batch = 2.2496e-01, Meta loss averaged over last 500 steps = 2.5871e-01, PNorm = 172.8107, GNorm = 0.2158
Meta loss on this task batch = 1.9972e-01, Meta loss averaged over last 500 steps = 2.5856e-01, PNorm = 172.8159, GNorm = 0.2164
Meta loss on this task batch = 2.2215e-01, Meta loss averaged over last 500 steps = 2.5846e-01, PNorm = 172.8214, GNorm = 0.1956
Meta loss on this task batch = 1.9474e-01, Meta loss averaged over last 500 steps = 2.5832e-01, PNorm = 172.8265, GNorm = 0.2329
Meta loss on this task batch = 2.4399e-01, Meta loss averaged over last 500 steps = 2.5820e-01, PNorm = 172.8307, GNorm = 0.2216
Meta loss on this task batch = 2.4104e-01, Meta loss averaged over last 500 steps = 2.5817e-01, PNorm = 172.8344, GNorm = 0.2229
Meta loss on this task batch = 2.6868e-01, Meta loss averaged over last 500 steps = 2.5815e-01, PNorm = 172.8383, GNorm = 0.2335
Meta loss on this task batch = 2.6877e-01, Meta loss averaged over last 500 steps = 2.5814e-01, PNorm = 172.8417, GNorm = 0.2351
Meta loss on this task batch = 3.1639e-01, Meta loss averaged over last 500 steps = 2.5820e-01, PNorm = 172.8450, GNorm = 0.2523
Meta loss on this task batch = 2.7910e-01, Meta loss averaged over last 500 steps = 2.5828e-01, PNorm = 172.8480, GNorm = 0.3384
Took 107.51216983795166 seconds to complete one epoch of meta training
Took 113.9434506893158 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506253
Epoch 863
Meta loss on this task batch = 2.3869e-01, Meta loss averaged over last 500 steps = 2.5824e-01, PNorm = 172.8521, GNorm = 0.2284
Meta loss on this task batch = 3.2862e-01, Meta loss averaged over last 500 steps = 2.5839e-01, PNorm = 172.8553, GNorm = 0.2895
Meta loss on this task batch = 2.2614e-01, Meta loss averaged over last 500 steps = 2.5835e-01, PNorm = 172.8581, GNorm = 0.1952
Meta loss on this task batch = 1.9562e-01, Meta loss averaged over last 500 steps = 2.5823e-01, PNorm = 172.8614, GNorm = 0.1990
Meta loss on this task batch = 2.5744e-01, Meta loss averaged over last 500 steps = 2.5820e-01, PNorm = 172.8653, GNorm = 0.1990
Meta loss on this task batch = 2.2635e-01, Meta loss averaged over last 500 steps = 2.5823e-01, PNorm = 172.8691, GNorm = 0.2379
Meta loss on this task batch = 2.8293e-01, Meta loss averaged over last 500 steps = 2.5837e-01, PNorm = 172.8723, GNorm = 0.2583
Meta loss on this task batch = 2.2342e-01, Meta loss averaged over last 500 steps = 2.5822e-01, PNorm = 172.8753, GNorm = 0.2226
Meta loss on this task batch = 2.6400e-01, Meta loss averaged over last 500 steps = 2.5823e-01, PNorm = 172.8784, GNorm = 0.2396
Meta loss on this task batch = 2.6592e-01, Meta loss averaged over last 500 steps = 2.5811e-01, PNorm = 172.8813, GNorm = 0.3273
Meta loss on this task batch = 2.7687e-01, Meta loss averaged over last 500 steps = 2.5818e-01, PNorm = 172.8836, GNorm = 0.2463
Meta loss on this task batch = 3.0013e-01, Meta loss averaged over last 500 steps = 2.5825e-01, PNorm = 172.8863, GNorm = 0.2975
Meta loss on this task batch = 2.3078e-01, Meta loss averaged over last 500 steps = 2.5817e-01, PNorm = 172.8894, GNorm = 0.2298
Meta loss on this task batch = 2.5204e-01, Meta loss averaged over last 500 steps = 2.5801e-01, PNorm = 172.8925, GNorm = 0.2301
Meta loss on this task batch = 2.3657e-01, Meta loss averaged over last 500 steps = 2.5804e-01, PNorm = 172.8958, GNorm = 0.2495
Meta loss on this task batch = 2.2461e-01, Meta loss averaged over last 500 steps = 2.5796e-01, PNorm = 172.8998, GNorm = 0.2237
Meta loss on this task batch = 3.1165e-01, Meta loss averaged over last 500 steps = 2.5811e-01, PNorm = 172.9036, GNorm = 0.2580
Meta loss on this task batch = 2.3594e-01, Meta loss averaged over last 500 steps = 2.5813e-01, PNorm = 172.9081, GNorm = 0.2158
Meta loss on this task batch = 3.1104e-01, Meta loss averaged over last 500 steps = 2.5820e-01, PNorm = 172.9125, GNorm = 0.3779
Took 150.8413050174713 seconds to complete one epoch of meta training
Took 157.78977751731873 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477355
Epoch 864
Meta loss on this task batch = 2.7353e-01, Meta loss averaged over last 500 steps = 2.5826e-01, PNorm = 172.9171, GNorm = 0.2275
Meta loss on this task batch = 2.5225e-01, Meta loss averaged over last 500 steps = 2.5828e-01, PNorm = 172.9222, GNorm = 0.2213
Meta loss on this task batch = 2.1939e-01, Meta loss averaged over last 500 steps = 2.5819e-01, PNorm = 172.9282, GNorm = 0.2251
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.5820e-01, PNorm = 172.9341, GNorm = 0.2350
Meta loss on this task batch = 2.3498e-01, Meta loss averaged over last 500 steps = 2.5824e-01, PNorm = 172.9398, GNorm = 0.2346
Meta loss on this task batch = 2.3362e-01, Meta loss averaged over last 500 steps = 2.5818e-01, PNorm = 172.9448, GNorm = 0.1983
Meta loss on this task batch = 2.8247e-01, Meta loss averaged over last 500 steps = 2.5824e-01, PNorm = 172.9500, GNorm = 0.2566
Meta loss on this task batch = 2.8387e-01, Meta loss averaged over last 500 steps = 2.5827e-01, PNorm = 172.9546, GNorm = 0.2258
Meta loss on this task batch = 2.5694e-01, Meta loss averaged over last 500 steps = 2.5823e-01, PNorm = 172.9594, GNorm = 0.2268
Meta loss on this task batch = 2.5774e-01, Meta loss averaged over last 500 steps = 2.5825e-01, PNorm = 172.9647, GNorm = 0.2405
Meta loss on this task batch = 2.3446e-01, Meta loss averaged over last 500 steps = 2.5818e-01, PNorm = 172.9694, GNorm = 0.2495
Meta loss on this task batch = 2.2062e-01, Meta loss averaged over last 500 steps = 2.5806e-01, PNorm = 172.9736, GNorm = 0.2256
Meta loss on this task batch = 3.3016e-01, Meta loss averaged over last 500 steps = 2.5818e-01, PNorm = 172.9765, GNorm = 0.2990
Meta loss on this task batch = 2.1722e-01, Meta loss averaged over last 500 steps = 2.5815e-01, PNorm = 172.9792, GNorm = 0.2208
Meta loss on this task batch = 3.0378e-01, Meta loss averaged over last 500 steps = 2.5817e-01, PNorm = 172.9819, GNorm = 0.2299
Meta loss on this task batch = 3.0117e-01, Meta loss averaged over last 500 steps = 2.5822e-01, PNorm = 172.9841, GNorm = 0.2226
Meta loss on this task batch = 2.2244e-01, Meta loss averaged over last 500 steps = 2.5813e-01, PNorm = 172.9872, GNorm = 0.2151
Meta loss on this task batch = 2.9696e-01, Meta loss averaged over last 500 steps = 2.5818e-01, PNorm = 172.9899, GNorm = 0.2390
Meta loss on this task batch = 2.7626e-01, Meta loss averaged over last 500 steps = 2.5832e-01, PNorm = 172.9931, GNorm = 0.2972
Took 104.2734203338623 seconds to complete one epoch of meta training
Took 111.45354700088501 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.468620
Epoch 865
Meta loss on this task batch = 2.7611e-01, Meta loss averaged over last 500 steps = 2.5837e-01, PNorm = 172.9969, GNorm = 0.2305
Meta loss on this task batch = 2.4635e-01, Meta loss averaged over last 500 steps = 2.5838e-01, PNorm = 173.0014, GNorm = 0.2205
Meta loss on this task batch = 1.8373e-01, Meta loss averaged over last 500 steps = 2.5827e-01, PNorm = 173.0064, GNorm = 0.2061
Meta loss on this task batch = 3.2486e-01, Meta loss averaged over last 500 steps = 2.5839e-01, PNorm = 173.0111, GNorm = 0.3195
Meta loss on this task batch = 2.5377e-01, Meta loss averaged over last 500 steps = 2.5844e-01, PNorm = 173.0153, GNorm = 0.2308
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 2.5849e-01, PNorm = 173.0201, GNorm = 0.2491
Meta loss on this task batch = 2.1638e-01, Meta loss averaged over last 500 steps = 2.5831e-01, PNorm = 173.0250, GNorm = 0.2626
Meta loss on this task batch = 2.3965e-01, Meta loss averaged over last 500 steps = 2.5826e-01, PNorm = 173.0290, GNorm = 0.2225
Meta loss on this task batch = 2.6975e-01, Meta loss averaged over last 500 steps = 2.5828e-01, PNorm = 173.0341, GNorm = 0.2446
Meta loss on this task batch = 2.8909e-01, Meta loss averaged over last 500 steps = 2.5841e-01, PNorm = 173.0381, GNorm = 0.2560
Meta loss on this task batch = 2.8285e-01, Meta loss averaged over last 500 steps = 2.5841e-01, PNorm = 173.0417, GNorm = 0.2490
Meta loss on this task batch = 2.9063e-01, Meta loss averaged over last 500 steps = 2.5833e-01, PNorm = 173.0453, GNorm = 0.2146
Meta loss on this task batch = 2.7211e-01, Meta loss averaged over last 500 steps = 2.5827e-01, PNorm = 173.0483, GNorm = 0.2933
Meta loss on this task batch = 2.7619e-01, Meta loss averaged over last 500 steps = 2.5830e-01, PNorm = 173.0501, GNorm = 0.2958
Meta loss on this task batch = 2.6607e-01, Meta loss averaged over last 500 steps = 2.5834e-01, PNorm = 173.0526, GNorm = 0.2328
Meta loss on this task batch = 2.9342e-01, Meta loss averaged over last 500 steps = 2.5845e-01, PNorm = 173.0557, GNorm = 0.2419
Meta loss on this task batch = 3.0762e-01, Meta loss averaged over last 500 steps = 2.5856e-01, PNorm = 173.0585, GNorm = 0.2453
Meta loss on this task batch = 2.2045e-01, Meta loss averaged over last 500 steps = 2.5852e-01, PNorm = 173.0609, GNorm = 0.2212
Meta loss on this task batch = 2.4310e-01, Meta loss averaged over last 500 steps = 2.5849e-01, PNorm = 173.0640, GNorm = 0.2831
Took 111.8687584400177 seconds to complete one epoch of meta training
Took 118.77331757545471 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.507299
Epoch 866
Meta loss on this task batch = 2.3122e-01, Meta loss averaged over last 500 steps = 2.5843e-01, PNorm = 173.0668, GNorm = 0.2245
Meta loss on this task batch = 2.3667e-01, Meta loss averaged over last 500 steps = 2.5838e-01, PNorm = 173.0701, GNorm = 0.2645
Meta loss on this task batch = 2.4503e-01, Meta loss averaged over last 500 steps = 2.5833e-01, PNorm = 173.0738, GNorm = 0.2462
Meta loss on this task batch = 2.9846e-01, Meta loss averaged over last 500 steps = 2.5841e-01, PNorm = 173.0783, GNorm = 0.2448
Meta loss on this task batch = 2.2579e-01, Meta loss averaged over last 500 steps = 2.5836e-01, PNorm = 173.0832, GNorm = 0.2113
Meta loss on this task batch = 2.4948e-01, Meta loss averaged over last 500 steps = 2.5839e-01, PNorm = 173.0878, GNorm = 0.2268
Meta loss on this task batch = 2.3646e-01, Meta loss averaged over last 500 steps = 2.5845e-01, PNorm = 173.0919, GNorm = 0.2142
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.5842e-01, PNorm = 173.0952, GNorm = 0.2442
Meta loss on this task batch = 2.6832e-01, Meta loss averaged over last 500 steps = 2.5842e-01, PNorm = 173.0985, GNorm = 0.2821
Meta loss on this task batch = 2.8444e-01, Meta loss averaged over last 500 steps = 2.5857e-01, PNorm = 173.1023, GNorm = 0.2285
Meta loss on this task batch = 2.3809e-01, Meta loss averaged over last 500 steps = 2.5848e-01, PNorm = 173.1061, GNorm = 0.2165
Meta loss on this task batch = 2.7675e-01, Meta loss averaged over last 500 steps = 2.5857e-01, PNorm = 173.1107, GNorm = 0.1980
Meta loss on this task batch = 2.7969e-01, Meta loss averaged over last 500 steps = 2.5860e-01, PNorm = 173.1154, GNorm = 0.2595
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.5854e-01, PNorm = 173.1213, GNorm = 0.2471
Meta loss on this task batch = 2.8637e-01, Meta loss averaged over last 500 steps = 2.5856e-01, PNorm = 173.1265, GNorm = 0.2639
Meta loss on this task batch = 2.9455e-01, Meta loss averaged over last 500 steps = 2.5853e-01, PNorm = 173.1322, GNorm = 0.2980
Meta loss on this task batch = 2.6962e-01, Meta loss averaged over last 500 steps = 2.5859e-01, PNorm = 173.1374, GNorm = 0.2549
Meta loss on this task batch = 2.7603e-01, Meta loss averaged over last 500 steps = 2.5858e-01, PNorm = 173.1423, GNorm = 0.2189
Meta loss on this task batch = 1.9599e-01, Meta loss averaged over last 500 steps = 2.5845e-01, PNorm = 173.1481, GNorm = 0.2816
Took 109.4163966178894 seconds to complete one epoch of meta training
Took 117.57196998596191 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482953
Epoch 867
Meta loss on this task batch = 2.8926e-01, Meta loss averaged over last 500 steps = 2.5850e-01, PNorm = 173.1532, GNorm = 0.2407
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.5845e-01, PNorm = 173.1588, GNorm = 0.2242
Meta loss on this task batch = 2.6925e-01, Meta loss averaged over last 500 steps = 2.5849e-01, PNorm = 173.1642, GNorm = 0.2229
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.5845e-01, PNorm = 173.1691, GNorm = 0.2362
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 2.5833e-01, PNorm = 173.1741, GNorm = 0.2450
Meta loss on this task batch = 2.3351e-01, Meta loss averaged over last 500 steps = 2.5833e-01, PNorm = 173.1797, GNorm = 0.1849
Meta loss on this task batch = 2.8111e-01, Meta loss averaged over last 500 steps = 2.5840e-01, PNorm = 173.1851, GNorm = 0.2168
Meta loss on this task batch = 2.6408e-01, Meta loss averaged over last 500 steps = 2.5843e-01, PNorm = 173.1900, GNorm = 0.2493
Meta loss on this task batch = 2.4922e-01, Meta loss averaged over last 500 steps = 2.5835e-01, PNorm = 173.1951, GNorm = 0.2484
Meta loss on this task batch = 3.2355e-01, Meta loss averaged over last 500 steps = 2.5855e-01, PNorm = 173.1998, GNorm = 0.2422
Meta loss on this task batch = 2.1784e-01, Meta loss averaged over last 500 steps = 2.5856e-01, PNorm = 173.2054, GNorm = 0.2131
Meta loss on this task batch = 2.5185e-01, Meta loss averaged over last 500 steps = 2.5855e-01, PNorm = 173.2108, GNorm = 0.2141
Meta loss on this task batch = 2.6157e-01, Meta loss averaged over last 500 steps = 2.5846e-01, PNorm = 173.2168, GNorm = 0.2397
Meta loss on this task batch = 2.7562e-01, Meta loss averaged over last 500 steps = 2.5863e-01, PNorm = 173.2222, GNorm = 0.2133
Meta loss on this task batch = 2.2292e-01, Meta loss averaged over last 500 steps = 2.5858e-01, PNorm = 173.2275, GNorm = 0.1918
Meta loss on this task batch = 1.7293e-01, Meta loss averaged over last 500 steps = 2.5847e-01, PNorm = 173.2328, GNorm = 0.2137
Meta loss on this task batch = 2.4575e-01, Meta loss averaged over last 500 steps = 2.5839e-01, PNorm = 173.2374, GNorm = 0.2432
Meta loss on this task batch = 2.7004e-01, Meta loss averaged over last 500 steps = 2.5835e-01, PNorm = 173.2419, GNorm = 0.2147
Meta loss on this task batch = 2.5281e-01, Meta loss averaged over last 500 steps = 2.5828e-01, PNorm = 173.2460, GNorm = 0.2811
Took 108.68111681938171 seconds to complete one epoch of meta training
Took 115.8272716999054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.454694
Epoch 868
Meta loss on this task batch = 2.1822e-01, Meta loss averaged over last 500 steps = 2.5804e-01, PNorm = 173.2513, GNorm = 0.2280
Meta loss on this task batch = 3.2107e-01, Meta loss averaged over last 500 steps = 2.5814e-01, PNorm = 173.2561, GNorm = 0.2560
Meta loss on this task batch = 2.8077e-01, Meta loss averaged over last 500 steps = 2.5821e-01, PNorm = 173.2604, GNorm = 0.2374
Meta loss on this task batch = 2.3460e-01, Meta loss averaged over last 500 steps = 2.5812e-01, PNorm = 173.2646, GNorm = 0.2136
Meta loss on this task batch = 2.4983e-01, Meta loss averaged over last 500 steps = 2.5811e-01, PNorm = 173.2685, GNorm = 0.2389
Meta loss on this task batch = 2.5351e-01, Meta loss averaged over last 500 steps = 2.5812e-01, PNorm = 173.2723, GNorm = 0.2352
Meta loss on this task batch = 2.5010e-01, Meta loss averaged over last 500 steps = 2.5806e-01, PNorm = 173.2760, GNorm = 0.1950
Meta loss on this task batch = 2.5771e-01, Meta loss averaged over last 500 steps = 2.5811e-01, PNorm = 173.2801, GNorm = 0.2255
Meta loss on this task batch = 3.0307e-01, Meta loss averaged over last 500 steps = 2.5812e-01, PNorm = 173.2844, GNorm = 0.3002
Meta loss on this task batch = 2.4192e-01, Meta loss averaged over last 500 steps = 2.5802e-01, PNorm = 173.2893, GNorm = 0.2079
Meta loss on this task batch = 3.1824e-01, Meta loss averaged over last 500 steps = 2.5811e-01, PNorm = 173.2942, GNorm = 0.2344
Meta loss on this task batch = 2.5367e-01, Meta loss averaged over last 500 steps = 2.5811e-01, PNorm = 173.2988, GNorm = 0.2358
Meta loss on this task batch = 2.9276e-01, Meta loss averaged over last 500 steps = 2.5824e-01, PNorm = 173.3033, GNorm = 0.2577
Meta loss on this task batch = 2.9349e-01, Meta loss averaged over last 500 steps = 2.5830e-01, PNorm = 173.3078, GNorm = 0.2177
Meta loss on this task batch = 2.4503e-01, Meta loss averaged over last 500 steps = 2.5822e-01, PNorm = 173.3123, GNorm = 0.2118
Meta loss on this task batch = 2.6561e-01, Meta loss averaged over last 500 steps = 2.5823e-01, PNorm = 173.3158, GNorm = 0.3206
Meta loss on this task batch = 2.0541e-01, Meta loss averaged over last 500 steps = 2.5816e-01, PNorm = 173.3193, GNorm = 0.1919
Meta loss on this task batch = 2.6016e-01, Meta loss averaged over last 500 steps = 2.5816e-01, PNorm = 173.3227, GNorm = 0.2284
Meta loss on this task batch = 2.3619e-01, Meta loss averaged over last 500 steps = 2.5812e-01, PNorm = 173.3268, GNorm = 0.2533
Took 103.92445588111877 seconds to complete one epoch of meta training
Took 111.32232117652893 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.464985
Epoch 869
Meta loss on this task batch = 2.4490e-01, Meta loss averaged over last 500 steps = 2.5812e-01, PNorm = 173.3312, GNorm = 0.2074
Meta loss on this task batch = 2.2340e-01, Meta loss averaged over last 500 steps = 2.5800e-01, PNorm = 173.3360, GNorm = 0.1914
Meta loss on this task batch = 2.6340e-01, Meta loss averaged over last 500 steps = 2.5796e-01, PNorm = 173.3418, GNorm = 0.2424
Meta loss on this task batch = 2.2061e-01, Meta loss averaged over last 500 steps = 2.5790e-01, PNorm = 173.3475, GNorm = 0.1830
Meta loss on this task batch = 2.4285e-01, Meta loss averaged over last 500 steps = 2.5799e-01, PNorm = 173.3526, GNorm = 0.2209
Meta loss on this task batch = 2.4225e-01, Meta loss averaged over last 500 steps = 2.5794e-01, PNorm = 173.3573, GNorm = 0.1991
Meta loss on this task batch = 2.1507e-01, Meta loss averaged over last 500 steps = 2.5793e-01, PNorm = 173.3618, GNorm = 0.2149
Meta loss on this task batch = 2.4879e-01, Meta loss averaged over last 500 steps = 2.5784e-01, PNorm = 173.3664, GNorm = 0.2054
Meta loss on this task batch = 2.3475e-01, Meta loss averaged over last 500 steps = 2.5771e-01, PNorm = 173.3708, GNorm = 0.1782
Meta loss on this task batch = 2.7996e-01, Meta loss averaged over last 500 steps = 2.5770e-01, PNorm = 173.3763, GNorm = 0.2658
Meta loss on this task batch = 2.3967e-01, Meta loss averaged over last 500 steps = 2.5768e-01, PNorm = 173.3813, GNorm = 0.2242
Meta loss on this task batch = 2.9323e-01, Meta loss averaged over last 500 steps = 2.5769e-01, PNorm = 173.3849, GNorm = 0.2560
Meta loss on this task batch = 2.9754e-01, Meta loss averaged over last 500 steps = 2.5779e-01, PNorm = 173.3883, GNorm = 0.2476
Meta loss on this task batch = 2.7671e-01, Meta loss averaged over last 500 steps = 2.5779e-01, PNorm = 173.3911, GNorm = 0.2842
Meta loss on this task batch = 3.0793e-01, Meta loss averaged over last 500 steps = 2.5788e-01, PNorm = 173.3939, GNorm = 0.2643
Meta loss on this task batch = 2.4273e-01, Meta loss averaged over last 500 steps = 2.5786e-01, PNorm = 173.3974, GNorm = 0.2120
Meta loss on this task batch = 2.6083e-01, Meta loss averaged over last 500 steps = 2.5782e-01, PNorm = 173.4000, GNorm = 0.3468
Meta loss on this task batch = 2.4155e-01, Meta loss averaged over last 500 steps = 2.5780e-01, PNorm = 173.4031, GNorm = 0.2204
Meta loss on this task batch = 2.9663e-01, Meta loss averaged over last 500 steps = 2.5789e-01, PNorm = 173.4063, GNorm = 0.3186
Took 109.95982098579407 seconds to complete one epoch of meta training
Took 117.1841127872467 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481842
Epoch 870
Meta loss on this task batch = 2.3121e-01, Meta loss averaged over last 500 steps = 2.5781e-01, PNorm = 173.4099, GNorm = 0.2337
Meta loss on this task batch = 2.7699e-01, Meta loss averaged over last 500 steps = 2.5787e-01, PNorm = 173.4139, GNorm = 0.2289
Meta loss on this task batch = 2.3269e-01, Meta loss averaged over last 500 steps = 2.5783e-01, PNorm = 173.4178, GNorm = 0.2357
Meta loss on this task batch = 2.2955e-01, Meta loss averaged over last 500 steps = 2.5769e-01, PNorm = 173.4217, GNorm = 0.2144
Meta loss on this task batch = 2.2756e-01, Meta loss averaged over last 500 steps = 2.5761e-01, PNorm = 173.4259, GNorm = 0.2189
Meta loss on this task batch = 3.0134e-01, Meta loss averaged over last 500 steps = 2.5778e-01, PNorm = 173.4303, GNorm = 0.2333
Meta loss on this task batch = 1.9754e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 173.4349, GNorm = 0.2053
Meta loss on this task batch = 2.5751e-01, Meta loss averaged over last 500 steps = 2.5760e-01, PNorm = 173.4391, GNorm = 0.2149
Meta loss on this task batch = 3.0496e-01, Meta loss averaged over last 500 steps = 2.5772e-01, PNorm = 173.4433, GNorm = 0.2296
Meta loss on this task batch = 2.5725e-01, Meta loss averaged over last 500 steps = 2.5782e-01, PNorm = 173.4480, GNorm = 0.2366
Meta loss on this task batch = 2.2032e-01, Meta loss averaged over last 500 steps = 2.5773e-01, PNorm = 173.4533, GNorm = 0.2154
Meta loss on this task batch = 2.6750e-01, Meta loss averaged over last 500 steps = 2.5769e-01, PNorm = 173.4588, GNorm = 0.2139
Meta loss on this task batch = 2.6756e-01, Meta loss averaged over last 500 steps = 2.5770e-01, PNorm = 173.4633, GNorm = 0.2275
Meta loss on this task batch = 2.4875e-01, Meta loss averaged over last 500 steps = 2.5778e-01, PNorm = 173.4682, GNorm = 0.2672
Meta loss on this task batch = 1.8640e-01, Meta loss averaged over last 500 steps = 2.5755e-01, PNorm = 173.4734, GNorm = 0.2076
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 173.4779, GNorm = 0.2824
Meta loss on this task batch = 2.8987e-01, Meta loss averaged over last 500 steps = 2.5772e-01, PNorm = 173.4822, GNorm = 0.2457
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 2.5772e-01, PNorm = 173.4869, GNorm = 0.2836
Meta loss on this task batch = 2.9894e-01, Meta loss averaged over last 500 steps = 2.5781e-01, PNorm = 173.4912, GNorm = 0.3327
Took 103.57677793502808 seconds to complete one epoch of meta training
Took 111.02066421508789 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484009
Epoch 871
Meta loss on this task batch = 2.4375e-01, Meta loss averaged over last 500 steps = 2.5765e-01, PNorm = 173.4958, GNorm = 0.2357
Meta loss on this task batch = 2.3911e-01, Meta loss averaged over last 500 steps = 2.5759e-01, PNorm = 173.5004, GNorm = 0.2179
Meta loss on this task batch = 2.7701e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 173.5051, GNorm = 0.2524
Meta loss on this task batch = 2.6483e-01, Meta loss averaged over last 500 steps = 2.5759e-01, PNorm = 173.5098, GNorm = 0.2382
Meta loss on this task batch = 2.2208e-01, Meta loss averaged over last 500 steps = 2.5758e-01, PNorm = 173.5141, GNorm = 0.2119
Meta loss on this task batch = 2.3797e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 173.5188, GNorm = 0.2417
Meta loss on this task batch = 2.0303e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 173.5234, GNorm = 0.1879
Meta loss on this task batch = 2.7007e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 173.5284, GNorm = 0.2239
Meta loss on this task batch = 1.9826e-01, Meta loss averaged over last 500 steps = 2.5735e-01, PNorm = 173.5341, GNorm = 0.1780
Meta loss on this task batch = 3.1675e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 173.5389, GNorm = 0.2645
Meta loss on this task batch = 2.8371e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 173.5436, GNorm = 0.2500
Meta loss on this task batch = 2.5664e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 173.5487, GNorm = 0.2275
Meta loss on this task batch = 2.8887e-01, Meta loss averaged over last 500 steps = 2.5765e-01, PNorm = 173.5538, GNorm = 0.2629
Meta loss on this task batch = 2.3411e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 173.5594, GNorm = 0.2053
Meta loss on this task batch = 2.9314e-01, Meta loss averaged over last 500 steps = 2.5747e-01, PNorm = 173.5648, GNorm = 0.2495
Meta loss on this task batch = 3.1279e-01, Meta loss averaged over last 500 steps = 2.5762e-01, PNorm = 173.5699, GNorm = 0.2298
Meta loss on this task batch = 2.8547e-01, Meta loss averaged over last 500 steps = 2.5771e-01, PNorm = 173.5752, GNorm = 0.2330
Meta loss on this task batch = 2.5178e-01, Meta loss averaged over last 500 steps = 2.5768e-01, PNorm = 173.5805, GNorm = 0.2109
Meta loss on this task batch = 2.6197e-01, Meta loss averaged over last 500 steps = 2.5761e-01, PNorm = 173.5860, GNorm = 0.2623
Took 104.18286895751953 seconds to complete one epoch of meta training
Took 111.73880243301392 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482606
Epoch 872
Meta loss on this task batch = 2.7668e-01, Meta loss averaged over last 500 steps = 2.5761e-01, PNorm = 173.5910, GNorm = 0.2256
Meta loss on this task batch = 2.5693e-01, Meta loss averaged over last 500 steps = 2.5763e-01, PNorm = 173.5959, GNorm = 0.2163
Meta loss on this task batch = 2.2873e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 173.6013, GNorm = 0.1912
Meta loss on this task batch = 2.3311e-01, Meta loss averaged over last 500 steps = 2.5749e-01, PNorm = 173.6066, GNorm = 0.2285
Meta loss on this task batch = 2.3061e-01, Meta loss averaged over last 500 steps = 2.5738e-01, PNorm = 173.6116, GNorm = 0.2043
Meta loss on this task batch = 2.7354e-01, Meta loss averaged over last 500 steps = 2.5733e-01, PNorm = 173.6164, GNorm = 0.2137
Meta loss on this task batch = 3.0811e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 173.6205, GNorm = 0.2587
Meta loss on this task batch = 2.7441e-01, Meta loss averaged over last 500 steps = 2.5741e-01, PNorm = 173.6244, GNorm = 0.2354
Meta loss on this task batch = 2.1648e-01, Meta loss averaged over last 500 steps = 2.5735e-01, PNorm = 173.6289, GNorm = 0.2423
Meta loss on this task batch = 2.5645e-01, Meta loss averaged over last 500 steps = 2.5729e-01, PNorm = 173.6330, GNorm = 0.2564
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 2.5737e-01, PNorm = 173.6371, GNorm = 0.2414
Meta loss on this task batch = 2.8772e-01, Meta loss averaged over last 500 steps = 2.5737e-01, PNorm = 173.6410, GNorm = 0.2650
Meta loss on this task batch = 2.3049e-01, Meta loss averaged over last 500 steps = 2.5735e-01, PNorm = 173.6444, GNorm = 0.2669
Meta loss on this task batch = 2.1702e-01, Meta loss averaged over last 500 steps = 2.5734e-01, PNorm = 173.6484, GNorm = 0.2128
Meta loss on this task batch = 2.7648e-01, Meta loss averaged over last 500 steps = 2.5735e-01, PNorm = 173.6525, GNorm = 0.2272
Meta loss on this task batch = 3.0599e-01, Meta loss averaged over last 500 steps = 2.5733e-01, PNorm = 173.6564, GNorm = 0.2406
Meta loss on this task batch = 2.8343e-01, Meta loss averaged over last 500 steps = 2.5736e-01, PNorm = 173.6609, GNorm = 0.2479
Meta loss on this task batch = 2.2049e-01, Meta loss averaged over last 500 steps = 2.5723e-01, PNorm = 173.6661, GNorm = 0.2141
Meta loss on this task batch = 2.2872e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 173.6714, GNorm = 0.2504
Took 103.74863696098328 seconds to complete one epoch of meta training
Took 110.81200742721558 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471564
Epoch 873
Meta loss on this task batch = 2.2415e-01, Meta loss averaged over last 500 steps = 2.5713e-01, PNorm = 173.6767, GNorm = 0.2073
Meta loss on this task batch = 2.2507e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 173.6814, GNorm = 0.2454
Meta loss on this task batch = 3.0515e-01, Meta loss averaged over last 500 steps = 2.5713e-01, PNorm = 173.6857, GNorm = 0.2375
Meta loss on this task batch = 2.5314e-01, Meta loss averaged over last 500 steps = 2.5719e-01, PNorm = 173.6898, GNorm = 0.2277
Meta loss on this task batch = 3.4187e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 173.6923, GNorm = 0.2840
Meta loss on this task batch = 2.5828e-01, Meta loss averaged over last 500 steps = 2.5746e-01, PNorm = 173.6946, GNorm = 0.2525
Meta loss on this task batch = 2.8285e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 173.6964, GNorm = 0.2856
Meta loss on this task batch = 2.5524e-01, Meta loss averaged over last 500 steps = 2.5755e-01, PNorm = 173.6981, GNorm = 0.2344
Meta loss on this task batch = 2.4207e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 173.7004, GNorm = 0.2223
Meta loss on this task batch = 2.5245e-01, Meta loss averaged over last 500 steps = 2.5759e-01, PNorm = 173.7033, GNorm = 0.2393
Meta loss on this task batch = 2.7134e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 173.7071, GNorm = 0.2415
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.5765e-01, PNorm = 173.7114, GNorm = 0.2152
Meta loss on this task batch = 2.2043e-01, Meta loss averaged over last 500 steps = 2.5754e-01, PNorm = 173.7161, GNorm = 0.2163
Meta loss on this task batch = 2.3393e-01, Meta loss averaged over last 500 steps = 2.5741e-01, PNorm = 173.7209, GNorm = 0.1882
Meta loss on this task batch = 2.1105e-01, Meta loss averaged over last 500 steps = 2.5728e-01, PNorm = 173.7260, GNorm = 0.2359
Meta loss on this task batch = 2.9041e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 173.7305, GNorm = 0.2392
Meta loss on this task batch = 2.4040e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 173.7350, GNorm = 0.2113
Meta loss on this task batch = 3.0890e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 173.7394, GNorm = 0.2602
Meta loss on this task batch = 2.5865e-01, Meta loss averaged over last 500 steps = 2.5758e-01, PNorm = 173.7440, GNorm = 0.2712
Took 111.1589584350586 seconds to complete one epoch of meta training
Took 118.51542639732361 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465850
Epoch 874
Meta loss on this task batch = 2.4212e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 173.7488, GNorm = 0.2263
Meta loss on this task batch = 2.9473e-01, Meta loss averaged over last 500 steps = 2.5758e-01, PNorm = 173.7526, GNorm = 0.2643
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.5755e-01, PNorm = 173.7568, GNorm = 0.2090
Meta loss on this task batch = 2.7033e-01, Meta loss averaged over last 500 steps = 2.5767e-01, PNorm = 173.7612, GNorm = 0.2443
Meta loss on this task batch = 2.5785e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 173.7650, GNorm = 0.2329
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 2.5760e-01, PNorm = 173.7677, GNorm = 0.2620
Meta loss on this task batch = 2.2763e-01, Meta loss averaged over last 500 steps = 2.5760e-01, PNorm = 173.7707, GNorm = 0.2167
Meta loss on this task batch = 2.4067e-01, Meta loss averaged over last 500 steps = 2.5759e-01, PNorm = 173.7739, GNorm = 0.2300
Meta loss on this task batch = 2.6513e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 173.7776, GNorm = 0.2029
Meta loss on this task batch = 2.2585e-01, Meta loss averaged over last 500 steps = 2.5749e-01, PNorm = 173.7822, GNorm = 0.2262
Meta loss on this task batch = 2.4239e-01, Meta loss averaged over last 500 steps = 2.5742e-01, PNorm = 173.7873, GNorm = 0.2309
Meta loss on this task batch = 2.8338e-01, Meta loss averaged over last 500 steps = 2.5747e-01, PNorm = 173.7922, GNorm = 0.2273
Meta loss on this task batch = 2.3360e-01, Meta loss averaged over last 500 steps = 2.5732e-01, PNorm = 173.7965, GNorm = 0.2384
Meta loss on this task batch = 2.7886e-01, Meta loss averaged over last 500 steps = 2.5737e-01, PNorm = 173.8005, GNorm = 0.2748
Meta loss on this task batch = 2.6859e-01, Meta loss averaged over last 500 steps = 2.5734e-01, PNorm = 173.8046, GNorm = 0.2000
Meta loss on this task batch = 1.7254e-01, Meta loss averaged over last 500 steps = 2.5719e-01, PNorm = 173.8086, GNorm = 0.1984
Meta loss on this task batch = 2.4872e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 173.8125, GNorm = 0.2177
Meta loss on this task batch = 2.5809e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 173.8170, GNorm = 0.2426
Meta loss on this task batch = 2.4049e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 173.8219, GNorm = 0.2310
Took 106.17893266677856 seconds to complete one epoch of meta training
Took 113.58039855957031 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474849
Epoch 875
Meta loss on this task batch = 2.3306e-01, Meta loss averaged over last 500 steps = 2.5723e-01, PNorm = 173.8264, GNorm = 0.2577
Meta loss on this task batch = 2.5939e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 173.8311, GNorm = 0.2179
Meta loss on this task batch = 2.4870e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 173.8358, GNorm = 0.2527
Meta loss on this task batch = 2.2885e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 173.8402, GNorm = 0.2272
Meta loss on this task batch = 2.9703e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 173.8444, GNorm = 0.2804
Meta loss on this task batch = 2.6249e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 173.8491, GNorm = 0.2503
Meta loss on this task batch = 2.7694e-01, Meta loss averaged over last 500 steps = 2.5723e-01, PNorm = 173.8546, GNorm = 0.2758
Meta loss on this task batch = 2.4509e-01, Meta loss averaged over last 500 steps = 2.5727e-01, PNorm = 173.8605, GNorm = 0.2231
Meta loss on this task batch = 2.9316e-01, Meta loss averaged over last 500 steps = 2.5727e-01, PNorm = 173.8649, GNorm = 0.2923
Meta loss on this task batch = 2.3159e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 173.8689, GNorm = 0.2101
Meta loss on this task batch = 2.3314e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 173.8729, GNorm = 0.2377
Meta loss on this task batch = 2.7649e-01, Meta loss averaged over last 500 steps = 2.5720e-01, PNorm = 173.8766, GNorm = 0.2553
Meta loss on this task batch = 2.8213e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 173.8800, GNorm = 0.2484
Meta loss on this task batch = 2.2635e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 173.8837, GNorm = 0.1983
Meta loss on this task batch = 2.9226e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 173.8879, GNorm = 0.2534
Meta loss on this task batch = 2.6790e-01, Meta loss averaged over last 500 steps = 2.5714e-01, PNorm = 173.8920, GNorm = 0.2518
Meta loss on this task batch = 2.7311e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 173.8962, GNorm = 0.2325
Meta loss on this task batch = 2.6328e-01, Meta loss averaged over last 500 steps = 2.5709e-01, PNorm = 173.8998, GNorm = 0.2520
Meta loss on this task batch = 2.0779e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 173.9034, GNorm = 0.2483
Took 105.56770038604736 seconds to complete one epoch of meta training
Took 112.68539881706238 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501697
Epoch 876
Meta loss on this task batch = 2.1888e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 173.9073, GNorm = 0.1883
Meta loss on this task batch = 2.5215e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 173.9113, GNorm = 0.1912
Meta loss on this task batch = 2.1901e-01, Meta loss averaged over last 500 steps = 2.5708e-01, PNorm = 173.9156, GNorm = 0.2164
Meta loss on this task batch = 2.8215e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 173.9202, GNorm = 0.2677
Meta loss on this task batch = 2.5281e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 173.9246, GNorm = 0.2201
Meta loss on this task batch = 2.6327e-01, Meta loss averaged over last 500 steps = 2.5701e-01, PNorm = 173.9292, GNorm = 0.2234
Meta loss on this task batch = 2.5242e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 173.9333, GNorm = 0.2145
Meta loss on this task batch = 2.3670e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 173.9379, GNorm = 0.2230
Meta loss on this task batch = 2.6638e-01, Meta loss averaged over last 500 steps = 2.5700e-01, PNorm = 173.9418, GNorm = 0.2423
Meta loss on this task batch = 2.6890e-01, Meta loss averaged over last 500 steps = 2.5689e-01, PNorm = 173.9462, GNorm = 0.2567
Meta loss on this task batch = 2.6864e-01, Meta loss averaged over last 500 steps = 2.5691e-01, PNorm = 173.9504, GNorm = 0.2507
Meta loss on this task batch = 2.5322e-01, Meta loss averaged over last 500 steps = 2.5698e-01, PNorm = 173.9547, GNorm = 0.2457
Meta loss on this task batch = 2.4574e-01, Meta loss averaged over last 500 steps = 2.5696e-01, PNorm = 173.9590, GNorm = 0.2654
Meta loss on this task batch = 3.0240e-01, Meta loss averaged over last 500 steps = 2.5708e-01, PNorm = 173.9633, GNorm = 0.2887
Meta loss on this task batch = 2.7953e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 173.9667, GNorm = 0.2483
Meta loss on this task batch = 2.9289e-01, Meta loss averaged over last 500 steps = 2.5703e-01, PNorm = 173.9696, GNorm = 0.2575
Meta loss on this task batch = 2.5979e-01, Meta loss averaged over last 500 steps = 2.5713e-01, PNorm = 173.9728, GNorm = 0.2304
Meta loss on this task batch = 2.1650e-01, Meta loss averaged over last 500 steps = 2.5708e-01, PNorm = 173.9756, GNorm = 0.2022
Meta loss on this task batch = 2.2564e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 173.9787, GNorm = 0.2440
Took 104.2161066532135 seconds to complete one epoch of meta training
Took 111.45117425918579 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487670
Epoch 877
Meta loss on this task batch = 2.9636e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 173.9817, GNorm = 0.2764
Meta loss on this task batch = 2.3882e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 173.9855, GNorm = 0.2537
Meta loss on this task batch = 2.7618e-01, Meta loss averaged over last 500 steps = 2.5719e-01, PNorm = 173.9894, GNorm = 0.2292
Meta loss on this task batch = 2.3647e-01, Meta loss averaged over last 500 steps = 2.5721e-01, PNorm = 173.9931, GNorm = 0.2060
Meta loss on this task batch = 2.7305e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 173.9968, GNorm = 0.2422
Meta loss on this task batch = 2.6379e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 174.0001, GNorm = 0.3079
Meta loss on this task batch = 2.0548e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 174.0040, GNorm = 0.1916
Meta loss on this task batch = 2.4805e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 174.0081, GNorm = 0.2351
Meta loss on this task batch = 1.7864e-01, Meta loss averaged over last 500 steps = 2.5699e-01, PNorm = 174.0126, GNorm = 0.1574
Meta loss on this task batch = 2.6412e-01, Meta loss averaged over last 500 steps = 2.5702e-01, PNorm = 174.0162, GNorm = 0.2305
Meta loss on this task batch = 2.4645e-01, Meta loss averaged over last 500 steps = 2.5701e-01, PNorm = 174.0201, GNorm = 0.1992
Meta loss on this task batch = 3.1532e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 174.0240, GNorm = 0.2623
Meta loss on this task batch = 2.8166e-01, Meta loss averaged over last 500 steps = 2.5727e-01, PNorm = 174.0273, GNorm = 0.2475
Meta loss on this task batch = 2.9560e-01, Meta loss averaged over last 500 steps = 2.5737e-01, PNorm = 174.0311, GNorm = 0.2355
Meta loss on this task batch = 2.8654e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 174.0348, GNorm = 0.2590
Meta loss on this task batch = 2.6046e-01, Meta loss averaged over last 500 steps = 2.5739e-01, PNorm = 174.0382, GNorm = 0.1958
Meta loss on this task batch = 2.7751e-01, Meta loss averaged over last 500 steps = 2.5744e-01, PNorm = 174.0423, GNorm = 0.2152
Meta loss on this task batch = 2.8951e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 174.0467, GNorm = 0.2678
Meta loss on this task batch = 2.2076e-01, Meta loss averaged over last 500 steps = 2.5734e-01, PNorm = 174.0513, GNorm = 0.2554
Took 108.37092447280884 seconds to complete one epoch of meta training
Took 115.4220757484436 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491031
Epoch 878
Meta loss on this task batch = 2.3841e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 174.0562, GNorm = 0.2262
Meta loss on this task batch = 2.2138e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 174.0610, GNorm = 0.1711
Meta loss on this task batch = 3.1913e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 174.0648, GNorm = 0.2524
Meta loss on this task batch = 2.7316e-01, Meta loss averaged over last 500 steps = 2.5744e-01, PNorm = 174.0670, GNorm = 0.2719
Meta loss on this task batch = 2.7062e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 174.0697, GNorm = 0.2845
Meta loss on this task batch = 2.7264e-01, Meta loss averaged over last 500 steps = 2.5738e-01, PNorm = 174.0724, GNorm = 0.2361
Meta loss on this task batch = 2.8632e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.0754, GNorm = 0.2199
Meta loss on this task batch = 2.6717e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.0787, GNorm = 0.2617
Meta loss on this task batch = 2.3672e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 174.0822, GNorm = 0.2715
Meta loss on this task batch = 2.6560e-01, Meta loss averaged over last 500 steps = 2.5736e-01, PNorm = 174.0865, GNorm = 0.2496
Meta loss on this task batch = 2.8247e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 174.0899, GNorm = 0.2354
Meta loss on this task batch = 2.1215e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 174.0942, GNorm = 0.2266
Meta loss on this task batch = 2.2772e-01, Meta loss averaged over last 500 steps = 2.5744e-01, PNorm = 174.0988, GNorm = 0.1990
Meta loss on this task batch = 2.4121e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 174.1032, GNorm = 0.2290
Meta loss on this task batch = 2.9066e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 174.1066, GNorm = 0.2339
Meta loss on this task batch = 2.4508e-01, Meta loss averaged over last 500 steps = 2.5747e-01, PNorm = 174.1108, GNorm = 0.2538
Meta loss on this task batch = 2.6810e-01, Meta loss averaged over last 500 steps = 2.5747e-01, PNorm = 174.1142, GNorm = 0.2219
Meta loss on this task batch = 2.5570e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.1174, GNorm = 0.2358
Meta loss on this task batch = 2.7948e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.1213, GNorm = 0.2604
Took 108.9696741104126 seconds to complete one epoch of meta training
Took 115.75030016899109 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483736
Epoch 879
Meta loss on this task batch = 2.4383e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.1256, GNorm = 0.2193
Meta loss on this task batch = 2.6022e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 174.1302, GNorm = 0.2531
Meta loss on this task batch = 2.6672e-01, Meta loss averaged over last 500 steps = 2.5769e-01, PNorm = 174.1346, GNorm = 0.2685
Meta loss on this task batch = 2.7964e-01, Meta loss averaged over last 500 steps = 2.5769e-01, PNorm = 174.1389, GNorm = 0.2358
Meta loss on this task batch = 2.3945e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.1426, GNorm = 0.2174
Meta loss on this task batch = 2.6738e-01, Meta loss averaged over last 500 steps = 2.5744e-01, PNorm = 174.1455, GNorm = 0.3161
Meta loss on this task batch = 2.5430e-01, Meta loss averaged over last 500 steps = 2.5747e-01, PNorm = 174.1487, GNorm = 0.2465
Meta loss on this task batch = 2.3099e-01, Meta loss averaged over last 500 steps = 2.5742e-01, PNorm = 174.1522, GNorm = 0.2257
Meta loss on this task batch = 2.8476e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.1554, GNorm = 0.2892
Meta loss on this task batch = 2.7670e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 174.1584, GNorm = 0.2367
Meta loss on this task batch = 2.2073e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 174.1621, GNorm = 0.2195
Meta loss on this task batch = 2.1636e-01, Meta loss averaged over last 500 steps = 2.5731e-01, PNorm = 174.1655, GNorm = 0.2217
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 2.5729e-01, PNorm = 174.1692, GNorm = 0.2215
Meta loss on this task batch = 3.1456e-01, Meta loss averaged over last 500 steps = 2.5731e-01, PNorm = 174.1724, GNorm = 0.2457
Meta loss on this task batch = 2.1143e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 174.1756, GNorm = 0.2242
Meta loss on this task batch = 3.0382e-01, Meta loss averaged over last 500 steps = 2.5722e-01, PNorm = 174.1792, GNorm = 0.2576
Meta loss on this task batch = 2.7859e-01, Meta loss averaged over last 500 steps = 2.5727e-01, PNorm = 174.1830, GNorm = 0.2586
Meta loss on this task batch = 2.5200e-01, Meta loss averaged over last 500 steps = 2.5739e-01, PNorm = 174.1876, GNorm = 0.2590
Meta loss on this task batch = 2.1498e-01, Meta loss averaged over last 500 steps = 2.5738e-01, PNorm = 174.1924, GNorm = 0.2349
Took 107.85620594024658 seconds to complete one epoch of meta training
Took 115.72521448135376 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486062
Epoch 880
Meta loss on this task batch = 2.8973e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 174.1977, GNorm = 0.2540
Meta loss on this task batch = 2.4917e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 174.2031, GNorm = 0.2228
Meta loss on this task batch = 2.8069e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 174.2087, GNorm = 0.2141
Meta loss on this task batch = 2.5113e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.2143, GNorm = 0.2839
Meta loss on this task batch = 2.9260e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 174.2195, GNorm = 0.2899
Meta loss on this task batch = 2.5967e-01, Meta loss averaged over last 500 steps = 2.5747e-01, PNorm = 174.2246, GNorm = 0.2400
Meta loss on this task batch = 3.2426e-01, Meta loss averaged over last 500 steps = 2.5758e-01, PNorm = 174.2287, GNorm = 0.2626
Meta loss on this task batch = 2.4782e-01, Meta loss averaged over last 500 steps = 2.5758e-01, PNorm = 174.2329, GNorm = 0.1715
Meta loss on this task batch = 2.3048e-01, Meta loss averaged over last 500 steps = 2.5749e-01, PNorm = 174.2376, GNorm = 0.1994
Meta loss on this task batch = 3.0331e-01, Meta loss averaged over last 500 steps = 2.5754e-01, PNorm = 174.2422, GNorm = 0.2639
Meta loss on this task batch = 2.1611e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.2471, GNorm = 0.2179
Meta loss on this task batch = 2.6128e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.2519, GNorm = 0.2044
Meta loss on this task batch = 2.5759e-01, Meta loss averaged over last 500 steps = 2.5760e-01, PNorm = 174.2562, GNorm = 0.2296
Meta loss on this task batch = 2.5703e-01, Meta loss averaged over last 500 steps = 2.5765e-01, PNorm = 174.2609, GNorm = 0.2185
Meta loss on this task batch = 2.4828e-01, Meta loss averaged over last 500 steps = 2.5773e-01, PNorm = 174.2660, GNorm = 0.2389
Meta loss on this task batch = 2.4665e-01, Meta loss averaged over last 500 steps = 2.5760e-01, PNorm = 174.2707, GNorm = 0.2009
Meta loss on this task batch = 2.1754e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 174.2755, GNorm = 0.2306
Meta loss on this task batch = 3.1946e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 174.2798, GNorm = 0.2489
Meta loss on this task batch = 2.4355e-01, Meta loss averaged over last 500 steps = 2.5768e-01, PNorm = 174.2841, GNorm = 0.3020
Took 107.21966552734375 seconds to complete one epoch of meta training
Took 113.68790292739868 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.514876
Epoch 881
Meta loss on this task batch = 2.8731e-01, Meta loss averaged over last 500 steps = 2.5775e-01, PNorm = 174.2883, GNorm = 0.2448
Meta loss on this task batch = 2.4377e-01, Meta loss averaged over last 500 steps = 2.5762e-01, PNorm = 174.2921, GNorm = 0.2461
Meta loss on this task batch = 2.5098e-01, Meta loss averaged over last 500 steps = 2.5755e-01, PNorm = 174.2944, GNorm = 0.3074
Meta loss on this task batch = 2.6878e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 174.2966, GNorm = 0.2437
Meta loss on this task batch = 3.0254e-01, Meta loss averaged over last 500 steps = 2.5769e-01, PNorm = 174.2986, GNorm = 0.2717
Meta loss on this task batch = 2.8689e-01, Meta loss averaged over last 500 steps = 2.5765e-01, PNorm = 174.3012, GNorm = 0.2254
Meta loss on this task batch = 2.6752e-01, Meta loss averaged over last 500 steps = 2.5766e-01, PNorm = 174.3037, GNorm = 0.2775
Meta loss on this task batch = 2.4446e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.3065, GNorm = 0.2149
Meta loss on this task batch = 2.2017e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 174.3095, GNorm = 0.2401
Meta loss on this task batch = 2.5646e-01, Meta loss averaged over last 500 steps = 2.5750e-01, PNorm = 174.3129, GNorm = 0.2209
Meta loss on this task batch = 2.2011e-01, Meta loss averaged over last 500 steps = 2.5749e-01, PNorm = 174.3172, GNorm = 0.2226
Meta loss on this task batch = 2.8894e-01, Meta loss averaged over last 500 steps = 2.5758e-01, PNorm = 174.3222, GNorm = 0.2136
Meta loss on this task batch = 2.1488e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.3278, GNorm = 0.1969
Meta loss on this task batch = 3.1145e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 174.3331, GNorm = 0.2729
Meta loss on this task batch = 2.1498e-01, Meta loss averaged over last 500 steps = 2.5764e-01, PNorm = 174.3385, GNorm = 0.2344
Meta loss on this task batch = 2.8741e-01, Meta loss averaged over last 500 steps = 2.5779e-01, PNorm = 174.3440, GNorm = 0.2886
Meta loss on this task batch = 2.6101e-01, Meta loss averaged over last 500 steps = 2.5785e-01, PNorm = 174.3498, GNorm = 0.2342
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.5779e-01, PNorm = 174.3563, GNorm = 0.2359
Meta loss on this task batch = 2.8318e-01, Meta loss averaged over last 500 steps = 2.5782e-01, PNorm = 174.3619, GNorm = 0.2908
Took 106.72493958473206 seconds to complete one epoch of meta training
Took 114.6751697063446 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517602
Epoch 882
Meta loss on this task batch = 2.5137e-01, Meta loss averaged over last 500 steps = 2.5782e-01, PNorm = 174.3678, GNorm = 0.2146
Meta loss on this task batch = 2.2393e-01, Meta loss averaged over last 500 steps = 2.5772e-01, PNorm = 174.3734, GNorm = 0.1999
Meta loss on this task batch = 2.3535e-01, Meta loss averaged over last 500 steps = 2.5767e-01, PNorm = 174.3789, GNorm = 0.2151
Meta loss on this task batch = 2.1770e-01, Meta loss averaged over last 500 steps = 2.5755e-01, PNorm = 174.3843, GNorm = 0.2195
Meta loss on this task batch = 2.6580e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.3902, GNorm = 0.2345
Meta loss on this task batch = 2.3520e-01, Meta loss averaged over last 500 steps = 2.5743e-01, PNorm = 174.3956, GNorm = 0.2413
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.4011, GNorm = 0.2680
Meta loss on this task batch = 2.5241e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.4066, GNorm = 0.2324
Meta loss on this task batch = 2.7486e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.4121, GNorm = 0.2490
Meta loss on this task batch = 2.2863e-01, Meta loss averaged over last 500 steps = 2.5738e-01, PNorm = 174.4173, GNorm = 0.2146
Meta loss on this task batch = 2.8980e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 174.4221, GNorm = 0.2689
Meta loss on this task batch = 2.3820e-01, Meta loss averaged over last 500 steps = 2.5734e-01, PNorm = 174.4274, GNorm = 0.2033
Meta loss on this task batch = 2.6244e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 174.4329, GNorm = 0.2308
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 2.5729e-01, PNorm = 174.4378, GNorm = 0.2556
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.5735e-01, PNorm = 174.4433, GNorm = 0.2258
Meta loss on this task batch = 2.8179e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.4494, GNorm = 0.2723
Meta loss on this task batch = 2.3652e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.4550, GNorm = 0.2288
Meta loss on this task batch = 2.3312e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.4608, GNorm = 0.2113
Meta loss on this task batch = 2.7002e-01, Meta loss averaged over last 500 steps = 2.5750e-01, PNorm = 174.4665, GNorm = 0.3018
Took 110.8975281715393 seconds to complete one epoch of meta training
Took 118.09095597267151 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506057
Epoch 883
Meta loss on this task batch = 2.7032e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.4725, GNorm = 0.2076
Meta loss on this task batch = 3.0032e-01, Meta loss averaged over last 500 steps = 2.5750e-01, PNorm = 174.4782, GNorm = 0.2487
Meta loss on this task batch = 2.5691e-01, Meta loss averaged over last 500 steps = 2.5752e-01, PNorm = 174.4840, GNorm = 0.2377
Meta loss on this task batch = 2.1306e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 174.4896, GNorm = 0.2257
Meta loss on this task batch = 2.6571e-01, Meta loss averaged over last 500 steps = 2.5757e-01, PNorm = 174.4950, GNorm = 0.2310
Meta loss on this task batch = 2.5055e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 174.4998, GNorm = 0.2665
Meta loss on this task batch = 2.3965e-01, Meta loss averaged over last 500 steps = 2.5741e-01, PNorm = 174.5042, GNorm = 0.2363
Meta loss on this task batch = 2.1897e-01, Meta loss averaged over last 500 steps = 2.5732e-01, PNorm = 174.5081, GNorm = 0.2247
Meta loss on this task batch = 2.8776e-01, Meta loss averaged over last 500 steps = 2.5739e-01, PNorm = 174.5124, GNorm = 0.2612
Meta loss on this task batch = 2.6601e-01, Meta loss averaged over last 500 steps = 2.5750e-01, PNorm = 174.5169, GNorm = 0.1931
Meta loss on this task batch = 2.6678e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.5216, GNorm = 0.2079
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.5759e-01, PNorm = 174.5263, GNorm = 0.2280
Meta loss on this task batch = 2.6448e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.5305, GNorm = 0.3274
Meta loss on this task batch = 2.5284e-01, Meta loss averaged over last 500 steps = 2.5744e-01, PNorm = 174.5350, GNorm = 0.2021
Meta loss on this task batch = 2.7055e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.5382, GNorm = 0.2523
Meta loss on this task batch = 2.1216e-01, Meta loss averaged over last 500 steps = 2.5751e-01, PNorm = 174.5419, GNorm = 0.1994
Meta loss on this task batch = 2.3134e-01, Meta loss averaged over last 500 steps = 2.5745e-01, PNorm = 174.5461, GNorm = 0.2030
Meta loss on this task batch = 2.4837e-01, Meta loss averaged over last 500 steps = 2.5749e-01, PNorm = 174.5508, GNorm = 0.2479
Meta loss on this task batch = 2.8211e-01, Meta loss averaged over last 500 steps = 2.5756e-01, PNorm = 174.5556, GNorm = 0.2812
Took 107.16330409049988 seconds to complete one epoch of meta training
Took 114.523108959198 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.475565
Epoch 884
Meta loss on this task batch = 2.4928e-01, Meta loss averaged over last 500 steps = 2.5755e-01, PNorm = 174.5609, GNorm = 0.2149
Meta loss on this task batch = 2.6097e-01, Meta loss averaged over last 500 steps = 2.5754e-01, PNorm = 174.5663, GNorm = 0.2129
Meta loss on this task batch = 2.4348e-01, Meta loss averaged over last 500 steps = 2.5753e-01, PNorm = 174.5715, GNorm = 0.2001
Meta loss on this task batch = 2.4933e-01, Meta loss averaged over last 500 steps = 2.5744e-01, PNorm = 174.5766, GNorm = 0.2458
Meta loss on this task batch = 2.4363e-01, Meta loss averaged over last 500 steps = 2.5742e-01, PNorm = 174.5814, GNorm = 0.2285
Meta loss on this task batch = 2.3171e-01, Meta loss averaged over last 500 steps = 2.5734e-01, PNorm = 174.5865, GNorm = 0.2411
Meta loss on this task batch = 2.5233e-01, Meta loss averaged over last 500 steps = 2.5724e-01, PNorm = 174.5915, GNorm = 0.2069
Meta loss on this task batch = 1.9643e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 174.5963, GNorm = 0.2060
Meta loss on this task batch = 2.5966e-01, Meta loss averaged over last 500 steps = 2.5714e-01, PNorm = 174.6010, GNorm = 0.2158
Meta loss on this task batch = 2.7111e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 174.6058, GNorm = 0.2400
Meta loss on this task batch = 2.5574e-01, Meta loss averaged over last 500 steps = 2.5722e-01, PNorm = 174.6103, GNorm = 0.2693
Meta loss on this task batch = 2.4394e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 174.6142, GNorm = 0.2870
Meta loss on this task batch = 2.7319e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 174.6190, GNorm = 0.2637
Meta loss on this task batch = 2.6617e-01, Meta loss averaged over last 500 steps = 2.5723e-01, PNorm = 174.6235, GNorm = 0.2385
Meta loss on this task batch = 2.7057e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 174.6280, GNorm = 0.2545
Meta loss on this task batch = 1.9621e-01, Meta loss averaged over last 500 steps = 2.5703e-01, PNorm = 174.6327, GNorm = 0.2150
Meta loss on this task batch = 2.1788e-01, Meta loss averaged over last 500 steps = 2.5702e-01, PNorm = 174.6367, GNorm = 0.1986
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 174.6410, GNorm = 0.2510
Meta loss on this task batch = 2.4151e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 174.6456, GNorm = 0.2487
Took 108.30789947509766 seconds to complete one epoch of meta training
Took 114.31720161437988 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491901
Epoch 885
Meta loss on this task batch = 3.2739e-01, Meta loss averaged over last 500 steps = 2.5724e-01, PNorm = 174.6497, GNorm = 0.2795
Meta loss on this task batch = 2.4789e-01, Meta loss averaged over last 500 steps = 2.5721e-01, PNorm = 174.6540, GNorm = 0.2093
Meta loss on this task batch = 2.1454e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 174.6580, GNorm = 0.2428
Meta loss on this task batch = 2.6734e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 174.6619, GNorm = 0.2179
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.5724e-01, PNorm = 174.6658, GNorm = 0.2817
Meta loss on this task batch = 2.3298e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 174.6700, GNorm = 0.2279
Meta loss on this task batch = 2.5563e-01, Meta loss averaged over last 500 steps = 2.5732e-01, PNorm = 174.6742, GNorm = 0.2162
Meta loss on this task batch = 2.5416e-01, Meta loss averaged over last 500 steps = 2.5732e-01, PNorm = 174.6787, GNorm = 0.2208
Meta loss on this task batch = 2.8432e-01, Meta loss averaged over last 500 steps = 2.5729e-01, PNorm = 174.6829, GNorm = 0.2657
Meta loss on this task batch = 2.7038e-01, Meta loss averaged over last 500 steps = 2.5722e-01, PNorm = 174.6872, GNorm = 0.2381
Meta loss on this task batch = 1.9723e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 174.6921, GNorm = 0.2089
Meta loss on this task batch = 2.3087e-01, Meta loss averaged over last 500 steps = 2.5727e-01, PNorm = 174.6973, GNorm = 0.2359
Meta loss on this task batch = 2.2275e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 174.7028, GNorm = 0.2531
Meta loss on this task batch = 3.0418e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 174.7082, GNorm = 0.2383
Meta loss on this task batch = 2.8725e-01, Meta loss averaged over last 500 steps = 2.5737e-01, PNorm = 174.7145, GNorm = 0.2368
Meta loss on this task batch = 2.6679e-01, Meta loss averaged over last 500 steps = 2.5731e-01, PNorm = 174.7205, GNorm = 0.2194
Meta loss on this task batch = 3.4239e-01, Meta loss averaged over last 500 steps = 2.5741e-01, PNorm = 174.7243, GNorm = 0.3086
Meta loss on this task batch = 2.8084e-01, Meta loss averaged over last 500 steps = 2.5748e-01, PNorm = 174.7276, GNorm = 0.2808
Meta loss on this task batch = 2.4487e-01, Meta loss averaged over last 500 steps = 2.5750e-01, PNorm = 174.7315, GNorm = 0.2628
Took 110.59794759750366 seconds to complete one epoch of meta training
Took 118.33303475379944 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500109
Epoch 886
Meta loss on this task batch = 2.3370e-01, Meta loss averaged over last 500 steps = 2.5740e-01, PNorm = 174.7357, GNorm = 0.2053
Meta loss on this task batch = 2.1110e-01, Meta loss averaged over last 500 steps = 2.5731e-01, PNorm = 174.7401, GNorm = 0.1756
Meta loss on this task batch = 2.7119e-01, Meta loss averaged over last 500 steps = 2.5727e-01, PNorm = 174.7440, GNorm = 0.2540
Meta loss on this task batch = 2.5179e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 174.7479, GNorm = 0.2276
Meta loss on this task batch = 2.0441e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 174.7523, GNorm = 0.2144
Meta loss on this task batch = 2.8869e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 174.7561, GNorm = 0.2185
Meta loss on this task batch = 2.5160e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 174.7601, GNorm = 0.2464
Meta loss on this task batch = 2.4920e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 174.7643, GNorm = 0.2358
Meta loss on this task batch = 2.2877e-01, Meta loss averaged over last 500 steps = 2.5701e-01, PNorm = 174.7695, GNorm = 0.2202
Meta loss on this task batch = 2.1567e-01, Meta loss averaged over last 500 steps = 2.5695e-01, PNorm = 174.7753, GNorm = 0.2454
Meta loss on this task batch = 2.6682e-01, Meta loss averaged over last 500 steps = 2.5709e-01, PNorm = 174.7814, GNorm = 0.2447
Meta loss on this task batch = 2.3805e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 174.7873, GNorm = 0.2398
Meta loss on this task batch = 2.7891e-01, Meta loss averaged over last 500 steps = 2.5708e-01, PNorm = 174.7926, GNorm = 0.2414
Meta loss on this task batch = 2.5184e-01, Meta loss averaged over last 500 steps = 2.5701e-01, PNorm = 174.7980, GNorm = 0.2348
Meta loss on this task batch = 2.6260e-01, Meta loss averaged over last 500 steps = 2.5703e-01, PNorm = 174.8030, GNorm = 0.2538
Meta loss on this task batch = 3.2757e-01, Meta loss averaged over last 500 steps = 2.5724e-01, PNorm = 174.8069, GNorm = 0.2886
Meta loss on this task batch = 2.3912e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 174.8107, GNorm = 0.2441
Meta loss on this task batch = 2.7882e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 174.8145, GNorm = 0.2776
Meta loss on this task batch = 2.5151e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 174.8185, GNorm = 0.2823
Took 112.23682856559753 seconds to complete one epoch of meta training
Took 119.64804983139038 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502149
Epoch 887
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 174.8222, GNorm = 0.2171
Meta loss on this task batch = 2.1112e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 174.8263, GNorm = 0.2039
Meta loss on this task batch = 3.3155e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 174.8301, GNorm = 0.2462
Meta loss on this task batch = 2.1000e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 174.8346, GNorm = 0.2173
Meta loss on this task batch = 1.7261e-01, Meta loss averaged over last 500 steps = 2.5698e-01, PNorm = 174.8392, GNorm = 0.1930
Meta loss on this task batch = 2.4783e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 174.8434, GNorm = 0.2418
Meta loss on this task batch = 2.9547e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 174.8474, GNorm = 0.2576
Meta loss on this task batch = 2.4001e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 174.8517, GNorm = 0.2717
Meta loss on this task batch = 2.5474e-01, Meta loss averaged over last 500 steps = 2.5709e-01, PNorm = 174.8564, GNorm = 0.2012
Meta loss on this task batch = 2.7374e-01, Meta loss averaged over last 500 steps = 2.5710e-01, PNorm = 174.8606, GNorm = 0.2094
Meta loss on this task batch = 2.9150e-01, Meta loss averaged over last 500 steps = 2.5719e-01, PNorm = 174.8648, GNorm = 0.2081
Meta loss on this task batch = 2.8879e-01, Meta loss averaged over last 500 steps = 2.5721e-01, PNorm = 174.8694, GNorm = 0.2969
Meta loss on this task batch = 2.2891e-01, Meta loss averaged over last 500 steps = 2.5720e-01, PNorm = 174.8743, GNorm = 0.2304
Meta loss on this task batch = 2.8820e-01, Meta loss averaged over last 500 steps = 2.5725e-01, PNorm = 174.8794, GNorm = 0.2525
Meta loss on this task batch = 2.0918e-01, Meta loss averaged over last 500 steps = 2.5719e-01, PNorm = 174.8845, GNorm = 0.2227
Meta loss on this task batch = 2.4097e-01, Meta loss averaged over last 500 steps = 2.5722e-01, PNorm = 174.8901, GNorm = 0.2312
Meta loss on this task batch = 2.6630e-01, Meta loss averaged over last 500 steps = 2.5728e-01, PNorm = 174.8951, GNorm = 0.2567
Meta loss on this task batch = 2.2225e-01, Meta loss averaged over last 500 steps = 2.5723e-01, PNorm = 174.9004, GNorm = 0.2048
Meta loss on this task batch = 2.3066e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 174.9046, GNorm = 0.3313
Took 110.07176637649536 seconds to complete one epoch of meta training
Took 117.23450136184692 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488861
Epoch 888
Meta loss on this task batch = 1.9552e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 174.9088, GNorm = 0.1897
Meta loss on this task batch = 2.4456e-01, Meta loss averaged over last 500 steps = 2.5692e-01, PNorm = 174.9135, GNorm = 0.2152
Meta loss on this task batch = 2.4806e-01, Meta loss averaged over last 500 steps = 2.5690e-01, PNorm = 174.9185, GNorm = 0.2747
Meta loss on this task batch = 2.1599e-01, Meta loss averaged over last 500 steps = 2.5687e-01, PNorm = 174.9241, GNorm = 0.2008
Meta loss on this task batch = 2.4420e-01, Meta loss averaged over last 500 steps = 2.5690e-01, PNorm = 174.9303, GNorm = 0.2415
Meta loss on this task batch = 2.5898e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 174.9354, GNorm = 0.2814
Meta loss on this task batch = 2.1524e-01, Meta loss averaged over last 500 steps = 2.5696e-01, PNorm = 174.9403, GNorm = 0.2040
Meta loss on this task batch = 2.8129e-01, Meta loss averaged over last 500 steps = 2.5693e-01, PNorm = 174.9443, GNorm = 0.3168
Meta loss on this task batch = 2.1043e-01, Meta loss averaged over last 500 steps = 2.5676e-01, PNorm = 174.9487, GNorm = 0.2119
Meta loss on this task batch = 2.4344e-01, Meta loss averaged over last 500 steps = 2.5672e-01, PNorm = 174.9528, GNorm = 0.2424
Meta loss on this task batch = 2.5688e-01, Meta loss averaged over last 500 steps = 2.5673e-01, PNorm = 174.9573, GNorm = 0.2526
Meta loss on this task batch = 2.7500e-01, Meta loss averaged over last 500 steps = 2.5676e-01, PNorm = 174.9622, GNorm = 0.2488
Meta loss on this task batch = 2.9081e-01, Meta loss averaged over last 500 steps = 2.5689e-01, PNorm = 174.9668, GNorm = 0.2381
Meta loss on this task batch = 2.6634e-01, Meta loss averaged over last 500 steps = 2.5688e-01, PNorm = 174.9711, GNorm = 0.2467
Meta loss on this task batch = 2.1909e-01, Meta loss averaged over last 500 steps = 2.5684e-01, PNorm = 174.9764, GNorm = 0.2395
Meta loss on this task batch = 2.3818e-01, Meta loss averaged over last 500 steps = 2.5686e-01, PNorm = 174.9808, GNorm = 0.2564
Meta loss on this task batch = 2.6881e-01, Meta loss averaged over last 500 steps = 2.5700e-01, PNorm = 174.9857, GNorm = 0.2703
Meta loss on this task batch = 2.9574e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 174.9906, GNorm = 0.2360
Meta loss on this task batch = 2.6711e-01, Meta loss averaged over last 500 steps = 2.5729e-01, PNorm = 174.9958, GNorm = 0.3240
Took 106.72136282920837 seconds to complete one epoch of meta training
Took 114.1601870059967 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515666
Epoch 889
Meta loss on this task batch = 2.3909e-01, Meta loss averaged over last 500 steps = 2.5728e-01, PNorm = 175.0015, GNorm = 0.2254
Meta loss on this task batch = 1.9953e-01, Meta loss averaged over last 500 steps = 2.5720e-01, PNorm = 175.0078, GNorm = 0.2636
Meta loss on this task batch = 2.4563e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 175.0145, GNorm = 0.2240
Meta loss on this task batch = 2.9380e-01, Meta loss averaged over last 500 steps = 2.5720e-01, PNorm = 175.0214, GNorm = 0.2280
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 175.0285, GNorm = 0.2726
Meta loss on this task batch = 2.6931e-01, Meta loss averaged over last 500 steps = 2.5713e-01, PNorm = 175.0351, GNorm = 0.2361
Meta loss on this task batch = 2.5454e-01, Meta loss averaged over last 500 steps = 2.5716e-01, PNorm = 175.0412, GNorm = 0.2396
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.5701e-01, PNorm = 175.0471, GNorm = 0.2301
Meta loss on this task batch = 2.5506e-01, Meta loss averaged over last 500 steps = 2.5707e-01, PNorm = 175.0526, GNorm = 0.2542
Meta loss on this task batch = 2.4638e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 175.0573, GNorm = 0.2012
Meta loss on this task batch = 2.3763e-01, Meta loss averaged over last 500 steps = 2.5713e-01, PNorm = 175.0616, GNorm = 0.2298
Meta loss on this task batch = 2.6712e-01, Meta loss averaged over last 500 steps = 2.5721e-01, PNorm = 175.0665, GNorm = 0.2023
Meta loss on this task batch = 2.3695e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 175.0711, GNorm = 0.2218
Meta loss on this task batch = 2.4481e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 175.0745, GNorm = 0.2438
Meta loss on this task batch = 2.6531e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 175.0783, GNorm = 0.2750
Meta loss on this task batch = 2.5203e-01, Meta loss averaged over last 500 steps = 2.5714e-01, PNorm = 175.0821, GNorm = 0.2114
Meta loss on this task batch = 2.8011e-01, Meta loss averaged over last 500 steps = 2.5715e-01, PNorm = 175.0870, GNorm = 0.3086
Meta loss on this task batch = 2.8034e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 175.0913, GNorm = 0.2855
Meta loss on this task batch = 2.3058e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 175.0955, GNorm = 0.2376
Took 110.9138503074646 seconds to complete one epoch of meta training
Took 118.53603076934814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485582
Epoch 890
Meta loss on this task batch = 2.5475e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 175.0994, GNorm = 0.2507
Meta loss on this task batch = 2.6778e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 175.1027, GNorm = 0.2528
Meta loss on this task batch = 2.3629e-01, Meta loss averaged over last 500 steps = 2.5720e-01, PNorm = 175.1055, GNorm = 0.2469
Meta loss on this task batch = 2.4462e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 175.1088, GNorm = 0.2322
Meta loss on this task batch = 2.9595e-01, Meta loss averaged over last 500 steps = 2.5718e-01, PNorm = 175.1126, GNorm = 0.2466
Meta loss on this task batch = 2.6027e-01, Meta loss averaged over last 500 steps = 2.5708e-01, PNorm = 175.1157, GNorm = 0.3400
Meta loss on this task batch = 2.3254e-01, Meta loss averaged over last 500 steps = 2.5700e-01, PNorm = 175.1189, GNorm = 0.2449
Meta loss on this task batch = 2.5946e-01, Meta loss averaged over last 500 steps = 2.5701e-01, PNorm = 175.1220, GNorm = 0.2587
Meta loss on this task batch = 2.4153e-01, Meta loss averaged over last 500 steps = 2.5706e-01, PNorm = 175.1256, GNorm = 0.2022
Meta loss on this task batch = 2.5742e-01, Meta loss averaged over last 500 steps = 2.5705e-01, PNorm = 175.1293, GNorm = 0.2313
Meta loss on this task batch = 2.5746e-01, Meta loss averaged over last 500 steps = 2.5709e-01, PNorm = 175.1334, GNorm = 0.2243
Meta loss on this task batch = 2.4275e-01, Meta loss averaged over last 500 steps = 2.5711e-01, PNorm = 175.1377, GNorm = 0.2163
Meta loss on this task batch = 3.1182e-01, Meta loss averaged over last 500 steps = 2.5717e-01, PNorm = 175.1421, GNorm = 0.2332
Meta loss on this task batch = 2.6231e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 175.1466, GNorm = 0.2478
Meta loss on this task batch = 2.9963e-01, Meta loss averaged over last 500 steps = 2.5721e-01, PNorm = 175.1517, GNorm = 0.2176
Meta loss on this task batch = 2.7376e-01, Meta loss averaged over last 500 steps = 2.5724e-01, PNorm = 175.1565, GNorm = 0.2318
Meta loss on this task batch = 2.5472e-01, Meta loss averaged over last 500 steps = 2.5728e-01, PNorm = 175.1615, GNorm = 0.2148
Meta loss on this task batch = 2.3477e-01, Meta loss averaged over last 500 steps = 2.5731e-01, PNorm = 175.1665, GNorm = 0.2334
Meta loss on this task batch = 2.8375e-01, Meta loss averaged over last 500 steps = 2.5722e-01, PNorm = 175.1714, GNorm = 0.2952
Took 109.53986358642578 seconds to complete one epoch of meta training
Took 116.59310793876648 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.478192
Epoch 891
Meta loss on this task batch = 2.3838e-01, Meta loss averaged over last 500 steps = 2.5726e-01, PNorm = 175.1769, GNorm = 0.2179
Meta loss on this task batch = 2.7683e-01, Meta loss averaged over last 500 steps = 2.5721e-01, PNorm = 175.1828, GNorm = 0.2419
Meta loss on this task batch = 2.2016e-01, Meta loss averaged over last 500 steps = 2.5704e-01, PNorm = 175.1888, GNorm = 0.2079
Meta loss on this task batch = 2.6241e-01, Meta loss averaged over last 500 steps = 2.5712e-01, PNorm = 175.1948, GNorm = 0.2120
Meta loss on this task batch = 2.7246e-01, Meta loss averaged over last 500 steps = 2.5708e-01, PNorm = 175.2000, GNorm = 0.2518
Meta loss on this task batch = 2.1061e-01, Meta loss averaged over last 500 steps = 2.5694e-01, PNorm = 175.2053, GNorm = 0.2139
Meta loss on this task batch = 2.1414e-01, Meta loss averaged over last 500 steps = 2.5682e-01, PNorm = 175.2108, GNorm = 0.2514
Meta loss on this task batch = 2.6831e-01, Meta loss averaged over last 500 steps = 2.5686e-01, PNorm = 175.2159, GNorm = 0.2847
Meta loss on this task batch = 2.2352e-01, Meta loss averaged over last 500 steps = 2.5694e-01, PNorm = 175.2209, GNorm = 0.2392
Meta loss on this task batch = 2.1316e-01, Meta loss averaged over last 500 steps = 2.5672e-01, PNorm = 175.2261, GNorm = 0.1928
Meta loss on this task batch = 2.7239e-01, Meta loss averaged over last 500 steps = 2.5676e-01, PNorm = 175.2307, GNorm = 0.2258
Meta loss on this task batch = 2.5224e-01, Meta loss averaged over last 500 steps = 2.5672e-01, PNorm = 175.2357, GNorm = 0.2510
Meta loss on this task batch = 2.6500e-01, Meta loss averaged over last 500 steps = 2.5681e-01, PNorm = 175.2407, GNorm = 0.2403
Meta loss on this task batch = 3.0252e-01, Meta loss averaged over last 500 steps = 2.5694e-01, PNorm = 175.2457, GNorm = 0.2350
Meta loss on this task batch = 3.1052e-01, Meta loss averaged over last 500 steps = 2.5702e-01, PNorm = 175.2503, GNorm = 0.2686
Meta loss on this task batch = 2.6478e-01, Meta loss averaged over last 500 steps = 2.5697e-01, PNorm = 175.2555, GNorm = 0.2256
Meta loss on this task batch = 2.7622e-01, Meta loss averaged over last 500 steps = 2.5696e-01, PNorm = 175.2595, GNorm = 0.2575
Meta loss on this task batch = 2.5877e-01, Meta loss averaged over last 500 steps = 2.5690e-01, PNorm = 175.2630, GNorm = 0.2487
Meta loss on this task batch = 2.1401e-01, Meta loss averaged over last 500 steps = 2.5678e-01, PNorm = 175.2673, GNorm = 0.2483
Took 103.06059575080872 seconds to complete one epoch of meta training
Took 110.19091272354126 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493294
Epoch 892
Meta loss on this task batch = 2.5464e-01, Meta loss averaged over last 500 steps = 2.5674e-01, PNorm = 175.2709, GNorm = 0.2397
Meta loss on this task batch = 2.3363e-01, Meta loss averaged over last 500 steps = 2.5667e-01, PNorm = 175.2745, GNorm = 0.2107
Meta loss on this task batch = 2.2547e-01, Meta loss averaged over last 500 steps = 2.5654e-01, PNorm = 175.2782, GNorm = 0.2262
Meta loss on this task batch = 2.2219e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 175.2825, GNorm = 0.2097
Meta loss on this task batch = 3.1249e-01, Meta loss averaged over last 500 steps = 2.5655e-01, PNorm = 175.2870, GNorm = 0.2196
Meta loss on this task batch = 2.2263e-01, Meta loss averaged over last 500 steps = 2.5651e-01, PNorm = 175.2914, GNorm = 0.2081
Meta loss on this task batch = 2.5695e-01, Meta loss averaged over last 500 steps = 2.5656e-01, PNorm = 175.2956, GNorm = 0.2238
Meta loss on this task batch = 2.1191e-01, Meta loss averaged over last 500 steps = 2.5651e-01, PNorm = 175.2997, GNorm = 0.2309
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.5651e-01, PNorm = 175.3042, GNorm = 0.2353
Meta loss on this task batch = 2.6733e-01, Meta loss averaged over last 500 steps = 2.5645e-01, PNorm = 175.3094, GNorm = 0.2231
Meta loss on this task batch = 2.7438e-01, Meta loss averaged over last 500 steps = 2.5655e-01, PNorm = 175.3138, GNorm = 0.2437
Meta loss on this task batch = 3.0123e-01, Meta loss averaged over last 500 steps = 2.5665e-01, PNorm = 175.3181, GNorm = 0.2292
Meta loss on this task batch = 2.3702e-01, Meta loss averaged over last 500 steps = 2.5665e-01, PNorm = 175.3222, GNorm = 0.2248
Meta loss on this task batch = 2.2580e-01, Meta loss averaged over last 500 steps = 2.5660e-01, PNorm = 175.3266, GNorm = 0.2369
Meta loss on this task batch = 2.4183e-01, Meta loss averaged over last 500 steps = 2.5655e-01, PNorm = 175.3312, GNorm = 0.2423
Meta loss on this task batch = 3.0837e-01, Meta loss averaged over last 500 steps = 2.5660e-01, PNorm = 175.3354, GNorm = 0.2548
Meta loss on this task batch = 2.6393e-01, Meta loss averaged over last 500 steps = 2.5665e-01, PNorm = 175.3394, GNorm = 0.2170
Meta loss on this task batch = 2.6113e-01, Meta loss averaged over last 500 steps = 2.5662e-01, PNorm = 175.3432, GNorm = 0.2567
Meta loss on this task batch = 2.5341e-01, Meta loss averaged over last 500 steps = 2.5656e-01, PNorm = 175.3478, GNorm = 0.2533
Took 105.1769073009491 seconds to complete one epoch of meta training
Took 112.0654890537262 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486792
Epoch 893
Meta loss on this task batch = 3.3412e-01, Meta loss averaged over last 500 steps = 2.5672e-01, PNorm = 175.3510, GNorm = 0.2999
Meta loss on this task batch = 2.5842e-01, Meta loss averaged over last 500 steps = 2.5667e-01, PNorm = 175.3543, GNorm = 0.2152
Meta loss on this task batch = 2.4412e-01, Meta loss averaged over last 500 steps = 2.5657e-01, PNorm = 175.3583, GNorm = 0.2993
Meta loss on this task batch = 2.2118e-01, Meta loss averaged over last 500 steps = 2.5647e-01, PNorm = 175.3629, GNorm = 0.2238
Meta loss on this task batch = 2.3855e-01, Meta loss averaged over last 500 steps = 2.5640e-01, PNorm = 175.3674, GNorm = 0.2120
Meta loss on this task batch = 2.6184e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 175.3721, GNorm = 0.2310
Meta loss on this task batch = 2.7270e-01, Meta loss averaged over last 500 steps = 2.5649e-01, PNorm = 175.3768, GNorm = 0.2404
Meta loss on this task batch = 2.8886e-01, Meta loss averaged over last 500 steps = 2.5663e-01, PNorm = 175.3810, GNorm = 0.2547
Meta loss on this task batch = 1.7823e-01, Meta loss averaged over last 500 steps = 2.5645e-01, PNorm = 175.3857, GNorm = 0.1976
Meta loss on this task batch = 2.1264e-01, Meta loss averaged over last 500 steps = 2.5635e-01, PNorm = 175.3894, GNorm = 0.2380
Meta loss on this task batch = 2.9930e-01, Meta loss averaged over last 500 steps = 2.5640e-01, PNorm = 175.3921, GNorm = 0.3213
Meta loss on this task batch = 2.8904e-01, Meta loss averaged over last 500 steps = 2.5651e-01, PNorm = 175.3948, GNorm = 0.2408
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 2.5650e-01, PNorm = 175.3977, GNorm = 0.2551
Meta loss on this task batch = 1.9508e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 175.4015, GNorm = 0.2016
Meta loss on this task batch = 2.8236e-01, Meta loss averaged over last 500 steps = 2.5643e-01, PNorm = 175.4051, GNorm = 0.2592
Meta loss on this task batch = 2.5385e-01, Meta loss averaged over last 500 steps = 2.5629e-01, PNorm = 175.4093, GNorm = 0.2051
Meta loss on this task batch = 2.2304e-01, Meta loss averaged over last 500 steps = 2.5630e-01, PNorm = 175.4129, GNorm = 0.2221
Meta loss on this task batch = 1.8329e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 175.4164, GNorm = 0.1973
Meta loss on this task batch = 2.7107e-01, Meta loss averaged over last 500 steps = 2.5618e-01, PNorm = 175.4200, GNorm = 0.2843
Took 101.8400137424469 seconds to complete one epoch of meta training
Took 108.88690257072449 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506429
Epoch 894
Meta loss on this task batch = 2.0104e-01, Meta loss averaged over last 500 steps = 2.5603e-01, PNorm = 175.4244, GNorm = 0.1899
Meta loss on this task batch = 2.3679e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 175.4292, GNorm = 0.2383
Meta loss on this task batch = 2.6173e-01, Meta loss averaged over last 500 steps = 2.5624e-01, PNorm = 175.4336, GNorm = 0.2060
Meta loss on this task batch = 2.4963e-01, Meta loss averaged over last 500 steps = 2.5625e-01, PNorm = 175.4376, GNorm = 0.2257
Meta loss on this task batch = 3.0529e-01, Meta loss averaged over last 500 steps = 2.5632e-01, PNorm = 175.4411, GNorm = 0.2469
Meta loss on this task batch = 2.7898e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 175.4449, GNorm = 0.2652
Meta loss on this task batch = 2.5691e-01, Meta loss averaged over last 500 steps = 2.5645e-01, PNorm = 175.4494, GNorm = 0.2244
Meta loss on this task batch = 2.6780e-01, Meta loss averaged over last 500 steps = 2.5634e-01, PNorm = 175.4545, GNorm = 0.2901
Meta loss on this task batch = 2.9119e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 175.4599, GNorm = 0.2434
Meta loss on this task batch = 2.3958e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 175.4654, GNorm = 0.2394
Meta loss on this task batch = 2.0769e-01, Meta loss averaged over last 500 steps = 2.5629e-01, PNorm = 175.4704, GNorm = 0.1975
Meta loss on this task batch = 2.3939e-01, Meta loss averaged over last 500 steps = 2.5626e-01, PNorm = 175.4754, GNorm = 0.2112
Meta loss on this task batch = 2.7347e-01, Meta loss averaged over last 500 steps = 2.5630e-01, PNorm = 175.4796, GNorm = 0.2681
Meta loss on this task batch = 2.9055e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 175.4838, GNorm = 0.2407
Meta loss on this task batch = 2.4270e-01, Meta loss averaged over last 500 steps = 2.5625e-01, PNorm = 175.4885, GNorm = 0.1914
Meta loss on this task batch = 2.4363e-01, Meta loss averaged over last 500 steps = 2.5625e-01, PNorm = 175.4934, GNorm = 0.2688
Meta loss on this task batch = 2.5141e-01, Meta loss averaged over last 500 steps = 2.5612e-01, PNorm = 175.4987, GNorm = 0.2215
Meta loss on this task batch = 2.5593e-01, Meta loss averaged over last 500 steps = 2.5612e-01, PNorm = 175.5042, GNorm = 0.1973
Meta loss on this task batch = 2.9919e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 175.5091, GNorm = 0.2837
Took 111.20843315124512 seconds to complete one epoch of meta training
Took 118.90889596939087 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499179
Epoch 895
Meta loss on this task batch = 2.7228e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 175.5132, GNorm = 0.2541
Meta loss on this task batch = 2.4030e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 175.5181, GNorm = 0.2110
Meta loss on this task batch = 2.1751e-01, Meta loss averaged over last 500 steps = 2.5599e-01, PNorm = 175.5227, GNorm = 0.1926
Meta loss on this task batch = 2.5993e-01, Meta loss averaged over last 500 steps = 2.5610e-01, PNorm = 175.5269, GNorm = 0.2120
Meta loss on this task batch = 3.3334e-01, Meta loss averaged over last 500 steps = 2.5624e-01, PNorm = 175.5307, GNorm = 0.2858
Meta loss on this task batch = 2.9556e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 175.5343, GNorm = 0.2790
Meta loss on this task batch = 2.8736e-01, Meta loss averaged over last 500 steps = 2.5645e-01, PNorm = 175.5383, GNorm = 0.2150
Meta loss on this task batch = 2.6568e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 175.5426, GNorm = 0.2144
Meta loss on this task batch = 2.0886e-01, Meta loss averaged over last 500 steps = 2.5642e-01, PNorm = 175.5468, GNorm = 0.2159
Meta loss on this task batch = 2.6406e-01, Meta loss averaged over last 500 steps = 2.5651e-01, PNorm = 175.5518, GNorm = 0.3098
Meta loss on this task batch = 2.7218e-01, Meta loss averaged over last 500 steps = 2.5657e-01, PNorm = 175.5573, GNorm = 0.2146
Meta loss on this task batch = 2.1282e-01, Meta loss averaged over last 500 steps = 2.5651e-01, PNorm = 175.5626, GNorm = 0.1900
Meta loss on this task batch = 2.3755e-01, Meta loss averaged over last 500 steps = 2.5655e-01, PNorm = 175.5677, GNorm = 0.2260
Meta loss on this task batch = 2.1387e-01, Meta loss averaged over last 500 steps = 2.5648e-01, PNorm = 175.5737, GNorm = 0.2416
Meta loss on this task batch = 2.8893e-01, Meta loss averaged over last 500 steps = 2.5659e-01, PNorm = 175.5792, GNorm = 0.1977
Meta loss on this task batch = 2.2404e-01, Meta loss averaged over last 500 steps = 2.5648e-01, PNorm = 175.5844, GNorm = 0.2271
Meta loss on this task batch = 2.8183e-01, Meta loss averaged over last 500 steps = 2.5656e-01, PNorm = 175.5892, GNorm = 0.2625
Meta loss on this task batch = 3.1245e-01, Meta loss averaged over last 500 steps = 2.5660e-01, PNorm = 175.5931, GNorm = 0.3033
Meta loss on this task batch = 2.7520e-01, Meta loss averaged over last 500 steps = 2.5656e-01, PNorm = 175.5974, GNorm = 0.2600
Took 106.70480990409851 seconds to complete one epoch of meta training
Took 114.27783966064453 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482188
Epoch 896
Meta loss on this task batch = 2.5673e-01, Meta loss averaged over last 500 steps = 2.5652e-01, PNorm = 175.6021, GNorm = 0.2093
Meta loss on this task batch = 2.3418e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 175.6072, GNorm = 0.2127
Meta loss on this task batch = 2.3498e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 175.6132, GNorm = 0.2326
Meta loss on this task batch = 2.2156e-01, Meta loss averaged over last 500 steps = 2.5628e-01, PNorm = 175.6188, GNorm = 0.2596
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 175.6243, GNorm = 0.2404
Meta loss on this task batch = 2.6387e-01, Meta loss averaged over last 500 steps = 2.5630e-01, PNorm = 175.6301, GNorm = 0.3762
Meta loss on this task batch = 2.2530e-01, Meta loss averaged over last 500 steps = 2.5628e-01, PNorm = 175.6357, GNorm = 0.2141
Meta loss on this task batch = 2.5264e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 175.6413, GNorm = 0.2471
Meta loss on this task batch = 3.0768e-01, Meta loss averaged over last 500 steps = 2.5638e-01, PNorm = 175.6465, GNorm = 0.2791
Meta loss on this task batch = 2.5096e-01, Meta loss averaged over last 500 steps = 2.5643e-01, PNorm = 175.6513, GNorm = 0.2132
Meta loss on this task batch = 2.6580e-01, Meta loss averaged over last 500 steps = 2.5650e-01, PNorm = 175.6558, GNorm = 0.2125
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.5643e-01, PNorm = 175.6609, GNorm = 0.2224
Meta loss on this task batch = 2.4468e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 175.6661, GNorm = 0.2266
Meta loss on this task batch = 2.2218e-01, Meta loss averaged over last 500 steps = 2.5646e-01, PNorm = 175.6709, GNorm = 0.2478
Meta loss on this task batch = 2.6555e-01, Meta loss averaged over last 500 steps = 2.5638e-01, PNorm = 175.6746, GNorm = 0.2609
Meta loss on this task batch = 2.5607e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 175.6785, GNorm = 0.2213
Meta loss on this task batch = 2.1328e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 175.6827, GNorm = 0.2209
Meta loss on this task batch = 2.7737e-01, Meta loss averaged over last 500 steps = 2.5638e-01, PNorm = 175.6861, GNorm = 0.2392
Meta loss on this task batch = 2.8261e-01, Meta loss averaged over last 500 steps = 2.5641e-01, PNorm = 175.6894, GNorm = 0.3135
Took 105.71033954620361 seconds to complete one epoch of meta training
Took 113.01776242256165 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.485646
Epoch 897
Meta loss on this task batch = 2.3624e-01, Meta loss averaged over last 500 steps = 2.5639e-01, PNorm = 175.6929, GNorm = 0.2047
Meta loss on this task batch = 2.7727e-01, Meta loss averaged over last 500 steps = 2.5657e-01, PNorm = 175.6962, GNorm = 0.2724
Meta loss on this task batch = 2.5560e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 175.6995, GNorm = 0.2129
Meta loss on this task batch = 2.8671e-01, Meta loss averaged over last 500 steps = 2.5652e-01, PNorm = 175.7020, GNorm = 0.2267
Meta loss on this task batch = 2.9072e-01, Meta loss averaged over last 500 steps = 2.5652e-01, PNorm = 175.7039, GNorm = 0.2270
Meta loss on this task batch = 2.4973e-01, Meta loss averaged over last 500 steps = 2.5642e-01, PNorm = 175.7062, GNorm = 0.2072
Meta loss on this task batch = 3.0428e-01, Meta loss averaged over last 500 steps = 2.5654e-01, PNorm = 175.7089, GNorm = 0.2302
Meta loss on this task batch = 2.5086e-01, Meta loss averaged over last 500 steps = 2.5656e-01, PNorm = 175.7123, GNorm = 0.2140
Meta loss on this task batch = 2.8958e-01, Meta loss averaged over last 500 steps = 2.5659e-01, PNorm = 175.7173, GNorm = 0.2567
Meta loss on this task batch = 3.0365e-01, Meta loss averaged over last 500 steps = 2.5667e-01, PNorm = 175.7221, GNorm = 0.2450
Meta loss on this task batch = 2.8578e-01, Meta loss averaged over last 500 steps = 2.5679e-01, PNorm = 175.7269, GNorm = 0.2165
Meta loss on this task batch = 2.4738e-01, Meta loss averaged over last 500 steps = 2.5681e-01, PNorm = 175.7324, GNorm = 0.2212
Meta loss on this task batch = 2.2520e-01, Meta loss averaged over last 500 steps = 2.5686e-01, PNorm = 175.7379, GNorm = 0.1965
Meta loss on this task batch = 2.1223e-01, Meta loss averaged over last 500 steps = 2.5674e-01, PNorm = 175.7436, GNorm = 0.2278
Meta loss on this task batch = 2.1368e-01, Meta loss averaged over last 500 steps = 2.5677e-01, PNorm = 175.7495, GNorm = 0.2185
Meta loss on this task batch = 2.1955e-01, Meta loss averaged over last 500 steps = 2.5658e-01, PNorm = 175.7554, GNorm = 0.2270
Meta loss on this task batch = 2.6353e-01, Meta loss averaged over last 500 steps = 2.5654e-01, PNorm = 175.7612, GNorm = 0.2543
Meta loss on this task batch = 2.2735e-01, Meta loss averaged over last 500 steps = 2.5648e-01, PNorm = 175.7659, GNorm = 0.2245
Meta loss on this task batch = 2.6751e-01, Meta loss averaged over last 500 steps = 2.5644e-01, PNorm = 175.7696, GNorm = 0.3511
Took 109.78236031532288 seconds to complete one epoch of meta training
Took 117.17565178871155 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.487835
Epoch 898
Meta loss on this task batch = 2.7828e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 175.7730, GNorm = 0.2277
Meta loss on this task batch = 2.2872e-01, Meta loss averaged over last 500 steps = 2.5640e-01, PNorm = 175.7768, GNorm = 0.1885
Meta loss on this task batch = 2.2750e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 175.7809, GNorm = 0.2089
Meta loss on this task batch = 2.5085e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 175.7845, GNorm = 0.2180
Meta loss on this task batch = 2.5608e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 175.7879, GNorm = 0.2264
Meta loss on this task batch = 2.9597e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 175.7924, GNorm = 0.2966
Meta loss on this task batch = 2.3113e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 175.7965, GNorm = 0.2410
Meta loss on this task batch = 2.2686e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 175.8010, GNorm = 0.2059
Meta loss on this task batch = 2.9355e-01, Meta loss averaged over last 500 steps = 2.5621e-01, PNorm = 175.8054, GNorm = 0.2296
Meta loss on this task batch = 2.4412e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 175.8098, GNorm = 0.2063
Meta loss on this task batch = 2.4677e-01, Meta loss averaged over last 500 steps = 2.5627e-01, PNorm = 175.8142, GNorm = 0.2470
Meta loss on this task batch = 2.1954e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 175.8187, GNorm = 0.1876
Meta loss on this task batch = 2.3603e-01, Meta loss averaged over last 500 steps = 2.5601e-01, PNorm = 175.8229, GNorm = 0.2397
Meta loss on this task batch = 2.3920e-01, Meta loss averaged over last 500 steps = 2.5594e-01, PNorm = 175.8274, GNorm = 0.2457
Meta loss on this task batch = 2.8172e-01, Meta loss averaged over last 500 steps = 2.5607e-01, PNorm = 175.8305, GNorm = 0.2649
Meta loss on this task batch = 2.2940e-01, Meta loss averaged over last 500 steps = 2.5602e-01, PNorm = 175.8343, GNorm = 0.1959
Meta loss on this task batch = 2.6583e-01, Meta loss averaged over last 500 steps = 2.5598e-01, PNorm = 175.8378, GNorm = 0.2366
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.5595e-01, PNorm = 175.8413, GNorm = 0.2538
Meta loss on this task batch = 2.5519e-01, Meta loss averaged over last 500 steps = 2.5600e-01, PNorm = 175.8452, GNorm = 0.3027
Took 107.47806310653687 seconds to complete one epoch of meta training
Took 113.7181146144867 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.469175
Epoch 899
Meta loss on this task batch = 2.6330e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 175.8494, GNorm = 0.2450
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 175.8529, GNorm = 0.2560
Meta loss on this task batch = 3.0192e-01, Meta loss averaged over last 500 steps = 2.5607e-01, PNorm = 175.8554, GNorm = 0.2763
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.5599e-01, PNorm = 175.8582, GNorm = 0.2056
Meta loss on this task batch = 2.3144e-01, Meta loss averaged over last 500 steps = 2.5601e-01, PNorm = 175.8611, GNorm = 0.2221
Meta loss on this task batch = 2.6732e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 175.8646, GNorm = 0.2405
Meta loss on this task batch = 2.5503e-01, Meta loss averaged over last 500 steps = 2.5615e-01, PNorm = 175.8678, GNorm = 0.2440
Meta loss on this task batch = 2.6588e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 175.8705, GNorm = 0.2446
Meta loss on this task batch = 2.6651e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 175.8741, GNorm = 0.2856
Meta loss on this task batch = 2.5842e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 175.8774, GNorm = 0.2624
Meta loss on this task batch = 2.6660e-01, Meta loss averaged over last 500 steps = 2.5602e-01, PNorm = 175.8805, GNorm = 0.2945
Meta loss on this task batch = 2.1667e-01, Meta loss averaged over last 500 steps = 2.5593e-01, PNorm = 175.8840, GNorm = 0.2022
Meta loss on this task batch = 2.6345e-01, Meta loss averaged over last 500 steps = 2.5589e-01, PNorm = 175.8868, GNorm = 0.2433
Meta loss on this task batch = 2.4364e-01, Meta loss averaged over last 500 steps = 2.5587e-01, PNorm = 175.8898, GNorm = 0.2307
Meta loss on this task batch = 1.9665e-01, Meta loss averaged over last 500 steps = 2.5578e-01, PNorm = 175.8931, GNorm = 0.2179
Meta loss on this task batch = 2.8308e-01, Meta loss averaged over last 500 steps = 2.5584e-01, PNorm = 175.8953, GNorm = 0.2568
Meta loss on this task batch = 2.6676e-01, Meta loss averaged over last 500 steps = 2.5583e-01, PNorm = 175.8972, GNorm = 0.2466
Meta loss on this task batch = 2.8862e-01, Meta loss averaged over last 500 steps = 2.5591e-01, PNorm = 175.8993, GNorm = 0.2364
Meta loss on this task batch = 2.6323e-01, Meta loss averaged over last 500 steps = 2.5599e-01, PNorm = 175.9015, GNorm = 0.2649
Took 109.22709131240845 seconds to complete one epoch of meta training
Took 115.44010138511658 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501909
Epoch 900
Meta loss on this task batch = 2.3143e-01, Meta loss averaged over last 500 steps = 2.5599e-01, PNorm = 175.9043, GNorm = 0.2312
Meta loss on this task batch = 2.6209e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 175.9069, GNorm = 0.2450
Meta loss on this task batch = 2.2171e-01, Meta loss averaged over last 500 steps = 2.5595e-01, PNorm = 175.9099, GNorm = 0.2198
Meta loss on this task batch = 2.9172e-01, Meta loss averaged over last 500 steps = 2.5605e-01, PNorm = 175.9131, GNorm = 0.2486
Meta loss on this task batch = 1.7716e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 175.9167, GNorm = 0.1845
Meta loss on this task batch = 2.6059e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 175.9201, GNorm = 0.2543
Meta loss on this task batch = 2.7000e-01, Meta loss averaged over last 500 steps = 2.5585e-01, PNorm = 175.9232, GNorm = 0.2577
Meta loss on this task batch = 2.6017e-01, Meta loss averaged over last 500 steps = 2.5578e-01, PNorm = 175.9265, GNorm = 0.2059
Meta loss on this task batch = 2.0855e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 175.9303, GNorm = 0.2093
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.5571e-01, PNorm = 175.9333, GNorm = 0.2659
Meta loss on this task batch = 2.3148e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 175.9362, GNorm = 0.2072
Meta loss on this task batch = 2.3773e-01, Meta loss averaged over last 500 steps = 2.5559e-01, PNorm = 175.9396, GNorm = 0.2130
Meta loss on this task batch = 2.4665e-01, Meta loss averaged over last 500 steps = 2.5563e-01, PNorm = 175.9433, GNorm = 0.2132
Meta loss on this task batch = 3.2809e-01, Meta loss averaged over last 500 steps = 2.5581e-01, PNorm = 175.9467, GNorm = 0.3074
Meta loss on this task batch = 3.0620e-01, Meta loss averaged over last 500 steps = 2.5589e-01, PNorm = 175.9501, GNorm = 0.3352
Meta loss on this task batch = 2.4915e-01, Meta loss averaged over last 500 steps = 2.5594e-01, PNorm = 175.9535, GNorm = 0.2564
Meta loss on this task batch = 2.8854e-01, Meta loss averaged over last 500 steps = 2.5603e-01, PNorm = 175.9569, GNorm = 0.2533
Meta loss on this task batch = 2.9037e-01, Meta loss averaged over last 500 steps = 2.5604e-01, PNorm = 175.9604, GNorm = 0.2082
Meta loss on this task batch = 2.3861e-01, Meta loss averaged over last 500 steps = 2.5605e-01, PNorm = 175.9643, GNorm = 0.2784
Took 108.16227459907532 seconds to complete one epoch of meta training
Took 115.554190158844 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490065
Epoch 901
Meta loss on this task batch = 3.1285e-01, Meta loss averaged over last 500 steps = 2.5612e-01, PNorm = 175.9679, GNorm = 0.2670
Meta loss on this task batch = 2.1427e-01, Meta loss averaged over last 500 steps = 2.5601e-01, PNorm = 175.9716, GNorm = 0.2217
Meta loss on this task batch = 2.5102e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 175.9751, GNorm = 0.2487
Meta loss on this task batch = 2.2701e-01, Meta loss averaged over last 500 steps = 2.5613e-01, PNorm = 175.9791, GNorm = 0.2538
Meta loss on this task batch = 2.8174e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 175.9836, GNorm = 0.1988
Meta loss on this task batch = 2.2407e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 175.9884, GNorm = 0.2417
Meta loss on this task batch = 2.3191e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 175.9927, GNorm = 0.2363
Meta loss on this task batch = 2.6598e-01, Meta loss averaged over last 500 steps = 2.5615e-01, PNorm = 175.9969, GNorm = 0.2278
Meta loss on this task batch = 2.6673e-01, Meta loss averaged over last 500 steps = 2.5619e-01, PNorm = 176.0013, GNorm = 0.2357
Meta loss on this task batch = 2.7001e-01, Meta loss averaged over last 500 steps = 2.5627e-01, PNorm = 176.0060, GNorm = 0.2274
Meta loss on this task batch = 2.3107e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 176.0104, GNorm = 0.2205
Meta loss on this task batch = 2.7318e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 176.0152, GNorm = 0.2120
Meta loss on this task batch = 3.1249e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 176.0189, GNorm = 0.2667
Meta loss on this task batch = 3.0101e-01, Meta loss averaged over last 500 steps = 2.5634e-01, PNorm = 176.0228, GNorm = 0.2937
Meta loss on this task batch = 2.5776e-01, Meta loss averaged over last 500 steps = 2.5627e-01, PNorm = 176.0260, GNorm = 0.2372
Meta loss on this task batch = 2.3939e-01, Meta loss averaged over last 500 steps = 2.5629e-01, PNorm = 176.0290, GNorm = 0.2379
Meta loss on this task batch = 2.1739e-01, Meta loss averaged over last 500 steps = 2.5625e-01, PNorm = 176.0320, GNorm = 0.1950
Meta loss on this task batch = 2.3997e-01, Meta loss averaged over last 500 steps = 2.5618e-01, PNorm = 176.0342, GNorm = 0.2543
Meta loss on this task batch = 2.7330e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 176.0367, GNorm = 0.2547
Took 109.68591928482056 seconds to complete one epoch of meta training
Took 115.93400621414185 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486130
Epoch 902
Meta loss on this task batch = 2.5969e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 176.0401, GNorm = 0.2625
Meta loss on this task batch = 2.6811e-01, Meta loss averaged over last 500 steps = 2.5618e-01, PNorm = 176.0436, GNorm = 0.2163
Meta loss on this task batch = 2.1526e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 176.0477, GNorm = 0.2136
Meta loss on this task batch = 2.9496e-01, Meta loss averaged over last 500 steps = 2.5612e-01, PNorm = 176.0519, GNorm = 0.2198
Meta loss on this task batch = 2.3404e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 176.0562, GNorm = 0.1920
Meta loss on this task batch = 2.2491e-01, Meta loss averaged over last 500 steps = 2.5610e-01, PNorm = 176.0606, GNorm = 0.2878
Meta loss on this task batch = 2.3019e-01, Meta loss averaged over last 500 steps = 2.5612e-01, PNorm = 176.0654, GNorm = 0.2492
Meta loss on this task batch = 2.9270e-01, Meta loss averaged over last 500 steps = 2.5620e-01, PNorm = 176.0697, GNorm = 0.2699
Meta loss on this task batch = 2.5945e-01, Meta loss averaged over last 500 steps = 2.5628e-01, PNorm = 176.0747, GNorm = 0.2370
Meta loss on this task batch = 2.5991e-01, Meta loss averaged over last 500 steps = 2.5624e-01, PNorm = 176.0797, GNorm = 0.2129
Meta loss on this task batch = 2.5644e-01, Meta loss averaged over last 500 steps = 2.5624e-01, PNorm = 176.0841, GNorm = 0.2514
Meta loss on this task batch = 2.1638e-01, Meta loss averaged over last 500 steps = 2.5615e-01, PNorm = 176.0889, GNorm = 0.1988
Meta loss on this task batch = 2.1748e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 176.0941, GNorm = 0.2195
Meta loss on this task batch = 2.6507e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 176.0993, GNorm = 0.1989
Meta loss on this task batch = 2.9028e-01, Meta loss averaged over last 500 steps = 2.5618e-01, PNorm = 176.1043, GNorm = 0.2597
Meta loss on this task batch = 2.4891e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 176.1090, GNorm = 0.2047
Meta loss on this task batch = 2.7049e-01, Meta loss averaged over last 500 steps = 2.5615e-01, PNorm = 176.1131, GNorm = 0.2517
Meta loss on this task batch = 2.6564e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 176.1172, GNorm = 0.2443
Meta loss on this task batch = 3.1031e-01, Meta loss averaged over last 500 steps = 2.5630e-01, PNorm = 176.1212, GNorm = 0.2974
Took 108.92227363586426 seconds to complete one epoch of meta training
Took 115.94814014434814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479194
Epoch 903
Meta loss on this task batch = 2.3182e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 176.1250, GNorm = 0.2419
Meta loss on this task batch = 2.7858e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 176.1281, GNorm = 0.2574
Meta loss on this task batch = 3.2705e-01, Meta loss averaged over last 500 steps = 2.5623e-01, PNorm = 176.1309, GNorm = 0.2201
Meta loss on this task batch = 2.6977e-01, Meta loss averaged over last 500 steps = 2.5625e-01, PNorm = 176.1341, GNorm = 0.2132
Meta loss on this task batch = 2.4922e-01, Meta loss averaged over last 500 steps = 2.5631e-01, PNorm = 176.1373, GNorm = 0.2278
Meta loss on this task batch = 2.7391e-01, Meta loss averaged over last 500 steps = 2.5641e-01, PNorm = 176.1405, GNorm = 0.2309
Meta loss on this task batch = 2.7257e-01, Meta loss averaged over last 500 steps = 2.5636e-01, PNorm = 176.1441, GNorm = 0.2273
Meta loss on this task batch = 2.6870e-01, Meta loss averaged over last 500 steps = 2.5642e-01, PNorm = 176.1480, GNorm = 0.2282
Meta loss on this task batch = 2.5251e-01, Meta loss averaged over last 500 steps = 2.5637e-01, PNorm = 176.1526, GNorm = 0.2116
Meta loss on this task batch = 2.4791e-01, Meta loss averaged over last 500 steps = 2.5640e-01, PNorm = 176.1567, GNorm = 0.2105
Meta loss on this task batch = 2.7182e-01, Meta loss averaged over last 500 steps = 2.5640e-01, PNorm = 176.1612, GNorm = 0.2192
Meta loss on this task batch = 2.9188e-01, Meta loss averaged over last 500 steps = 2.5645e-01, PNorm = 176.1654, GNorm = 0.2209
Meta loss on this task batch = 2.4626e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 176.1700, GNorm = 0.2132
Meta loss on this task batch = 2.4744e-01, Meta loss averaged over last 500 steps = 2.5653e-01, PNorm = 176.1748, GNorm = 0.2197
Meta loss on this task batch = 2.2000e-01, Meta loss averaged over last 500 steps = 2.5661e-01, PNorm = 176.1797, GNorm = 0.1997
Meta loss on this task batch = 2.6459e-01, Meta loss averaged over last 500 steps = 2.5662e-01, PNorm = 176.1848, GNorm = 0.2414
Meta loss on this task batch = 2.4375e-01, Meta loss averaged over last 500 steps = 2.5661e-01, PNorm = 176.1896, GNorm = 0.2004
Meta loss on this task batch = 2.4473e-01, Meta loss averaged over last 500 steps = 2.5647e-01, PNorm = 176.1941, GNorm = 0.2258
Meta loss on this task batch = 2.0784e-01, Meta loss averaged over last 500 steps = 2.5632e-01, PNorm = 176.1991, GNorm = 0.2560
Took 107.5487585067749 seconds to complete one epoch of meta training
Took 114.93416118621826 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483232
Epoch 904
Meta loss on this task batch = 2.6861e-01, Meta loss averaged over last 500 steps = 2.5627e-01, PNorm = 176.2045, GNorm = 0.2621
Meta loss on this task batch = 2.8608e-01, Meta loss averaged over last 500 steps = 2.5627e-01, PNorm = 176.2097, GNorm = 0.2404
Meta loss on this task batch = 2.3284e-01, Meta loss averaged over last 500 steps = 2.5621e-01, PNorm = 176.2150, GNorm = 0.1992
Meta loss on this task batch = 2.5706e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 176.2211, GNorm = 0.2304
Meta loss on this task batch = 2.9166e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 176.2256, GNorm = 0.3119
Meta loss on this task batch = 2.3501e-01, Meta loss averaged over last 500 steps = 2.5620e-01, PNorm = 176.2297, GNorm = 0.2280
Meta loss on this task batch = 3.0228e-01, Meta loss averaged over last 500 steps = 2.5633e-01, PNorm = 176.2343, GNorm = 0.2341
Meta loss on this task batch = 1.9259e-01, Meta loss averaged over last 500 steps = 2.5627e-01, PNorm = 176.2390, GNorm = 0.1982
Meta loss on this task batch = 2.2482e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 176.2433, GNorm = 0.2067
Meta loss on this task batch = 2.7517e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 176.2467, GNorm = 0.2805
Meta loss on this task batch = 2.8113e-01, Meta loss averaged over last 500 steps = 2.5611e-01, PNorm = 176.2505, GNorm = 0.2173
Meta loss on this task batch = 2.9578e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 176.2538, GNorm = 0.2502
Meta loss on this task batch = 2.3884e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 176.2577, GNorm = 0.2353
Meta loss on this task batch = 2.6796e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 176.2610, GNorm = 0.2664
Meta loss on this task batch = 2.8686e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 176.2651, GNorm = 0.2270
Meta loss on this task batch = 2.6929e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 176.2688, GNorm = 0.2818
Meta loss on this task batch = 2.1117e-01, Meta loss averaged over last 500 steps = 2.5603e-01, PNorm = 176.2730, GNorm = 0.2616
Meta loss on this task batch = 2.4513e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 176.2774, GNorm = 0.1930
Meta loss on this task batch = 2.1127e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 176.2818, GNorm = 0.2477
Took 103.94540548324585 seconds to complete one epoch of meta training
Took 111.83660292625427 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501686
Epoch 905
Meta loss on this task batch = 2.9402e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 176.2859, GNorm = 0.2137
Meta loss on this task batch = 2.5172e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 176.2901, GNorm = 0.2452
Meta loss on this task batch = 2.6497e-01, Meta loss averaged over last 500 steps = 2.5613e-01, PNorm = 176.2941, GNorm = 0.2238
Meta loss on this task batch = 2.2904e-01, Meta loss averaged over last 500 steps = 2.5605e-01, PNorm = 176.2989, GNorm = 0.2070
Meta loss on this task batch = 2.4709e-01, Meta loss averaged over last 500 steps = 2.5603e-01, PNorm = 176.3044, GNorm = 0.2281
Meta loss on this task batch = 2.1306e-01, Meta loss averaged over last 500 steps = 2.5590e-01, PNorm = 176.3103, GNorm = 0.1935
Meta loss on this task batch = 2.8898e-01, Meta loss averaged over last 500 steps = 2.5599e-01, PNorm = 176.3156, GNorm = 0.2368
Meta loss on this task batch = 2.1359e-01, Meta loss averaged over last 500 steps = 2.5590e-01, PNorm = 176.3212, GNorm = 0.2206
Meta loss on this task batch = 2.9174e-01, Meta loss averaged over last 500 steps = 2.5595e-01, PNorm = 176.3255, GNorm = 0.2425
Meta loss on this task batch = 3.0914e-01, Meta loss averaged over last 500 steps = 2.5601e-01, PNorm = 176.3299, GNorm = 0.2636
Meta loss on this task batch = 2.0427e-01, Meta loss averaged over last 500 steps = 2.5594e-01, PNorm = 176.3348, GNorm = 0.2243
Meta loss on this task batch = 2.6147e-01, Meta loss averaged over last 500 steps = 2.5592e-01, PNorm = 176.3396, GNorm = 0.2179
Meta loss on this task batch = 2.2211e-01, Meta loss averaged over last 500 steps = 2.5586e-01, PNorm = 176.3438, GNorm = 0.1919
Meta loss on this task batch = 2.4918e-01, Meta loss averaged over last 500 steps = 2.5590e-01, PNorm = 176.3478, GNorm = 0.1931
Meta loss on this task batch = 2.3578e-01, Meta loss averaged over last 500 steps = 2.5580e-01, PNorm = 176.3514, GNorm = 0.2037
Meta loss on this task batch = 2.3276e-01, Meta loss averaged over last 500 steps = 2.5571e-01, PNorm = 176.3550, GNorm = 0.2333
Meta loss on this task batch = 2.8676e-01, Meta loss averaged over last 500 steps = 2.5584e-01, PNorm = 176.3595, GNorm = 0.2696
Meta loss on this task batch = 2.1176e-01, Meta loss averaged over last 500 steps = 2.5583e-01, PNorm = 176.3641, GNorm = 0.2286
Meta loss on this task batch = 2.1186e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 176.3693, GNorm = 0.2455
Took 110.01967692375183 seconds to complete one epoch of meta training
Took 117.39054465293884 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484234
Epoch 906
Meta loss on this task batch = 2.4955e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 176.3744, GNorm = 0.2389
Meta loss on this task batch = 2.8333e-01, Meta loss averaged over last 500 steps = 2.5580e-01, PNorm = 176.3804, GNorm = 0.3070
Meta loss on this task batch = 2.4749e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 176.3859, GNorm = 0.2707
Meta loss on this task batch = 2.6090e-01, Meta loss averaged over last 500 steps = 2.5565e-01, PNorm = 176.3917, GNorm = 0.2408
Meta loss on this task batch = 2.9045e-01, Meta loss averaged over last 500 steps = 2.5573e-01, PNorm = 176.3978, GNorm = 0.2513
Meta loss on this task batch = 2.5652e-01, Meta loss averaged over last 500 steps = 2.5581e-01, PNorm = 176.4032, GNorm = 0.2609
Meta loss on this task batch = 2.5891e-01, Meta loss averaged over last 500 steps = 2.5575e-01, PNorm = 176.4088, GNorm = 0.2282
Meta loss on this task batch = 2.5946e-01, Meta loss averaged over last 500 steps = 2.5577e-01, PNorm = 176.4132, GNorm = 0.3102
Meta loss on this task batch = 2.7771e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 176.4169, GNorm = 0.2340
Meta loss on this task batch = 2.5149e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 176.4205, GNorm = 0.2167
Meta loss on this task batch = 2.5031e-01, Meta loss averaged over last 500 steps = 2.5568e-01, PNorm = 176.4242, GNorm = 0.2499
Meta loss on this task batch = 2.5125e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 176.4279, GNorm = 0.2279
Meta loss on this task batch = 2.0968e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 176.4312, GNorm = 0.2424
Meta loss on this task batch = 2.9174e-01, Meta loss averaged over last 500 steps = 2.5552e-01, PNorm = 176.4342, GNorm = 0.2252
Meta loss on this task batch = 2.5217e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 176.4372, GNorm = 0.2396
Meta loss on this task batch = 2.4636e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 176.4410, GNorm = 0.2424
Meta loss on this task batch = 2.7836e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 176.4449, GNorm = 0.2715
Meta loss on this task batch = 2.7453e-01, Meta loss averaged over last 500 steps = 2.5560e-01, PNorm = 176.4483, GNorm = 0.2458
Meta loss on this task batch = 2.6316e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 176.4515, GNorm = 0.2546
Took 108.77292728424072 seconds to complete one epoch of meta training
Took 116.81666350364685 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503204
Epoch 907
Meta loss on this task batch = 2.6414e-01, Meta loss averaged over last 500 steps = 2.5563e-01, PNorm = 176.4549, GNorm = 0.2269
Meta loss on this task batch = 2.6914e-01, Meta loss averaged over last 500 steps = 2.5567e-01, PNorm = 176.4582, GNorm = 0.2488
Meta loss on this task batch = 2.7788e-01, Meta loss averaged over last 500 steps = 2.5573e-01, PNorm = 176.4620, GNorm = 0.2297
Meta loss on this task batch = 2.9969e-01, Meta loss averaged over last 500 steps = 2.5590e-01, PNorm = 176.4654, GNorm = 0.3588
Meta loss on this task batch = 2.0252e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 176.4698, GNorm = 0.2058
Meta loss on this task batch = 2.2712e-01, Meta loss averaged over last 500 steps = 2.5563e-01, PNorm = 176.4748, GNorm = 0.2216
Meta loss on this task batch = 2.4671e-01, Meta loss averaged over last 500 steps = 2.5555e-01, PNorm = 176.4794, GNorm = 0.2213
Meta loss on this task batch = 3.0664e-01, Meta loss averaged over last 500 steps = 2.5567e-01, PNorm = 176.4839, GNorm = 0.2629
Meta loss on this task batch = 2.6546e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 176.4884, GNorm = 0.2282
Meta loss on this task batch = 2.2616e-01, Meta loss averaged over last 500 steps = 2.5562e-01, PNorm = 176.4933, GNorm = 0.1924
Meta loss on this task batch = 2.2082e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 176.4984, GNorm = 0.2117
Meta loss on this task batch = 2.2764e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 176.5042, GNorm = 0.2180
Meta loss on this task batch = 2.2079e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 176.5099, GNorm = 0.2214
Meta loss on this task batch = 2.7529e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 176.5151, GNorm = 0.2299
Meta loss on this task batch = 3.0307e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 176.5202, GNorm = 0.2353
Meta loss on this task batch = 2.7430e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 176.5250, GNorm = 0.2086
Meta loss on this task batch = 2.8324e-01, Meta loss averaged over last 500 steps = 2.5563e-01, PNorm = 176.5295, GNorm = 0.2293
Meta loss on this task batch = 2.2565e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 176.5349, GNorm = 0.2057
Meta loss on this task batch = 2.4922e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 176.5403, GNorm = 0.2519
Took 108.74517869949341 seconds to complete one epoch of meta training
Took 116.64954876899719 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.470742
Epoch 908
Meta loss on this task batch = 2.5144e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 176.5458, GNorm = 0.2224
Meta loss on this task batch = 2.2391e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 176.5506, GNorm = 0.2537
Meta loss on this task batch = 2.5166e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 176.5554, GNorm = 0.2127
Meta loss on this task batch = 2.0977e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 176.5598, GNorm = 0.2104
Meta loss on this task batch = 2.5370e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 176.5639, GNorm = 0.2233
Meta loss on this task batch = 2.1518e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 176.5684, GNorm = 0.2174
Meta loss on this task batch = 2.4224e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 176.5733, GNorm = 0.2142
Meta loss on this task batch = 2.5484e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 176.5783, GNorm = 0.2070
Meta loss on this task batch = 2.5587e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 176.5835, GNorm = 0.2030
Meta loss on this task batch = 2.8337e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 176.5881, GNorm = 0.2528
Meta loss on this task batch = 2.3913e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 176.5933, GNorm = 0.2388
Meta loss on this task batch = 2.5629e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 176.5987, GNorm = 0.2489
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 176.6035, GNorm = 0.2584
Meta loss on this task batch = 2.4062e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 176.6085, GNorm = 0.2395
Meta loss on this task batch = 2.4561e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 176.6140, GNorm = 0.2256
Meta loss on this task batch = 2.4355e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 176.6192, GNorm = 0.2248
Meta loss on this task batch = 2.9802e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 176.6243, GNorm = 0.2426
Meta loss on this task batch = 3.4467e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 176.6292, GNorm = 0.3211
Meta loss on this task batch = 2.1798e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 176.6341, GNorm = 0.2754
Took 109.40948581695557 seconds to complete one epoch of meta training
Took 116.68892025947571 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494398
Epoch 909
Meta loss on this task batch = 2.4282e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 176.6387, GNorm = 0.2404
Meta loss on this task batch = 2.9698e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 176.6430, GNorm = 0.2568
Meta loss on this task batch = 3.1192e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 176.6473, GNorm = 0.2163
Meta loss on this task batch = 2.7218e-01, Meta loss averaged over last 500 steps = 2.5560e-01, PNorm = 176.6513, GNorm = 0.2399
Meta loss on this task batch = 2.7696e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 176.6556, GNorm = 0.2386
Meta loss on this task batch = 2.1826e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 176.6602, GNorm = 0.2237
Meta loss on this task batch = 2.1753e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 176.6650, GNorm = 0.2079
Meta loss on this task batch = 2.2394e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 176.6702, GNorm = 0.2357
Meta loss on this task batch = 2.7516e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 176.6756, GNorm = 0.2343
Meta loss on this task batch = 2.3845e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 176.6810, GNorm = 0.2271
Meta loss on this task batch = 2.5157e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 176.6859, GNorm = 0.2281
Meta loss on this task batch = 2.9528e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 176.6896, GNorm = 0.2879
Meta loss on this task batch = 1.8684e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 176.6934, GNorm = 0.1863
Meta loss on this task batch = 3.1028e-01, Meta loss averaged over last 500 steps = 2.5555e-01, PNorm = 176.6978, GNorm = 0.2428
Meta loss on this task batch = 2.6420e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 176.7024, GNorm = 0.1903
Meta loss on this task batch = 2.4244e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 176.7074, GNorm = 0.2342
Meta loss on this task batch = 2.2733e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 176.7119, GNorm = 0.2304
Meta loss on this task batch = 1.9425e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 176.7164, GNorm = 0.1851
Meta loss on this task batch = 2.6620e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 176.7218, GNorm = 0.2909
Took 109.35470724105835 seconds to complete one epoch of meta training
Took 116.4835684299469 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486771
Epoch 910
Meta loss on this task batch = 2.5512e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 176.7273, GNorm = 0.2286
Meta loss on this task batch = 2.7444e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 176.7325, GNorm = 0.2870
Meta loss on this task batch = 2.3398e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 176.7382, GNorm = 0.2397
Meta loss on this task batch = 2.4482e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 176.7440, GNorm = 0.2456
Meta loss on this task batch = 2.2274e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 176.7496, GNorm = 0.2165
Meta loss on this task batch = 2.6754e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 176.7552, GNorm = 0.2756
Meta loss on this task batch = 2.6823e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 176.7603, GNorm = 0.2391
Meta loss on this task batch = 2.2929e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 176.7656, GNorm = 0.1947
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 176.7711, GNorm = 0.2527
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 176.7762, GNorm = 0.2387
Meta loss on this task batch = 2.6295e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 176.7806, GNorm = 0.2491
Meta loss on this task batch = 1.9866e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 176.7854, GNorm = 0.1816
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 176.7905, GNorm = 0.2218
Meta loss on this task batch = 2.4021e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 176.7959, GNorm = 0.2341
Meta loss on this task batch = 2.9451e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 176.8000, GNorm = 0.3025
Meta loss on this task batch = 2.4644e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 176.8042, GNorm = 0.1946
Meta loss on this task batch = 2.4094e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 176.8087, GNorm = 0.2458
Meta loss on this task batch = 2.4926e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 176.8130, GNorm = 0.2474
Meta loss on this task batch = 3.2998e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 176.8166, GNorm = 0.3216
Took 106.27050447463989 seconds to complete one epoch of meta training
Took 113.0627555847168 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502111
Epoch 911
Meta loss on this task batch = 2.3385e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 176.8196, GNorm = 0.2207
Meta loss on this task batch = 2.8609e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 176.8226, GNorm = 0.2483
Meta loss on this task batch = 2.9386e-01, Meta loss averaged over last 500 steps = 2.5565e-01, PNorm = 176.8253, GNorm = 0.2443
Meta loss on this task batch = 2.2991e-01, Meta loss averaged over last 500 steps = 2.5567e-01, PNorm = 176.8285, GNorm = 0.2161
Meta loss on this task batch = 2.4638e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 176.8317, GNorm = 0.2249
Meta loss on this task batch = 2.4088e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 176.8351, GNorm = 0.2031
Meta loss on this task batch = 2.7897e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 176.8388, GNorm = 0.2427
Meta loss on this task batch = 2.4314e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 176.8425, GNorm = 0.2211
Meta loss on this task batch = 2.7037e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 176.8469, GNorm = 0.2464
Meta loss on this task batch = 2.7873e-01, Meta loss averaged over last 500 steps = 2.5564e-01, PNorm = 176.8515, GNorm = 0.2161
Meta loss on this task batch = 2.8475e-01, Meta loss averaged over last 500 steps = 2.5572e-01, PNorm = 176.8560, GNorm = 0.2410
Meta loss on this task batch = 2.6991e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 176.8605, GNorm = 0.2060
Meta loss on this task batch = 2.7042e-01, Meta loss averaged over last 500 steps = 2.5582e-01, PNorm = 176.8648, GNorm = 0.2273
Meta loss on this task batch = 2.4197e-01, Meta loss averaged over last 500 steps = 2.5580e-01, PNorm = 176.8691, GNorm = 0.2042
Meta loss on this task batch = 2.3361e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 176.8734, GNorm = 0.2878
Meta loss on this task batch = 3.0895e-01, Meta loss averaged over last 500 steps = 2.5577e-01, PNorm = 176.8777, GNorm = 0.2281
Meta loss on this task batch = 2.2052e-01, Meta loss averaged over last 500 steps = 2.5582e-01, PNorm = 176.8820, GNorm = 0.1953
Meta loss on this task batch = 2.1851e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 176.8867, GNorm = 0.2297
Meta loss on this task batch = 2.3735e-01, Meta loss averaged over last 500 steps = 2.5582e-01, PNorm = 176.8910, GNorm = 0.2844
Took 111.59190821647644 seconds to complete one epoch of meta training
Took 118.74565196037292 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481760
Epoch 912
Meta loss on this task batch = 2.6498e-01, Meta loss averaged over last 500 steps = 2.5574e-01, PNorm = 176.8946, GNorm = 0.2197
Meta loss on this task batch = 2.5879e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 176.8972, GNorm = 0.2665
Meta loss on this task batch = 2.6533e-01, Meta loss averaged over last 500 steps = 2.5568e-01, PNorm = 176.8993, GNorm = 0.2334
Meta loss on this task batch = 2.6655e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 176.9023, GNorm = 0.2253
Meta loss on this task batch = 2.6679e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 176.9055, GNorm = 0.2590
Meta loss on this task batch = 2.7052e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 176.9089, GNorm = 0.2463
Meta loss on this task batch = 2.1058e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 176.9127, GNorm = 0.1854
Meta loss on this task batch = 2.6046e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 176.9167, GNorm = 0.2293
Meta loss on this task batch = 2.5852e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 176.9208, GNorm = 0.2343
Meta loss on this task batch = 2.9130e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 176.9247, GNorm = 0.2367
Meta loss on this task batch = 2.8587e-01, Meta loss averaged over last 500 steps = 2.5582e-01, PNorm = 176.9288, GNorm = 0.2288
Meta loss on this task batch = 2.5438e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 176.9328, GNorm = 0.2519
Meta loss on this task batch = 2.3920e-01, Meta loss averaged over last 500 steps = 2.5573e-01, PNorm = 176.9372, GNorm = 0.1925
Meta loss on this task batch = 2.9852e-01, Meta loss averaged over last 500 steps = 2.5583e-01, PNorm = 176.9411, GNorm = 0.2853
Meta loss on this task batch = 2.5825e-01, Meta loss averaged over last 500 steps = 2.5589e-01, PNorm = 176.9449, GNorm = 0.2211
Meta loss on this task batch = 2.6634e-01, Meta loss averaged over last 500 steps = 2.5599e-01, PNorm = 176.9480, GNorm = 0.2395
Meta loss on this task batch = 2.2568e-01, Meta loss averaged over last 500 steps = 2.5591e-01, PNorm = 176.9513, GNorm = 0.2123
Meta loss on this task batch = 2.2818e-01, Meta loss averaged over last 500 steps = 2.5589e-01, PNorm = 176.9552, GNorm = 0.2547
Meta loss on this task batch = 2.7522e-01, Meta loss averaged over last 500 steps = 2.5588e-01, PNorm = 176.9591, GNorm = 0.3004
Took 109.74281048774719 seconds to complete one epoch of meta training
Took 117.28562331199646 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496132
Epoch 913
Meta loss on this task batch = 2.7550e-01, Meta loss averaged over last 500 steps = 2.5593e-01, PNorm = 176.9630, GNorm = 0.2722
Meta loss on this task batch = 2.1772e-01, Meta loss averaged over last 500 steps = 2.5584e-01, PNorm = 176.9677, GNorm = 0.2020
Meta loss on this task batch = 2.2972e-01, Meta loss averaged over last 500 steps = 2.5564e-01, PNorm = 176.9726, GNorm = 0.2266
Meta loss on this task batch = 2.6907e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 176.9772, GNorm = 0.2674
Meta loss on this task batch = 2.0874e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 176.9814, GNorm = 0.2062
Meta loss on this task batch = 2.1191e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 176.9852, GNorm = 0.2025
Meta loss on this task batch = 2.4394e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 176.9888, GNorm = 0.2395
Meta loss on this task batch = 3.2516e-01, Meta loss averaged over last 500 steps = 2.5567e-01, PNorm = 176.9914, GNorm = 0.2805
Meta loss on this task batch = 2.4289e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 176.9939, GNorm = 0.2192
Meta loss on this task batch = 2.4802e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 176.9966, GNorm = 0.2267
Meta loss on this task batch = 2.6322e-01, Meta loss averaged over last 500 steps = 2.5575e-01, PNorm = 176.9992, GNorm = 0.2561
Meta loss on this task batch = 2.5545e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 177.0023, GNorm = 0.2284
Meta loss on this task batch = 3.0634e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 177.0057, GNorm = 0.2801
Meta loss on this task batch = 2.8653e-01, Meta loss averaged over last 500 steps = 2.5588e-01, PNorm = 177.0097, GNorm = 0.2973
Meta loss on this task batch = 2.1958e-01, Meta loss averaged over last 500 steps = 2.5581e-01, PNorm = 177.0139, GNorm = 0.1830
Meta loss on this task batch = 2.7059e-01, Meta loss averaged over last 500 steps = 2.5580e-01, PNorm = 177.0188, GNorm = 0.2375
Meta loss on this task batch = 2.5823e-01, Meta loss averaged over last 500 steps = 2.5574e-01, PNorm = 177.0231, GNorm = 0.2652
Meta loss on this task batch = 2.6502e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 177.0274, GNorm = 0.2885
Meta loss on this task batch = 2.4857e-01, Meta loss averaged over last 500 steps = 2.5573e-01, PNorm = 177.0316, GNorm = 0.2632
Took 106.84542942047119 seconds to complete one epoch of meta training
Took 114.06437039375305 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.521215
Epoch 914
Meta loss on this task batch = 2.8932e-01, Meta loss averaged over last 500 steps = 2.5573e-01, PNorm = 177.0361, GNorm = 0.2619
Meta loss on this task batch = 2.5787e-01, Meta loss averaged over last 500 steps = 2.5583e-01, PNorm = 177.0412, GNorm = 0.2247
Meta loss on this task batch = 2.9917e-01, Meta loss averaged over last 500 steps = 2.5594e-01, PNorm = 177.0464, GNorm = 0.2410
Meta loss on this task batch = 2.3007e-01, Meta loss averaged over last 500 steps = 2.5587e-01, PNorm = 177.0517, GNorm = 0.2175
Meta loss on this task batch = 2.6405e-01, Meta loss averaged over last 500 steps = 2.5595e-01, PNorm = 177.0571, GNorm = 0.2859
Meta loss on this task batch = 2.1695e-01, Meta loss averaged over last 500 steps = 2.5593e-01, PNorm = 177.0628, GNorm = 0.2038
Meta loss on this task batch = 2.3085e-01, Meta loss averaged over last 500 steps = 2.5600e-01, PNorm = 177.0693, GNorm = 0.2419
Meta loss on this task batch = 2.3199e-01, Meta loss averaged over last 500 steps = 2.5597e-01, PNorm = 177.0750, GNorm = 0.2634
Meta loss on this task batch = 2.5384e-01, Meta loss averaged over last 500 steps = 2.5598e-01, PNorm = 177.0805, GNorm = 0.2134
Meta loss on this task batch = 2.0320e-01, Meta loss averaged over last 500 steps = 2.5596e-01, PNorm = 177.0863, GNorm = 0.2247
Meta loss on this task batch = 2.0273e-01, Meta loss averaged over last 500 steps = 2.5588e-01, PNorm = 177.0923, GNorm = 0.2014
Meta loss on this task batch = 2.9853e-01, Meta loss averaged over last 500 steps = 2.5595e-01, PNorm = 177.0980, GNorm = 0.2456
Meta loss on this task batch = 2.6587e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 177.1024, GNorm = 0.2491
Meta loss on this task batch = 2.5717e-01, Meta loss averaged over last 500 steps = 2.5601e-01, PNorm = 177.1069, GNorm = 0.2084
Meta loss on this task batch = 2.7066e-01, Meta loss averaged over last 500 steps = 2.5613e-01, PNorm = 177.1110, GNorm = 0.2505
Meta loss on this task batch = 2.5978e-01, Meta loss averaged over last 500 steps = 2.5616e-01, PNorm = 177.1155, GNorm = 0.2131
Meta loss on this task batch = 2.6232e-01, Meta loss averaged over last 500 steps = 2.5617e-01, PNorm = 177.1201, GNorm = 0.2473
Meta loss on this task batch = 2.3845e-01, Meta loss averaged over last 500 steps = 2.5610e-01, PNorm = 177.1244, GNorm = 0.2135
Meta loss on this task batch = 2.8835e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 177.1286, GNorm = 0.2908
Took 109.68492412567139 seconds to complete one epoch of meta training
Took 117.31581211090088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.499539
Epoch 915
Meta loss on this task batch = 2.3834e-01, Meta loss averaged over last 500 steps = 2.5604e-01, PNorm = 177.1330, GNorm = 0.2191
Meta loss on this task batch = 2.2279e-01, Meta loss averaged over last 500 steps = 2.5605e-01, PNorm = 177.1380, GNorm = 0.2197
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.5609e-01, PNorm = 177.1435, GNorm = 0.2547
Meta loss on this task batch = 2.5832e-01, Meta loss averaged over last 500 steps = 2.5606e-01, PNorm = 177.1486, GNorm = 0.2513
Meta loss on this task batch = 2.8073e-01, Meta loss averaged over last 500 steps = 2.5603e-01, PNorm = 177.1533, GNorm = 0.2361
Meta loss on this task batch = 2.5086e-01, Meta loss averaged over last 500 steps = 2.5600e-01, PNorm = 177.1570, GNorm = 0.2521
Meta loss on this task batch = 2.8002e-01, Meta loss averaged over last 500 steps = 2.5608e-01, PNorm = 177.1602, GNorm = 0.2531
Meta loss on this task batch = 2.6621e-01, Meta loss averaged over last 500 steps = 2.5622e-01, PNorm = 177.1637, GNorm = 0.2220
Meta loss on this task batch = 2.0883e-01, Meta loss averaged over last 500 steps = 2.5614e-01, PNorm = 177.1675, GNorm = 0.2082
Meta loss on this task batch = 2.2410e-01, Meta loss averaged over last 500 steps = 2.5600e-01, PNorm = 177.1721, GNorm = 0.2138
Meta loss on this task batch = 2.4595e-01, Meta loss averaged over last 500 steps = 2.5592e-01, PNorm = 177.1773, GNorm = 0.2241
Meta loss on this task batch = 2.4144e-01, Meta loss averaged over last 500 steps = 2.5586e-01, PNorm = 177.1825, GNorm = 0.2310
Meta loss on this task batch = 2.3156e-01, Meta loss averaged over last 500 steps = 2.5582e-01, PNorm = 177.1867, GNorm = 0.2465
Meta loss on this task batch = 2.3315e-01, Meta loss averaged over last 500 steps = 2.5577e-01, PNorm = 177.1910, GNorm = 0.2145
Meta loss on this task batch = 3.1397e-01, Meta loss averaged over last 500 steps = 2.5589e-01, PNorm = 177.1946, GNorm = 0.2717
Meta loss on this task batch = 2.7196e-01, Meta loss averaged over last 500 steps = 2.5594e-01, PNorm = 177.1987, GNorm = 0.2585
Meta loss on this task batch = 2.2894e-01, Meta loss averaged over last 500 steps = 2.5593e-01, PNorm = 177.2034, GNorm = 0.2312
Meta loss on this task batch = 2.6072e-01, Meta loss averaged over last 500 steps = 2.5591e-01, PNorm = 177.2084, GNorm = 0.2433
Meta loss on this task batch = 2.5699e-01, Meta loss averaged over last 500 steps = 2.5595e-01, PNorm = 177.2138, GNorm = 0.3238
Took 104.25004172325134 seconds to complete one epoch of meta training
Took 111.59784078598022 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511316
Epoch 916
Meta loss on this task batch = 2.6975e-01, Meta loss averaged over last 500 steps = 2.5600e-01, PNorm = 177.2189, GNorm = 0.2068
Meta loss on this task batch = 2.0080e-01, Meta loss averaged over last 500 steps = 2.5587e-01, PNorm = 177.2242, GNorm = 0.1989
Meta loss on this task batch = 2.6628e-01, Meta loss averaged over last 500 steps = 2.5590e-01, PNorm = 177.2296, GNorm = 0.2358
Meta loss on this task batch = 2.0374e-01, Meta loss averaged over last 500 steps = 2.5575e-01, PNorm = 177.2350, GNorm = 0.1830
Meta loss on this task batch = 2.8583e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 177.2395, GNorm = 0.2435
Meta loss on this task batch = 1.9697e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 177.2438, GNorm = 0.2304
Meta loss on this task batch = 2.6721e-01, Meta loss averaged over last 500 steps = 2.5572e-01, PNorm = 177.2478, GNorm = 0.2167
Meta loss on this task batch = 3.0644e-01, Meta loss averaged over last 500 steps = 2.5580e-01, PNorm = 177.2513, GNorm = 0.2661
Meta loss on this task batch = 2.1946e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 177.2547, GNorm = 0.1953
Meta loss on this task batch = 2.9069e-01, Meta loss averaged over last 500 steps = 2.5585e-01, PNorm = 177.2583, GNorm = 0.2505
Meta loss on this task batch = 2.8320e-01, Meta loss averaged over last 500 steps = 2.5583e-01, PNorm = 177.2610, GNorm = 0.2773
Meta loss on this task batch = 2.5879e-01, Meta loss averaged over last 500 steps = 2.5583e-01, PNorm = 177.2639, GNorm = 0.2230
Meta loss on this task batch = 2.1806e-01, Meta loss averaged over last 500 steps = 2.5580e-01, PNorm = 177.2665, GNorm = 0.2063
Meta loss on this task batch = 2.0125e-01, Meta loss averaged over last 500 steps = 2.5568e-01, PNorm = 177.2698, GNorm = 0.2066
Meta loss on this task batch = 2.3552e-01, Meta loss averaged over last 500 steps = 2.5567e-01, PNorm = 177.2736, GNorm = 0.2246
Meta loss on this task batch = 2.7442e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 177.2775, GNorm = 0.2353
Meta loss on this task batch = 2.5405e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 177.2814, GNorm = 0.2986
Meta loss on this task batch = 2.7509e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 177.2858, GNorm = 0.2272
Meta loss on this task batch = 2.3069e-01, Meta loss averaged over last 500 steps = 2.5560e-01, PNorm = 177.2901, GNorm = 0.2832
Took 108.02544021606445 seconds to complete one epoch of meta training
Took 116.07540702819824 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491922
Epoch 917
Meta loss on this task batch = 2.5073e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 177.2943, GNorm = 0.2178
Meta loss on this task batch = 2.8879e-01, Meta loss averaged over last 500 steps = 2.5555e-01, PNorm = 177.2983, GNorm = 0.3045
Meta loss on this task batch = 2.2879e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 177.3023, GNorm = 0.1899
Meta loss on this task batch = 2.5720e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 177.3061, GNorm = 0.2670
Meta loss on this task batch = 2.2416e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 177.3092, GNorm = 0.2023
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.3119, GNorm = 0.2229
Meta loss on this task batch = 2.9195e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 177.3155, GNorm = 0.2391
Meta loss on this task batch = 2.2030e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.3190, GNorm = 0.2107
Meta loss on this task batch = 2.6212e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 177.3220, GNorm = 0.2630
Meta loss on this task batch = 2.3878e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 177.3258, GNorm = 0.2156
Meta loss on this task batch = 2.9391e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 177.3300, GNorm = 0.2703
Meta loss on this task batch = 2.4928e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 177.3344, GNorm = 0.2240
Meta loss on this task batch = 2.4970e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 177.3385, GNorm = 0.2207
Meta loss on this task batch = 2.5890e-01, Meta loss averaged over last 500 steps = 2.5559e-01, PNorm = 177.3428, GNorm = 0.2721
Meta loss on this task batch = 2.5735e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 177.3468, GNorm = 0.2116
Meta loss on this task batch = 2.5949e-01, Meta loss averaged over last 500 steps = 2.5575e-01, PNorm = 177.3505, GNorm = 0.2211
Meta loss on this task batch = 2.4730e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 177.3533, GNorm = 0.2941
Meta loss on this task batch = 2.4331e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 177.3563, GNorm = 0.2792
Meta loss on this task batch = 2.4753e-01, Meta loss averaged over last 500 steps = 2.5565e-01, PNorm = 177.3591, GNorm = 0.2841
Took 107.39099097251892 seconds to complete one epoch of meta training
Took 113.57408833503723 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503056
Epoch 918
Meta loss on this task batch = 2.5454e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 177.3622, GNorm = 0.2178
Meta loss on this task batch = 2.8327e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 177.3653, GNorm = 0.2375
Meta loss on this task batch = 2.4305e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 177.3688, GNorm = 0.2871
Meta loss on this task batch = 2.4775e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 177.3732, GNorm = 0.2970
Meta loss on this task batch = 2.4564e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 177.3779, GNorm = 0.2214
Meta loss on this task batch = 2.9036e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 177.3829, GNorm = 0.2483
Meta loss on this task batch = 2.5260e-01, Meta loss averaged over last 500 steps = 2.5552e-01, PNorm = 177.3884, GNorm = 0.2113
Meta loss on this task batch = 2.3897e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 177.3938, GNorm = 0.2198
Meta loss on this task batch = 2.1606e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 177.3995, GNorm = 0.2415
Meta loss on this task batch = 2.7556e-01, Meta loss averaged over last 500 steps = 2.5562e-01, PNorm = 177.4046, GNorm = 0.2329
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 177.4102, GNorm = 0.1907
Meta loss on this task batch = 2.3243e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 177.4161, GNorm = 0.2595
Meta loss on this task batch = 2.5439e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 177.4217, GNorm = 0.2671
Meta loss on this task batch = 2.2702e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 177.4273, GNorm = 0.2412
Meta loss on this task batch = 2.4731e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.4328, GNorm = 0.2048
Meta loss on this task batch = 2.6700e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.4374, GNorm = 0.2536
Meta loss on this task batch = 2.6402e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 177.4417, GNorm = 0.2598
Meta loss on this task batch = 2.6471e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.4461, GNorm = 0.2469
Meta loss on this task batch = 3.0503e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 177.4495, GNorm = 0.3099
Took 111.31053447723389 seconds to complete one epoch of meta training
Took 118.68249154090881 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504013
Epoch 919
Meta loss on this task batch = 2.7635e-01, Meta loss averaged over last 500 steps = 2.5563e-01, PNorm = 177.4524, GNorm = 0.2524
Meta loss on this task batch = 2.5351e-01, Meta loss averaged over last 500 steps = 2.5565e-01, PNorm = 177.4552, GNorm = 0.2393
Meta loss on this task batch = 2.4425e-01, Meta loss averaged over last 500 steps = 2.5552e-01, PNorm = 177.4576, GNorm = 0.2774
Meta loss on this task batch = 2.1490e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.4609, GNorm = 0.2099
Meta loss on this task batch = 2.2005e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 177.4648, GNorm = 0.2037
Meta loss on this task batch = 2.9493e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.4679, GNorm = 0.2632
Meta loss on this task batch = 1.7480e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 177.4716, GNorm = 0.1760
Meta loss on this task batch = 2.4800e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 177.4757, GNorm = 0.2242
Meta loss on this task batch = 2.4281e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 177.4804, GNorm = 0.2300
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 177.4855, GNorm = 0.2697
Meta loss on this task batch = 2.0886e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 177.4908, GNorm = 0.1845
Meta loss on this task batch = 2.6770e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 177.4964, GNorm = 0.2195
Meta loss on this task batch = 3.0959e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 177.5023, GNorm = 0.2443
Meta loss on this task batch = 2.5044e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 177.5086, GNorm = 0.2055
Meta loss on this task batch = 2.7280e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 177.5151, GNorm = 0.3791
Meta loss on this task batch = 3.1170e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 177.5207, GNorm = 0.2536
Meta loss on this task batch = 2.7879e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.5266, GNorm = 0.2322
Meta loss on this task batch = 2.8271e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 177.5320, GNorm = 0.2643
Meta loss on this task batch = 2.2539e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 177.5371, GNorm = 0.3179
Took 103.45316553115845 seconds to complete one epoch of meta training
Took 111.2636365890503 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491925
Epoch 920
Meta loss on this task batch = 2.3507e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 177.5418, GNorm = 0.2448
Meta loss on this task batch = 2.4964e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.5461, GNorm = 0.1994
Meta loss on this task batch = 2.4198e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 177.5501, GNorm = 0.2403
Meta loss on this task batch = 1.8686e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 177.5543, GNorm = 0.2039
Meta loss on this task batch = 2.9252e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 177.5584, GNorm = 0.2770
Meta loss on this task batch = 2.2667e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.5627, GNorm = 0.2202
Meta loss on this task batch = 2.3490e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.5671, GNorm = 0.2288
Meta loss on this task batch = 2.0637e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.5713, GNorm = 0.2117
Meta loss on this task batch = 3.3475e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 177.5754, GNorm = 0.2667
Meta loss on this task batch = 2.5379e-01, Meta loss averaged over last 500 steps = 2.5559e-01, PNorm = 177.5794, GNorm = 0.3415
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.5836, GNorm = 0.2532
Meta loss on this task batch = 2.1475e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 177.5885, GNorm = 0.2092
Meta loss on this task batch = 2.3724e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 177.5932, GNorm = 0.2403
Meta loss on this task batch = 2.7505e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 177.5982, GNorm = 0.2648
Meta loss on this task batch = 2.4572e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 177.6026, GNorm = 0.2334
Meta loss on this task batch = 2.6128e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 177.6076, GNorm = 0.2282
Meta loss on this task batch = 3.1495e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 177.6118, GNorm = 0.2501
Meta loss on this task batch = 3.1870e-01, Meta loss averaged over last 500 steps = 2.5566e-01, PNorm = 177.6160, GNorm = 0.3015
Meta loss on this task batch = 2.3927e-01, Meta loss averaged over last 500 steps = 2.5560e-01, PNorm = 177.6209, GNorm = 0.2517
Took 106.64056944847107 seconds to complete one epoch of meta training
Took 114.48975706100464 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.511429
Epoch 921
Meta loss on this task batch = 2.4351e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 177.6261, GNorm = 0.2002
Meta loss on this task batch = 2.7841e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 177.6307, GNorm = 0.2555
Meta loss on this task batch = 2.2914e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 177.6359, GNorm = 0.2300
Meta loss on this task batch = 2.5740e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 177.6412, GNorm = 0.2289
Meta loss on this task batch = 2.2941e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 177.6465, GNorm = 0.2285
Meta loss on this task batch = 2.3415e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 177.6519, GNorm = 0.1844
Meta loss on this task batch = 2.6208e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 177.6571, GNorm = 0.2312
Meta loss on this task batch = 2.3010e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 177.6626, GNorm = 0.1851
Meta loss on this task batch = 2.0330e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 177.6684, GNorm = 0.2197
Meta loss on this task batch = 2.5431e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 177.6732, GNorm = 0.2423
Meta loss on this task batch = 2.4256e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 177.6774, GNorm = 0.2724
Meta loss on this task batch = 2.5553e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 177.6817, GNorm = 0.2133
Meta loss on this task batch = 2.6040e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 177.6854, GNorm = 0.2301
Meta loss on this task batch = 3.2047e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 177.6884, GNorm = 0.2525
Meta loss on this task batch = 2.8853e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 177.6913, GNorm = 0.2356
Meta loss on this task batch = 2.9753e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 177.6932, GNorm = 0.2955
Meta loss on this task batch = 2.1702e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 177.6951, GNorm = 0.2194
Meta loss on this task batch = 2.7622e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 177.6972, GNorm = 0.2461
Meta loss on this task batch = 2.3920e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 177.7000, GNorm = 0.2508
Took 111.43557596206665 seconds to complete one epoch of meta training
Took 117.83336281776428 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471255
Epoch 922
Meta loss on this task batch = 2.6054e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.7041, GNorm = 0.4190
Meta loss on this task batch = 2.4915e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 177.7086, GNorm = 0.2261
Meta loss on this task batch = 2.2908e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 177.7131, GNorm = 0.2054
Meta loss on this task batch = 2.6541e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 177.7171, GNorm = 0.2143
Meta loss on this task batch = 2.6179e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 177.7220, GNorm = 0.2212
Meta loss on this task batch = 3.0200e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 177.7263, GNorm = 0.2362
Meta loss on this task batch = 2.9327e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 177.7314, GNorm = 0.2624
Meta loss on this task batch = 2.6430e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 177.7364, GNorm = 0.2266
Meta loss on this task batch = 2.7335e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.7409, GNorm = 0.2297
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.5564e-01, PNorm = 177.7455, GNorm = 0.2643
Meta loss on this task batch = 2.6576e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 177.7496, GNorm = 0.2626
Meta loss on this task batch = 1.9427e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 177.7543, GNorm = 0.1983
Meta loss on this task batch = 2.3910e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 177.7592, GNorm = 0.2414
Meta loss on this task batch = 2.7086e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 177.7641, GNorm = 0.2354
Meta loss on this task batch = 2.5850e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.7695, GNorm = 0.2324
Meta loss on this task batch = 2.5743e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 177.7751, GNorm = 0.2154
Meta loss on this task batch = 2.4226e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 177.7812, GNorm = 0.1988
Meta loss on this task batch = 3.0392e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 177.7864, GNorm = 0.2443
Meta loss on this task batch = 1.9837e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 177.7917, GNorm = 0.2364
Took 112.31835865974426 seconds to complete one epoch of meta training
Took 118.83904910087585 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498393
Epoch 923
Meta loss on this task batch = 2.2662e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.7964, GNorm = 0.2045
Meta loss on this task batch = 2.4971e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 177.8010, GNorm = 0.2197
Meta loss on this task batch = 2.4957e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 177.8053, GNorm = 0.1927
Meta loss on this task batch = 2.3113e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 177.8096, GNorm = 0.2336
Meta loss on this task batch = 2.1864e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 177.8137, GNorm = 0.1941
Meta loss on this task batch = 2.6438e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 177.8170, GNorm = 0.2535
Meta loss on this task batch = 2.8712e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 177.8195, GNorm = 0.2586
Meta loss on this task batch = 2.4521e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 177.8217, GNorm = 0.2339
Meta loss on this task batch = 2.7072e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 177.8257, GNorm = 0.2341
Meta loss on this task batch = 1.9218e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 177.8301, GNorm = 0.1915
Meta loss on this task batch = 2.9231e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 177.8335, GNorm = 0.3163
Meta loss on this task batch = 2.4943e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 177.8376, GNorm = 0.2307
Meta loss on this task batch = 2.0753e-01, Meta loss averaged over last 500 steps = 2.5492e-01, PNorm = 177.8416, GNorm = 0.1904
Meta loss on this task batch = 2.2354e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 177.8457, GNorm = 0.2389
Meta loss on this task batch = 2.8750e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 177.8498, GNorm = 0.2507
Meta loss on this task batch = 2.4688e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 177.8540, GNorm = 0.2290
Meta loss on this task batch = 3.1984e-01, Meta loss averaged over last 500 steps = 2.5481e-01, PNorm = 177.8575, GNorm = 0.2779
Meta loss on this task batch = 2.9353e-01, Meta loss averaged over last 500 steps = 2.5490e-01, PNorm = 177.8613, GNorm = 0.2315
Meta loss on this task batch = 2.7623e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 177.8652, GNorm = 0.4439
Took 107.21536564826965 seconds to complete one epoch of meta training
Took 114.31877732276917 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490720
Epoch 924
Meta loss on this task batch = 2.7085e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 177.8695, GNorm = 0.2188
Meta loss on this task batch = 2.4643e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 177.8741, GNorm = 0.2272
Meta loss on this task batch = 2.4064e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 177.8790, GNorm = 0.2287
Meta loss on this task batch = 2.3842e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 177.8843, GNorm = 0.2331
Meta loss on this task batch = 2.2245e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 177.8902, GNorm = 0.2053
Meta loss on this task batch = 2.6922e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 177.8959, GNorm = 0.2384
Meta loss on this task batch = 2.5053e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 177.9013, GNorm = 0.2529
Meta loss on this task batch = 2.3969e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 177.9067, GNorm = 0.2179
Meta loss on this task batch = 2.9568e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 177.9114, GNorm = 0.2900
Meta loss on this task batch = 2.3213e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 177.9163, GNorm = 0.2375
Meta loss on this task batch = 2.9194e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 177.9210, GNorm = 0.2826
Meta loss on this task batch = 2.6283e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 177.9251, GNorm = 0.2783
Meta loss on this task batch = 2.7783e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 177.9291, GNorm = 0.2472
Meta loss on this task batch = 2.5084e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.9331, GNorm = 0.2404
Meta loss on this task batch = 2.4834e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 177.9372, GNorm = 0.2245
Meta loss on this task batch = 2.7547e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 177.9421, GNorm = 0.2700
Meta loss on this task batch = 2.4191e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 177.9475, GNorm = 0.1906
Meta loss on this task batch = 2.5579e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 177.9528, GNorm = 0.2817
Meta loss on this task batch = 2.1785e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.9589, GNorm = 0.2398
Took 109.39295291900635 seconds to complete one epoch of meta training
Took 116.66019773483276 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508052
Epoch 925
Meta loss on this task batch = 2.6641e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 177.9644, GNorm = 0.2551
Meta loss on this task batch = 2.7548e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 177.9694, GNorm = 0.2743
Meta loss on this task batch = 2.1019e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 177.9753, GNorm = 0.2062
Meta loss on this task batch = 2.3226e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 177.9812, GNorm = 0.2060
Meta loss on this task batch = 2.5789e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 177.9865, GNorm = 0.2342
Meta loss on this task batch = 2.9157e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 177.9905, GNorm = 0.2915
Meta loss on this task batch = 2.7829e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 177.9943, GNorm = 0.2459
Meta loss on this task batch = 3.2508e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 177.9977, GNorm = 0.2510
Meta loss on this task batch = 2.8567e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 178.0010, GNorm = 0.2315
Meta loss on this task batch = 2.4584e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 178.0045, GNorm = 0.2235
Meta loss on this task batch = 2.4606e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 178.0079, GNorm = 0.2176
Meta loss on this task batch = 2.2008e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 178.0111, GNorm = 0.2494
Meta loss on this task batch = 2.5210e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 178.0141, GNorm = 0.2283
Meta loss on this task batch = 2.8461e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 178.0155, GNorm = 0.3332
Meta loss on this task batch = 2.5226e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 178.0159, GNorm = 0.2895
Meta loss on this task batch = 1.8526e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 178.0165, GNorm = 0.2177
Meta loss on this task batch = 2.3196e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 178.0181, GNorm = 0.2233
Meta loss on this task batch = 2.9604e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.0193, GNorm = 0.2513
Meta loss on this task batch = 2.6015e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.0206, GNorm = 0.2707
Took 108.74079203605652 seconds to complete one epoch of meta training
Took 116.53200602531433 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481559
Epoch 926
Meta loss on this task batch = 2.7143e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 178.0231, GNorm = 0.2745
Meta loss on this task batch = 2.0171e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 178.0262, GNorm = 0.2160
Meta loss on this task batch = 2.3470e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 178.0302, GNorm = 0.1851
Meta loss on this task batch = 2.1724e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 178.0345, GNorm = 0.2118
Meta loss on this task batch = 2.4079e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 178.0394, GNorm = 0.2514
Meta loss on this task batch = 2.8910e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 178.0439, GNorm = 0.2600
Meta loss on this task batch = 2.5403e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 178.0487, GNorm = 0.2205
Meta loss on this task batch = 2.6432e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 178.0536, GNorm = 0.2523
Meta loss on this task batch = 2.2793e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 178.0583, GNorm = 0.2002
Meta loss on this task batch = 2.3893e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 178.0627, GNorm = 0.2869
Meta loss on this task batch = 2.9775e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 178.0669, GNorm = 0.2781
Meta loss on this task batch = 2.4974e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.0706, GNorm = 0.2737
Meta loss on this task batch = 2.5014e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 178.0746, GNorm = 0.2342
Meta loss on this task batch = 2.6922e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 178.0788, GNorm = 0.2339
Meta loss on this task batch = 2.9039e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 178.0829, GNorm = 0.2977
Meta loss on this task batch = 2.5849e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 178.0870, GNorm = 0.2279
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 178.0909, GNorm = 0.2198
Meta loss on this task batch = 2.7523e-01, Meta loss averaged over last 500 steps = 2.5563e-01, PNorm = 178.0953, GNorm = 0.2136
Meta loss on this task batch = 2.1430e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 178.0990, GNorm = 0.2843
Took 107.688227891922 seconds to complete one epoch of meta training
Took 115.26892900466919 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495320
Epoch 927
Meta loss on this task batch = 3.0886e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 178.1028, GNorm = 0.2995
Meta loss on this task batch = 2.6424e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 178.1064, GNorm = 0.2924
Meta loss on this task batch = 2.5010e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 178.1105, GNorm = 0.2342
Meta loss on this task batch = 2.8017e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.1151, GNorm = 0.2163
Meta loss on this task batch = 2.4789e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.1192, GNorm = 0.3031
Meta loss on this task batch = 2.3499e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.1232, GNorm = 0.2257
Meta loss on this task batch = 2.1162e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 178.1275, GNorm = 0.2202
Meta loss on this task batch = 2.9715e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 178.1319, GNorm = 0.2522
Meta loss on this task batch = 2.6860e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.1357, GNorm = 0.2498
Meta loss on this task batch = 2.4370e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 178.1395, GNorm = 0.2014
Meta loss on this task batch = 3.3639e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 178.1430, GNorm = 0.2852
Meta loss on this task batch = 2.5354e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 178.1463, GNorm = 0.2257
Meta loss on this task batch = 2.4896e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 178.1492, GNorm = 0.2491
Meta loss on this task batch = 2.4141e-01, Meta loss averaged over last 500 steps = 2.5552e-01, PNorm = 178.1520, GNorm = 0.2457
Meta loss on this task batch = 2.9084e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 178.1550, GNorm = 0.2911
Meta loss on this task batch = 2.7090e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 178.1582, GNorm = 0.2283
Meta loss on this task batch = 2.1035e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 178.1614, GNorm = 0.2034
Meta loss on this task batch = 2.5535e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 178.1646, GNorm = 0.2305
Meta loss on this task batch = 2.4417e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 178.1688, GNorm = 0.2565
Took 111.97667646408081 seconds to complete one epoch of meta training
Took 119.34022617340088 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.517394
Epoch 928
Meta loss on this task batch = 2.7484e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 178.1735, GNorm = 0.2693
Meta loss on this task batch = 2.7075e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 178.1793, GNorm = 0.2601
Meta loss on this task batch = 2.4294e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.1851, GNorm = 0.1872
Meta loss on this task batch = 2.6422e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.1916, GNorm = 0.2309
Meta loss on this task batch = 2.8862e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 178.1978, GNorm = 0.2333
Meta loss on this task batch = 2.2136e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.2033, GNorm = 0.2044
Meta loss on this task batch = 2.2070e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.2095, GNorm = 0.2495
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 178.2148, GNorm = 0.2608
Meta loss on this task batch = 2.6383e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 178.2198, GNorm = 0.2100
Meta loss on this task batch = 2.8538e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.2243, GNorm = 0.2428
Meta loss on this task batch = 2.7350e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 178.2284, GNorm = 0.2289
Meta loss on this task batch = 2.3886e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 178.2328, GNorm = 0.2157
Meta loss on this task batch = 2.4373e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 178.2372, GNorm = 0.2318
Meta loss on this task batch = 2.5841e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 178.2418, GNorm = 0.2230
Meta loss on this task batch = 2.3782e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 178.2463, GNorm = 0.2313
Meta loss on this task batch = 2.3756e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 178.2513, GNorm = 0.2368
Meta loss on this task batch = 2.3375e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 178.2557, GNorm = 0.2284
Meta loss on this task batch = 2.2841e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 178.2601, GNorm = 0.2352
Meta loss on this task batch = 2.6693e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 178.2645, GNorm = 0.2609
Took 107.43481373786926 seconds to complete one epoch of meta training
Took 115.84948945045471 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493580
Epoch 929
Meta loss on this task batch = 2.3637e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.2691, GNorm = 0.2166
Meta loss on this task batch = 2.4759e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.2740, GNorm = 0.2493
Meta loss on this task batch = 2.5349e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.2791, GNorm = 0.2699
Meta loss on this task batch = 2.5235e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 178.2842, GNorm = 0.2104
Meta loss on this task batch = 2.9750e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 178.2892, GNorm = 0.2475
Meta loss on this task batch = 3.1442e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 178.2929, GNorm = 0.2986
Meta loss on this task batch = 2.6051e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 178.2967, GNorm = 0.2586
Meta loss on this task batch = 3.3518e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 178.3006, GNorm = 0.2425
Meta loss on this task batch = 2.4582e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 178.3043, GNorm = 0.2114
Meta loss on this task batch = 1.9573e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 178.3079, GNorm = 0.2117
Meta loss on this task batch = 2.1574e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 178.3116, GNorm = 0.2255
Meta loss on this task batch = 2.5685e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 178.3148, GNorm = 0.2199
Meta loss on this task batch = 2.2670e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 178.3182, GNorm = 0.2189
Meta loss on this task batch = 2.7914e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 178.3217, GNorm = 0.2145
Meta loss on this task batch = 2.4071e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 178.3256, GNorm = 0.2271
Meta loss on this task batch = 2.5274e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 178.3296, GNorm = 0.2494
Meta loss on this task batch = 2.2316e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 178.3344, GNorm = 0.2043
Meta loss on this task batch = 2.2307e-01, Meta loss averaged over last 500 steps = 2.5483e-01, PNorm = 178.3392, GNorm = 0.2183
Meta loss on this task batch = 2.2997e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 178.3438, GNorm = 0.2397
Took 108.858717918396 seconds to complete one epoch of meta training
Took 115.92038536071777 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.520950
Epoch 930
Meta loss on this task batch = 2.4865e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 178.3474, GNorm = 0.2664
Meta loss on this task batch = 2.6661e-01, Meta loss averaged over last 500 steps = 2.5489e-01, PNorm = 178.3508, GNorm = 0.2817
Meta loss on this task batch = 2.4515e-01, Meta loss averaged over last 500 steps = 2.5485e-01, PNorm = 178.3541, GNorm = 0.2238
Meta loss on this task batch = 3.3242e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 178.3570, GNorm = 0.2528
Meta loss on this task batch = 2.4569e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 178.3599, GNorm = 0.2016
Meta loss on this task batch = 2.0620e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 178.3630, GNorm = 0.1989
Meta loss on this task batch = 2.7294e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 178.3656, GNorm = 0.2669
Meta loss on this task batch = 2.6921e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 178.3686, GNorm = 0.2300
Meta loss on this task batch = 2.7808e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 178.3716, GNorm = 0.2485
Meta loss on this task batch = 2.4200e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 178.3753, GNorm = 0.2417
Meta loss on this task batch = 2.4206e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 178.3789, GNorm = 0.2690
Meta loss on this task batch = 2.3602e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 178.3825, GNorm = 0.2289
Meta loss on this task batch = 3.0821e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 178.3855, GNorm = 0.2666
Meta loss on this task batch = 2.4686e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 178.3889, GNorm = 0.2696
Meta loss on this task batch = 2.4238e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 178.3921, GNorm = 0.2600
Meta loss on this task batch = 2.1113e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 178.3957, GNorm = 0.1876
Meta loss on this task batch = 2.3867e-01, Meta loss averaged over last 500 steps = 2.5491e-01, PNorm = 178.3998, GNorm = 0.2949
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 178.4038, GNorm = 0.2501
Meta loss on this task batch = 2.1205e-01, Meta loss averaged over last 500 steps = 2.5482e-01, PNorm = 178.4077, GNorm = 0.2682
Took 99.2651994228363 seconds to complete one epoch of meta training
Took 106.092289686203 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.518423
Epoch 931
Meta loss on this task batch = 2.7734e-01, Meta loss averaged over last 500 steps = 2.5484e-01, PNorm = 178.4116, GNorm = 0.2264
Meta loss on this task batch = 2.3671e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 178.4156, GNorm = 0.2539
Meta loss on this task batch = 2.5720e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 178.4194, GNorm = 0.2544
Meta loss on this task batch = 2.0996e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 178.4239, GNorm = 0.2037
Meta loss on this task batch = 2.5913e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 178.4282, GNorm = 0.2403
Meta loss on this task batch = 2.7449e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 178.4325, GNorm = 0.2435
Meta loss on this task batch = 1.6996e-01, Meta loss averaged over last 500 steps = 2.5462e-01, PNorm = 178.4372, GNorm = 0.1953
Meta loss on this task batch = 2.2236e-01, Meta loss averaged over last 500 steps = 2.5456e-01, PNorm = 178.4424, GNorm = 0.2268
Meta loss on this task batch = 2.8895e-01, Meta loss averaged over last 500 steps = 2.5461e-01, PNorm = 178.4473, GNorm = 0.2761
Meta loss on this task batch = 2.0592e-01, Meta loss averaged over last 500 steps = 2.5456e-01, PNorm = 178.4527, GNorm = 0.2366
Meta loss on this task batch = 2.7517e-01, Meta loss averaged over last 500 steps = 2.5462e-01, PNorm = 178.4583, GNorm = 0.2788
Meta loss on this task batch = 2.3225e-01, Meta loss averaged over last 500 steps = 2.5466e-01, PNorm = 178.4637, GNorm = 0.2469
Meta loss on this task batch = 3.2734e-01, Meta loss averaged over last 500 steps = 2.5473e-01, PNorm = 178.4680, GNorm = 0.2902
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.5482e-01, PNorm = 178.4721, GNorm = 0.2368
Meta loss on this task batch = 2.6979e-01, Meta loss averaged over last 500 steps = 2.5477e-01, PNorm = 178.4759, GNorm = 0.3229
Meta loss on this task batch = 3.0868e-01, Meta loss averaged over last 500 steps = 2.5477e-01, PNorm = 178.4788, GNorm = 0.2781
Meta loss on this task batch = 2.4100e-01, Meta loss averaged over last 500 steps = 2.5485e-01, PNorm = 178.4811, GNorm = 0.2471
Meta loss on this task batch = 2.5427e-01, Meta loss averaged over last 500 steps = 2.5483e-01, PNorm = 178.4842, GNorm = 0.2572
Meta loss on this task batch = 2.7652e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 178.4878, GNorm = 0.2503
Took 101.42779493331909 seconds to complete one epoch of meta training
Took 108.25314807891846 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.494931
Epoch 932
Meta loss on this task batch = 2.7457e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 178.4906, GNorm = 0.2475
Meta loss on this task batch = 2.8288e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 178.4937, GNorm = 0.2707
Meta loss on this task batch = 3.0361e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 178.4968, GNorm = 0.2031
Meta loss on this task batch = 2.8838e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 178.5007, GNorm = 0.2642
Meta loss on this task batch = 2.5030e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 178.5039, GNorm = 0.2521
Meta loss on this task batch = 2.1941e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 178.5074, GNorm = 0.1934
Meta loss on this task batch = 2.8057e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 178.5113, GNorm = 0.2363
Meta loss on this task batch = 2.6236e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.5151, GNorm = 0.2120
Meta loss on this task batch = 2.5218e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.5180, GNorm = 0.2063
Meta loss on this task batch = 2.5483e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.5205, GNorm = 0.2286
Meta loss on this task batch = 2.6531e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 178.5230, GNorm = 0.2339
Meta loss on this task batch = 2.2555e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 178.5249, GNorm = 0.2361
Meta loss on this task batch = 2.8167e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 178.5266, GNorm = 0.2457
Meta loss on this task batch = 2.2097e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 178.5286, GNorm = 0.1942
Meta loss on this task batch = 2.5359e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 178.5308, GNorm = 0.2207
Meta loss on this task batch = 2.8073e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 178.5323, GNorm = 0.2411
Meta loss on this task batch = 1.9786e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 178.5345, GNorm = 0.1795
Meta loss on this task batch = 2.4710e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 178.5371, GNorm = 0.2242
Meta loss on this task batch = 2.8627e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 178.5401, GNorm = 0.2678
Took 104.60726761817932 seconds to complete one epoch of meta training
Took 111.66007971763611 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472860
Epoch 933
Meta loss on this task batch = 2.4436e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 178.5436, GNorm = 0.2518
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 178.5463, GNorm = 0.2366
Meta loss on this task batch = 2.7113e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 178.5493, GNorm = 0.2572
Meta loss on this task batch = 2.4266e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 178.5525, GNorm = 0.2159
Meta loss on this task batch = 2.5907e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 178.5566, GNorm = 0.2076
Meta loss on this task batch = 1.9814e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 178.5609, GNorm = 0.1916
Meta loss on this task batch = 3.1244e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 178.5646, GNorm = 0.2693
Meta loss on this task batch = 2.3915e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 178.5686, GNorm = 0.2083
Meta loss on this task batch = 3.0554e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 178.5727, GNorm = 0.2622
Meta loss on this task batch = 2.5933e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 178.5768, GNorm = 0.2252
Meta loss on this task batch = 2.4339e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 178.5811, GNorm = 0.2056
Meta loss on this task batch = 2.4712e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 178.5851, GNorm = 0.2188
Meta loss on this task batch = 2.6557e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 178.5899, GNorm = 0.2504
Meta loss on this task batch = 2.1876e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 178.5949, GNorm = 0.2314
Meta loss on this task batch = 2.5037e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 178.5997, GNorm = 0.2338
Meta loss on this task batch = 2.3327e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 178.6048, GNorm = 0.2099
Meta loss on this task batch = 2.7195e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 178.6092, GNorm = 0.2536
Meta loss on this task batch = 2.5844e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 178.6134, GNorm = 0.2232
Meta loss on this task batch = 3.3146e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 178.6177, GNorm = 0.2779
Took 102.45973181724548 seconds to complete one epoch of meta training
Took 109.37375450134277 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500878
Epoch 934
Meta loss on this task batch = 2.2668e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 178.6223, GNorm = 0.2047
Meta loss on this task batch = 2.8841e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 178.6275, GNorm = 0.2200
Meta loss on this task batch = 2.4931e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 178.6328, GNorm = 0.2082
Meta loss on this task batch = 2.5292e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 178.6383, GNorm = 0.2083
Meta loss on this task batch = 2.5700e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 178.6435, GNorm = 0.2216
Meta loss on this task batch = 2.3934e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 178.6485, GNorm = 0.2019
Meta loss on this task batch = 2.1537e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 178.6541, GNorm = 0.2207
Meta loss on this task batch = 2.8847e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 178.6591, GNorm = 0.2579
Meta loss on this task batch = 2.1867e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 178.6640, GNorm = 0.2117
Meta loss on this task batch = 2.3250e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 178.6688, GNorm = 0.2295
Meta loss on this task batch = 2.9066e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 178.6739, GNorm = 0.2521
Meta loss on this task batch = 2.4529e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.6792, GNorm = 0.2353
Meta loss on this task batch = 2.9516e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 178.6845, GNorm = 0.2418
Meta loss on this task batch = 3.0073e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 178.6889, GNorm = 0.2404
Meta loss on this task batch = 2.0080e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.6936, GNorm = 0.3228
Meta loss on this task batch = 2.0800e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 178.6987, GNorm = 0.2075
Meta loss on this task batch = 2.7044e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.7042, GNorm = 0.2275
Meta loss on this task batch = 2.4336e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 178.7096, GNorm = 0.2482
Meta loss on this task batch = 3.0847e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.7147, GNorm = 0.2913
Took 102.89645099639893 seconds to complete one epoch of meta training
Took 109.59615230560303 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.486030
Epoch 935
Meta loss on this task batch = 2.8384e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 178.7190, GNorm = 0.2700
Meta loss on this task batch = 2.7926e-01, Meta loss averaged over last 500 steps = 2.5550e-01, PNorm = 178.7236, GNorm = 0.2271
Meta loss on this task batch = 2.6376e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 178.7284, GNorm = 0.3228
Meta loss on this task batch = 2.6889e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 178.7326, GNorm = 0.2500
Meta loss on this task batch = 2.8910e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 178.7370, GNorm = 0.2638
Meta loss on this task batch = 3.0471e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 178.7413, GNorm = 0.2117
Meta loss on this task batch = 2.7750e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 178.7457, GNorm = 0.2380
Meta loss on this task batch = 2.3698e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 178.7501, GNorm = 0.2265
Meta loss on this task batch = 2.8892e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 178.7549, GNorm = 0.2309
Meta loss on this task batch = 2.4473e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 178.7604, GNorm = 0.2458
Meta loss on this task batch = 2.1313e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 178.7661, GNorm = 0.1996
Meta loss on this task batch = 1.9919e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 178.7715, GNorm = 0.1900
Meta loss on this task batch = 2.3617e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 178.7770, GNorm = 0.2182
Meta loss on this task batch = 2.6304e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.7814, GNorm = 0.2519
Meta loss on this task batch = 2.5872e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 178.7856, GNorm = 0.2439
Meta loss on this task batch = 2.4996e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 178.7897, GNorm = 0.2243
Meta loss on this task batch = 2.5364e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 178.7937, GNorm = 0.2116
Meta loss on this task batch = 2.4563e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 178.7974, GNorm = 0.2453
Meta loss on this task batch = 2.3697e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 178.8012, GNorm = 0.2621
Took 103.88111233711243 seconds to complete one epoch of meta training
Took 109.77816367149353 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515302
Epoch 936
Meta loss on this task batch = 2.9077e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 178.8049, GNorm = 0.2150
Meta loss on this task batch = 1.7779e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 178.8084, GNorm = 0.2301
Meta loss on this task batch = 2.5576e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 178.8120, GNorm = 0.2575
Meta loss on this task batch = 2.7632e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 178.8159, GNorm = 0.2103
Meta loss on this task batch = 2.3932e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 178.8199, GNorm = 0.2510
Meta loss on this task batch = 2.4306e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 178.8234, GNorm = 0.2301
Meta loss on this task batch = 2.8956e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 178.8264, GNorm = 0.2325
Meta loss on this task batch = 2.2078e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 178.8296, GNorm = 0.1931
Meta loss on this task batch = 2.9058e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 178.8324, GNorm = 0.2349
Meta loss on this task batch = 2.8084e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 178.8357, GNorm = 0.2547
Meta loss on this task batch = 2.3754e-01, Meta loss averaged over last 500 steps = 2.5547e-01, PNorm = 178.8387, GNorm = 0.2370
Meta loss on this task batch = 2.6394e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 178.8414, GNorm = 0.2665
Meta loss on this task batch = 2.1334e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.8438, GNorm = 0.2634
Meta loss on this task batch = 2.3006e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 178.8467, GNorm = 0.2129
Meta loss on this task batch = 2.5551e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 178.8505, GNorm = 0.2370
Meta loss on this task batch = 2.3298e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 178.8548, GNorm = 0.2090
Meta loss on this task batch = 2.3294e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 178.8590, GNorm = 0.2172
Meta loss on this task batch = 2.9080e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 178.8629, GNorm = 0.2972
Meta loss on this task batch = 2.8716e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 178.8669, GNorm = 0.2923
Took 103.30029439926147 seconds to complete one epoch of meta training
Took 111.28651905059814 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496411
Epoch 937
Meta loss on this task batch = 2.4599e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 178.8710, GNorm = 0.2164
Meta loss on this task batch = 2.2865e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 178.8750, GNorm = 0.2187
Meta loss on this task batch = 2.3324e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 178.8791, GNorm = 0.2577
Meta loss on this task batch = 2.1881e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 178.8834, GNorm = 0.2295
Meta loss on this task batch = 2.3414e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 178.8873, GNorm = 0.2490
Meta loss on this task batch = 2.6261e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 178.8910, GNorm = 0.2656
Meta loss on this task batch = 2.2923e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 178.8953, GNorm = 0.2256
Meta loss on this task batch = 2.8606e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 178.9004, GNorm = 0.2479
Meta loss on this task batch = 2.6271e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 178.9049, GNorm = 0.2388
Meta loss on this task batch = 3.1161e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 178.9102, GNorm = 0.2731
Meta loss on this task batch = 2.4493e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 178.9151, GNorm = 0.2483
Meta loss on this task batch = 2.7998e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 178.9187, GNorm = 0.2695
Meta loss on this task batch = 2.9150e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 178.9223, GNorm = 0.2307
Meta loss on this task batch = 2.4789e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 178.9260, GNorm = 0.2382
Meta loss on this task batch = 2.3042e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 178.9302, GNorm = 0.2145
Meta loss on this task batch = 2.5569e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 178.9351, GNorm = 0.2421
Meta loss on this task batch = 2.3343e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 178.9404, GNorm = 0.2416
Meta loss on this task batch = 2.3339e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 178.9461, GNorm = 0.2255
Meta loss on this task batch = 2.2595e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 178.9526, GNorm = 0.2658
Took 107.92163586616516 seconds to complete one epoch of meta training
Took 114.89273762702942 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484375
Epoch 938
Meta loss on this task batch = 2.5111e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 178.9587, GNorm = 0.2384
Meta loss on this task batch = 2.7174e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 178.9655, GNorm = 0.2129
Meta loss on this task batch = 2.1292e-01, Meta loss averaged over last 500 steps = 2.5476e-01, PNorm = 178.9727, GNorm = 0.2296
Meta loss on this task batch = 2.8693e-01, Meta loss averaged over last 500 steps = 2.5490e-01, PNorm = 178.9800, GNorm = 0.2530
Meta loss on this task batch = 3.0555e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 178.9859, GNorm = 0.2227
Meta loss on this task batch = 2.5971e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 178.9916, GNorm = 0.2258
Meta loss on this task batch = 2.8842e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 178.9963, GNorm = 0.2501
Meta loss on this task batch = 2.8658e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.0005, GNorm = 0.2446
Meta loss on this task batch = 2.3794e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 179.0050, GNorm = 0.2297
Meta loss on this task batch = 3.1417e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 179.0088, GNorm = 0.3044
Meta loss on this task batch = 2.6131e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.0122, GNorm = 0.2856
Meta loss on this task batch = 2.2164e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.0159, GNorm = 0.2020
Meta loss on this task batch = 2.2746e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 179.0196, GNorm = 0.2413
Meta loss on this task batch = 2.4747e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 179.0233, GNorm = 0.2467
Meta loss on this task batch = 2.5263e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 179.0267, GNorm = 0.2413
Meta loss on this task batch = 2.1991e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 179.0308, GNorm = 0.2497
Meta loss on this task batch = 1.8062e-01, Meta loss averaged over last 500 steps = 2.5479e-01, PNorm = 179.0354, GNorm = 0.2296
Meta loss on this task batch = 2.1972e-01, Meta loss averaged over last 500 steps = 2.5472e-01, PNorm = 179.0394, GNorm = 0.2301
Meta loss on this task batch = 3.1148e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 179.0441, GNorm = 0.3617
Took 102.70942735671997 seconds to complete one epoch of meta training
Took 110.15340757369995 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496647
Epoch 939
Meta loss on this task batch = 2.4355e-01, Meta loss averaged over last 500 steps = 2.5476e-01, PNorm = 179.0481, GNorm = 0.2336
Meta loss on this task batch = 2.6327e-01, Meta loss averaged over last 500 steps = 2.5477e-01, PNorm = 179.0521, GNorm = 0.2792
Meta loss on this task batch = 2.1032e-01, Meta loss averaged over last 500 steps = 2.5465e-01, PNorm = 179.0566, GNorm = 0.2001
Meta loss on this task batch = 2.7838e-01, Meta loss averaged over last 500 steps = 2.5476e-01, PNorm = 179.0620, GNorm = 0.2622
Meta loss on this task batch = 2.7805e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 179.0668, GNorm = 0.2253
Meta loss on this task batch = 2.7260e-01, Meta loss averaged over last 500 steps = 2.5485e-01, PNorm = 179.0716, GNorm = 0.2244
Meta loss on this task batch = 2.9870e-01, Meta loss averaged over last 500 steps = 2.5490e-01, PNorm = 179.0762, GNorm = 0.2552
Meta loss on this task batch = 2.3584e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 179.0806, GNorm = 0.2264
Meta loss on this task batch = 2.5110e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 179.0855, GNorm = 0.1915
Meta loss on this task batch = 2.3390e-01, Meta loss averaged over last 500 steps = 2.5491e-01, PNorm = 179.0905, GNorm = 0.1982
Meta loss on this task batch = 2.2097e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 179.0957, GNorm = 0.2272
Meta loss on this task batch = 2.1149e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 179.1012, GNorm = 0.2196
Meta loss on this task batch = 2.1833e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 179.1068, GNorm = 0.1962
Meta loss on this task batch = 2.5360e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 179.1115, GNorm = 0.2400
Meta loss on this task batch = 3.0776e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 179.1158, GNorm = 0.2518
Meta loss on this task batch = 2.7390e-01, Meta loss averaged over last 500 steps = 2.5492e-01, PNorm = 179.1196, GNorm = 0.2496
Meta loss on this task batch = 2.3164e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 179.1240, GNorm = 0.2279
Meta loss on this task batch = 2.6585e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 179.1279, GNorm = 0.2808
Meta loss on this task batch = 2.4224e-01, Meta loss averaged over last 500 steps = 2.5475e-01, PNorm = 179.1310, GNorm = 0.2767
Took 104.97289752960205 seconds to complete one epoch of meta training
Took 111.9899492263794 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.510652
Epoch 940
Meta loss on this task batch = 2.4712e-01, Meta loss averaged over last 500 steps = 2.5467e-01, PNorm = 179.1339, GNorm = 0.2287
Meta loss on this task batch = 2.3981e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 179.1370, GNorm = 0.2335
Meta loss on this task batch = 2.6986e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 179.1397, GNorm = 0.2254
Meta loss on this task batch = 2.6624e-01, Meta loss averaged over last 500 steps = 2.5473e-01, PNorm = 179.1427, GNorm = 0.2516
Meta loss on this task batch = 2.9506e-01, Meta loss averaged over last 500 steps = 2.5479e-01, PNorm = 179.1466, GNorm = 0.3544
Meta loss on this task batch = 3.0745e-01, Meta loss averaged over last 500 steps = 2.5490e-01, PNorm = 179.1501, GNorm = 0.2630
Meta loss on this task batch = 2.0715e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 179.1539, GNorm = 0.2230
Meta loss on this task batch = 2.0695e-01, Meta loss averaged over last 500 steps = 2.5464e-01, PNorm = 179.1578, GNorm = 0.2277
Meta loss on this task batch = 2.3351e-01, Meta loss averaged over last 500 steps = 2.5451e-01, PNorm = 179.1623, GNorm = 0.2142
Meta loss on this task batch = 3.0872e-01, Meta loss averaged over last 500 steps = 2.5466e-01, PNorm = 179.1658, GNorm = 0.2746
Meta loss on this task batch = 2.5399e-01, Meta loss averaged over last 500 steps = 2.5464e-01, PNorm = 179.1695, GNorm = 0.2759
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 2.5473e-01, PNorm = 179.1741, GNorm = 0.2247
Meta loss on this task batch = 2.4128e-01, Meta loss averaged over last 500 steps = 2.5475e-01, PNorm = 179.1784, GNorm = 0.2233
Meta loss on this task batch = 2.9368e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 179.1821, GNorm = 0.2589
Meta loss on this task batch = 2.7994e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 179.1857, GNorm = 0.2496
Meta loss on this task batch = 2.7146e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 179.1901, GNorm = 0.2088
Meta loss on this task batch = 2.2800e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 179.1945, GNorm = 0.2293
Meta loss on this task batch = 2.4507e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 179.1989, GNorm = 0.2496
Meta loss on this task batch = 2.6969e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 179.2032, GNorm = 0.2594
Took 103.74980807304382 seconds to complete one epoch of meta training
Took 110.90439248085022 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465695
Epoch 941
Meta loss on this task batch = 2.7078e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 179.2077, GNorm = 0.2345
Meta loss on this task batch = 3.0393e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 179.2113, GNorm = 0.2886
Meta loss on this task batch = 2.0372e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 179.2154, GNorm = 0.2050
Meta loss on this task batch = 2.6248e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 179.2199, GNorm = 0.2265
Meta loss on this task batch = 2.1874e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 179.2245, GNorm = 0.2021
Meta loss on this task batch = 2.8329e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 179.2288, GNorm = 0.2327
Meta loss on this task batch = 2.8771e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 179.2328, GNorm = 0.2223
Meta loss on this task batch = 2.6360e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 179.2379, GNorm = 0.3021
Meta loss on this task batch = 2.1678e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 179.2423, GNorm = 0.2028
Meta loss on this task batch = 2.4679e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 179.2470, GNorm = 0.2216
Meta loss on this task batch = 2.8497e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 179.2515, GNorm = 0.3006
Meta loss on this task batch = 2.3458e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 179.2556, GNorm = 0.2271
Meta loss on this task batch = 2.9968e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 179.2594, GNorm = 0.2563
Meta loss on this task batch = 2.5451e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 179.2627, GNorm = 0.2543
Meta loss on this task batch = 2.3561e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 179.2652, GNorm = 0.2544
Meta loss on this task batch = 2.2698e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 179.2677, GNorm = 0.2261
Meta loss on this task batch = 2.4050e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 179.2707, GNorm = 0.2166
Meta loss on this task batch = 2.5195e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 179.2734, GNorm = 0.2464
Meta loss on this task batch = 2.3258e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 179.2773, GNorm = 0.2350
Took 105.50478982925415 seconds to complete one epoch of meta training
Took 112.56485915184021 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.463731
Epoch 942
Meta loss on this task batch = 3.1689e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.2816, GNorm = 0.3089
Meta loss on this task batch = 2.9496e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.2852, GNorm = 0.2743
Meta loss on this task batch = 2.6264e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 179.2894, GNorm = 0.2365
Meta loss on this task batch = 2.0610e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.2941, GNorm = 0.1966
Meta loss on this task batch = 2.0341e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 179.2991, GNorm = 0.2175
Meta loss on this task batch = 2.9711e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 179.3046, GNorm = 0.2685
Meta loss on this task batch = 2.2977e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 179.3102, GNorm = 0.2168
Meta loss on this task batch = 2.4736e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 179.3158, GNorm = 0.2252
Meta loss on this task batch = 2.9111e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 179.3210, GNorm = 0.2205
Meta loss on this task batch = 2.3638e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.3262, GNorm = 0.2261
Meta loss on this task batch = 2.4834e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 179.3315, GNorm = 0.2029
Meta loss on this task batch = 2.6628e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 179.3362, GNorm = 0.2077
Meta loss on this task batch = 2.3686e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.3412, GNorm = 0.2510
Meta loss on this task batch = 2.2575e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 179.3459, GNorm = 0.2207
Meta loss on this task batch = 2.2155e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 179.3511, GNorm = 0.2251
Meta loss on this task batch = 2.4699e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 179.3562, GNorm = 0.2374
Meta loss on this task batch = 2.8178e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 179.3620, GNorm = 0.2292
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 179.3681, GNorm = 0.2820
Meta loss on this task batch = 2.7252e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 179.3741, GNorm = 0.2831
Took 104.96067762374878 seconds to complete one epoch of meta training
Took 112.05785346031189 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493593
Epoch 943
Meta loss on this task batch = 2.3505e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 179.3800, GNorm = 0.2565
Meta loss on this task batch = 2.4633e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.3857, GNorm = 0.2136
Meta loss on this task batch = 2.3763e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.3915, GNorm = 0.2237
Meta loss on this task batch = 2.6944e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 179.3974, GNorm = 0.2028
Meta loss on this task batch = 2.2324e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 179.4027, GNorm = 0.2288
Meta loss on this task batch = 2.8442e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 179.4072, GNorm = 0.2696
Meta loss on this task batch = 3.0349e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 179.4105, GNorm = 0.3028
Meta loss on this task batch = 2.1494e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 179.4133, GNorm = 0.2565
Meta loss on this task batch = 2.3868e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 179.4154, GNorm = 0.2201
Meta loss on this task batch = 2.4068e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 179.4178, GNorm = 0.2211
Meta loss on this task batch = 2.7732e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 179.4201, GNorm = 0.2258
Meta loss on this task batch = 2.6537e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.4232, GNorm = 0.3019
Meta loss on this task batch = 2.2966e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 179.4276, GNorm = 0.1995
Meta loss on this task batch = 2.3509e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 179.4324, GNorm = 0.2064
Meta loss on this task batch = 2.8837e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 179.4376, GNorm = 0.2552
Meta loss on this task batch = 2.6532e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 179.4438, GNorm = 0.2646
Meta loss on this task batch = 2.6003e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 179.4497, GNorm = 0.2373
Meta loss on this task batch = 2.3115e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 179.4552, GNorm = 0.2283
Meta loss on this task batch = 2.7451e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 179.4604, GNorm = 0.2608
Took 106.7846462726593 seconds to complete one epoch of meta training
Took 113.74476933479309 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488673
Epoch 944
Meta loss on this task batch = 2.8233e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.4655, GNorm = 0.1934
Meta loss on this task batch = 2.2320e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 179.4706, GNorm = 0.2185
Meta loss on this task batch = 2.6409e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 179.4756, GNorm = 0.2504
Meta loss on this task batch = 2.6067e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.4804, GNorm = 0.2390
Meta loss on this task batch = 2.3999e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 179.4849, GNorm = 0.2787
Meta loss on this task batch = 2.7148e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 179.4891, GNorm = 0.2330
Meta loss on this task batch = 2.3134e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.4931, GNorm = 0.1999
Meta loss on this task batch = 2.4818e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.4970, GNorm = 0.2337
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 179.5011, GNorm = 0.2601
Meta loss on this task batch = 2.6683e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 179.5043, GNorm = 0.2199
Meta loss on this task batch = 2.9653e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 179.5074, GNorm = 0.2704
Meta loss on this task batch = 2.4976e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.5100, GNorm = 0.2314
Meta loss on this task batch = 2.3586e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 179.5130, GNorm = 0.2112
Meta loss on this task batch = 2.7177e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.5162, GNorm = 0.2415
Meta loss on this task batch = 1.9360e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 179.5198, GNorm = 0.1937
Meta loss on this task batch = 2.4927e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.5234, GNorm = 0.2129
Meta loss on this task batch = 2.6478e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 179.5274, GNorm = 0.2599
Meta loss on this task batch = 2.7937e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 179.5318, GNorm = 0.2682
Meta loss on this task batch = 2.8741e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 179.5367, GNorm = 0.2591
Took 103.16209769248962 seconds to complete one epoch of meta training
Took 110.58231115341187 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.495542
Epoch 945
Meta loss on this task batch = 2.5538e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 179.5398, GNorm = 0.2577
Meta loss on this task batch = 2.9108e-01, Meta loss averaged over last 500 steps = 2.5554e-01, PNorm = 179.5428, GNorm = 0.2453
Meta loss on this task batch = 2.4330e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 179.5458, GNorm = 0.2131
Meta loss on this task batch = 2.6385e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 179.5490, GNorm = 0.2267
Meta loss on this task batch = 2.7481e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 179.5523, GNorm = 0.2456
Meta loss on this task batch = 2.5144e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 179.5556, GNorm = 0.2328
Meta loss on this task batch = 2.1834e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 179.5593, GNorm = 0.2272
Meta loss on this task batch = 3.1313e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 179.5630, GNorm = 0.2605
Meta loss on this task batch = 2.6468e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 179.5668, GNorm = 0.2360
Meta loss on this task batch = 2.3099e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 179.5708, GNorm = 0.2199
Meta loss on this task batch = 2.8091e-01, Meta loss averaged over last 500 steps = 2.5560e-01, PNorm = 179.5751, GNorm = 0.2371
Meta loss on this task batch = 2.7618e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 179.5790, GNorm = 0.2866
Meta loss on this task batch = 2.6110e-01, Meta loss averaged over last 500 steps = 2.5574e-01, PNorm = 179.5830, GNorm = 0.2569
Meta loss on this task batch = 2.5965e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 179.5871, GNorm = 0.2062
Meta loss on this task batch = 2.5661e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 179.5917, GNorm = 0.2189
Meta loss on this task batch = 2.4988e-01, Meta loss averaged over last 500 steps = 2.5574e-01, PNorm = 179.5964, GNorm = 0.2121
Meta loss on this task batch = 2.3117e-01, Meta loss averaged over last 500 steps = 2.5578e-01, PNorm = 179.6017, GNorm = 0.1990
Meta loss on this task batch = 2.3283e-01, Meta loss averaged over last 500 steps = 2.5571e-01, PNorm = 179.6067, GNorm = 0.2929
Meta loss on this task batch = 2.8730e-01, Meta loss averaged over last 500 steps = 2.5567e-01, PNorm = 179.6121, GNorm = 0.2947
Took 100.4957685470581 seconds to complete one epoch of meta training
Took 107.35695028305054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.477289
Epoch 946
Meta loss on this task batch = 2.4119e-01, Meta loss averaged over last 500 steps = 2.5565e-01, PNorm = 179.6166, GNorm = 0.3290
Meta loss on this task batch = 2.6687e-01, Meta loss averaged over last 500 steps = 2.5564e-01, PNorm = 179.6211, GNorm = 0.2238
Meta loss on this task batch = 2.8297e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 179.6257, GNorm = 0.2032
Meta loss on this task batch = 2.6162e-01, Meta loss averaged over last 500 steps = 2.5555e-01, PNorm = 179.6304, GNorm = 0.2985
Meta loss on this task batch = 3.1537e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 179.6355, GNorm = 0.2482
Meta loss on this task batch = 2.3136e-01, Meta loss averaged over last 500 steps = 2.5562e-01, PNorm = 179.6407, GNorm = 0.2050
Meta loss on this task batch = 2.9062e-01, Meta loss averaged over last 500 steps = 2.5573e-01, PNorm = 179.6455, GNorm = 0.2390
Meta loss on this task batch = 2.0636e-01, Meta loss averaged over last 500 steps = 2.5565e-01, PNorm = 179.6509, GNorm = 0.2062
Meta loss on this task batch = 2.2955e-01, Meta loss averaged over last 500 steps = 2.5562e-01, PNorm = 179.6563, GNorm = 0.2581
Meta loss on this task batch = 2.6793e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 179.6591, GNorm = 0.3142
Meta loss on this task batch = 2.7872e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 179.6623, GNorm = 0.2386
Meta loss on this task batch = 2.8657e-01, Meta loss averaged over last 500 steps = 2.5588e-01, PNorm = 179.6653, GNorm = 0.2281
Meta loss on this task batch = 2.5075e-01, Meta loss averaged over last 500 steps = 2.5591e-01, PNorm = 179.6690, GNorm = 0.2220
Meta loss on this task batch = 2.3206e-01, Meta loss averaged over last 500 steps = 2.5596e-01, PNorm = 179.6730, GNorm = 0.1981
Meta loss on this task batch = 2.9512e-01, Meta loss averaged over last 500 steps = 2.5588e-01, PNorm = 179.6773, GNorm = 0.2625
Meta loss on this task batch = 2.6475e-01, Meta loss averaged over last 500 steps = 2.5590e-01, PNorm = 179.6817, GNorm = 0.2349
Meta loss on this task batch = 2.3881e-01, Meta loss averaged over last 500 steps = 2.5587e-01, PNorm = 179.6862, GNorm = 0.2134
Meta loss on this task batch = 2.0552e-01, Meta loss averaged over last 500 steps = 2.5585e-01, PNorm = 179.6907, GNorm = 0.1862
Meta loss on this task batch = 2.2320e-01, Meta loss averaged over last 500 steps = 2.5582e-01, PNorm = 179.6947, GNorm = 0.2836
Took 97.28732705116272 seconds to complete one epoch of meta training
Took 104.22577667236328 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509780
Epoch 947
Meta loss on this task batch = 2.4429e-01, Meta loss averaged over last 500 steps = 2.5576e-01, PNorm = 179.6986, GNorm = 0.2291
Meta loss on this task batch = 2.2322e-01, Meta loss averaged over last 500 steps = 2.5571e-01, PNorm = 179.7022, GNorm = 0.2321
Meta loss on this task batch = 2.4388e-01, Meta loss averaged over last 500 steps = 2.5568e-01, PNorm = 179.7064, GNorm = 0.2746
Meta loss on this task batch = 2.5836e-01, Meta loss averaged over last 500 steps = 2.5557e-01, PNorm = 179.7105, GNorm = 0.2273
Meta loss on this task batch = 2.3263e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 179.7150, GNorm = 0.2111
Meta loss on this task batch = 2.0080e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 179.7193, GNorm = 0.1796
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 179.7234, GNorm = 0.2691
Meta loss on this task batch = 2.5121e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 179.7271, GNorm = 0.2347
Meta loss on this task batch = 2.7964e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 179.7303, GNorm = 0.2726
Meta loss on this task batch = 2.3186e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 179.7335, GNorm = 0.2028
Meta loss on this task batch = 2.5950e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 179.7366, GNorm = 0.2508
Meta loss on this task batch = 2.9301e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 179.7396, GNorm = 0.2631
Meta loss on this task batch = 2.5185e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 179.7427, GNorm = 0.2364
Meta loss on this task batch = 2.1762e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 179.7449, GNorm = 0.2455
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 2.5560e-01, PNorm = 179.7474, GNorm = 0.2402
Meta loss on this task batch = 2.9849e-01, Meta loss averaged over last 500 steps = 2.5569e-01, PNorm = 179.7500, GNorm = 0.2477
Meta loss on this task batch = 2.5590e-01, Meta loss averaged over last 500 steps = 2.5572e-01, PNorm = 179.7520, GNorm = 0.2225
Meta loss on this task batch = 2.8235e-01, Meta loss averaged over last 500 steps = 2.5577e-01, PNorm = 179.7544, GNorm = 0.2485
Meta loss on this task batch = 2.6366e-01, Meta loss averaged over last 500 steps = 2.5578e-01, PNorm = 179.7563, GNorm = 0.2821
Took 98.6973443031311 seconds to complete one epoch of meta training
Took 105.60056018829346 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.474978
Epoch 948
Meta loss on this task batch = 3.0593e-01, Meta loss averaged over last 500 steps = 2.5575e-01, PNorm = 179.7588, GNorm = 0.2430
Meta loss on this task batch = 2.0530e-01, Meta loss averaged over last 500 steps = 2.5558e-01, PNorm = 179.7613, GNorm = 0.2053
Meta loss on this task batch = 2.3713e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 179.7647, GNorm = 0.2156
Meta loss on this task batch = 2.3147e-01, Meta loss averaged over last 500 steps = 2.5549e-01, PNorm = 179.7683, GNorm = 0.2175
Meta loss on this task batch = 2.5729e-01, Meta loss averaged over last 500 steps = 2.5545e-01, PNorm = 179.7723, GNorm = 0.2139
Meta loss on this task batch = 2.2914e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 179.7768, GNorm = 0.2673
Meta loss on this task batch = 2.2193e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 179.7816, GNorm = 0.2195
Meta loss on this task batch = 2.3229e-01, Meta loss averaged over last 500 steps = 2.5532e-01, PNorm = 179.7862, GNorm = 0.2358
Meta loss on this task batch = 2.8860e-01, Meta loss averaged over last 500 steps = 2.5544e-01, PNorm = 179.7907, GNorm = 0.3094
Meta loss on this task batch = 2.4235e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 179.7951, GNorm = 0.2282
Meta loss on this task batch = 2.6794e-01, Meta loss averaged over last 500 steps = 2.5541e-01, PNorm = 179.7981, GNorm = 0.2860
Meta loss on this task batch = 2.6824e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 179.8016, GNorm = 0.2355
Meta loss on this task batch = 2.2012e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 179.8050, GNorm = 0.2301
Meta loss on this task batch = 2.7327e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 179.8086, GNorm = 0.2619
Meta loss on this task batch = 2.2427e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 179.8123, GNorm = 0.1909
Meta loss on this task batch = 2.4009e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 179.8152, GNorm = 0.2381
Meta loss on this task batch = 2.2187e-01, Meta loss averaged over last 500 steps = 2.5491e-01, PNorm = 179.8182, GNorm = 0.1848
Meta loss on this task batch = 2.7741e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 179.8212, GNorm = 0.2646
Meta loss on this task batch = 2.5290e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 179.8242, GNorm = 0.2946
Took 99.03280782699585 seconds to complete one epoch of meta training
Took 105.95081686973572 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504222
Epoch 949
Meta loss on this task batch = 2.0038e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 179.8277, GNorm = 0.2122
Meta loss on this task batch = 2.6138e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 179.8316, GNorm = 0.2156
Meta loss on this task batch = 2.5871e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 179.8362, GNorm = 0.2254
Meta loss on this task batch = 2.6307e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 179.8406, GNorm = 0.2223
Meta loss on this task batch = 2.5507e-01, Meta loss averaged over last 500 steps = 2.5491e-01, PNorm = 179.8451, GNorm = 0.2655
Meta loss on this task batch = 2.4506e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 179.8504, GNorm = 0.2726
Meta loss on this task batch = 2.7738e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 179.8564, GNorm = 0.2491
Meta loss on this task batch = 2.4333e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 179.8621, GNorm = 0.2146
Meta loss on this task batch = 2.7892e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.8678, GNorm = 0.2356
Meta loss on this task batch = 2.1620e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 179.8742, GNorm = 0.2305
Meta loss on this task batch = 2.3016e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.8805, GNorm = 0.2204
Meta loss on this task batch = 2.2819e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 179.8863, GNorm = 0.2474
Meta loss on this task batch = 2.9985e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 179.8918, GNorm = 0.2580
Meta loss on this task batch = 2.8639e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 179.8967, GNorm = 0.2472
Meta loss on this task batch = 2.4729e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 179.9012, GNorm = 0.2204
Meta loss on this task batch = 1.7998e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 179.9060, GNorm = 0.1790
Meta loss on this task batch = 2.4455e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 179.9110, GNorm = 0.2614
Meta loss on this task batch = 2.3514e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 179.9160, GNorm = 0.2203
Meta loss on this task batch = 2.9159e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.9208, GNorm = 0.2720
Took 103.1520459651947 seconds to complete one epoch of meta training
Took 110.44786381721497 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501346
Epoch 950
Meta loss on this task batch = 2.7369e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 179.9253, GNorm = 0.2202
Meta loss on this task batch = 2.3346e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.9300, GNorm = 0.2448
Meta loss on this task batch = 2.4748e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 179.9344, GNorm = 0.2541
Meta loss on this task batch = 2.9619e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 179.9381, GNorm = 0.2547
Meta loss on this task batch = 2.2316e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 179.9425, GNorm = 0.2063
Meta loss on this task batch = 2.6842e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 179.9467, GNorm = 0.2732
Meta loss on this task batch = 2.7693e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 179.9508, GNorm = 0.2536
Meta loss on this task batch = 2.7967e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 179.9551, GNorm = 0.2564
Meta loss on this task batch = 2.0856e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 179.9590, GNorm = 0.2509
Meta loss on this task batch = 2.8517e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 179.9622, GNorm = 0.2277
Meta loss on this task batch = 2.3409e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 179.9656, GNorm = 0.2457
Meta loss on this task batch = 2.6300e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 179.9694, GNorm = 0.2431
Meta loss on this task batch = 3.2763e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.9731, GNorm = 0.3179
Meta loss on this task batch = 2.4019e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 179.9776, GNorm = 0.2631
Meta loss on this task batch = 2.5024e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 179.9815, GNorm = 0.2925
Meta loss on this task batch = 2.5509e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 179.9859, GNorm = 0.2253
Meta loss on this task batch = 2.4420e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 179.9905, GNorm = 0.2384
Meta loss on this task batch = 2.4987e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 179.9951, GNorm = 0.2236
Meta loss on this task batch = 2.2222e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.0000, GNorm = 0.2668
Took 105.6600272655487 seconds to complete one epoch of meta training
Took 113.31791734695435 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502319
Epoch 951
Meta loss on this task batch = 2.7952e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.0049, GNorm = 0.2555
Meta loss on this task batch = 2.2692e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 180.0086, GNorm = 0.2717
Meta loss on this task batch = 2.3337e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.0125, GNorm = 0.2280
Meta loss on this task batch = 2.7303e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 180.0166, GNorm = 0.2270
Meta loss on this task batch = 2.7654e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.0201, GNorm = 0.2348
Meta loss on this task batch = 2.3516e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 180.0236, GNorm = 0.2432
Meta loss on this task batch = 2.6385e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.0280, GNorm = 0.1783
Meta loss on this task batch = 2.8633e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 180.0319, GNorm = 0.2720
Meta loss on this task batch = 2.3549e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 180.0362, GNorm = 0.1878
Meta loss on this task batch = 2.7864e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 180.0404, GNorm = 0.2303
Meta loss on this task batch = 2.6111e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 180.0448, GNorm = 0.2788
Meta loss on this task batch = 2.5942e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 180.0489, GNorm = 0.2474
Meta loss on this task batch = 2.8302e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 180.0523, GNorm = 0.2457
Meta loss on this task batch = 2.3467e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.0562, GNorm = 0.2179
Meta loss on this task batch = 3.0285e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.0592, GNorm = 0.2858
Meta loss on this task batch = 2.4903e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 180.0629, GNorm = 0.2528
Meta loss on this task batch = 2.7612e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.0667, GNorm = 0.2247
Meta loss on this task batch = 2.3081e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 180.0711, GNorm = 0.2174
Meta loss on this task batch = 1.5756e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.0751, GNorm = 0.2326
Took 109.40026521682739 seconds to complete one epoch of meta training
Took 116.68735957145691 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488273
Epoch 952
Meta loss on this task batch = 2.4267e-01, Meta loss averaged over last 500 steps = 2.5479e-01, PNorm = 180.0797, GNorm = 0.2539
Meta loss on this task batch = 2.4779e-01, Meta loss averaged over last 500 steps = 2.5478e-01, PNorm = 180.0835, GNorm = 0.3109
Meta loss on this task batch = 2.6236e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 180.0876, GNorm = 0.2178
Meta loss on this task batch = 2.2614e-01, Meta loss averaged over last 500 steps = 2.5492e-01, PNorm = 180.0915, GNorm = 0.2108
Meta loss on this task batch = 2.3314e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 180.0958, GNorm = 0.2264
Meta loss on this task batch = 2.5482e-01, Meta loss averaged over last 500 steps = 2.5479e-01, PNorm = 180.0997, GNorm = 0.2579
Meta loss on this task batch = 2.6362e-01, Meta loss averaged over last 500 steps = 2.5477e-01, PNorm = 180.1027, GNorm = 0.2753
Meta loss on this task batch = 2.8178e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 180.1061, GNorm = 0.2728
Meta loss on this task batch = 2.4166e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.1096, GNorm = 0.2359
Meta loss on this task batch = 2.5320e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 180.1129, GNorm = 0.2412
Meta loss on this task batch = 3.0125e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 180.1160, GNorm = 0.2467
Meta loss on this task batch = 2.6194e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 180.1193, GNorm = 0.2047
Meta loss on this task batch = 2.6367e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 180.1217, GNorm = 0.2500
Meta loss on this task batch = 2.4399e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 180.1243, GNorm = 0.2211
Meta loss on this task batch = 2.9938e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 180.1270, GNorm = 0.2239
Meta loss on this task batch = 2.4196e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.1304, GNorm = 0.2156
Meta loss on this task batch = 2.8314e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 180.1334, GNorm = 0.2557
Meta loss on this task batch = 2.4695e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 180.1370, GNorm = 0.2367
Meta loss on this task batch = 2.9437e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 180.1405, GNorm = 0.2778
Took 108.13929557800293 seconds to complete one epoch of meta training
Took 115.53876948356628 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483545
Epoch 953
Meta loss on this task batch = 2.4338e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.1443, GNorm = 0.2026
Meta loss on this task batch = 2.9286e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 180.1479, GNorm = 0.2852
Meta loss on this task batch = 2.2143e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 180.1523, GNorm = 0.2436
Meta loss on this task batch = 2.3607e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 180.1574, GNorm = 0.2432
Meta loss on this task batch = 2.6878e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.1627, GNorm = 0.2610
Meta loss on this task batch = 2.1237e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.1679, GNorm = 0.2122
Meta loss on this task batch = 3.3297e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 180.1724, GNorm = 0.2760
Meta loss on this task batch = 2.3367e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 180.1771, GNorm = 0.2003
Meta loss on this task batch = 2.3583e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.1815, GNorm = 0.2057
Meta loss on this task batch = 2.7541e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 180.1860, GNorm = 0.2107
Meta loss on this task batch = 2.4943e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 180.1901, GNorm = 0.2322
Meta loss on this task batch = 2.6933e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 180.1946, GNorm = 0.2306
Meta loss on this task batch = 1.8174e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.1992, GNorm = 0.1710
Meta loss on this task batch = 2.9568e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.2023, GNorm = 0.3067
Meta loss on this task batch = 2.6083e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 180.2052, GNorm = 0.2923
Meta loss on this task batch = 2.7028e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 180.2080, GNorm = 0.2363
Meta loss on this task batch = 3.2940e-01, Meta loss averaged over last 500 steps = 2.5502e-01, PNorm = 180.2089, GNorm = 0.3056
Meta loss on this task batch = 2.5123e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 180.2102, GNorm = 0.2567
Meta loss on this task batch = 2.2756e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.2121, GNorm = 0.2329
Took 106.70923185348511 seconds to complete one epoch of meta training
Took 114.00738739967346 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.493439
Epoch 954
Meta loss on this task batch = 2.9422e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 180.2137, GNorm = 0.2651
Meta loss on this task batch = 2.4597e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 180.2162, GNorm = 0.2135
Meta loss on this task batch = 2.5176e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 180.2194, GNorm = 0.2113
Meta loss on this task batch = 2.1434e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 180.2227, GNorm = 0.2295
Meta loss on this task batch = 2.9497e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.2263, GNorm = 0.2470
Meta loss on this task batch = 2.7153e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 180.2303, GNorm = 0.2560
Meta loss on this task batch = 2.5287e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 180.2340, GNorm = 0.2834
Meta loss on this task batch = 2.7830e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 180.2374, GNorm = 0.2538
Meta loss on this task batch = 2.7962e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 180.2417, GNorm = 0.2671
Meta loss on this task batch = 1.7616e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 180.2464, GNorm = 0.1743
Meta loss on this task batch = 2.4770e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 180.2514, GNorm = 0.2308
Meta loss on this task batch = 2.7370e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 180.2563, GNorm = 0.2227
Meta loss on this task batch = 2.1834e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 180.2615, GNorm = 0.2249
Meta loss on this task batch = 2.5894e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 180.2659, GNorm = 0.2261
Meta loss on this task batch = 2.9321e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 180.2697, GNorm = 0.2770
Meta loss on this task batch = 2.3735e-01, Meta loss averaged over last 500 steps = 2.5492e-01, PNorm = 180.2737, GNorm = 0.2187
Meta loss on this task batch = 2.9989e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.2769, GNorm = 0.2817
Meta loss on this task batch = 2.9668e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 180.2797, GNorm = 0.3502
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.2828, GNorm = 0.3342
Took 98.959144115448 seconds to complete one epoch of meta training
Took 106.00766134262085 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.544648
Found better MAML checkpoint after meta validation, saving now
Epoch 955
Meta loss on this task batch = 2.3339e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 180.2861, GNorm = 0.2648
Meta loss on this task batch = 2.0944e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 180.2905, GNorm = 0.2377
Meta loss on this task batch = 2.7497e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 180.2951, GNorm = 0.2595
Meta loss on this task batch = 2.1143e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 180.3001, GNorm = 0.1868
Meta loss on this task batch = 2.4739e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.3053, GNorm = 0.2125
Meta loss on this task batch = 2.0924e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 180.3101, GNorm = 0.2220
Meta loss on this task batch = 2.2190e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 180.3151, GNorm = 0.2292
Meta loss on this task batch = 2.1901e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 180.3201, GNorm = 0.2133
Meta loss on this task batch = 2.5439e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 180.3252, GNorm = 0.2082
Meta loss on this task batch = 3.0813e-01, Meta loss averaged over last 500 steps = 2.5491e-01, PNorm = 180.3303, GNorm = 0.2973
Meta loss on this task batch = 2.6365e-01, Meta loss averaged over last 500 steps = 2.5484e-01, PNorm = 180.3354, GNorm = 0.2307
Meta loss on this task batch = 2.5756e-01, Meta loss averaged over last 500 steps = 2.5473e-01, PNorm = 180.3409, GNorm = 0.3288
Meta loss on this task batch = 3.4152e-01, Meta loss averaged over last 500 steps = 2.5489e-01, PNorm = 180.3451, GNorm = 0.3286
Meta loss on this task batch = 2.6172e-01, Meta loss averaged over last 500 steps = 2.5475e-01, PNorm = 180.3478, GNorm = 0.3015
Meta loss on this task batch = 2.6678e-01, Meta loss averaged over last 500 steps = 2.5479e-01, PNorm = 180.3502, GNorm = 0.2337
Meta loss on this task batch = 2.6964e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.3523, GNorm = 0.2212
Meta loss on this task batch = 1.9840e-01, Meta loss averaged over last 500 steps = 2.5490e-01, PNorm = 180.3552, GNorm = 0.2271
Meta loss on this task batch = 2.9097e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.3586, GNorm = 0.2333
Meta loss on this task batch = 2.6256e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.3620, GNorm = 0.2870
Took 100.3628261089325 seconds to complete one epoch of meta training
Took 107.29625177383423 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502984
Epoch 956
Meta loss on this task batch = 2.7148e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 180.3660, GNorm = 0.1987
Meta loss on this task batch = 2.2752e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 180.3704, GNorm = 0.2061
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.5501e-01, PNorm = 180.3744, GNorm = 0.2411
Meta loss on this task batch = 2.3370e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 180.3786, GNorm = 0.2216
Meta loss on this task batch = 2.7630e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 180.3829, GNorm = 0.2372
Meta loss on this task batch = 2.6930e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.3875, GNorm = 0.2431
Meta loss on this task batch = 2.5935e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 180.3921, GNorm = 0.2282
Meta loss on this task batch = 2.7819e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 180.3972, GNorm = 0.2217
Meta loss on this task batch = 2.3531e-01, Meta loss averaged over last 500 steps = 2.5524e-01, PNorm = 180.4014, GNorm = 0.2743
Meta loss on this task batch = 2.3127e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.4059, GNorm = 0.2318
Meta loss on this task batch = 2.4677e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.4106, GNorm = 0.2180
Meta loss on this task batch = 2.8357e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 180.4156, GNorm = 0.3126
Meta loss on this task batch = 2.8158e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.4198, GNorm = 0.2661
Meta loss on this task batch = 2.0351e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 180.4243, GNorm = 0.2157
Meta loss on this task batch = 3.3542e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 180.4275, GNorm = 0.2829
Meta loss on this task batch = 2.6886e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 180.4311, GNorm = 0.2604
Meta loss on this task batch = 2.8798e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 180.4348, GNorm = 0.2366
Meta loss on this task batch = 2.1816e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 180.4388, GNorm = 0.2301
Meta loss on this task batch = 2.7259e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 180.4432, GNorm = 0.2793
Took 98.09579372406006 seconds to complete one epoch of meta training
Took 105.06464862823486 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489661
Epoch 957
Meta loss on this task batch = 2.5584e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 180.4482, GNorm = 0.2045
Meta loss on this task batch = 3.0392e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 180.4523, GNorm = 0.2430
Meta loss on this task batch = 2.2166e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 180.4560, GNorm = 0.2243
Meta loss on this task batch = 2.1730e-01, Meta loss averaged over last 500 steps = 2.5535e-01, PNorm = 180.4601, GNorm = 0.2309
Meta loss on this task batch = 2.7431e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 180.4640, GNorm = 0.2720
Meta loss on this task batch = 2.4180e-01, Meta loss averaged over last 500 steps = 2.5540e-01, PNorm = 180.4681, GNorm = 0.1931
Meta loss on this task batch = 2.1394e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 180.4732, GNorm = 0.2093
Meta loss on this task batch = 2.4271e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 180.4788, GNorm = 0.2160
Meta loss on this task batch = 2.8436e-01, Meta loss averaged over last 500 steps = 2.5534e-01, PNorm = 180.4841, GNorm = 0.2644
Meta loss on this task batch = 2.3274e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 180.4893, GNorm = 0.2204
Meta loss on this task batch = 2.5352e-01, Meta loss averaged over last 500 steps = 2.5538e-01, PNorm = 180.4944, GNorm = 0.2364
Meta loss on this task batch = 2.4923e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 180.4993, GNorm = 0.2306
Meta loss on this task batch = 1.9946e-01, Meta loss averaged over last 500 steps = 2.5539e-01, PNorm = 180.5044, GNorm = 0.2039
Meta loss on this task batch = 2.8760e-01, Meta loss averaged over last 500 steps = 2.5552e-01, PNorm = 180.5094, GNorm = 0.2275
Meta loss on this task batch = 2.8659e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 180.5140, GNorm = 0.2225
Meta loss on this task batch = 3.2204e-01, Meta loss averaged over last 500 steps = 2.5574e-01, PNorm = 180.5182, GNorm = 0.2785
Meta loss on this task batch = 2.5392e-01, Meta loss averaged over last 500 steps = 2.5570e-01, PNorm = 180.5224, GNorm = 0.2223
Meta loss on this task batch = 2.7717e-01, Meta loss averaged over last 500 steps = 2.5579e-01, PNorm = 180.5265, GNorm = 0.2340
Meta loss on this task batch = 2.3429e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 180.5309, GNorm = 0.2532
Took 103.09642434120178 seconds to complete one epoch of meta training
Took 110.12869668006897 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488525
Epoch 958
Meta loss on this task batch = 2.4687e-01, Meta loss averaged over last 500 steps = 2.5559e-01, PNorm = 180.5347, GNorm = 0.2445
Meta loss on this task batch = 2.5649e-01, Meta loss averaged over last 500 steps = 2.5556e-01, PNorm = 180.5384, GNorm = 0.2220
Meta loss on this task batch = 2.5975e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 180.5426, GNorm = 0.2216
Meta loss on this task batch = 2.6736e-01, Meta loss averaged over last 500 steps = 2.5551e-01, PNorm = 180.5470, GNorm = 0.2256
Meta loss on this task batch = 3.1628e-01, Meta loss averaged over last 500 steps = 2.5564e-01, PNorm = 180.5515, GNorm = 0.2980
Meta loss on this task batch = 2.6377e-01, Meta loss averaged over last 500 steps = 2.5561e-01, PNorm = 180.5561, GNorm = 0.2346
Meta loss on this task batch = 2.2876e-01, Meta loss averaged over last 500 steps = 2.5552e-01, PNorm = 180.5604, GNorm = 0.2393
Meta loss on this task batch = 2.5998e-01, Meta loss averaged over last 500 steps = 2.5548e-01, PNorm = 180.5650, GNorm = 0.2139
Meta loss on this task batch = 2.2088e-01, Meta loss averaged over last 500 steps = 2.5531e-01, PNorm = 180.5699, GNorm = 0.2343
Meta loss on this task batch = 2.4694e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 180.5752, GNorm = 0.2190
Meta loss on this task batch = 2.2981e-01, Meta loss averaged over last 500 steps = 2.5519e-01, PNorm = 180.5806, GNorm = 0.2188
Meta loss on this task batch = 2.7362e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 180.5855, GNorm = 0.2304
Meta loss on this task batch = 2.2413e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 180.5907, GNorm = 0.1772
Meta loss on this task batch = 2.4038e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 180.5955, GNorm = 0.2339
Meta loss on this task batch = 3.1485e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 180.6002, GNorm = 0.2878
Meta loss on this task batch = 2.3699e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 180.6044, GNorm = 0.2200
Meta loss on this task batch = 2.5815e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.6088, GNorm = 0.2225
Meta loss on this task batch = 2.5859e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 180.6120, GNorm = 0.2513
Meta loss on this task batch = 2.6743e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 180.6160, GNorm = 0.3031
Took 104.6942503452301 seconds to complete one epoch of meta training
Took 111.70157814025879 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.505634
Epoch 959
Meta loss on this task batch = 2.6098e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 180.6197, GNorm = 0.2161
Meta loss on this task batch = 2.7219e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 180.6226, GNorm = 0.2582
Meta loss on this task batch = 2.8381e-01, Meta loss averaged over last 500 steps = 2.5537e-01, PNorm = 180.6255, GNorm = 0.2592
Meta loss on this task batch = 2.7732e-01, Meta loss averaged over last 500 steps = 2.5553e-01, PNorm = 180.6285, GNorm = 0.2297
Meta loss on this task batch = 1.9139e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 180.6321, GNorm = 0.2271
Meta loss on this task batch = 3.0562e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 180.6360, GNorm = 0.2236
Meta loss on this task batch = 2.4500e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 180.6406, GNorm = 0.2179
Meta loss on this task batch = 2.5442e-01, Meta loss averaged over last 500 steps = 2.5543e-01, PNorm = 180.6453, GNorm = 0.2102
Meta loss on this task batch = 2.0470e-01, Meta loss averaged over last 500 steps = 2.5530e-01, PNorm = 180.6497, GNorm = 0.2294
Meta loss on this task batch = 2.2631e-01, Meta loss averaged over last 500 steps = 2.5527e-01, PNorm = 180.6548, GNorm = 0.2333
Meta loss on this task batch = 2.9309e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 180.6593, GNorm = 0.2175
Meta loss on this task batch = 2.5910e-01, Meta loss averaged over last 500 steps = 2.5546e-01, PNorm = 180.6637, GNorm = 0.2471
Meta loss on this task batch = 2.3066e-01, Meta loss averaged over last 500 steps = 2.5529e-01, PNorm = 180.6675, GNorm = 0.2440
Meta loss on this task batch = 2.2103e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 180.6717, GNorm = 0.1866
Meta loss on this task batch = 2.6752e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 180.6760, GNorm = 0.2216
Meta loss on this task batch = 2.0506e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 180.6807, GNorm = 0.1868
Meta loss on this task batch = 2.3048e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 180.6854, GNorm = 0.1971
Meta loss on this task batch = 2.4891e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 180.6890, GNorm = 0.2669
Meta loss on this task batch = 3.0171e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 180.6919, GNorm = 0.3206
Took 101.88434362411499 seconds to complete one epoch of meta training
Took 108.92709946632385 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.504544
Epoch 960
Meta loss on this task batch = 2.9693e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 180.6950, GNorm = 0.2552
Meta loss on this task batch = 2.3735e-01, Meta loss averaged over last 500 steps = 2.5525e-01, PNorm = 180.6980, GNorm = 0.2054
Meta loss on this task batch = 2.8737e-01, Meta loss averaged over last 500 steps = 2.5536e-01, PNorm = 180.7016, GNorm = 0.3416
Meta loss on this task batch = 2.5511e-01, Meta loss averaged over last 500 steps = 2.5533e-01, PNorm = 180.7056, GNorm = 0.2538
Meta loss on this task batch = 3.0759e-01, Meta loss averaged over last 500 steps = 2.5542e-01, PNorm = 180.7098, GNorm = 0.2645
Meta loss on this task batch = 2.1808e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 180.7143, GNorm = 0.2563
Meta loss on this task batch = 2.4066e-01, Meta loss averaged over last 500 steps = 2.5523e-01, PNorm = 180.7189, GNorm = 0.1858
Meta loss on this task batch = 2.3836e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 180.7239, GNorm = 0.2172
Meta loss on this task batch = 2.4276e-01, Meta loss averaged over last 500 steps = 2.5511e-01, PNorm = 180.7281, GNorm = 0.2298
Meta loss on this task batch = 2.6089e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 180.7321, GNorm = 0.2258
Meta loss on this task batch = 2.1275e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.7360, GNorm = 0.2073
Meta loss on this task batch = 2.6253e-01, Meta loss averaged over last 500 steps = 2.5509e-01, PNorm = 180.7397, GNorm = 0.2143
Meta loss on this task batch = 2.4971e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 180.7438, GNorm = 0.2311
Meta loss on this task batch = 2.7756e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 180.7477, GNorm = 0.2315
Meta loss on this task batch = 2.5233e-01, Meta loss averaged over last 500 steps = 2.5520e-01, PNorm = 180.7515, GNorm = 0.2526
Meta loss on this task batch = 2.3981e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.7547, GNorm = 0.2112
Meta loss on this task batch = 2.4685e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 180.7570, GNorm = 0.2565
Meta loss on this task batch = 2.5451e-01, Meta loss averaged over last 500 steps = 2.5515e-01, PNorm = 180.7596, GNorm = 0.2671
Meta loss on this task batch = 2.1101e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 180.7625, GNorm = 0.2563
Took 107.06206202507019 seconds to complete one epoch of meta training
Took 114.70981121063232 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.526249
Epoch 961
Meta loss on this task batch = 2.4977e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 180.7660, GNorm = 0.2121
Meta loss on this task batch = 2.0033e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.7701, GNorm = 0.2359
Meta loss on this task batch = 2.8655e-01, Meta loss averaged over last 500 steps = 2.5503e-01, PNorm = 180.7744, GNorm = 0.2376
Meta loss on this task batch = 2.2912e-01, Meta loss averaged over last 500 steps = 2.5495e-01, PNorm = 180.7781, GNorm = 0.2184
Meta loss on this task batch = 2.3151e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 180.7822, GNorm = 0.2165
Meta loss on this task batch = 2.5155e-01, Meta loss averaged over last 500 steps = 2.5481e-01, PNorm = 180.7863, GNorm = 0.2397
Meta loss on this task batch = 3.1942e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 180.7903, GNorm = 0.2592
Meta loss on this task batch = 2.0383e-01, Meta loss averaged over last 500 steps = 2.5473e-01, PNorm = 180.7939, GNorm = 0.2149
Meta loss on this task batch = 2.6919e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 180.7970, GNorm = 0.2454
Meta loss on this task batch = 2.7487e-01, Meta loss averaged over last 500 steps = 2.5475e-01, PNorm = 180.8005, GNorm = 0.2345
Meta loss on this task batch = 2.5763e-01, Meta loss averaged over last 500 steps = 2.5469e-01, PNorm = 180.8037, GNorm = 0.2323
Meta loss on this task batch = 2.4921e-01, Meta loss averaged over last 500 steps = 2.5458e-01, PNorm = 180.8061, GNorm = 0.2752
Meta loss on this task batch = 2.7968e-01, Meta loss averaged over last 500 steps = 2.5458e-01, PNorm = 180.8091, GNorm = 0.2377
Meta loss on this task batch = 2.0952e-01, Meta loss averaged over last 500 steps = 2.5453e-01, PNorm = 180.8120, GNorm = 0.1831
Meta loss on this task batch = 2.8430e-01, Meta loss averaged over last 500 steps = 2.5452e-01, PNorm = 180.8147, GNorm = 0.2399
Meta loss on this task batch = 3.1055e-01, Meta loss averaged over last 500 steps = 2.5465e-01, PNorm = 180.8173, GNorm = 0.2976
Meta loss on this task batch = 2.6428e-01, Meta loss averaged over last 500 steps = 2.5475e-01, PNorm = 180.8190, GNorm = 0.2939
Meta loss on this task batch = 2.5711e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.8214, GNorm = 0.2360
Meta loss on this task batch = 2.2860e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 180.8240, GNorm = 0.2515
Took 107.45863938331604 seconds to complete one epoch of meta training
Took 114.78359007835388 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.509181
Epoch 962
Meta loss on this task batch = 2.7464e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 180.8268, GNorm = 0.2129
Meta loss on this task batch = 2.5601e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.8300, GNorm = 0.2240
Meta loss on this task batch = 2.4946e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.8338, GNorm = 0.2077
Meta loss on this task batch = 2.8947e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.8375, GNorm = 0.2080
Meta loss on this task batch = 2.6628e-01, Meta loss averaged over last 500 steps = 2.5498e-01, PNorm = 180.8412, GNorm = 0.2327
Meta loss on this task batch = 2.2828e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.8451, GNorm = 0.2199
Meta loss on this task batch = 2.1758e-01, Meta loss averaged over last 500 steps = 2.5482e-01, PNorm = 180.8495, GNorm = 0.2059
Meta loss on this task batch = 2.3260e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 180.8535, GNorm = 0.2087
Meta loss on this task batch = 2.3882e-01, Meta loss averaged over last 500 steps = 2.5490e-01, PNorm = 180.8577, GNorm = 0.2320
Meta loss on this task batch = 2.4923e-01, Meta loss averaged over last 500 steps = 2.5484e-01, PNorm = 180.8621, GNorm = 0.2278
Meta loss on this task batch = 2.3934e-01, Meta loss averaged over last 500 steps = 2.5484e-01, PNorm = 180.8670, GNorm = 0.2174
Meta loss on this task batch = 2.1328e-01, Meta loss averaged over last 500 steps = 2.5478e-01, PNorm = 180.8716, GNorm = 0.2541
Meta loss on this task batch = 2.5231e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 180.8769, GNorm = 0.2656
Meta loss on this task batch = 2.8642e-01, Meta loss averaged over last 500 steps = 2.5484e-01, PNorm = 180.8814, GNorm = 0.2621
Meta loss on this task batch = 2.2768e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 180.8855, GNorm = 0.2152
Meta loss on this task batch = 2.6487e-01, Meta loss averaged over last 500 steps = 2.5468e-01, PNorm = 180.8894, GNorm = 0.2412
Meta loss on this task batch = 2.4898e-01, Meta loss averaged over last 500 steps = 2.5471e-01, PNorm = 180.8929, GNorm = 0.2670
Meta loss on this task batch = 2.6302e-01, Meta loss averaged over last 500 steps = 2.5470e-01, PNorm = 180.8962, GNorm = 0.2612
Meta loss on this task batch = 2.5321e-01, Meta loss averaged over last 500 steps = 2.5478e-01, PNorm = 180.8993, GNorm = 0.2400
Took 105.09298658370972 seconds to complete one epoch of meta training
Took 113.03771138191223 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.515403
Epoch 963
Meta loss on this task batch = 2.7234e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 180.9018, GNorm = 0.2658
Meta loss on this task batch = 3.1448e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 180.9046, GNorm = 0.2428
Meta loss on this task batch = 2.8997e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 180.9069, GNorm = 0.2273
Meta loss on this task batch = 2.0685e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 180.9099, GNorm = 0.1735
Meta loss on this task batch = 2.3615e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.9123, GNorm = 0.2404
Meta loss on this task batch = 3.1302e-01, Meta loss averaged over last 500 steps = 2.5499e-01, PNorm = 180.9146, GNorm = 0.2570
Meta loss on this task batch = 3.0281e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 180.9166, GNorm = 0.2634
Meta loss on this task batch = 2.4311e-01, Meta loss averaged over last 500 steps = 2.5513e-01, PNorm = 180.9191, GNorm = 0.2298
Meta loss on this task batch = 1.9568e-01, Meta loss averaged over last 500 steps = 2.5506e-01, PNorm = 180.9220, GNorm = 0.2340
Meta loss on this task batch = 2.7137e-01, Meta loss averaged over last 500 steps = 2.5516e-01, PNorm = 180.9249, GNorm = 0.2202
Meta loss on this task batch = 2.6276e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 180.9277, GNorm = 0.2295
Meta loss on this task batch = 2.5954e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.9303, GNorm = 0.2482
Meta loss on this task batch = 2.6243e-01, Meta loss averaged over last 500 steps = 2.5528e-01, PNorm = 180.9331, GNorm = 0.2408
Meta loss on this task batch = 2.1460e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 180.9362, GNorm = 0.2184
Meta loss on this task batch = 2.4579e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 180.9398, GNorm = 0.2148
Meta loss on this task batch = 2.7880e-01, Meta loss averaged over last 500 steps = 2.5504e-01, PNorm = 180.9434, GNorm = 0.2606
Meta loss on this task batch = 1.9857e-01, Meta loss averaged over last 500 steps = 2.5494e-01, PNorm = 180.9477, GNorm = 0.2016
Meta loss on this task batch = 2.6686e-01, Meta loss averaged over last 500 steps = 2.5492e-01, PNorm = 180.9519, GNorm = 0.2477
Meta loss on this task batch = 2.4612e-01, Meta loss averaged over last 500 steps = 2.5483e-01, PNorm = 180.9558, GNorm = 0.2329
Took 108.70858979225159 seconds to complete one epoch of meta training
Took 115.0763692855835 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506149
Epoch 964
Meta loss on this task batch = 2.4734e-01, Meta loss averaged over last 500 steps = 2.5483e-01, PNorm = 180.9596, GNorm = 0.2274
Meta loss on this task batch = 2.5881e-01, Meta loss averaged over last 500 steps = 2.5488e-01, PNorm = 180.9641, GNorm = 0.1979
Meta loss on this task batch = 2.2473e-01, Meta loss averaged over last 500 steps = 2.5482e-01, PNorm = 180.9689, GNorm = 0.1861
Meta loss on this task batch = 2.3398e-01, Meta loss averaged over last 500 steps = 2.5482e-01, PNorm = 180.9740, GNorm = 0.2276
Meta loss on this task batch = 3.0666e-01, Meta loss averaged over last 500 steps = 2.5497e-01, PNorm = 180.9782, GNorm = 0.2405
Meta loss on this task batch = 2.7883e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 180.9821, GNorm = 0.2346
Meta loss on this task batch = 2.8492e-01, Meta loss averaged over last 500 steps = 2.5514e-01, PNorm = 180.9858, GNorm = 0.2437
Meta loss on this task batch = 2.5909e-01, Meta loss averaged over last 500 steps = 2.5512e-01, PNorm = 180.9894, GNorm = 0.2364
Meta loss on this task batch = 2.5925e-01, Meta loss averaged over last 500 steps = 2.5521e-01, PNorm = 180.9933, GNorm = 0.2666
Meta loss on this task batch = 3.0987e-01, Meta loss averaged over last 500 steps = 2.5526e-01, PNorm = 180.9968, GNorm = 0.2449
Meta loss on this task batch = 2.8610e-01, Meta loss averaged over last 500 steps = 2.5522e-01, PNorm = 180.9999, GNorm = 0.2461
Meta loss on this task batch = 2.3559e-01, Meta loss averaged over last 500 steps = 2.5517e-01, PNorm = 181.0033, GNorm = 0.2262
Meta loss on this task batch = 2.5268e-01, Meta loss averaged over last 500 steps = 2.5510e-01, PNorm = 181.0070, GNorm = 0.2335
Meta loss on this task batch = 2.0251e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 181.0103, GNorm = 0.2237
Meta loss on this task batch = 2.2825e-01, Meta loss averaged over last 500 steps = 2.5491e-01, PNorm = 181.0139, GNorm = 0.2374
Meta loss on this task batch = 2.5256e-01, Meta loss averaged over last 500 steps = 2.5479e-01, PNorm = 181.0179, GNorm = 0.2545
Meta loss on this task batch = 2.8290e-01, Meta loss averaged over last 500 steps = 2.5483e-01, PNorm = 181.0210, GNorm = 0.3100
Meta loss on this task batch = 2.2921e-01, Meta loss averaged over last 500 steps = 2.5484e-01, PNorm = 181.0244, GNorm = 0.2119
Meta loss on this task batch = 1.7154e-01, Meta loss averaged over last 500 steps = 2.5473e-01, PNorm = 181.0280, GNorm = 0.2022
Took 106.4107084274292 seconds to complete one epoch of meta training
Took 113.765700340271 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471853
Epoch 965
Meta loss on this task batch = 3.1673e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 181.0314, GNorm = 0.2981
Meta loss on this task batch = 2.6143e-01, Meta loss averaged over last 500 steps = 2.5489e-01, PNorm = 181.0350, GNorm = 0.2074
Meta loss on this task batch = 2.1030e-01, Meta loss averaged over last 500 steps = 2.5487e-01, PNorm = 181.0389, GNorm = 0.2058
Meta loss on this task batch = 2.1275e-01, Meta loss averaged over last 500 steps = 2.5493e-01, PNorm = 181.0436, GNorm = 0.2284
Meta loss on this task batch = 3.4427e-01, Meta loss averaged over last 500 steps = 2.5518e-01, PNorm = 181.0473, GNorm = 0.2772
Meta loss on this task batch = 2.4250e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 181.0513, GNorm = 0.2287
Meta loss on this task batch = 2.6237e-01, Meta loss averaged over last 500 steps = 2.5508e-01, PNorm = 181.0552, GNorm = 0.2110
Meta loss on this task batch = 2.0163e-01, Meta loss averaged over last 500 steps = 2.5496e-01, PNorm = 181.0594, GNorm = 0.1998
Meta loss on this task batch = 2.5716e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 181.0631, GNorm = 0.2381
Meta loss on this task batch = 1.5097e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 181.0671, GNorm = 0.1860
Meta loss on this task batch = 2.6025e-01, Meta loss averaged over last 500 steps = 2.5476e-01, PNorm = 181.0711, GNorm = 0.2451
Meta loss on this task batch = 2.6437e-01, Meta loss averaged over last 500 steps = 2.5475e-01, PNorm = 181.0752, GNorm = 0.2524
Meta loss on this task batch = 2.7175e-01, Meta loss averaged over last 500 steps = 2.5469e-01, PNorm = 181.0792, GNorm = 0.2603
Meta loss on this task batch = 2.2811e-01, Meta loss averaged over last 500 steps = 2.5468e-01, PNorm = 181.0831, GNorm = 0.2307
Meta loss on this task batch = 3.2430e-01, Meta loss averaged over last 500 steps = 2.5482e-01, PNorm = 181.0861, GNorm = 0.2355
Meta loss on this task batch = 2.2742e-01, Meta loss averaged over last 500 steps = 2.5481e-01, PNorm = 181.0890, GNorm = 0.2276
Meta loss on this task batch = 2.5971e-01, Meta loss averaged over last 500 steps = 2.5489e-01, PNorm = 181.0918, GNorm = 0.2270
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 2.5500e-01, PNorm = 181.0953, GNorm = 0.2587
Meta loss on this task batch = 2.5164e-01, Meta loss averaged over last 500 steps = 2.5507e-01, PNorm = 181.0984, GNorm = 0.2332
Took 108.9129786491394 seconds to complete one epoch of meta training
Took 116.43060374259949 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491895
Epoch 966
Meta loss on this task batch = 2.4516e-01, Meta loss averaged over last 500 steps = 2.5505e-01, PNorm = 181.1017, GNorm = 0.2572
Meta loss on this task batch = 2.0015e-01, Meta loss averaged over last 500 steps = 2.5483e-01, PNorm = 181.1049, GNorm = 0.2077
Meta loss on this task batch = 2.8555e-01, Meta loss averaged over last 500 steps = 2.5486e-01, PNorm = 181.1081, GNorm = 0.2217
Meta loss on this task batch = 2.0949e-01, Meta loss averaged over last 500 steps = 2.5481e-01, PNorm = 181.1121, GNorm = 0.1932
Meta loss on this task batch = 2.4871e-01, Meta loss averaged over last 500 steps = 2.5478e-01, PNorm = 181.1161, GNorm = 0.2284
Meta loss on this task batch = 2.5131e-01, Meta loss averaged over last 500 steps = 2.5480e-01, PNorm = 181.1204, GNorm = 0.2238
Meta loss on this task batch = 2.2100e-01, Meta loss averaged over last 500 steps = 2.5474e-01, PNorm = 181.1244, GNorm = 0.2389
Meta loss on this task batch = 2.2808e-01, Meta loss averaged over last 500 steps = 2.5472e-01, PNorm = 181.1281, GNorm = 0.2215
Meta loss on this task batch = 2.2891e-01, Meta loss averaged over last 500 steps = 2.5464e-01, PNorm = 181.1324, GNorm = 0.2073
Meta loss on this task batch = 2.5983e-01, Meta loss averaged over last 500 steps = 2.5463e-01, PNorm = 181.1360, GNorm = 0.2612
Meta loss on this task batch = 2.2964e-01, Meta loss averaged over last 500 steps = 2.5450e-01, PNorm = 181.1396, GNorm = 0.2228
Meta loss on this task batch = 2.8087e-01, Meta loss averaged over last 500 steps = 2.5444e-01, PNorm = 181.1429, GNorm = 0.2555
Meta loss on this task batch = 2.2823e-01, Meta loss averaged over last 500 steps = 2.5448e-01, PNorm = 181.1461, GNorm = 0.2343
Meta loss on this task batch = 1.8563e-01, Meta loss averaged over last 500 steps = 2.5444e-01, PNorm = 181.1500, GNorm = 0.2125
Meta loss on this task batch = 2.6100e-01, Meta loss averaged over last 500 steps = 2.5450e-01, PNorm = 181.1538, GNorm = 0.2279
Meta loss on this task batch = 2.6282e-01, Meta loss averaged over last 500 steps = 2.5441e-01, PNorm = 181.1577, GNorm = 0.2337
Meta loss on this task batch = 2.8189e-01, Meta loss averaged over last 500 steps = 2.5446e-01, PNorm = 181.1617, GNorm = 0.2324
Meta loss on this task batch = 2.5143e-01, Meta loss averaged over last 500 steps = 2.5444e-01, PNorm = 181.1659, GNorm = 0.2792
Meta loss on this task batch = 3.1468e-01, Meta loss averaged over last 500 steps = 2.5459e-01, PNorm = 181.1695, GNorm = 0.3055
Took 107.31604433059692 seconds to complete one epoch of meta training
Took 114.73082995414734 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488129
Epoch 967
Meta loss on this task batch = 2.2040e-01, Meta loss averaged over last 500 steps = 2.5444e-01, PNorm = 181.1734, GNorm = 0.2563
Meta loss on this task batch = 2.3908e-01, Meta loss averaged over last 500 steps = 2.5436e-01, PNorm = 181.1771, GNorm = 0.2400
Meta loss on this task batch = 2.1578e-01, Meta loss averaged over last 500 steps = 2.5425e-01, PNorm = 181.1815, GNorm = 0.2467
Meta loss on this task batch = 2.3856e-01, Meta loss averaged over last 500 steps = 2.5427e-01, PNorm = 181.1860, GNorm = 0.2327
Meta loss on this task batch = 2.8807e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.1902, GNorm = 0.2437
Meta loss on this task batch = 2.7529e-01, Meta loss averaged over last 500 steps = 2.5437e-01, PNorm = 181.1937, GNorm = 0.2370
Meta loss on this task batch = 2.7415e-01, Meta loss averaged over last 500 steps = 2.5437e-01, PNorm = 181.1976, GNorm = 0.2400
Meta loss on this task batch = 2.6906e-01, Meta loss averaged over last 500 steps = 2.5430e-01, PNorm = 181.2018, GNorm = 0.2171
Meta loss on this task batch = 2.4496e-01, Meta loss averaged over last 500 steps = 2.5438e-01, PNorm = 181.2065, GNorm = 0.2252
Meta loss on this task batch = 2.6714e-01, Meta loss averaged over last 500 steps = 2.5439e-01, PNorm = 181.2110, GNorm = 0.2189
Meta loss on this task batch = 2.0816e-01, Meta loss averaged over last 500 steps = 2.5437e-01, PNorm = 181.2157, GNorm = 0.1822
Meta loss on this task batch = 2.1165e-01, Meta loss averaged over last 500 steps = 2.5423e-01, PNorm = 181.2204, GNorm = 0.2066
Meta loss on this task batch = 2.2706e-01, Meta loss averaged over last 500 steps = 2.5411e-01, PNorm = 181.2255, GNorm = 0.2598
Meta loss on this task batch = 2.6001e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 181.2304, GNorm = 0.2617
Meta loss on this task batch = 2.6413e-01, Meta loss averaged over last 500 steps = 2.5420e-01, PNorm = 181.2344, GNorm = 0.3067
Meta loss on this task batch = 2.0093e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 181.2385, GNorm = 0.1962
Meta loss on this task batch = 2.4188e-01, Meta loss averaged over last 500 steps = 2.5402e-01, PNorm = 181.2426, GNorm = 0.2403
Meta loss on this task batch = 3.4494e-01, Meta loss averaged over last 500 steps = 2.5424e-01, PNorm = 181.2460, GNorm = 0.2985
Meta loss on this task batch = 2.5117e-01, Meta loss averaged over last 500 steps = 2.5414e-01, PNorm = 181.2499, GNorm = 0.2746
Took 108.11491203308105 seconds to complete one epoch of meta training
Took 115.3751106262207 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.480784
Epoch 968
Meta loss on this task batch = 2.6033e-01, Meta loss averaged over last 500 steps = 2.5415e-01, PNorm = 181.2543, GNorm = 0.2182
Meta loss on this task batch = 2.3415e-01, Meta loss averaged over last 500 steps = 2.5415e-01, PNorm = 181.2588, GNorm = 0.2674
Meta loss on this task batch = 2.1082e-01, Meta loss averaged over last 500 steps = 2.5412e-01, PNorm = 181.2638, GNorm = 0.2448
Meta loss on this task batch = 2.6278e-01, Meta loss averaged over last 500 steps = 2.5416e-01, PNorm = 181.2689, GNorm = 0.2431
Meta loss on this task batch = 2.6824e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.2743, GNorm = 0.2300
Meta loss on this task batch = 3.0768e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.2797, GNorm = 0.2576
Meta loss on this task batch = 2.4790e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 181.2854, GNorm = 0.2506
Meta loss on this task batch = 2.0744e-01, Meta loss averaged over last 500 steps = 2.5403e-01, PNorm = 181.2911, GNorm = 0.2060
Meta loss on this task batch = 2.3639e-01, Meta loss averaged over last 500 steps = 2.5398e-01, PNorm = 181.2965, GNorm = 0.2187
Meta loss on this task batch = 3.2529e-01, Meta loss averaged over last 500 steps = 2.5422e-01, PNorm = 181.3020, GNorm = 0.2796
Meta loss on this task batch = 2.7142e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.3077, GNorm = 0.2623
Meta loss on this task batch = 1.8120e-01, Meta loss averaged over last 500 steps = 2.5412e-01, PNorm = 181.3135, GNorm = 0.2160
Meta loss on this task batch = 2.4836e-01, Meta loss averaged over last 500 steps = 2.5416e-01, PNorm = 181.3191, GNorm = 0.2190
Meta loss on this task batch = 2.5429e-01, Meta loss averaged over last 500 steps = 2.5417e-01, PNorm = 181.3238, GNorm = 0.2510
Meta loss on this task batch = 2.8937e-01, Meta loss averaged over last 500 steps = 2.5417e-01, PNorm = 181.3277, GNorm = 0.2406
Meta loss on this task batch = 2.3352e-01, Meta loss averaged over last 500 steps = 2.5416e-01, PNorm = 181.3317, GNorm = 0.2059
Meta loss on this task batch = 2.6195e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.3360, GNorm = 0.2095
Meta loss on this task batch = 2.6519e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.3405, GNorm = 0.2328
Meta loss on this task batch = 2.7277e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.3446, GNorm = 0.2755
Took 186.9098880290985 seconds to complete one epoch of meta training
Took 193.84957098960876 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.471462
Epoch 969
Meta loss on this task batch = 2.5314e-01, Meta loss averaged over last 500 steps = 2.5432e-01, PNorm = 181.3491, GNorm = 0.2377
Meta loss on this task batch = 2.0681e-01, Meta loss averaged over last 500 steps = 2.5429e-01, PNorm = 181.3543, GNorm = 0.1959
Meta loss on this task batch = 2.6293e-01, Meta loss averaged over last 500 steps = 2.5432e-01, PNorm = 181.3595, GNorm = 0.2274
Meta loss on this task batch = 2.9503e-01, Meta loss averaged over last 500 steps = 2.5434e-01, PNorm = 181.3642, GNorm = 0.2910
Meta loss on this task batch = 2.6334e-01, Meta loss averaged over last 500 steps = 2.5433e-01, PNorm = 181.3675, GNorm = 0.2629
Meta loss on this task batch = 2.4329e-01, Meta loss averaged over last 500 steps = 2.5428e-01, PNorm = 181.3707, GNorm = 0.2289
Meta loss on this task batch = 2.2607e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.3738, GNorm = 0.2173
Meta loss on this task batch = 2.7307e-01, Meta loss averaged over last 500 steps = 2.5431e-01, PNorm = 181.3768, GNorm = 0.2254
Meta loss on this task batch = 2.3547e-01, Meta loss averaged over last 500 steps = 2.5431e-01, PNorm = 181.3804, GNorm = 0.2091
Meta loss on this task batch = 2.5908e-01, Meta loss averaged over last 500 steps = 2.5429e-01, PNorm = 181.3844, GNorm = 0.2862
Meta loss on this task batch = 2.7892e-01, Meta loss averaged over last 500 steps = 2.5440e-01, PNorm = 181.3885, GNorm = 0.2322
Meta loss on this task batch = 2.6470e-01, Meta loss averaged over last 500 steps = 2.5436e-01, PNorm = 181.3924, GNorm = 0.2207
Meta loss on this task batch = 2.6511e-01, Meta loss averaged over last 500 steps = 2.5428e-01, PNorm = 181.3963, GNorm = 0.2416
Meta loss on this task batch = 2.2958e-01, Meta loss averaged over last 500 steps = 2.5431e-01, PNorm = 181.4004, GNorm = 0.2191
Meta loss on this task batch = 2.8897e-01, Meta loss averaged over last 500 steps = 2.5441e-01, PNorm = 181.4044, GNorm = 0.2429
Meta loss on this task batch = 2.7527e-01, Meta loss averaged over last 500 steps = 2.5448e-01, PNorm = 181.4081, GNorm = 0.2327
Meta loss on this task batch = 2.3076e-01, Meta loss averaged over last 500 steps = 2.5439e-01, PNorm = 181.4119, GNorm = 0.2265
Meta loss on this task batch = 2.4802e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.4153, GNorm = 0.2350
Meta loss on this task batch = 2.5087e-01, Meta loss averaged over last 500 steps = 2.5440e-01, PNorm = 181.4191, GNorm = 0.2738
Took 153.95477557182312 seconds to complete one epoch of meta training
Took 160.95143914222717 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491783
Epoch 970
Meta loss on this task batch = 2.7410e-01, Meta loss averaged over last 500 steps = 2.5447e-01, PNorm = 181.4216, GNorm = 0.2934
Meta loss on this task batch = 2.9956e-01, Meta loss averaged over last 500 steps = 2.5450e-01, PNorm = 181.4239, GNorm = 0.2258
Meta loss on this task batch = 3.0125e-01, Meta loss averaged over last 500 steps = 2.5457e-01, PNorm = 181.4263, GNorm = 0.2062
Meta loss on this task batch = 2.6382e-01, Meta loss averaged over last 500 steps = 2.5457e-01, PNorm = 181.4291, GNorm = 0.2401
Meta loss on this task batch = 2.8374e-01, Meta loss averaged over last 500 steps = 2.5468e-01, PNorm = 181.4314, GNorm = 0.2523
Meta loss on this task batch = 2.4734e-01, Meta loss averaged over last 500 steps = 2.5463e-01, PNorm = 181.4343, GNorm = 0.2251
Meta loss on this task batch = 2.4413e-01, Meta loss averaged over last 500 steps = 2.5455e-01, PNorm = 181.4374, GNorm = 0.2484
Meta loss on this task batch = 1.9324e-01, Meta loss averaged over last 500 steps = 2.5449e-01, PNorm = 181.4408, GNorm = 0.2175
Meta loss on this task batch = 2.4840e-01, Meta loss averaged over last 500 steps = 2.5446e-01, PNorm = 181.4441, GNorm = 0.2353
Meta loss on this task batch = 2.3731e-01, Meta loss averaged over last 500 steps = 2.5441e-01, PNorm = 181.4479, GNorm = 0.1986
Meta loss on this task batch = 2.5927e-01, Meta loss averaged over last 500 steps = 2.5445e-01, PNorm = 181.4519, GNorm = 0.2334
Meta loss on this task batch = 2.8508e-01, Meta loss averaged over last 500 steps = 2.5448e-01, PNorm = 181.4559, GNorm = 0.2187
Meta loss on this task batch = 2.9504e-01, Meta loss averaged over last 500 steps = 2.5460e-01, PNorm = 181.4598, GNorm = 0.2503
Meta loss on this task batch = 2.4221e-01, Meta loss averaged over last 500 steps = 2.5459e-01, PNorm = 181.4627, GNorm = 0.2697
Meta loss on this task batch = 2.0906e-01, Meta loss averaged over last 500 steps = 2.5451e-01, PNorm = 181.4662, GNorm = 0.1995
Meta loss on this task batch = 2.2025e-01, Meta loss averaged over last 500 steps = 2.5442e-01, PNorm = 181.4702, GNorm = 0.2049
Meta loss on this task batch = 2.6635e-01, Meta loss averaged over last 500 steps = 2.5436e-01, PNorm = 181.4744, GNorm = 0.2339
Meta loss on this task batch = 2.4063e-01, Meta loss averaged over last 500 steps = 2.5434e-01, PNorm = 181.4791, GNorm = 0.2641
Meta loss on this task batch = 2.3998e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.4837, GNorm = 0.2784
Took 156.75504064559937 seconds to complete one epoch of meta training
Took 163.5921847820282 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502039
Epoch 971
Meta loss on this task batch = 2.4788e-01, Meta loss averaged over last 500 steps = 2.5430e-01, PNorm = 181.4886, GNorm = 0.2302
Meta loss on this task batch = 2.6142e-01, Meta loss averaged over last 500 steps = 2.5444e-01, PNorm = 181.4938, GNorm = 0.2435
Meta loss on this task batch = 2.2474e-01, Meta loss averaged over last 500 steps = 2.5439e-01, PNorm = 181.4991, GNorm = 0.1865
Meta loss on this task batch = 2.4786e-01, Meta loss averaged over last 500 steps = 2.5436e-01, PNorm = 181.5044, GNorm = 0.2322
Meta loss on this task batch = 2.0936e-01, Meta loss averaged over last 500 steps = 2.5422e-01, PNorm = 181.5097, GNorm = 0.2351
Meta loss on this task batch = 2.4657e-01, Meta loss averaged over last 500 steps = 2.5413e-01, PNorm = 181.5151, GNorm = 0.2396
Meta loss on this task batch = 2.5365e-01, Meta loss averaged over last 500 steps = 2.5413e-01, PNorm = 181.5208, GNorm = 0.2223
Meta loss on this task batch = 3.1803e-01, Meta loss averaged over last 500 steps = 2.5418e-01, PNorm = 181.5264, GNorm = 0.2737
Meta loss on this task batch = 2.5678e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 181.5310, GNorm = 0.2374
Meta loss on this task batch = 2.6569e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 181.5350, GNorm = 0.3036
Meta loss on this task batch = 2.3737e-01, Meta loss averaged over last 500 steps = 2.5414e-01, PNorm = 181.5388, GNorm = 0.2333
Meta loss on this task batch = 3.5610e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.5422, GNorm = 0.2941
Meta loss on this task batch = 3.0186e-01, Meta loss averaged over last 500 steps = 2.5452e-01, PNorm = 181.5454, GNorm = 0.2881
Meta loss on this task batch = 2.2342e-01, Meta loss averaged over last 500 steps = 2.5434e-01, PNorm = 181.5487, GNorm = 0.2323
Meta loss on this task batch = 2.1658e-01, Meta loss averaged over last 500 steps = 2.5424e-01, PNorm = 181.5525, GNorm = 0.1871
Meta loss on this task batch = 2.3580e-01, Meta loss averaged over last 500 steps = 2.5425e-01, PNorm = 181.5569, GNorm = 0.2117
Meta loss on this task batch = 2.1456e-01, Meta loss averaged over last 500 steps = 2.5412e-01, PNorm = 181.5614, GNorm = 0.2452
Meta loss on this task batch = 2.6830e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 181.5662, GNorm = 0.2296
Meta loss on this task batch = 1.9618e-01, Meta loss averaged over last 500 steps = 2.5397e-01, PNorm = 181.5714, GNorm = 0.2158
Took 101.33088684082031 seconds to complete one epoch of meta training
Took 108.68891549110413 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.489860
Epoch 972
Meta loss on this task batch = 2.3317e-01, Meta loss averaged over last 500 steps = 2.5392e-01, PNorm = 181.5763, GNorm = 0.2566
Meta loss on this task batch = 2.5467e-01, Meta loss averaged over last 500 steps = 2.5391e-01, PNorm = 181.5808, GNorm = 0.2169
Meta loss on this task batch = 2.6341e-01, Meta loss averaged over last 500 steps = 2.5394e-01, PNorm = 181.5849, GNorm = 0.2236
Meta loss on this task batch = 3.3430e-01, Meta loss averaged over last 500 steps = 2.5415e-01, PNorm = 181.5879, GNorm = 0.3737
Meta loss on this task batch = 2.8717e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.5905, GNorm = 0.2797
Meta loss on this task batch = 2.1102e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 181.5933, GNorm = 0.2360
Meta loss on this task batch = 2.8436e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.5960, GNorm = 0.2861
Meta loss on this task batch = 2.3954e-01, Meta loss averaged over last 500 steps = 2.5414e-01, PNorm = 181.5987, GNorm = 0.2177
Meta loss on this task batch = 2.3185e-01, Meta loss averaged over last 500 steps = 2.5403e-01, PNorm = 181.6018, GNorm = 0.2383
Meta loss on this task batch = 2.7997e-01, Meta loss averaged over last 500 steps = 2.5407e-01, PNorm = 181.6043, GNorm = 0.3061
Meta loss on this task batch = 2.5381e-01, Meta loss averaged over last 500 steps = 2.5395e-01, PNorm = 181.6070, GNorm = 0.2349
Meta loss on this task batch = 2.0195e-01, Meta loss averaged over last 500 steps = 2.5389e-01, PNorm = 181.6105, GNorm = 0.1996
Meta loss on this task batch = 2.0509e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 181.6142, GNorm = 0.2193
Meta loss on this task batch = 2.6611e-01, Meta loss averaged over last 500 steps = 2.5384e-01, PNorm = 181.6177, GNorm = 0.2601
Meta loss on this task batch = 2.8118e-01, Meta loss averaged over last 500 steps = 2.5394e-01, PNorm = 181.6214, GNorm = 0.2477
Meta loss on this task batch = 2.4327e-01, Meta loss averaged over last 500 steps = 2.5389e-01, PNorm = 181.6253, GNorm = 0.2103
Meta loss on this task batch = 2.3116e-01, Meta loss averaged over last 500 steps = 2.5380e-01, PNorm = 181.6294, GNorm = 0.2726
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.5374e-01, PNorm = 181.6336, GNorm = 0.2222
Meta loss on this task batch = 2.8672e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 181.6373, GNorm = 0.2975
Took 107.19679832458496 seconds to complete one epoch of meta training
Took 114.42913126945496 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488342
Epoch 973
Meta loss on this task batch = 3.1344e-01, Meta loss averaged over last 500 steps = 2.5397e-01, PNorm = 181.6408, GNorm = 0.2518
Meta loss on this task batch = 2.3074e-01, Meta loss averaged over last 500 steps = 2.5384e-01, PNorm = 181.6445, GNorm = 0.2257
Meta loss on this task batch = 2.4940e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 181.6486, GNorm = 0.2050
Meta loss on this task batch = 2.5154e-01, Meta loss averaged over last 500 steps = 2.5384e-01, PNorm = 181.6527, GNorm = 0.2315
Meta loss on this task batch = 2.8047e-01, Meta loss averaged over last 500 steps = 2.5399e-01, PNorm = 181.6577, GNorm = 0.2512
Meta loss on this task batch = 2.3284e-01, Meta loss averaged over last 500 steps = 2.5401e-01, PNorm = 181.6626, GNorm = 0.2107
Meta loss on this task batch = 2.2210e-01, Meta loss averaged over last 500 steps = 2.5396e-01, PNorm = 181.6677, GNorm = 0.2052
Meta loss on this task batch = 3.2199e-01, Meta loss averaged over last 500 steps = 2.5416e-01, PNorm = 181.6721, GNorm = 0.2597
Meta loss on this task batch = 2.0944e-01, Meta loss averaged over last 500 steps = 2.5409e-01, PNorm = 181.6766, GNorm = 0.2044
Meta loss on this task batch = 2.1095e-01, Meta loss averaged over last 500 steps = 2.5399e-01, PNorm = 181.6811, GNorm = 0.2237
Meta loss on this task batch = 2.3591e-01, Meta loss averaged over last 500 steps = 2.5400e-01, PNorm = 181.6850, GNorm = 0.2439
Meta loss on this task batch = 1.8727e-01, Meta loss averaged over last 500 steps = 2.5397e-01, PNorm = 181.6889, GNorm = 0.1894
Meta loss on this task batch = 2.8038e-01, Meta loss averaged over last 500 steps = 2.5400e-01, PNorm = 181.6924, GNorm = 0.2458
Meta loss on this task batch = 3.0598e-01, Meta loss averaged over last 500 steps = 2.5411e-01, PNorm = 181.6965, GNorm = 0.2388
Meta loss on this task batch = 1.9419e-01, Meta loss averaged over last 500 steps = 2.5394e-01, PNorm = 181.7010, GNorm = 0.1972
Meta loss on this task batch = 3.0717e-01, Meta loss averaged over last 500 steps = 2.5409e-01, PNorm = 181.7050, GNorm = 0.2644
Meta loss on this task batch = 2.8076e-01, Meta loss averaged over last 500 steps = 2.5414e-01, PNorm = 181.7094, GNorm = 0.2330
Meta loss on this task batch = 2.6884e-01, Meta loss averaged over last 500 steps = 2.5409e-01, PNorm = 181.7137, GNorm = 0.2676
Meta loss on this task batch = 2.1941e-01, Meta loss averaged over last 500 steps = 2.5402e-01, PNorm = 181.7175, GNorm = 0.2509
Took 107.07446026802063 seconds to complete one epoch of meta training
Took 114.16657042503357 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.473920
Epoch 974
Meta loss on this task batch = 1.8741e-01, Meta loss averaged over last 500 steps = 2.5396e-01, PNorm = 181.7214, GNorm = 0.1915
Meta loss on this task batch = 2.4241e-01, Meta loss averaged over last 500 steps = 2.5393e-01, PNorm = 181.7251, GNorm = 0.2453
Meta loss on this task batch = 2.4278e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 181.7288, GNorm = 0.2397
Meta loss on this task batch = 2.8052e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 181.7332, GNorm = 0.2778
Meta loss on this task batch = 2.5911e-01, Meta loss averaged over last 500 steps = 2.5382e-01, PNorm = 181.7377, GNorm = 0.2161
Meta loss on this task batch = 2.9899e-01, Meta loss averaged over last 500 steps = 2.5389e-01, PNorm = 181.7416, GNorm = 0.2382
Meta loss on this task batch = 2.2317e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 181.7453, GNorm = 0.1875
Meta loss on this task batch = 2.6536e-01, Meta loss averaged over last 500 steps = 2.5384e-01, PNorm = 181.7492, GNorm = 0.2615
Meta loss on this task batch = 2.7033e-01, Meta loss averaged over last 500 steps = 2.5391e-01, PNorm = 181.7525, GNorm = 0.2548
Meta loss on this task batch = 2.7654e-01, Meta loss averaged over last 500 steps = 2.5400e-01, PNorm = 181.7558, GNorm = 0.2545
Meta loss on this task batch = 2.5940e-01, Meta loss averaged over last 500 steps = 2.5400e-01, PNorm = 181.7592, GNorm = 0.2307
Meta loss on this task batch = 2.3723e-01, Meta loss averaged over last 500 steps = 2.5402e-01, PNorm = 181.7628, GNorm = 0.2027
Meta loss on this task batch = 3.0758e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.7668, GNorm = 0.3474
Meta loss on this task batch = 2.4261e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 181.7708, GNorm = 0.2367
Meta loss on this task batch = 2.1312e-01, Meta loss averaged over last 500 steps = 2.5406e-01, PNorm = 181.7743, GNorm = 0.2079
Meta loss on this task batch = 2.3858e-01, Meta loss averaged over last 500 steps = 2.5405e-01, PNorm = 181.7783, GNorm = 0.2134
Meta loss on this task batch = 2.5286e-01, Meta loss averaged over last 500 steps = 2.5402e-01, PNorm = 181.7817, GNorm = 0.2239
Meta loss on this task batch = 2.4371e-01, Meta loss averaged over last 500 steps = 2.5397e-01, PNorm = 181.7860, GNorm = 0.2085
Meta loss on this task batch = 2.9950e-01, Meta loss averaged over last 500 steps = 2.5413e-01, PNorm = 181.7911, GNorm = 0.2925
Took 106.95713233947754 seconds to complete one epoch of meta training
Took 114.38249015808105 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.482606
Epoch 975
Meta loss on this task batch = 2.5716e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 181.7964, GNorm = 0.2573
Meta loss on this task batch = 2.6769e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.8015, GNorm = 0.2145
Meta loss on this task batch = 2.2427e-01, Meta loss averaged over last 500 steps = 2.5416e-01, PNorm = 181.8062, GNorm = 0.2124
Meta loss on this task batch = 2.5462e-01, Meta loss averaged over last 500 steps = 2.5422e-01, PNorm = 181.8108, GNorm = 0.2321
Meta loss on this task batch = 2.4014e-01, Meta loss averaged over last 500 steps = 2.5415e-01, PNorm = 181.8154, GNorm = 0.1919
Meta loss on this task batch = 2.7397e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 181.8197, GNorm = 0.2269
Meta loss on this task batch = 2.4813e-01, Meta loss averaged over last 500 steps = 2.5428e-01, PNorm = 181.8242, GNorm = 0.2637
Meta loss on this task batch = 2.4423e-01, Meta loss averaged over last 500 steps = 2.5425e-01, PNorm = 181.8295, GNorm = 0.2333
Meta loss on this task batch = 2.1841e-01, Meta loss averaged over last 500 steps = 2.5417e-01, PNorm = 181.8347, GNorm = 0.2351
Meta loss on this task batch = 2.1328e-01, Meta loss averaged over last 500 steps = 2.5407e-01, PNorm = 181.8395, GNorm = 0.2047
Meta loss on this task batch = 2.9366e-01, Meta loss averaged over last 500 steps = 2.5415e-01, PNorm = 181.8437, GNorm = 0.2406
Meta loss on this task batch = 2.2902e-01, Meta loss averaged over last 500 steps = 2.5411e-01, PNorm = 181.8487, GNorm = 0.2157
Meta loss on this task batch = 2.5425e-01, Meta loss averaged over last 500 steps = 2.5407e-01, PNorm = 181.8535, GNorm = 0.2391
Meta loss on this task batch = 2.5069e-01, Meta loss averaged over last 500 steps = 2.5408e-01, PNorm = 181.8585, GNorm = 0.2230
Meta loss on this task batch = 2.7054e-01, Meta loss averaged over last 500 steps = 2.5407e-01, PNorm = 181.8625, GNorm = 0.2743
Meta loss on this task batch = 2.3200e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 181.8666, GNorm = 0.2399
Meta loss on this task batch = 2.8103e-01, Meta loss averaged over last 500 steps = 2.5420e-01, PNorm = 181.8704, GNorm = 0.2322
Meta loss on this task batch = 2.8696e-01, Meta loss averaged over last 500 steps = 2.5432e-01, PNorm = 181.8735, GNorm = 0.2475
Meta loss on this task batch = 3.0263e-01, Meta loss averaged over last 500 steps = 2.5432e-01, PNorm = 181.8769, GNorm = 0.3183
Took 115.60743045806885 seconds to complete one epoch of meta training
Took 122.56747794151306 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481214
Epoch 976
Meta loss on this task batch = 2.3329e-01, Meta loss averaged over last 500 steps = 2.5422e-01, PNorm = 181.8799, GNorm = 0.2341
Meta loss on this task batch = 2.7947e-01, Meta loss averaged over last 500 steps = 2.5428e-01, PNorm = 181.8825, GNorm = 0.2352
Meta loss on this task batch = 2.5391e-01, Meta loss averaged over last 500 steps = 2.5443e-01, PNorm = 181.8847, GNorm = 0.2213
Meta loss on this task batch = 2.4048e-01, Meta loss averaged over last 500 steps = 2.5442e-01, PNorm = 181.8875, GNorm = 0.2119
Meta loss on this task batch = 2.0617e-01, Meta loss averaged over last 500 steps = 2.5436e-01, PNorm = 181.8900, GNorm = 0.1683
Meta loss on this task batch = 2.6381e-01, Meta loss averaged over last 500 steps = 2.5431e-01, PNorm = 181.8925, GNorm = 0.2132
Meta loss on this task batch = 3.0207e-01, Meta loss averaged over last 500 steps = 2.5436e-01, PNorm = 181.8953, GNorm = 0.2423
Meta loss on this task batch = 2.4579e-01, Meta loss averaged over last 500 steps = 2.5439e-01, PNorm = 181.8981, GNorm = 0.2388
Meta loss on this task batch = 2.7660e-01, Meta loss averaged over last 500 steps = 2.5445e-01, PNorm = 181.9003, GNorm = 0.2532
Meta loss on this task batch = 2.0209e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.9030, GNorm = 0.1690
Meta loss on this task batch = 2.4762e-01, Meta loss averaged over last 500 steps = 2.5431e-01, PNorm = 181.9062, GNorm = 0.2142
Meta loss on this task batch = 2.7760e-01, Meta loss averaged over last 500 steps = 2.5433e-01, PNorm = 181.9097, GNorm = 0.2393
Meta loss on this task batch = 2.4694e-01, Meta loss averaged over last 500 steps = 2.5427e-01, PNorm = 181.9130, GNorm = 0.2188
Meta loss on this task batch = 2.5646e-01, Meta loss averaged over last 500 steps = 2.5422e-01, PNorm = 181.9164, GNorm = 0.2478
Meta loss on this task batch = 2.1471e-01, Meta loss averaged over last 500 steps = 2.5423e-01, PNorm = 181.9199, GNorm = 0.2057
Meta loss on this task batch = 3.1801e-01, Meta loss averaged over last 500 steps = 2.5430e-01, PNorm = 181.9233, GNorm = 0.2412
Meta loss on this task batch = 2.5982e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.9268, GNorm = 0.2173
Meta loss on this task batch = 2.4981e-01, Meta loss averaged over last 500 steps = 2.5432e-01, PNorm = 181.9308, GNorm = 0.2337
Meta loss on this task batch = 2.7286e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 181.9346, GNorm = 0.3001
Took 111.81083989143372 seconds to complete one epoch of meta training
Took 119.25825810432434 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.498451
Epoch 977
Meta loss on this task batch = 2.6433e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.9386, GNorm = 0.2319
Meta loss on this task batch = 2.6451e-01, Meta loss averaged over last 500 steps = 2.5429e-01, PNorm = 181.9431, GNorm = 0.2523
Meta loss on this task batch = 2.6604e-01, Meta loss averaged over last 500 steps = 2.5431e-01, PNorm = 181.9466, GNorm = 0.2726
Meta loss on this task batch = 2.7731e-01, Meta loss averaged over last 500 steps = 2.5438e-01, PNorm = 181.9499, GNorm = 0.2465
Meta loss on this task batch = 3.2497e-01, Meta loss averaged over last 500 steps = 2.5453e-01, PNorm = 181.9530, GNorm = 0.2834
Meta loss on this task batch = 1.6306e-01, Meta loss averaged over last 500 steps = 2.5441e-01, PNorm = 181.9569, GNorm = 0.1883
Meta loss on this task batch = 2.1845e-01, Meta loss averaged over last 500 steps = 2.5429e-01, PNorm = 181.9610, GNorm = 0.2289
Meta loss on this task batch = 2.5538e-01, Meta loss averaged over last 500 steps = 2.5434e-01, PNorm = 181.9651, GNorm = 0.2437
Meta loss on this task batch = 2.5936e-01, Meta loss averaged over last 500 steps = 2.5440e-01, PNorm = 181.9694, GNorm = 0.2535
Meta loss on this task batch = 2.8078e-01, Meta loss averaged over last 500 steps = 2.5441e-01, PNorm = 181.9734, GNorm = 0.3029
Meta loss on this task batch = 2.5373e-01, Meta loss averaged over last 500 steps = 2.5437e-01, PNorm = 181.9773, GNorm = 0.2251
Meta loss on this task batch = 2.0982e-01, Meta loss averaged over last 500 steps = 2.5432e-01, PNorm = 181.9820, GNorm = 0.2558
Meta loss on this task batch = 2.8092e-01, Meta loss averaged over last 500 steps = 2.5435e-01, PNorm = 181.9862, GNorm = 0.2353
Meta loss on this task batch = 2.4326e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.9900, GNorm = 0.2310
Meta loss on this task batch = 2.3545e-01, Meta loss averaged over last 500 steps = 2.5426e-01, PNorm = 181.9932, GNorm = 0.2286
Meta loss on this task batch = 2.6092e-01, Meta loss averaged over last 500 steps = 2.5423e-01, PNorm = 181.9966, GNorm = 0.2506
Meta loss on this task batch = 2.6937e-01, Meta loss averaged over last 500 steps = 2.5424e-01, PNorm = 182.0004, GNorm = 0.2647
Meta loss on this task batch = 2.6200e-01, Meta loss averaged over last 500 steps = 2.5425e-01, PNorm = 182.0035, GNorm = 0.2874
Meta loss on this task batch = 2.4637e-01, Meta loss averaged over last 500 steps = 2.5418e-01, PNorm = 182.0066, GNorm = 0.2652
Took 108.38178133964539 seconds to complete one epoch of meta training
Took 115.6751561164856 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484236
Epoch 978
Meta loss on this task batch = 2.8694e-01, Meta loss averaged over last 500 steps = 2.5428e-01, PNorm = 182.0100, GNorm = 0.2303
Meta loss on this task batch = 2.8391e-01, Meta loss averaged over last 500 steps = 2.5424e-01, PNorm = 182.0131, GNorm = 0.2390
Meta loss on this task batch = 2.4034e-01, Meta loss averaged over last 500 steps = 2.5423e-01, PNorm = 182.0163, GNorm = 0.2145
Meta loss on this task batch = 2.5783e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 182.0200, GNorm = 0.2693
Meta loss on this task batch = 2.5026e-01, Meta loss averaged over last 500 steps = 2.5423e-01, PNorm = 182.0239, GNorm = 0.2182
Meta loss on this task batch = 3.1630e-01, Meta loss averaged over last 500 steps = 2.5455e-01, PNorm = 182.0281, GNorm = 0.2568
Meta loss on this task batch = 3.0039e-01, Meta loss averaged over last 500 steps = 2.5466e-01, PNorm = 182.0332, GNorm = 0.2532
Meta loss on this task batch = 2.5027e-01, Meta loss averaged over last 500 steps = 2.5467e-01, PNorm = 182.0388, GNorm = 0.2265
Meta loss on this task batch = 2.2332e-01, Meta loss averaged over last 500 steps = 2.5459e-01, PNorm = 182.0445, GNorm = 0.2410
Meta loss on this task batch = 2.4891e-01, Meta loss averaged over last 500 steps = 2.5463e-01, PNorm = 182.0503, GNorm = 0.2123
Meta loss on this task batch = 2.3378e-01, Meta loss averaged over last 500 steps = 2.5463e-01, PNorm = 182.0560, GNorm = 0.2278
Meta loss on this task batch = 2.4276e-01, Meta loss averaged over last 500 steps = 2.5461e-01, PNorm = 182.0624, GNorm = 0.2152
Meta loss on this task batch = 2.2507e-01, Meta loss averaged over last 500 steps = 2.5453e-01, PNorm = 182.0692, GNorm = 0.2231
Meta loss on this task batch = 2.6578e-01, Meta loss averaged over last 500 steps = 2.5450e-01, PNorm = 182.0757, GNorm = 0.2293
Meta loss on this task batch = 2.1182e-01, Meta loss averaged over last 500 steps = 2.5444e-01, PNorm = 182.0819, GNorm = 0.1933
Meta loss on this task batch = 2.4506e-01, Meta loss averaged over last 500 steps = 2.5443e-01, PNorm = 182.0886, GNorm = 0.2601
Meta loss on this task batch = 2.5999e-01, Meta loss averaged over last 500 steps = 2.5434e-01, PNorm = 182.0955, GNorm = 0.2307
Meta loss on this task batch = 2.3075e-01, Meta loss averaged over last 500 steps = 2.5428e-01, PNorm = 182.1017, GNorm = 0.2146
Meta loss on this task batch = 2.1671e-01, Meta loss averaged over last 500 steps = 2.5419e-01, PNorm = 182.1077, GNorm = 0.2303
Took 103.00938653945923 seconds to complete one epoch of meta training
Took 110.32968926429749 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492117
Epoch 979
Meta loss on this task batch = 2.5635e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 182.1133, GNorm = 0.2169
Meta loss on this task batch = 2.6748e-01, Meta loss averaged over last 500 steps = 2.5415e-01, PNorm = 182.1189, GNorm = 0.2385
Meta loss on this task batch = 2.3164e-01, Meta loss averaged over last 500 steps = 2.5413e-01, PNorm = 182.1242, GNorm = 0.2243
Meta loss on this task batch = 2.1115e-01, Meta loss averaged over last 500 steps = 2.5398e-01, PNorm = 182.1293, GNorm = 0.1902
Meta loss on this task batch = 2.6529e-01, Meta loss averaged over last 500 steps = 2.5402e-01, PNorm = 182.1334, GNorm = 0.2236
Meta loss on this task batch = 2.5796e-01, Meta loss averaged over last 500 steps = 2.5395e-01, PNorm = 182.1366, GNorm = 0.2500
Meta loss on this task batch = 2.6900e-01, Meta loss averaged over last 500 steps = 2.5400e-01, PNorm = 182.1408, GNorm = 0.2713
Meta loss on this task batch = 2.5479e-01, Meta loss averaged over last 500 steps = 2.5392e-01, PNorm = 182.1442, GNorm = 0.2517
Meta loss on this task batch = 2.9064e-01, Meta loss averaged over last 500 steps = 2.5406e-01, PNorm = 182.1473, GNorm = 0.2434
Meta loss on this task batch = 2.6834e-01, Meta loss averaged over last 500 steps = 2.5412e-01, PNorm = 182.1498, GNorm = 0.2600
Meta loss on this task batch = 2.3532e-01, Meta loss averaged over last 500 steps = 2.5406e-01, PNorm = 182.1530, GNorm = 0.2141
Meta loss on this task batch = 2.3435e-01, Meta loss averaged over last 500 steps = 2.5410e-01, PNorm = 182.1573, GNorm = 0.2347
Meta loss on this task batch = 3.1646e-01, Meta loss averaged over last 500 steps = 2.5407e-01, PNorm = 182.1615, GNorm = 0.2213
Meta loss on this task batch = 2.1180e-01, Meta loss averaged over last 500 steps = 2.5403e-01, PNorm = 182.1660, GNorm = 0.1995
Meta loss on this task batch = 2.0509e-01, Meta loss averaged over last 500 steps = 2.5396e-01, PNorm = 182.1710, GNorm = 0.2054
Meta loss on this task batch = 2.6111e-01, Meta loss averaged over last 500 steps = 2.5393e-01, PNorm = 182.1755, GNorm = 0.2260
Meta loss on this task batch = 2.3835e-01, Meta loss averaged over last 500 steps = 2.5391e-01, PNorm = 182.1804, GNorm = 0.2386
Meta loss on this task batch = 2.6094e-01, Meta loss averaged over last 500 steps = 2.5390e-01, PNorm = 182.1847, GNorm = 0.2535
Meta loss on this task batch = 3.2058e-01, Meta loss averaged over last 500 steps = 2.5417e-01, PNorm = 182.1886, GNorm = 0.3473
Took 101.91178846359253 seconds to complete one epoch of meta training
Took 108.73229908943176 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.492529
Epoch 980
Meta loss on this task batch = 2.8083e-01, Meta loss averaged over last 500 steps = 2.5414e-01, PNorm = 182.1926, GNorm = 0.2277
Meta loss on this task batch = 2.1478e-01, Meta loss averaged over last 500 steps = 2.5405e-01, PNorm = 182.1963, GNorm = 0.2188
Meta loss on this task batch = 3.4731e-01, Meta loss averaged over last 500 steps = 2.5421e-01, PNorm = 182.1993, GNorm = 0.3129
Meta loss on this task batch = 2.8439e-01, Meta loss averaged over last 500 steps = 2.5412e-01, PNorm = 182.2017, GNorm = 0.2181
Meta loss on this task batch = 2.8176e-01, Meta loss averaged over last 500 steps = 2.5418e-01, PNorm = 182.2039, GNorm = 0.1854
Meta loss on this task batch = 2.0082e-01, Meta loss averaged over last 500 steps = 2.5412e-01, PNorm = 182.2065, GNorm = 0.1847
Meta loss on this task batch = 2.9567e-01, Meta loss averaged over last 500 steps = 2.5413e-01, PNorm = 182.2089, GNorm = 0.2380
Meta loss on this task batch = 2.4565e-01, Meta loss averaged over last 500 steps = 2.5413e-01, PNorm = 182.2120, GNorm = 0.2043
Meta loss on this task batch = 2.2737e-01, Meta loss averaged over last 500 steps = 2.5408e-01, PNorm = 182.2152, GNorm = 0.2271
Meta loss on this task batch = 2.6296e-01, Meta loss averaged over last 500 steps = 2.5417e-01, PNorm = 182.2189, GNorm = 0.2662
Meta loss on this task batch = 2.4020e-01, Meta loss averaged over last 500 steps = 2.5406e-01, PNorm = 182.2225, GNorm = 0.2173
Meta loss on this task batch = 2.3182e-01, Meta loss averaged over last 500 steps = 2.5399e-01, PNorm = 182.2268, GNorm = 0.2693
Meta loss on this task batch = 2.5435e-01, Meta loss averaged over last 500 steps = 2.5399e-01, PNorm = 182.2310, GNorm = 0.2341
Meta loss on this task batch = 2.1287e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 182.2354, GNorm = 0.2113
Meta loss on this task batch = 2.3306e-01, Meta loss averaged over last 500 steps = 2.5376e-01, PNorm = 182.2398, GNorm = 0.2056
Meta loss on this task batch = 2.9848e-01, Meta loss averaged over last 500 steps = 2.5401e-01, PNorm = 182.2442, GNorm = 0.2774
Meta loss on this task batch = 2.3193e-01, Meta loss averaged over last 500 steps = 2.5398e-01, PNorm = 182.2485, GNorm = 0.2694
Meta loss on this task batch = 2.4473e-01, Meta loss averaged over last 500 steps = 2.5392e-01, PNorm = 182.2528, GNorm = 0.2837
Meta loss on this task batch = 2.3133e-01, Meta loss averaged over last 500 steps = 2.5395e-01, PNorm = 182.2572, GNorm = 0.2483
Took 102.53555798530579 seconds to complete one epoch of meta training
Took 108.4268810749054 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.503597
Epoch 981
Meta loss on this task batch = 2.2111e-01, Meta loss averaged over last 500 steps = 2.5387e-01, PNorm = 182.2614, GNorm = 0.1893
Meta loss on this task batch = 2.1808e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 182.2658, GNorm = 0.2121
Meta loss on this task batch = 2.8388e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 182.2696, GNorm = 0.2528
Meta loss on this task batch = 2.1883e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 182.2738, GNorm = 0.2132
Meta loss on this task batch = 1.8454e-01, Meta loss averaged over last 500 steps = 2.5343e-01, PNorm = 182.2778, GNorm = 0.2200
Meta loss on this task batch = 2.7610e-01, Meta loss averaged over last 500 steps = 2.5354e-01, PNorm = 182.2819, GNorm = 0.2230
Meta loss on this task batch = 2.6071e-01, Meta loss averaged over last 500 steps = 2.5359e-01, PNorm = 182.2859, GNorm = 0.2284
Meta loss on this task batch = 2.4055e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 182.2898, GNorm = 0.2201
Meta loss on this task batch = 2.6990e-01, Meta loss averaged over last 500 steps = 2.5364e-01, PNorm = 182.2935, GNorm = 0.2262
Meta loss on this task batch = 2.5669e-01, Meta loss averaged over last 500 steps = 2.5373e-01, PNorm = 182.2968, GNorm = 0.2304
Meta loss on this task batch = 2.5331e-01, Meta loss averaged over last 500 steps = 2.5375e-01, PNorm = 182.3003, GNorm = 0.2528
Meta loss on this task batch = 2.3966e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 182.3037, GNorm = 0.2188
Meta loss on this task batch = 2.4840e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 182.3078, GNorm = 0.2945
Meta loss on this task batch = 2.1791e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 182.3121, GNorm = 0.2065
Meta loss on this task batch = 2.4481e-01, Meta loss averaged over last 500 steps = 2.5384e-01, PNorm = 182.3163, GNorm = 0.2978
Meta loss on this task batch = 2.3768e-01, Meta loss averaged over last 500 steps = 2.5370e-01, PNorm = 182.3208, GNorm = 0.2573
Meta loss on this task batch = 3.2368e-01, Meta loss averaged over last 500 steps = 2.5382e-01, PNorm = 182.3252, GNorm = 0.2636
Meta loss on this task batch = 2.6003e-01, Meta loss averaged over last 500 steps = 2.5382e-01, PNorm = 182.3299, GNorm = 0.2385
Meta loss on this task batch = 2.8299e-01, Meta loss averaged over last 500 steps = 2.5370e-01, PNorm = 182.3348, GNorm = 0.2996
Took 101.51183724403381 seconds to complete one epoch of meta training
Took 108.6214485168457 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476158
Epoch 982
Meta loss on this task batch = 2.3308e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 182.3402, GNorm = 0.2103
Meta loss on this task batch = 2.2445e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 182.3448, GNorm = 0.2593
Meta loss on this task batch = 2.1927e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 182.3488, GNorm = 0.2357
Meta loss on this task batch = 2.8103e-01, Meta loss averaged over last 500 steps = 2.5363e-01, PNorm = 182.3517, GNorm = 0.2382
Meta loss on this task batch = 2.8924e-01, Meta loss averaged over last 500 steps = 2.5362e-01, PNorm = 182.3549, GNorm = 0.2575
Meta loss on this task batch = 2.5246e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 182.3584, GNorm = 0.2360
Meta loss on this task batch = 2.9700e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 182.3615, GNorm = 0.2262
Meta loss on this task batch = 2.5654e-01, Meta loss averaged over last 500 steps = 2.5371e-01, PNorm = 182.3646, GNorm = 0.2658
Meta loss on this task batch = 2.5542e-01, Meta loss averaged over last 500 steps = 2.5371e-01, PNorm = 182.3679, GNorm = 0.2153
Meta loss on this task batch = 2.2993e-01, Meta loss averaged over last 500 steps = 2.5370e-01, PNorm = 182.3714, GNorm = 0.2149
Meta loss on this task batch = 2.5973e-01, Meta loss averaged over last 500 steps = 2.5367e-01, PNorm = 182.3749, GNorm = 0.2615
Meta loss on this task batch = 2.3369e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 182.3783, GNorm = 0.2162
Meta loss on this task batch = 2.4678e-01, Meta loss averaged over last 500 steps = 2.5357e-01, PNorm = 182.3816, GNorm = 0.2818
Meta loss on this task batch = 2.3674e-01, Meta loss averaged over last 500 steps = 2.5349e-01, PNorm = 182.3847, GNorm = 0.2365
Meta loss on this task batch = 2.8983e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 182.3879, GNorm = 0.2438
Meta loss on this task batch = 2.1862e-01, Meta loss averaged over last 500 steps = 2.5357e-01, PNorm = 182.3910, GNorm = 0.2086
Meta loss on this task batch = 2.2005e-01, Meta loss averaged over last 500 steps = 2.5352e-01, PNorm = 182.3942, GNorm = 0.2093
Meta loss on this task batch = 2.8487e-01, Meta loss averaged over last 500 steps = 2.5352e-01, PNorm = 182.3967, GNorm = 0.2858
Meta loss on this task batch = 2.4113e-01, Meta loss averaged over last 500 steps = 2.5344e-01, PNorm = 182.3994, GNorm = 0.2830
Took 103.19013476371765 seconds to complete one epoch of meta training
Took 110.10741949081421 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.491505
Epoch 983
Meta loss on this task batch = 2.9937e-01, Meta loss averaged over last 500 steps = 2.5363e-01, PNorm = 182.4016, GNorm = 0.2577
Meta loss on this task batch = 2.4853e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 182.4042, GNorm = 0.2356
Meta loss on this task batch = 2.4596e-01, Meta loss averaged over last 500 steps = 2.5342e-01, PNorm = 182.4070, GNorm = 0.2122
Meta loss on this task batch = 2.1447e-01, Meta loss averaged over last 500 steps = 2.5327e-01, PNorm = 182.4105, GNorm = 0.2153
Meta loss on this task batch = 2.2692e-01, Meta loss averaged over last 500 steps = 2.5329e-01, PNorm = 182.4142, GNorm = 0.2079
Meta loss on this task batch = 1.9633e-01, Meta loss averaged over last 500 steps = 2.5313e-01, PNorm = 182.4185, GNorm = 0.1960
Meta loss on this task batch = 2.5927e-01, Meta loss averaged over last 500 steps = 2.5314e-01, PNorm = 182.4228, GNorm = 0.2375
Meta loss on this task batch = 2.6145e-01, Meta loss averaged over last 500 steps = 2.5306e-01, PNorm = 182.4266, GNorm = 0.2641
Meta loss on this task batch = 2.3380e-01, Meta loss averaged over last 500 steps = 2.5308e-01, PNorm = 182.4304, GNorm = 0.2398
Meta loss on this task batch = 2.7619e-01, Meta loss averaged over last 500 steps = 2.5320e-01, PNorm = 182.4344, GNorm = 0.2739
Meta loss on this task batch = 2.6048e-01, Meta loss averaged over last 500 steps = 2.5317e-01, PNorm = 182.4386, GNorm = 0.2680
Meta loss on this task batch = 2.7605e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 182.4417, GNorm = 0.3444
Meta loss on this task batch = 2.5621e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 182.4457, GNorm = 0.2490
Meta loss on this task batch = 2.6863e-01, Meta loss averaged over last 500 steps = 2.5337e-01, PNorm = 182.4495, GNorm = 0.2111
Meta loss on this task batch = 2.5976e-01, Meta loss averaged over last 500 steps = 2.5333e-01, PNorm = 182.4537, GNorm = 0.2649
Meta loss on this task batch = 2.6785e-01, Meta loss averaged over last 500 steps = 2.5340e-01, PNorm = 182.4582, GNorm = 0.2391
Meta loss on this task batch = 2.1940e-01, Meta loss averaged over last 500 steps = 2.5333e-01, PNorm = 182.4632, GNorm = 0.2011
Meta loss on this task batch = 2.2138e-01, Meta loss averaged over last 500 steps = 2.5327e-01, PNorm = 182.4681, GNorm = 0.2188
Meta loss on this task batch = 2.9292e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 182.4729, GNorm = 0.2856
Took 99.9323239326477 seconds to complete one epoch of meta training
Took 106.90602970123291 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.476383
Epoch 984
Meta loss on this task batch = 2.8390e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 182.4781, GNorm = 0.2435
Meta loss on this task batch = 2.1927e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 182.4838, GNorm = 0.2374
Meta loss on this task batch = 3.0804e-01, Meta loss averaged over last 500 steps = 2.5329e-01, PNorm = 182.4886, GNorm = 0.2564
Meta loss on this task batch = 2.2682e-01, Meta loss averaged over last 500 steps = 2.5323e-01, PNorm = 182.4934, GNorm = 0.1966
Meta loss on this task batch = 2.4008e-01, Meta loss averaged over last 500 steps = 2.5316e-01, PNorm = 182.4981, GNorm = 0.2187
Meta loss on this task batch = 2.3735e-01, Meta loss averaged over last 500 steps = 2.5317e-01, PNorm = 182.5031, GNorm = 0.2073
Meta loss on this task batch = 2.4621e-01, Meta loss averaged over last 500 steps = 2.5316e-01, PNorm = 182.5080, GNorm = 0.2315
Meta loss on this task batch = 2.7246e-01, Meta loss averaged over last 500 steps = 2.5320e-01, PNorm = 182.5126, GNorm = 0.2160
Meta loss on this task batch = 2.7678e-01, Meta loss averaged over last 500 steps = 2.5323e-01, PNorm = 182.5159, GNorm = 0.2561
Meta loss on this task batch = 2.9614e-01, Meta loss averaged over last 500 steps = 2.5329e-01, PNorm = 182.5189, GNorm = 0.2611
Meta loss on this task batch = 2.5364e-01, Meta loss averaged over last 500 steps = 2.5316e-01, PNorm = 182.5223, GNorm = 0.2321
Meta loss on this task batch = 1.8665e-01, Meta loss averaged over last 500 steps = 2.5301e-01, PNorm = 182.5254, GNorm = 0.2010
Meta loss on this task batch = 2.4501e-01, Meta loss averaged over last 500 steps = 2.5304e-01, PNorm = 182.5292, GNorm = 0.2282
Meta loss on this task batch = 2.0791e-01, Meta loss averaged over last 500 steps = 2.5294e-01, PNorm = 182.5331, GNorm = 0.2359
Meta loss on this task batch = 2.5148e-01, Meta loss averaged over last 500 steps = 2.5300e-01, PNorm = 182.5369, GNorm = 0.2306
Meta loss on this task batch = 2.2008e-01, Meta loss averaged over last 500 steps = 2.5294e-01, PNorm = 182.5409, GNorm = 0.2102
Meta loss on this task batch = 2.6650e-01, Meta loss averaged over last 500 steps = 2.5302e-01, PNorm = 182.5450, GNorm = 0.2365
Meta loss on this task batch = 3.1193e-01, Meta loss averaged over last 500 steps = 2.5309e-01, PNorm = 182.5480, GNorm = 0.2566
Meta loss on this task batch = 2.1901e-01, Meta loss averaged over last 500 steps = 2.5308e-01, PNorm = 182.5514, GNorm = 0.2495
Took 101.2302393913269 seconds to complete one epoch of meta training
Took 108.3736982345581 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.508414
Epoch 985
Meta loss on this task batch = 3.0949e-01, Meta loss averaged over last 500 steps = 2.5322e-01, PNorm = 182.5543, GNorm = 0.2704
Meta loss on this task batch = 2.4082e-01, Meta loss averaged over last 500 steps = 2.5307e-01, PNorm = 182.5574, GNorm = 0.2264
Meta loss on this task batch = 2.4171e-01, Meta loss averaged over last 500 steps = 2.5308e-01, PNorm = 182.5604, GNorm = 0.2357
Meta loss on this task batch = 2.1379e-01, Meta loss averaged over last 500 steps = 2.5300e-01, PNorm = 182.5633, GNorm = 0.2127
Meta loss on this task batch = 2.4434e-01, Meta loss averaged over last 500 steps = 2.5297e-01, PNorm = 182.5662, GNorm = 0.2316
Meta loss on this task batch = 2.6515e-01, Meta loss averaged over last 500 steps = 2.5296e-01, PNorm = 182.5693, GNorm = 0.2376
Meta loss on this task batch = 2.2510e-01, Meta loss averaged over last 500 steps = 2.5289e-01, PNorm = 182.5723, GNorm = 0.1970
Meta loss on this task batch = 2.2888e-01, Meta loss averaged over last 500 steps = 2.5280e-01, PNorm = 182.5759, GNorm = 0.2216
Meta loss on this task batch = 3.2164e-01, Meta loss averaged over last 500 steps = 2.5288e-01, PNorm = 182.5797, GNorm = 0.2696
Meta loss on this task batch = 2.6910e-01, Meta loss averaged over last 500 steps = 2.5286e-01, PNorm = 182.5832, GNorm = 0.2223
Meta loss on this task batch = 2.7074e-01, Meta loss averaged over last 500 steps = 2.5302e-01, PNorm = 182.5871, GNorm = 0.2485
Meta loss on this task batch = 2.5025e-01, Meta loss averaged over last 500 steps = 2.5291e-01, PNorm = 182.5909, GNorm = 0.2472
Meta loss on this task batch = 2.7697e-01, Meta loss averaged over last 500 steps = 2.5297e-01, PNorm = 182.5951, GNorm = 0.2210
Meta loss on this task batch = 2.0321e-01, Meta loss averaged over last 500 steps = 2.5287e-01, PNorm = 182.6001, GNorm = 0.1803
Meta loss on this task batch = 2.5030e-01, Meta loss averaged over last 500 steps = 2.5296e-01, PNorm = 182.6049, GNorm = 0.2439
Meta loss on this task batch = 2.5642e-01, Meta loss averaged over last 500 steps = 2.5302e-01, PNorm = 182.6096, GNorm = 0.2290
Meta loss on this task batch = 2.4820e-01, Meta loss averaged over last 500 steps = 2.5293e-01, PNorm = 182.6139, GNorm = 0.2346
Meta loss on this task batch = 2.7588e-01, Meta loss averaged over last 500 steps = 2.5297e-01, PNorm = 182.6188, GNorm = 0.2946
Meta loss on this task batch = 2.7295e-01, Meta loss averaged over last 500 steps = 2.5305e-01, PNorm = 182.6240, GNorm = 0.2753
Took 102.62974405288696 seconds to complete one epoch of meta training
Took 109.75353288650513 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.501652
Epoch 986
Meta loss on this task batch = 2.4580e-01, Meta loss averaged over last 500 steps = 2.5310e-01, PNorm = 182.6290, GNorm = 0.2277
Meta loss on this task batch = 2.5423e-01, Meta loss averaged over last 500 steps = 2.5308e-01, PNorm = 182.6340, GNorm = 0.2289
Meta loss on this task batch = 2.1343e-01, Meta loss averaged over last 500 steps = 2.5309e-01, PNorm = 182.6393, GNorm = 0.1935
Meta loss on this task batch = 2.3948e-01, Meta loss averaged over last 500 steps = 2.5311e-01, PNorm = 182.6446, GNorm = 0.2387
Meta loss on this task batch = 2.4979e-01, Meta loss averaged over last 500 steps = 2.5311e-01, PNorm = 182.6494, GNorm = 0.2812
Meta loss on this task batch = 2.4428e-01, Meta loss averaged over last 500 steps = 2.5300e-01, PNorm = 182.6540, GNorm = 0.2195
Meta loss on this task batch = 2.8531e-01, Meta loss averaged over last 500 steps = 2.5297e-01, PNorm = 182.6579, GNorm = 0.2430
Meta loss on this task batch = 2.5722e-01, Meta loss averaged over last 500 steps = 2.5301e-01, PNorm = 182.6615, GNorm = 0.2940
Meta loss on this task batch = 2.5032e-01, Meta loss averaged over last 500 steps = 2.5294e-01, PNorm = 182.6648, GNorm = 0.2121
Meta loss on this task batch = 2.6649e-01, Meta loss averaged over last 500 steps = 2.5296e-01, PNorm = 182.6681, GNorm = 0.2993
Meta loss on this task batch = 2.3945e-01, Meta loss averaged over last 500 steps = 2.5283e-01, PNorm = 182.6715, GNorm = 0.2059
Meta loss on this task batch = 1.7994e-01, Meta loss averaged over last 500 steps = 2.5275e-01, PNorm = 182.6753, GNorm = 0.2443
Meta loss on this task batch = 2.4476e-01, Meta loss averaged over last 500 steps = 2.5276e-01, PNorm = 182.6793, GNorm = 0.2403
Meta loss on this task batch = 2.5066e-01, Meta loss averaged over last 500 steps = 2.5278e-01, PNorm = 182.6835, GNorm = 0.2406
Meta loss on this task batch = 2.6388e-01, Meta loss averaged over last 500 steps = 2.5282e-01, PNorm = 182.6881, GNorm = 0.2157
Meta loss on this task batch = 2.5933e-01, Meta loss averaged over last 500 steps = 2.5282e-01, PNorm = 182.6926, GNorm = 0.2692
Meta loss on this task batch = 2.6221e-01, Meta loss averaged over last 500 steps = 2.5292e-01, PNorm = 182.6972, GNorm = 0.2270
Meta loss on this task batch = 2.8485e-01, Meta loss averaged over last 500 steps = 2.5296e-01, PNorm = 182.7027, GNorm = 0.2523
Meta loss on this task batch = 3.2339e-01, Meta loss averaged over last 500 steps = 2.5311e-01, PNorm = 182.7080, GNorm = 0.3237
Took 101.90143156051636 seconds to complete one epoch of meta training
Took 108.85179591178894 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483823
Epoch 987
Meta loss on this task batch = 2.8694e-01, Meta loss averaged over last 500 steps = 2.5313e-01, PNorm = 182.7137, GNorm = 0.2625
Meta loss on this task batch = 1.9222e-01, Meta loss averaged over last 500 steps = 2.5301e-01, PNorm = 182.7193, GNorm = 0.1950
Meta loss on this task batch = 2.6805e-01, Meta loss averaged over last 500 steps = 2.5307e-01, PNorm = 182.7244, GNorm = 0.2343
Meta loss on this task batch = 2.8173e-01, Meta loss averaged over last 500 steps = 2.5314e-01, PNorm = 182.7295, GNorm = 0.2276
Meta loss on this task batch = 2.4560e-01, Meta loss averaged over last 500 steps = 2.5312e-01, PNorm = 182.7348, GNorm = 0.2258
Meta loss on this task batch = 2.8033e-01, Meta loss averaged over last 500 steps = 2.5326e-01, PNorm = 182.7399, GNorm = 0.2503
Meta loss on this task batch = 2.1690e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 182.7452, GNorm = 0.2122
Meta loss on this task batch = 2.9255e-01, Meta loss averaged over last 500 steps = 2.5338e-01, PNorm = 182.7502, GNorm = 0.2428
Meta loss on this task batch = 2.2003e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 182.7554, GNorm = 0.2236
Meta loss on this task batch = 2.6211e-01, Meta loss averaged over last 500 steps = 2.5331e-01, PNorm = 182.7609, GNorm = 0.2391
Meta loss on this task batch = 2.6136e-01, Meta loss averaged over last 500 steps = 2.5337e-01, PNorm = 182.7663, GNorm = 0.2334
Meta loss on this task batch = 2.6125e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 182.7710, GNorm = 0.2571
Meta loss on this task batch = 2.2109e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 182.7761, GNorm = 0.1946
Meta loss on this task batch = 3.0853e-01, Meta loss averaged over last 500 steps = 2.5340e-01, PNorm = 182.7812, GNorm = 0.3198
Meta loss on this task batch = 2.4546e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 182.7861, GNorm = 0.2535
Meta loss on this task batch = 2.5680e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 182.7904, GNorm = 0.2066
Meta loss on this task batch = 2.5661e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 182.7946, GNorm = 0.2413
Meta loss on this task batch = 2.5986e-01, Meta loss averaged over last 500 steps = 2.5334e-01, PNorm = 182.7985, GNorm = 0.2589
Meta loss on this task batch = 3.2358e-01, Meta loss averaged over last 500 steps = 2.5342e-01, PNorm = 182.8028, GNorm = 0.3018
Took 100.78869414329529 seconds to complete one epoch of meta training
Took 107.57473635673523 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.502793
Epoch 988
Meta loss on this task batch = 2.4387e-01, Meta loss averaged over last 500 steps = 2.5349e-01, PNorm = 182.8085, GNorm = 0.2374
Meta loss on this task batch = 2.7749e-01, Meta loss averaged over last 500 steps = 2.5348e-01, PNorm = 182.8144, GNorm = 0.2485
Meta loss on this task batch = 2.1668e-01, Meta loss averaged over last 500 steps = 2.5329e-01, PNorm = 182.8208, GNorm = 0.1987
Meta loss on this task batch = 2.3010e-01, Meta loss averaged over last 500 steps = 2.5322e-01, PNorm = 182.8275, GNorm = 0.2179
Meta loss on this task batch = 2.9542e-01, Meta loss averaged over last 500 steps = 2.5330e-01, PNorm = 182.8335, GNorm = 0.2477
Meta loss on this task batch = 2.5232e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 182.8391, GNorm = 0.2178
Meta loss on this task batch = 2.4392e-01, Meta loss averaged over last 500 steps = 2.5329e-01, PNorm = 182.8447, GNorm = 0.2152
Meta loss on this task batch = 2.3623e-01, Meta loss averaged over last 500 steps = 2.5325e-01, PNorm = 182.8502, GNorm = 0.2214
Meta loss on this task batch = 2.6417e-01, Meta loss averaged over last 500 steps = 2.5328e-01, PNorm = 182.8558, GNorm = 0.2223
Meta loss on this task batch = 2.4828e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 182.8614, GNorm = 0.2160
Meta loss on this task batch = 2.5263e-01, Meta loss averaged over last 500 steps = 2.5317e-01, PNorm = 182.8666, GNorm = 0.2672
Meta loss on this task batch = 2.6542e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 182.8714, GNorm = 0.2908
Meta loss on this task batch = 2.9401e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 182.8748, GNorm = 0.2511
Meta loss on this task batch = 2.5922e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 182.8770, GNorm = 0.2694
Meta loss on this task batch = 2.4094e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 182.8795, GNorm = 0.2051
Meta loss on this task batch = 3.0313e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 182.8825, GNorm = 0.2306
Meta loss on this task batch = 2.4035e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 182.8854, GNorm = 0.2351
Meta loss on this task batch = 2.8180e-01, Meta loss averaged over last 500 steps = 2.5370e-01, PNorm = 182.8876, GNorm = 0.2186
Meta loss on this task batch = 2.3173e-01, Meta loss averaged over last 500 steps = 2.5366e-01, PNorm = 182.8901, GNorm = 0.2534
Took 100.54265403747559 seconds to complete one epoch of meta training
Took 107.47536063194275 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.496962
Epoch 989
Meta loss on this task batch = 2.6031e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 182.8932, GNorm = 0.2346
Meta loss on this task batch = 2.3926e-01, Meta loss averaged over last 500 steps = 2.5363e-01, PNorm = 182.8959, GNorm = 0.2396
Meta loss on this task batch = 2.8905e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 182.8987, GNorm = 0.2731
Meta loss on this task batch = 2.5741e-01, Meta loss averaged over last 500 steps = 2.5369e-01, PNorm = 182.9016, GNorm = 0.2391
Meta loss on this task batch = 2.9050e-01, Meta loss averaged over last 500 steps = 2.5375e-01, PNorm = 182.9046, GNorm = 0.2063
Meta loss on this task batch = 2.8856e-01, Meta loss averaged over last 500 steps = 2.5382e-01, PNorm = 182.9073, GNorm = 0.3023
Meta loss on this task batch = 2.9679e-01, Meta loss averaged over last 500 steps = 2.5387e-01, PNorm = 182.9098, GNorm = 0.2070
Meta loss on this task batch = 3.1076e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 182.9118, GNorm = 0.2394
Meta loss on this task batch = 2.3655e-01, Meta loss averaged over last 500 steps = 2.5375e-01, PNorm = 182.9138, GNorm = 0.2306
Meta loss on this task batch = 2.9100e-01, Meta loss averaged over last 500 steps = 2.5392e-01, PNorm = 182.9173, GNorm = 0.2453
Meta loss on this task batch = 2.9210e-01, Meta loss averaged over last 500 steps = 2.5403e-01, PNorm = 182.9213, GNorm = 0.2114
Meta loss on this task batch = 2.4181e-01, Meta loss averaged over last 500 steps = 2.5389e-01, PNorm = 182.9258, GNorm = 0.2382
Meta loss on this task batch = 2.6652e-01, Meta loss averaged over last 500 steps = 2.5382e-01, PNorm = 182.9298, GNorm = 0.2514
Meta loss on this task batch = 1.7236e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 182.9347, GNorm = 0.1940
Meta loss on this task batch = 2.3795e-01, Meta loss averaged over last 500 steps = 2.5376e-01, PNorm = 182.9400, GNorm = 0.1873
Meta loss on this task batch = 2.1685e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 182.9457, GNorm = 0.2010
Meta loss on this task batch = 2.6044e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 182.9508, GNorm = 0.2239
Meta loss on this task batch = 1.9292e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 182.9559, GNorm = 0.1921
Meta loss on this task batch = 2.3603e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 182.9610, GNorm = 0.2917
Took 102.1941282749176 seconds to complete one epoch of meta training
Took 109.18918561935425 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.488951
Epoch 990
Meta loss on this task batch = 2.7766e-01, Meta loss averaged over last 500 steps = 2.5359e-01, PNorm = 182.9661, GNorm = 0.2170
Meta loss on this task batch = 2.2987e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 182.9711, GNorm = 0.2160
Meta loss on this task batch = 2.2453e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 182.9760, GNorm = 0.2529
Meta loss on this task batch = 2.0141e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 182.9813, GNorm = 0.2166
Meta loss on this task batch = 2.3536e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 182.9857, GNorm = 0.2219
Meta loss on this task batch = 2.4611e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 182.9899, GNorm = 0.2234
Meta loss on this task batch = 2.8563e-01, Meta loss averaged over last 500 steps = 2.5347e-01, PNorm = 182.9926, GNorm = 0.2844
Meta loss on this task batch = 2.6012e-01, Meta loss averaged over last 500 steps = 2.5347e-01, PNorm = 182.9957, GNorm = 0.2275
Meta loss on this task batch = 2.9465e-01, Meta loss averaged over last 500 steps = 2.5361e-01, PNorm = 182.9984, GNorm = 0.2675
Meta loss on this task batch = 2.7105e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 183.0013, GNorm = 0.2161
Meta loss on this task batch = 2.2038e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 183.0045, GNorm = 0.2179
Meta loss on this task batch = 2.4823e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 183.0084, GNorm = 0.2048
Meta loss on this task batch = 2.5522e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 183.0122, GNorm = 0.2358
Meta loss on this task batch = 2.4322e-01, Meta loss averaged over last 500 steps = 2.5336e-01, PNorm = 183.0167, GNorm = 0.2661
Meta loss on this task batch = 2.9090e-01, Meta loss averaged over last 500 steps = 2.5342e-01, PNorm = 183.0209, GNorm = 0.2399
Meta loss on this task batch = 2.2264e-01, Meta loss averaged over last 500 steps = 2.5325e-01, PNorm = 183.0258, GNorm = 0.2402
Meta loss on this task batch = 2.6656e-01, Meta loss averaged over last 500 steps = 2.5321e-01, PNorm = 183.0298, GNorm = 0.2911
Meta loss on this task batch = 2.9761e-01, Meta loss averaged over last 500 steps = 2.5333e-01, PNorm = 183.0343, GNorm = 0.2599
Meta loss on this task batch = 2.8214e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 183.0388, GNorm = 0.2666
Took 103.24476623535156 seconds to complete one epoch of meta training
Took 110.01665043830872 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.484676
Epoch 991
Meta loss on this task batch = 2.3567e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 183.0433, GNorm = 0.2578
Meta loss on this task batch = 2.8582e-01, Meta loss averaged over last 500 steps = 2.5357e-01, PNorm = 183.0475, GNorm = 0.2665
Meta loss on this task batch = 2.6688e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 183.0518, GNorm = 0.2728
Meta loss on this task batch = 2.3077e-01, Meta loss averaged over last 500 steps = 2.5350e-01, PNorm = 183.0565, GNorm = 0.2171
Meta loss on this task batch = 2.4331e-01, Meta loss averaged over last 500 steps = 2.5353e-01, PNorm = 183.0614, GNorm = 0.2258
Meta loss on this task batch = 2.4107e-01, Meta loss averaged over last 500 steps = 2.5366e-01, PNorm = 183.0662, GNorm = 0.2225
Meta loss on this task batch = 2.4424e-01, Meta loss averaged over last 500 steps = 2.5352e-01, PNorm = 183.0710, GNorm = 0.2164
Meta loss on this task batch = 2.2516e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 183.0756, GNorm = 0.2024
Meta loss on this task batch = 2.7999e-01, Meta loss averaged over last 500 steps = 2.5359e-01, PNorm = 183.0786, GNorm = 0.3077
Meta loss on this task batch = 2.8537e-01, Meta loss averaged over last 500 steps = 2.5373e-01, PNorm = 183.0814, GNorm = 0.2345
Meta loss on this task batch = 2.5081e-01, Meta loss averaged over last 500 steps = 2.5354e-01, PNorm = 183.0845, GNorm = 0.2354
Meta loss on this task batch = 2.4082e-01, Meta loss averaged over last 500 steps = 2.5354e-01, PNorm = 183.0880, GNorm = 0.2379
Meta loss on this task batch = 2.3190e-01, Meta loss averaged over last 500 steps = 2.5348e-01, PNorm = 183.0922, GNorm = 0.2544
Meta loss on this task batch = 2.4238e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 183.0966, GNorm = 0.1949
Meta loss on this task batch = 2.4137e-01, Meta loss averaged over last 500 steps = 2.5353e-01, PNorm = 183.1013, GNorm = 0.2155
Meta loss on this task batch = 2.6967e-01, Meta loss averaged over last 500 steps = 2.5377e-01, PNorm = 183.1066, GNorm = 0.2477
Meta loss on this task batch = 2.3840e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 183.1120, GNorm = 0.2299
Meta loss on this task batch = 2.4581e-01, Meta loss averaged over last 500 steps = 2.5369e-01, PNorm = 183.1174, GNorm = 0.2277
Meta loss on this task batch = 2.6783e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 183.1226, GNorm = 0.2746
Took 104.12587475776672 seconds to complete one epoch of meta training
Took 110.93013715744019 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.506510
Epoch 992
Meta loss on this task batch = 2.6002e-01, Meta loss averaged over last 500 steps = 2.5374e-01, PNorm = 183.1281, GNorm = 0.2401
Meta loss on this task batch = 2.2882e-01, Meta loss averaged over last 500 steps = 2.5355e-01, PNorm = 183.1338, GNorm = 0.2231
Meta loss on this task batch = 2.3402e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 183.1396, GNorm = 0.2519
Meta loss on this task batch = 2.6256e-01, Meta loss averaged over last 500 steps = 2.5357e-01, PNorm = 183.1455, GNorm = 0.2333
Meta loss on this task batch = 2.2377e-01, Meta loss averaged over last 500 steps = 2.5348e-01, PNorm = 183.1519, GNorm = 0.2081
Meta loss on this task batch = 2.9265e-01, Meta loss averaged over last 500 steps = 2.5357e-01, PNorm = 183.1579, GNorm = 0.2442
Meta loss on this task batch = 2.8697e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 183.1641, GNorm = 0.2927
Meta loss on this task batch = 1.8254e-01, Meta loss averaged over last 500 steps = 2.5361e-01, PNorm = 183.1704, GNorm = 0.2108
Meta loss on this task batch = 2.3370e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 183.1766, GNorm = 0.2169
Meta loss on this task batch = 2.6995e-01, Meta loss averaged over last 500 steps = 2.5363e-01, PNorm = 183.1822, GNorm = 0.2419
Meta loss on this task batch = 2.6226e-01, Meta loss averaged over last 500 steps = 2.5366e-01, PNorm = 183.1875, GNorm = 0.2331
Meta loss on this task batch = 2.5021e-01, Meta loss averaged over last 500 steps = 2.5366e-01, PNorm = 183.1928, GNorm = 0.2392
Meta loss on this task batch = 2.9200e-01, Meta loss averaged over last 500 steps = 2.5380e-01, PNorm = 183.1976, GNorm = 0.2404
Meta loss on this task batch = 2.5766e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 183.2029, GNorm = 0.2302
Meta loss on this task batch = 2.2233e-01, Meta loss averaged over last 500 steps = 2.5384e-01, PNorm = 183.2077, GNorm = 0.2450
Meta loss on this task batch = 2.9253e-01, Meta loss averaged over last 500 steps = 2.5391e-01, PNorm = 183.2111, GNorm = 0.2693
Meta loss on this task batch = 2.1176e-01, Meta loss averaged over last 500 steps = 2.5387e-01, PNorm = 183.2148, GNorm = 0.2186
Meta loss on this task batch = 2.2416e-01, Meta loss averaged over last 500 steps = 2.5376e-01, PNorm = 183.2182, GNorm = 0.2534
Meta loss on this task batch = 2.3801e-01, Meta loss averaged over last 500 steps = 2.5378e-01, PNorm = 183.2215, GNorm = 0.2244
Took 95.06192183494568 seconds to complete one epoch of meta training
Took 101.81700301170349 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.519121
Epoch 993
Meta loss on this task batch = 2.2901e-01, Meta loss averaged over last 500 steps = 2.5387e-01, PNorm = 183.2252, GNorm = 0.2156
Meta loss on this task batch = 2.3190e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 183.2281, GNorm = 0.2408
Meta loss on this task batch = 2.3848e-01, Meta loss averaged over last 500 steps = 2.5376e-01, PNorm = 183.2315, GNorm = 0.2342
Meta loss on this task batch = 2.7147e-01, Meta loss averaged over last 500 steps = 2.5374e-01, PNorm = 183.2343, GNorm = 0.2533
Meta loss on this task batch = 2.2220e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 183.2381, GNorm = 0.2089
Meta loss on this task batch = 2.0257e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 183.2424, GNorm = 0.2002
Meta loss on this task batch = 2.8445e-01, Meta loss averaged over last 500 steps = 2.5358e-01, PNorm = 183.2470, GNorm = 0.2405
Meta loss on this task batch = 2.5147e-01, Meta loss averaged over last 500 steps = 2.5361e-01, PNorm = 183.2522, GNorm = 0.2187
Meta loss on this task batch = 2.3169e-01, Meta loss averaged over last 500 steps = 2.5364e-01, PNorm = 183.2568, GNorm = 0.2383
Meta loss on this task batch = 2.4468e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 183.2619, GNorm = 0.2201
Meta loss on this task batch = 2.4144e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 183.2663, GNorm = 0.2534
Meta loss on this task batch = 2.5101e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 183.2697, GNorm = 0.2835
Meta loss on this task batch = 2.1075e-01, Meta loss averaged over last 500 steps = 2.5338e-01, PNorm = 183.2735, GNorm = 0.2218
Meta loss on this task batch = 2.6087e-01, Meta loss averaged over last 500 steps = 2.5337e-01, PNorm = 183.2769, GNorm = 0.2232
Meta loss on this task batch = 2.8557e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 183.2803, GNorm = 0.2019
Meta loss on this task batch = 2.4218e-01, Meta loss averaged over last 500 steps = 2.5340e-01, PNorm = 183.2832, GNorm = 0.2218
Meta loss on this task batch = 3.0386e-01, Meta loss averaged over last 500 steps = 2.5359e-01, PNorm = 183.2860, GNorm = 0.2718
Meta loss on this task batch = 2.5140e-01, Meta loss averaged over last 500 steps = 2.5367e-01, PNorm = 183.2884, GNorm = 0.2481
Meta loss on this task batch = 3.2121e-01, Meta loss averaged over last 500 steps = 2.5386e-01, PNorm = 183.2914, GNorm = 0.3219
Took 96.91924905776978 seconds to complete one epoch of meta training
Took 103.57063698768616 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.465248
Epoch 994
Meta loss on this task batch = 2.8847e-01, Meta loss averaged over last 500 steps = 2.5392e-01, PNorm = 183.2948, GNorm = 0.2746
Meta loss on this task batch = 2.4568e-01, Meta loss averaged over last 500 steps = 2.5388e-01, PNorm = 183.2984, GNorm = 0.2330
Meta loss on this task batch = 3.0123e-01, Meta loss averaged over last 500 steps = 2.5408e-01, PNorm = 183.3022, GNorm = 0.2921
Meta loss on this task batch = 2.1197e-01, Meta loss averaged over last 500 steps = 2.5402e-01, PNorm = 183.3060, GNorm = 0.2355
Meta loss on this task batch = 2.4312e-01, Meta loss averaged over last 500 steps = 2.5382e-01, PNorm = 183.3093, GNorm = 0.2620
Meta loss on this task batch = 2.3554e-01, Meta loss averaged over last 500 steps = 2.5378e-01, PNorm = 183.3131, GNorm = 0.2249
Meta loss on this task batch = 2.5372e-01, Meta loss averaged over last 500 steps = 2.5377e-01, PNorm = 183.3173, GNorm = 0.2248
Meta loss on this task batch = 2.0415e-01, Meta loss averaged over last 500 steps = 2.5371e-01, PNorm = 183.3218, GNorm = 0.1895
Meta loss on this task batch = 2.2859e-01, Meta loss averaged over last 500 steps = 2.5375e-01, PNorm = 183.3262, GNorm = 0.2095
Meta loss on this task batch = 2.2636e-01, Meta loss averaged over last 500 steps = 2.5367e-01, PNorm = 183.3302, GNorm = 0.2710
Meta loss on this task batch = 2.3115e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 183.3338, GNorm = 0.2141
Meta loss on this task batch = 2.4345e-01, Meta loss averaged over last 500 steps = 2.5347e-01, PNorm = 183.3381, GNorm = 0.2436
Meta loss on this task batch = 2.7232e-01, Meta loss averaged over last 500 steps = 2.5352e-01, PNorm = 183.3417, GNorm = 0.2432
Meta loss on this task batch = 2.5943e-01, Meta loss averaged over last 500 steps = 2.5362e-01, PNorm = 183.3452, GNorm = 0.2538
Meta loss on this task batch = 2.6356e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 183.3491, GNorm = 0.2201
Meta loss on this task batch = 3.0482e-01, Meta loss averaged over last 500 steps = 2.5364e-01, PNorm = 183.3525, GNorm = 0.2449
Meta loss on this task batch = 3.1804e-01, Meta loss averaged over last 500 steps = 2.5373e-01, PNorm = 183.3548, GNorm = 0.2446
Meta loss on this task batch = 2.4841e-01, Meta loss averaged over last 500 steps = 2.5387e-01, PNorm = 183.3576, GNorm = 0.2256
Meta loss on this task batch = 2.1904e-01, Meta loss averaged over last 500 steps = 2.5381e-01, PNorm = 183.3610, GNorm = 0.2657
Took 95.24200391769409 seconds to complete one epoch of meta training
Took 101.9833128452301 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.472996
Epoch 995
Meta loss on this task batch = 1.9047e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 183.3649, GNorm = 0.1766
Meta loss on this task batch = 2.6758e-01, Meta loss averaged over last 500 steps = 2.5364e-01, PNorm = 183.3689, GNorm = 0.2044
Meta loss on this task batch = 2.5401e-01, Meta loss averaged over last 500 steps = 2.5368e-01, PNorm = 183.3734, GNorm = 0.2156
Meta loss on this task batch = 2.8395e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 183.3774, GNorm = 0.2753
Meta loss on this task batch = 2.3414e-01, Meta loss averaged over last 500 steps = 2.5366e-01, PNorm = 183.3817, GNorm = 0.2236
Meta loss on this task batch = 3.0356e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 183.3854, GNorm = 0.2817
Meta loss on this task batch = 1.9489e-01, Meta loss averaged over last 500 steps = 2.5360e-01, PNorm = 183.3893, GNorm = 0.2131
Meta loss on this task batch = 2.9417e-01, Meta loss averaged over last 500 steps = 2.5378e-01, PNorm = 183.3930, GNorm = 0.2772
Meta loss on this task batch = 1.9926e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 183.3971, GNorm = 0.2080
Meta loss on this task batch = 2.2508e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 183.4018, GNorm = 0.2154
Meta loss on this task batch = 2.4637e-01, Meta loss averaged over last 500 steps = 2.5348e-01, PNorm = 183.4066, GNorm = 0.2070
Meta loss on this task batch = 2.7020e-01, Meta loss averaged over last 500 steps = 2.5353e-01, PNorm = 183.4108, GNorm = 0.2751
Meta loss on this task batch = 3.2079e-01, Meta loss averaged over last 500 steps = 2.5372e-01, PNorm = 183.4134, GNorm = 0.2441
Meta loss on this task batch = 2.4923e-01, Meta loss averaged over last 500 steps = 2.5367e-01, PNorm = 183.4164, GNorm = 0.2181
Meta loss on this task batch = 2.1019e-01, Meta loss averaged over last 500 steps = 2.5362e-01, PNorm = 183.4197, GNorm = 0.2169
Meta loss on this task batch = 3.0354e-01, Meta loss averaged over last 500 steps = 2.5371e-01, PNorm = 183.4227, GNorm = 0.2617
Meta loss on this task batch = 2.7152e-01, Meta loss averaged over last 500 steps = 2.5370e-01, PNorm = 183.4257, GNorm = 0.2676
Meta loss on this task batch = 2.6064e-01, Meta loss averaged over last 500 steps = 2.5369e-01, PNorm = 183.4293, GNorm = 0.2398
Meta loss on this task batch = 2.4405e-01, Meta loss averaged over last 500 steps = 2.5365e-01, PNorm = 183.4330, GNorm = 0.2499
Took 95.83955693244934 seconds to complete one epoch of meta training
Took 102.4649178981781 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.481670
Epoch 996
Meta loss on this task batch = 2.5236e-01, Meta loss averaged over last 500 steps = 2.5369e-01, PNorm = 183.4368, GNorm = 0.2468
Meta loss on this task batch = 2.3233e-01, Meta loss averaged over last 500 steps = 2.5358e-01, PNorm = 183.4407, GNorm = 0.2125
Meta loss on this task batch = 1.9125e-01, Meta loss averaged over last 500 steps = 2.5341e-01, PNorm = 183.4446, GNorm = 0.2021
Meta loss on this task batch = 2.0861e-01, Meta loss averaged over last 500 steps = 2.5337e-01, PNorm = 183.4491, GNorm = 0.1904
Meta loss on this task batch = 2.5331e-01, Meta loss averaged over last 500 steps = 2.5338e-01, PNorm = 183.4539, GNorm = 0.2203
Meta loss on this task batch = 2.4118e-01, Meta loss averaged over last 500 steps = 2.5336e-01, PNorm = 183.4593, GNorm = 0.2406
Meta loss on this task batch = 1.8609e-01, Meta loss averaged over last 500 steps = 2.5318e-01, PNorm = 183.4642, GNorm = 0.2236
Meta loss on this task batch = 2.3949e-01, Meta loss averaged over last 500 steps = 2.5306e-01, PNorm = 183.4693, GNorm = 0.2238
Meta loss on this task batch = 2.9463e-01, Meta loss averaged over last 500 steps = 2.5305e-01, PNorm = 183.4743, GNorm = 0.2373
Meta loss on this task batch = 2.5240e-01, Meta loss averaged over last 500 steps = 2.5303e-01, PNorm = 183.4793, GNorm = 0.2599
Meta loss on this task batch = 2.7902e-01, Meta loss averaged over last 500 steps = 2.5302e-01, PNorm = 183.4844, GNorm = 0.2425
Meta loss on this task batch = 2.1615e-01, Meta loss averaged over last 500 steps = 2.5295e-01, PNorm = 183.4895, GNorm = 0.1708
Meta loss on this task batch = 2.6355e-01, Meta loss averaged over last 500 steps = 2.5299e-01, PNorm = 183.4949, GNorm = 0.2236
Meta loss on this task batch = 2.5092e-01, Meta loss averaged over last 500 steps = 2.5311e-01, PNorm = 183.4995, GNorm = 0.2331
Meta loss on this task batch = 3.0860e-01, Meta loss averaged over last 500 steps = 2.5323e-01, PNorm = 183.5036, GNorm = 0.2693
Meta loss on this task batch = 2.5630e-01, Meta loss averaged over last 500 steps = 2.5327e-01, PNorm = 183.5076, GNorm = 0.2129
Meta loss on this task batch = 2.6719e-01, Meta loss averaged over last 500 steps = 2.5328e-01, PNorm = 183.5112, GNorm = 0.3019
Meta loss on this task batch = 2.4053e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 183.5144, GNorm = 0.2300
Meta loss on this task batch = 2.2784e-01, Meta loss averaged over last 500 steps = 2.5306e-01, PNorm = 183.5172, GNorm = 0.2665
Took 97.00381374359131 seconds to complete one epoch of meta training
Took 103.71247577667236 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.490452
Epoch 997
Meta loss on this task batch = 2.8404e-01, Meta loss averaged over last 500 steps = 2.5314e-01, PNorm = 183.5205, GNorm = 0.2649
Meta loss on this task batch = 2.3688e-01, Meta loss averaged over last 500 steps = 2.5320e-01, PNorm = 183.5240, GNorm = 0.2280
Meta loss on this task batch = 2.5197e-01, Meta loss averaged over last 500 steps = 2.5326e-01, PNorm = 183.5275, GNorm = 0.2288
Meta loss on this task batch = 2.3679e-01, Meta loss averaged over last 500 steps = 2.5320e-01, PNorm = 183.5315, GNorm = 0.1916
Meta loss on this task batch = 2.3626e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 183.5352, GNorm = 0.2194
Meta loss on this task batch = 2.8081e-01, Meta loss averaged over last 500 steps = 2.5327e-01, PNorm = 183.5384, GNorm = 0.2655
Meta loss on this task batch = 2.5628e-01, Meta loss averaged over last 500 steps = 2.5329e-01, PNorm = 183.5422, GNorm = 0.2268
Meta loss on this task batch = 2.7553e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 183.5458, GNorm = 0.2232
Meta loss on this task batch = 2.1619e-01, Meta loss averaged over last 500 steps = 2.5330e-01, PNorm = 183.5501, GNorm = 0.2197
Meta loss on this task batch = 2.9433e-01, Meta loss averaged over last 500 steps = 2.5340e-01, PNorm = 183.5544, GNorm = 0.2406
Meta loss on this task batch = 2.6181e-01, Meta loss averaged over last 500 steps = 2.5350e-01, PNorm = 183.5585, GNorm = 0.2580
Meta loss on this task batch = 2.5324e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 183.5621, GNorm = 0.2208
Meta loss on this task batch = 2.5303e-01, Meta loss averaged over last 500 steps = 2.5351e-01, PNorm = 183.5653, GNorm = 0.2520
Meta loss on this task batch = 2.5655e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 183.5687, GNorm = 0.2387
Meta loss on this task batch = 2.4200e-01, Meta loss averaged over last 500 steps = 2.5336e-01, PNorm = 183.5720, GNorm = 0.2516
Meta loss on this task batch = 2.4386e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 183.5755, GNorm = 0.2164
Meta loss on this task batch = 2.7621e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 183.5788, GNorm = 0.2475
Meta loss on this task batch = 2.7893e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 183.5817, GNorm = 0.2820
Meta loss on this task batch = 2.8808e-01, Meta loss averaged over last 500 steps = 2.5321e-01, PNorm = 183.5847, GNorm = 0.4527
Took 94.81045889854431 seconds to complete one epoch of meta training
Took 101.53967428207397 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.483596
Epoch 998
Meta loss on this task batch = 2.2149e-01, Meta loss averaged over last 500 steps = 2.5321e-01, PNorm = 183.5877, GNorm = 0.2066
Meta loss on this task batch = 2.3146e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 183.5908, GNorm = 0.2321
Meta loss on this task batch = 2.3316e-01, Meta loss averaged over last 500 steps = 2.5323e-01, PNorm = 183.5936, GNorm = 0.2714
Meta loss on this task batch = 1.9196e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 183.5965, GNorm = 0.1914
Meta loss on this task batch = 2.9641e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 183.5998, GNorm = 0.2597
Meta loss on this task batch = 2.8083e-01, Meta loss averaged over last 500 steps = 2.5341e-01, PNorm = 183.6027, GNorm = 0.2541
Meta loss on this task batch = 3.0729e-01, Meta loss averaged over last 500 steps = 2.5356e-01, PNorm = 183.6056, GNorm = 0.2387
Meta loss on this task batch = 2.8496e-01, Meta loss averaged over last 500 steps = 2.5362e-01, PNorm = 183.6093, GNorm = 0.2702
Meta loss on this task batch = 2.4917e-01, Meta loss averaged over last 500 steps = 2.5359e-01, PNorm = 183.6124, GNorm = 0.2231
Meta loss on this task batch = 2.1477e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 183.6158, GNorm = 0.2386
Meta loss on this task batch = 2.8074e-01, Meta loss averaged over last 500 steps = 2.5334e-01, PNorm = 183.6193, GNorm = 0.2373
Meta loss on this task batch = 2.5165e-01, Meta loss averaged over last 500 steps = 2.5342e-01, PNorm = 183.6229, GNorm = 0.2327
Meta loss on this task batch = 2.4585e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 183.6265, GNorm = 0.2297
Meta loss on this task batch = 2.7755e-01, Meta loss averaged over last 500 steps = 2.5342e-01, PNorm = 183.6304, GNorm = 0.3086
Meta loss on this task batch = 2.2375e-01, Meta loss averaged over last 500 steps = 2.5341e-01, PNorm = 183.6338, GNorm = 0.2476
Meta loss on this task batch = 2.5631e-01, Meta loss averaged over last 500 steps = 2.5336e-01, PNorm = 183.6375, GNorm = 0.1960
Meta loss on this task batch = 2.8844e-01, Meta loss averaged over last 500 steps = 2.5343e-01, PNorm = 183.6418, GNorm = 0.2315
Meta loss on this task batch = 2.3623e-01, Meta loss averaged over last 500 steps = 2.5350e-01, PNorm = 183.6457, GNorm = 0.2493
Meta loss on this task batch = 2.1806e-01, Meta loss averaged over last 500 steps = 2.5352e-01, PNorm = 183.6502, GNorm = 0.2285
Took 95.20283603668213 seconds to complete one epoch of meta training
Took 101.97632813453674 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.500223
Epoch 999
Meta loss on this task batch = 2.5753e-01, Meta loss averaged over last 500 steps = 2.5350e-01, PNorm = 183.6542, GNorm = 0.2326
Meta loss on this task batch = 2.7289e-01, Meta loss averaged over last 500 steps = 2.5349e-01, PNorm = 183.6585, GNorm = 0.2219
Meta loss on this task batch = 2.2286e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 183.6627, GNorm = 0.2062
Meta loss on this task batch = 2.5159e-01, Meta loss averaged over last 500 steps = 2.5349e-01, PNorm = 183.6670, GNorm = 0.2250
Meta loss on this task batch = 2.4367e-01, Meta loss averaged over last 500 steps = 2.5346e-01, PNorm = 183.6714, GNorm = 0.2241
Meta loss on this task batch = 2.5150e-01, Meta loss averaged over last 500 steps = 2.5339e-01, PNorm = 183.6756, GNorm = 0.2132
Meta loss on this task batch = 2.1895e-01, Meta loss averaged over last 500 steps = 2.5320e-01, PNorm = 183.6799, GNorm = 0.2213
Meta loss on this task batch = 2.5869e-01, Meta loss averaged over last 500 steps = 2.5326e-01, PNorm = 183.6840, GNorm = 0.2088
Meta loss on this task batch = 2.9713e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 183.6879, GNorm = 0.2298
Meta loss on this task batch = 2.4467e-01, Meta loss averaged over last 500 steps = 2.5334e-01, PNorm = 183.6917, GNorm = 0.2265
Meta loss on this task batch = 2.8056e-01, Meta loss averaged over last 500 steps = 2.5334e-01, PNorm = 183.6959, GNorm = 0.2424
Meta loss on this task batch = 2.3690e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 183.7007, GNorm = 0.2284
Meta loss on this task batch = 2.2138e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 183.7061, GNorm = 0.2401
Meta loss on this task batch = 2.4158e-01, Meta loss averaged over last 500 steps = 2.5319e-01, PNorm = 183.7107, GNorm = 0.2122
Meta loss on this task batch = 2.3779e-01, Meta loss averaged over last 500 steps = 2.5324e-01, PNorm = 183.7156, GNorm = 0.2360
Meta loss on this task batch = 2.4839e-01, Meta loss averaged over last 500 steps = 2.5332e-01, PNorm = 183.7199, GNorm = 0.2515
Meta loss on this task batch = 2.4937e-01, Meta loss averaged over last 500 steps = 2.5335e-01, PNorm = 183.7244, GNorm = 0.2662
Meta loss on this task batch = 2.6375e-01, Meta loss averaged over last 500 steps = 2.5350e-01, PNorm = 183.7281, GNorm = 0.2506
Meta loss on this task batch = 2.5664e-01, Meta loss averaged over last 500 steps = 2.5345e-01, PNorm = 183.7315, GNorm = 0.2530
Took 95.779705286026 seconds to complete one epoch of meta training
Took 101.63047122955322 seconds to complete one epoch of meta training and validating
Meta validation prc-auc = 0.479914
Best validation prc-auc = 0.544648 on epoch 954
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Beginning meta testing
Meta testing on task: CHEMBL1794355
New best model for test task CHEMBL1794355 at epoch 1 with val loss 0.6084539890289307
New best model for test task CHEMBL1794355 at epoch 2 with val loss 0.6048250794410706
New best model for test task CHEMBL1794355 at epoch 3 with val loss 0.6011549234390259
New best model for test task CHEMBL1794355 at epoch 4 with val loss 0.5973616242408752
New best model for test task CHEMBL1794355 at epoch 5 with val loss 0.5936523675918579
New best model for test task CHEMBL1794355 at epoch 6 with val loss 0.5896384119987488
New best model for test task CHEMBL1794355 at epoch 7 with val loss 0.5858709216117859
New best model for test task CHEMBL1794355 at epoch 8 with val loss 0.5819538235664368
New best model for test task CHEMBL1794355 at epoch 9 with val loss 0.5780521035194397
New best model for test task CHEMBL1794355 at epoch 10 with val loss 0.5742774605751038
New best model for test task CHEMBL1794355 at epoch 11 with val loss 0.5702540874481201
New best model for test task CHEMBL1794355 at epoch 12 with val loss 0.5664038062095642
New best model for test task CHEMBL1794355 at epoch 13 with val loss 0.5624823570251465
New best model for test task CHEMBL1794355 at epoch 14 with val loss 0.5584564208984375
New best model for test task CHEMBL1794355 at epoch 15 with val loss 0.5545614957809448
New best model for test task CHEMBL1794355 at epoch 16 with val loss 0.5505152940750122
New best model for test task CHEMBL1794355 at epoch 17 with val loss 0.5464770793914795
New best model for test task CHEMBL1794355 at epoch 18 with val loss 0.5425464510917664
New best model for test task CHEMBL1794355 at epoch 19 with val loss 0.5384756326675415
New best model for test task CHEMBL1794355 at epoch 20 with val loss 0.5344775915145874
New best model for test task CHEMBL1794355 at epoch 21 with val loss 0.5307853817939758
New best model for test task CHEMBL1794355 at epoch 22 with val loss 0.5271262526512146
New best model for test task CHEMBL1794355 at epoch 23 with val loss 0.5235286355018616
New best model for test task CHEMBL1794355 at epoch 24 with val loss 0.5199397802352905
New best model for test task CHEMBL1794355 at epoch 25 with val loss 0.5164119601249695
New best model for test task CHEMBL1794355 at epoch 26 with val loss 0.5128650665283203
New best model for test task CHEMBL1794355 at epoch 27 with val loss 0.5093019008636475
New best model for test task CHEMBL1794355 at epoch 28 with val loss 0.5058525204658508
New best model for test task CHEMBL1794355 at epoch 29 with val loss 0.5024237036705017
New best model for test task CHEMBL1794355 at epoch 30 with val loss 0.499181866645813
Finished early stopping for task CHEMBL1794355, beginning testing
Meta testing on task: CHEMBL2098499
New best model for test task CHEMBL2098499 at epoch 1 with val loss 0.7180087566375732
Val loss: 0.7181847095489502
Val loss: 0.7183597683906555
Val loss: 0.7184618711471558
Val loss: 0.7186094522476196
Val loss: 0.7187114357948303
Val loss: 0.7189348936080933
Val loss: 0.7190569639205933
Val loss: 0.7192171812057495
Val loss: 0.7193796038627625
Val loss: 0.7195286750793457
Val loss: 0.7196539640426636
Val loss: 0.7198526859283447
Val loss: 0.7199892401695251
Val loss: 0.7201403379440308
Val loss: 0.7202798128128052
Val loss: 0.7204474210739136
Val loss: 0.7206075191497803
Val loss: 0.7207481861114502
Val loss: 0.7208876013755798
Val loss: 0.7210090756416321
Val loss: 0.7211613655090332
Val loss: 0.721276581287384
Val loss: 0.7214643955230713
Val loss: 0.7216074466705322
Val loss: 0.721768856048584
Val loss: 0.7219020128250122
Val loss: 0.7220450639724731
Val loss: 0.7221822142601013
Val loss: 0.7222946286201477
Finished early stopping for task CHEMBL2098499, beginning testing
Meta testing on task: CHEMBL1738131
New best model for test task CHEMBL1738131 at epoch 1 with val loss 0.7128200531005859
New best model for test task CHEMBL1738131 at epoch 2 with val loss 0.709055483341217
Val loss: 0.7189522683620453
New best model for test task CHEMBL1738131 at epoch 4 with val loss 0.7089972794055939
Val loss: 0.7295472323894501
New best model for test task CHEMBL1738131 at epoch 6 with val loss 0.6965529024600983
Val loss: 0.7115523815155029
Val loss: 0.7148620784282684
Val loss: 0.7253380715847015
Val loss: 0.7253504693508148
Val loss: 0.7015341520309448
Val loss: 0.7273492515087128
Val loss: 0.7283161580562592
Val loss: 0.7216659784317017
Val loss: 0.720874696969986
Val loss: 0.7199955880641937
Val loss: 0.7203052639961243
Val loss: 0.7252508401870728
Val loss: 0.7151914834976196
Val loss: 0.7299261391162872
Val loss: 0.7207337021827698
Val loss: 0.7196540534496307
Val loss: 0.7118497788906097
Val loss: 0.729893147945404
Val loss: 0.723462849855423
Val loss: 0.7062431573867798
Val loss: 0.7309267520904541
Val loss: 0.7252388596534729
Val loss: 0.7366321086883545
Val loss: 0.7234471142292023
Finished early stopping for task CHEMBL1738131, beginning testing
Meta testing on task: CHEMBL918058
New best model for test task CHEMBL918058 at epoch 1 with val loss 0.7935864925384521
Val loss: 0.7937107086181641
Val loss: 0.793787956237793
Val loss: 0.7938050627708435
Val loss: 0.7938733696937561
Val loss: 0.79407799243927
Val loss: 0.7942180633544922
Val loss: 0.7944032549858093
Val loss: 0.7944688200950623
Val loss: 0.7945672869682312
Val loss: 0.7948362231254578
Val loss: 0.7950143218040466
Val loss: 0.7951123118400574
Val loss: 0.7951545715332031
Val loss: 0.7952378392219543
Val loss: 0.7953323125839233
Val loss: 0.7955235838890076
Val loss: 0.7956125140190125
Val loss: 0.7958060503005981
Val loss: 0.7959115505218506
Val loss: 0.796108067035675
Val loss: 0.7962155938148499
Val loss: 0.7963374853134155
Val loss: 0.7963451147079468
Val loss: 0.7963557243347168
Val loss: 0.7964263558387756
Val loss: 0.7966338396072388
Val loss: 0.7965905070304871
Val loss: 0.7966210246086121
Val loss: 0.7966048121452332
Finished early stopping for task CHEMBL918058, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL2095143
New best model for test task CHEMBL2095143 at epoch 1 with val loss 0.7922070622444153
New best model for test task CHEMBL2095143 at epoch 2 with val loss 0.7918746471405029
New best model for test task CHEMBL2095143 at epoch 3 with val loss 0.7915785908699036
New best model for test task CHEMBL2095143 at epoch 4 with val loss 0.7912424802780151
New best model for test task CHEMBL2095143 at epoch 5 with val loss 0.7910105586051941
New best model for test task CHEMBL2095143 at epoch 6 with val loss 0.7907097935676575
New best model for test task CHEMBL2095143 at epoch 7 with val loss 0.7902988195419312
New best model for test task CHEMBL2095143 at epoch 8 with val loss 0.7900270819664001
New best model for test task CHEMBL2095143 at epoch 9 with val loss 0.7897545099258423
New best model for test task CHEMBL2095143 at epoch 10 with val loss 0.7895243763923645
New best model for test task CHEMBL2095143 at epoch 11 with val loss 0.7892786264419556
New best model for test task CHEMBL2095143 at epoch 12 with val loss 0.7890790104866028
New best model for test task CHEMBL2095143 at epoch 13 with val loss 0.788853108882904
New best model for test task CHEMBL2095143 at epoch 14 with val loss 0.7884836196899414
New best model for test task CHEMBL2095143 at epoch 15 with val loss 0.7881770730018616
New best model for test task CHEMBL2095143 at epoch 16 with val loss 0.787905216217041
New best model for test task CHEMBL2095143 at epoch 17 with val loss 0.7875787019729614
New best model for test task CHEMBL2095143 at epoch 18 with val loss 0.7873167395591736
New best model for test task CHEMBL2095143 at epoch 19 with val loss 0.7871523499488831
New best model for test task CHEMBL2095143 at epoch 20 with val loss 0.7869739532470703
New best model for test task CHEMBL2095143 at epoch 21 with val loss 0.7867329716682434
New best model for test task CHEMBL2095143 at epoch 22 with val loss 0.7865077257156372
New best model for test task CHEMBL2095143 at epoch 23 with val loss 0.7862457036972046
New best model for test task CHEMBL2095143 at epoch 24 with val loss 0.7859799265861511
New best model for test task CHEMBL2095143 at epoch 25 with val loss 0.7857624888420105
New best model for test task CHEMBL2095143 at epoch 26 with val loss 0.7855737805366516
New best model for test task CHEMBL2095143 at epoch 27 with val loss 0.7853181958198547
New best model for test task CHEMBL2095143 at epoch 28 with val loss 0.7849882245063782
New best model for test task CHEMBL2095143 at epoch 29 with val loss 0.7847568392753601
New best model for test task CHEMBL2095143 at epoch 30 with val loss 0.7845509648323059
Finished early stopping for task CHEMBL2095143, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1614170
New best model for test task CHEMBL1614170 at epoch 1 with val loss 0.690695732831955
Val loss: 0.7051624655723572
Val loss: 0.6967984437942505
Val loss: 0.6909245252609253
Val loss: 0.6934210658073425
Val loss: 0.7069220244884491
Val loss: 0.7062663435935974
Val loss: 0.7071773409843445
Val loss: 0.6934602558612823
Val loss: 0.7006618976593018
Val loss: 0.7077852785587311
Val loss: 0.6928284168243408
Val loss: 0.7164566516876221
Val loss: 0.6999256908893585
Val loss: 0.7005271315574646
Val loss: 0.7001947164535522
Val loss: 0.6995072066783905
Val loss: 0.7076025009155273
Val loss: 0.7062122225761414
Val loss: 0.7176890075206757
Val loss: 0.7055315673351288
New best model for test task CHEMBL1614170 at epoch 22 with val loss 0.6895855367183685
Val loss: 0.7092075645923615
Val loss: 0.7117950022220612
Val loss: 0.7134028077125549
Val loss: 0.6925086677074432
Val loss: 0.7035874724388123
Val loss: 0.7163688838481903
Val loss: 0.7117480933666229
Val loss: 0.7102729678153992
Finished early stopping for task CHEMBL1614170, beginning testing
Meta testing on task: CHEMBL1963705
New best model for test task CHEMBL1963705 at epoch 1 with val loss 0.611060639222463
Val loss: 0.6255372762680054
New best model for test task CHEMBL1963705 at epoch 3 with val loss 0.5941037734349569
Val loss: 0.6520837148030599
Val loss: 0.6066219210624695
Val loss: 0.6091237465540568
Val loss: 0.6099517345428467
Val loss: 0.6025608777999878
Val loss: 0.616053024927775
Val loss: 0.5981671214103699
Val loss: 0.6170417666435242
Val loss: 0.6057109634081522
Val loss: 0.6454229156176249
New best model for test task CHEMBL1963705 at epoch 14 with val loss 0.5884047547976176
Val loss: 0.6088512937227885
Val loss: 0.593788206577301
Val loss: 0.6138879060745239
Val loss: 0.6165072719256083
Val loss: 0.5931371649106344
Val loss: 0.5999647577603658
New best model for test task CHEMBL1963705 at epoch 21 with val loss 0.5805320739746094
New best model for test task CHEMBL1963705 at epoch 22 with val loss 0.5691518584887186
Val loss: 0.5932663480440775
New best model for test task CHEMBL1963705 at epoch 24 with val loss 0.5633246600627899
Val loss: 0.5711350043614706
Val loss: 0.5827782154083252
Val loss: 0.5638290445009867
Val loss: 0.5735899806022644
Val loss: 0.6249210437138876
Val loss: 0.5805694063504537
Finished early stopping for task CHEMBL1963705, beginning testing
Meta testing on task: CHEMBL1909212
New best model for test task CHEMBL1909212 at epoch 1 with val loss 0.7402065992355347
New best model for test task CHEMBL1909212 at epoch 2 with val loss 0.7376362880071005
Val loss: 0.7383810877799988
Val loss: 0.7421893080075582
Val loss: 0.7380593021710714
Val loss: 0.7386863827705383
New best model for test task CHEMBL1909212 at epoch 7 with val loss 0.7309580643971761
Val loss: 0.7417303125063578
Val loss: 0.7360674540201823
Val loss: 0.7340496381123861
Val loss: 0.7327350378036499
Val loss: 0.7332351207733154
New best model for test task CHEMBL1909212 at epoch 13 with val loss 0.7299060225486755
Val loss: 0.7318666179974874
New best model for test task CHEMBL1909212 at epoch 15 with val loss 0.7286725044250488
Val loss: 0.7311156193415324
Val loss: 0.7318998773892721
Val loss: 0.7318870425224304
Val loss: 0.7293194731076559
Val loss: 0.7286965250968933
Val loss: 0.731982409954071
Val loss: 0.7305155595143636
Val loss: 0.7317648927370707
Val loss: 0.7300119598706564
Val loss: 0.7296967307726542
Val loss: 0.7294535636901855
New best model for test task CHEMBL1909212 at epoch 27 with val loss 0.7284780343373617
New best model for test task CHEMBL1909212 at epoch 28 with val loss 0.724435011545817
Val loss: 0.7285622159639994
New best model for test task CHEMBL1909212 at epoch 30 with val loss 0.723760982354482
Finished early stopping for task CHEMBL1909212, beginning testing
Meta testing on task: CHEMBL2114797
New best model for test task CHEMBL2114797 at epoch 1 with val loss 0.7572841048240662
Val loss: 0.757655918598175
Val loss: 0.7579209804534912
Val loss: 0.7583079934120178
Val loss: 0.7586457133293152
Val loss: 0.7589188814163208
Val loss: 0.7592549920082092
Val loss: 0.7596797943115234
Val loss: 0.7599790096282959
Val loss: 0.7603248357772827
Val loss: 0.7606255412101746
Val loss: 0.7609265446662903
Val loss: 0.7612733244895935
Val loss: 0.7616002559661865
Val loss: 0.761927604675293
Val loss: 0.7622444033622742
Val loss: 0.762539803981781
Val loss: 0.7627957463264465
Val loss: 0.7630336284637451
Val loss: 0.7634248733520508
Val loss: 0.7637057900428772
Val loss: 0.7639589309692383
Val loss: 0.7642877697944641
Val loss: 0.7646335363388062
Val loss: 0.7649219632148743
Val loss: 0.7652371525764465
Val loss: 0.7655623555183411
Val loss: 0.7657722234725952
Val loss: 0.7661100029945374
Val loss: 0.7664304375648499
Finished early stopping for task CHEMBL2114797, beginning testing
Meta testing on task: CHEMBL1963934
New best model for test task CHEMBL1963934 at epoch 1 with val loss 0.5914177894592285
New best model for test task CHEMBL1963934 at epoch 2 with val loss 0.5882550477981567
New best model for test task CHEMBL1963934 at epoch 3 with val loss 0.5850942134857178
New best model for test task CHEMBL1963934 at epoch 4 with val loss 0.5820721983909607
New best model for test task CHEMBL1963934 at epoch 5 with val loss 0.5790102481842041
New best model for test task CHEMBL1963934 at epoch 6 with val loss 0.5757225751876831
New best model for test task CHEMBL1963934 at epoch 7 with val loss 0.5728081464767456
New best model for test task CHEMBL1963934 at epoch 8 with val loss 0.5699050426483154
New best model for test task CHEMBL1963934 at epoch 9 with val loss 0.5671895742416382
New best model for test task CHEMBL1963934 at epoch 10 with val loss 0.5643563270568848
New best model for test task CHEMBL1963934 at epoch 11 with val loss 0.5616151690483093
New best model for test task CHEMBL1963934 at epoch 12 with val loss 0.5585136413574219
New best model for test task CHEMBL1963934 at epoch 13 with val loss 0.5555464029312134
New best model for test task CHEMBL1963934 at epoch 14 with val loss 0.5526418685913086
New best model for test task CHEMBL1963934 at epoch 15 with val loss 0.5499151945114136
New best model for test task CHEMBL1963934 at epoch 16 with val loss 0.547091007232666
New best model for test task CHEMBL1963934 at epoch 17 with val loss 0.544296383857727
New best model for test task CHEMBL1963934 at epoch 18 with val loss 0.5416896939277649
New best model for test task CHEMBL1963934 at epoch 19 with val loss 0.5387600660324097
New best model for test task CHEMBL1963934 at epoch 20 with val loss 0.5360166430473328
New best model for test task CHEMBL1963934 at epoch 21 with val loss 0.5332415103912354
New best model for test task CHEMBL1963934 at epoch 22 with val loss 0.530764102935791
New best model for test task CHEMBL1963934 at epoch 23 with val loss 0.5278481245040894
New best model for test task CHEMBL1963934 at epoch 24 with val loss 0.5254120826721191
New best model for test task CHEMBL1963934 at epoch 25 with val loss 0.5228462815284729
New best model for test task CHEMBL1963934 at epoch 26 with val loss 0.5202629566192627
New best model for test task CHEMBL1963934 at epoch 27 with val loss 0.517437219619751
New best model for test task CHEMBL1963934 at epoch 28 with val loss 0.5145629048347473
New best model for test task CHEMBL1963934 at epoch 29 with val loss 0.512082576751709
New best model for test task CHEMBL1963934 at epoch 30 with val loss 0.5092462301254272
Finished early stopping for task CHEMBL1963934, beginning testing
Meta testing on task: CHEMBL1909209
New best model for test task CHEMBL1909209 at epoch 1 with val loss 0.7066089113553365
New best model for test task CHEMBL1909209 at epoch 2 with val loss 0.7046888073285421
New best model for test task CHEMBL1909209 at epoch 3 with val loss 0.7035284439722697
New best model for test task CHEMBL1909209 at epoch 4 with val loss 0.7034740447998047
New best model for test task CHEMBL1909209 at epoch 5 with val loss 0.7005404829978943
Val loss: 0.7048181692759196
Val loss: 0.7012093265851339
Val loss: 0.7027225295702616
Val loss: 0.7055192391077677
Val loss: 0.7070134282112122
Val loss: 0.7044725020726522
Val loss: 0.7079551219940186
Val loss: 0.7027946511904398
Val loss: 0.7013187607129415
Val loss: 0.7068713903427124
Val loss: 0.7057945728302002
Val loss: 0.7044152021408081
Val loss: 0.7027887503306071
Val loss: 0.7059105038642883
Val loss: 0.7027493715286255
Val loss: 0.7036679784456888
Val loss: 0.7039920091629028
Val loss: 0.7039970954259237
Val loss: 0.7039712270100912
Val loss: 0.7006711363792419
Val loss: 0.7035422722498575
Val loss: 0.7016422351201376
Val loss: 0.7043792208035787
Val loss: 0.7014362613360087
Val loss: 0.7053892811139425
Finished early stopping for task CHEMBL1909209, beginning testing
Meta testing on task: CHEMBL1909211
New best model for test task CHEMBL1909211 at epoch 1 with val loss 0.7084476947784424
New best model for test task CHEMBL1909211 at epoch 2 with val loss 0.7076885104179382
Val loss: 0.7102635900179545
Val loss: 0.7081339160601298
Val loss: 0.7128025889396667
New best model for test task CHEMBL1909211 at epoch 6 with val loss 0.7074865301450094
New best model for test task CHEMBL1909211 at epoch 7 with val loss 0.7044057250022888
Val loss: 0.7122614582379659
Val loss: 0.707873543103536
Val loss: 0.7083287437756857
Val loss: 0.706045667330424
Val loss: 0.7100265820821127
Val loss: 0.7049554785092672
Val loss: 0.7093688448270162
Val loss: 0.7081316113471985
Val loss: 0.7072225014368693
Val loss: 0.708660880724589
Val loss: 0.7095891435941061
Val loss: 0.7066574096679688
Val loss: 0.7053860823313395
Val loss: 0.7081526517868042
Val loss: 0.7062017718950907
Val loss: 0.7055346767107645
New best model for test task CHEMBL1909211 at epoch 24 with val loss 0.7034537394841512
New best model for test task CHEMBL1909211 at epoch 25 with val loss 0.702560822168986
Val loss: 0.7046196262041727
Val loss: 0.7034348249435425
Val loss: 0.706749439239502
Val loss: 0.7039418617884318
Val loss: 0.7065152128537496
Finished early stopping for task CHEMBL1909211, beginning testing
Meta testing on task: CHEMBL1909085
New best model for test task CHEMBL1909085 at epoch 1 with val loss 0.6943387786547343
New best model for test task CHEMBL1909085 at epoch 2 with val loss 0.6932250658671061
Val loss: 0.6944898764292399
New best model for test task CHEMBL1909085 at epoch 4 with val loss 0.6899391214052836
Val loss: 0.6955999334653219
Val loss: 0.6973461707433065
Val loss: 0.6986719965934753
Val loss: 0.6919005314509074
Val loss: 0.6942300399144491
Val loss: 0.6938290596008301
Val loss: 0.6923538645108541
Val loss: 0.6941514611244202
Val loss: 0.6929267644882202
Val loss: 0.69447922706604
Val loss: 0.6935820579528809
Val loss: 0.6920247077941895
Val loss: 0.6930790742238363
New best model for test task CHEMBL1909085 at epoch 18 with val loss 0.6854857802391052
Val loss: 0.6953813831011454
Val loss: 0.6943771044413248
Val loss: 0.6911942561467489
Val loss: 0.6907856663068136
Val loss: 0.6903221805890402
Val loss: 0.6942821542421976
Val loss: 0.6881389419237772
Val loss: 0.6958232919375101
Val loss: 0.697374959786733
Val loss: 0.6953786412874857
Val loss: 0.6882599194844564
Val loss: 0.6909032662709554
Finished early stopping for task CHEMBL1909085, beginning testing
Meta testing on task: CHEMBL1738202
New best model for test task CHEMBL1738202 at epoch 1 with val loss 0.6665655970573425
New best model for test task CHEMBL1738202 at epoch 2 with val loss 0.6653807759284973
New best model for test task CHEMBL1738202 at epoch 3 with val loss 0.664269208908081
New best model for test task CHEMBL1738202 at epoch 4 with val loss 0.6630367636680603
New best model for test task CHEMBL1738202 at epoch 5 with val loss 0.6618008017539978
New best model for test task CHEMBL1738202 at epoch 6 with val loss 0.660495936870575
New best model for test task CHEMBL1738202 at epoch 7 with val loss 0.659250020980835
New best model for test task CHEMBL1738202 at epoch 8 with val loss 0.6580131649971008
New best model for test task CHEMBL1738202 at epoch 9 with val loss 0.6567308902740479
New best model for test task CHEMBL1738202 at epoch 10 with val loss 0.6554293036460876
New best model for test task CHEMBL1738202 at epoch 11 with val loss 0.6540533304214478
New best model for test task CHEMBL1738202 at epoch 12 with val loss 0.6525564193725586
New best model for test task CHEMBL1738202 at epoch 13 with val loss 0.6510992050170898
New best model for test task CHEMBL1738202 at epoch 14 with val loss 0.6496492624282837
New best model for test task CHEMBL1738202 at epoch 15 with val loss 0.6480835676193237
New best model for test task CHEMBL1738202 at epoch 16 with val loss 0.6464542746543884
New best model for test task CHEMBL1738202 at epoch 17 with val loss 0.644993245601654
New best model for test task CHEMBL1738202 at epoch 18 with val loss 0.6433823108673096
New best model for test task CHEMBL1738202 at epoch 19 with val loss 0.6417101621627808
New best model for test task CHEMBL1738202 at epoch 20 with val loss 0.6400758028030396
New best model for test task CHEMBL1738202 at epoch 21 with val loss 0.6383897662162781
New best model for test task CHEMBL1738202 at epoch 22 with val loss 0.6366575956344604
New best model for test task CHEMBL1738202 at epoch 23 with val loss 0.6348758935928345
New best model for test task CHEMBL1738202 at epoch 24 with val loss 0.6331489086151123
New best model for test task CHEMBL1738202 at epoch 25 with val loss 0.6312005519866943
New best model for test task CHEMBL1738202 at epoch 26 with val loss 0.6293694376945496
New best model for test task CHEMBL1738202 at epoch 27 with val loss 0.6274678707122803
New best model for test task CHEMBL1738202 at epoch 28 with val loss 0.6255382299423218
New best model for test task CHEMBL1738202 at epoch 29 with val loss 0.6235689520835876
New best model for test task CHEMBL1738202 at epoch 30 with val loss 0.6215053796768188
Finished early stopping for task CHEMBL1738202, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1738019
New best model for test task CHEMBL1738019 at epoch 1 with val loss 0.6544493436813354
New best model for test task CHEMBL1738019 at epoch 2 with val loss 0.6535397171974182
New best model for test task CHEMBL1738019 at epoch 3 with val loss 0.6524257659912109
New best model for test task CHEMBL1738019 at epoch 4 with val loss 0.6513020992279053
New best model for test task CHEMBL1738019 at epoch 5 with val loss 0.6503095030784607
New best model for test task CHEMBL1738019 at epoch 6 with val loss 0.6494828462600708
New best model for test task CHEMBL1738019 at epoch 7 with val loss 0.6484172344207764
New best model for test task CHEMBL1738019 at epoch 8 with val loss 0.6474896669387817
New best model for test task CHEMBL1738019 at epoch 9 with val loss 0.6463823318481445
New best model for test task CHEMBL1738019 at epoch 10 with val loss 0.6455862522125244
New best model for test task CHEMBL1738019 at epoch 11 with val loss 0.6446468234062195
New best model for test task CHEMBL1738019 at epoch 12 with val loss 0.6436221599578857
New best model for test task CHEMBL1738019 at epoch 13 with val loss 0.6425594091415405
New best model for test task CHEMBL1738019 at epoch 14 with val loss 0.6414158940315247
New best model for test task CHEMBL1738019 at epoch 15 with val loss 0.6403089761734009
New best model for test task CHEMBL1738019 at epoch 16 with val loss 0.6392403841018677
New best model for test task CHEMBL1738019 at epoch 17 with val loss 0.6380490660667419
New best model for test task CHEMBL1738019 at epoch 18 with val loss 0.6371278762817383
New best model for test task CHEMBL1738019 at epoch 19 with val loss 0.6360225081443787
New best model for test task CHEMBL1738019 at epoch 20 with val loss 0.635208249092102
New best model for test task CHEMBL1738019 at epoch 21 with val loss 0.6340530514717102
New best model for test task CHEMBL1738019 at epoch 22 with val loss 0.6330347061157227
New best model for test task CHEMBL1738019 at epoch 23 with val loss 0.6322237253189087
New best model for test task CHEMBL1738019 at epoch 24 with val loss 0.6312429904937744
New best model for test task CHEMBL1738019 at epoch 25 with val loss 0.6301296949386597
New best model for test task CHEMBL1738019 at epoch 26 with val loss 0.6292020082473755
New best model for test task CHEMBL1738019 at epoch 27 with val loss 0.6282122135162354
New best model for test task CHEMBL1738019 at epoch 28 with val loss 0.6271660327911377
New best model for test task CHEMBL1738019 at epoch 29 with val loss 0.6262348890304565
New best model for test task CHEMBL1738019 at epoch 30 with val loss 0.6253355741500854
Finished early stopping for task CHEMBL1738019, beginning testing
Meta testing on task: CHEMBL1909092
New best model for test task CHEMBL1909092 at epoch 1 with val loss 0.7355387608210245
Val loss: 0.7417448163032532
Val loss: 0.7448614835739136
Val loss: 0.7428296605745951
New best model for test task CHEMBL1909092 at epoch 5 with val loss 0.7354886929194132
New best model for test task CHEMBL1909092 at epoch 6 with val loss 0.7346801360448202
New best model for test task CHEMBL1909092 at epoch 7 with val loss 0.7337368329366049
New best model for test task CHEMBL1909092 at epoch 8 with val loss 0.7333679993947347
Val loss: 0.7353414098421732
New best model for test task CHEMBL1909092 at epoch 10 with val loss 0.7332305312156677
Val loss: 0.7347662250200907
New best model for test task CHEMBL1909092 at epoch 12 with val loss 0.7328911622365316
Val loss: 0.7334146896998087
New best model for test task CHEMBL1909092 at epoch 14 with val loss 0.7328088680903116
New best model for test task CHEMBL1909092 at epoch 15 with val loss 0.7278145750363668
Val loss: 0.7335580984751383
Val loss: 0.7345425883928934
Val loss: 0.729971726735433
New best model for test task CHEMBL1909092 at epoch 19 with val loss 0.7261233727137247
Val loss: 0.7313891251881918
Val loss: 0.7290093302726746
Val loss: 0.7282000780105591
New best model for test task CHEMBL1909092 at epoch 23 with val loss 0.7260098656018575
Val loss: 0.7274407744407654
New best model for test task CHEMBL1909092 at epoch 25 with val loss 0.7249300877253214
New best model for test task CHEMBL1909092 at epoch 26 with val loss 0.7241710821787516
New best model for test task CHEMBL1909092 at epoch 27 with val loss 0.7238048513730367
Val loss: 0.727159341176351
Val loss: 0.7247231403986613
New best model for test task CHEMBL1909092 at epoch 30 with val loss 0.7225097020467123
Finished early stopping for task CHEMBL1909092, beginning testing
Meta testing on task: CHEMBL1909192
New best model for test task CHEMBL1909192 at epoch 1 with val loss 0.744085947672526
New best model for test task CHEMBL1909192 at epoch 2 with val loss 0.7386883497238159
Val loss: 0.7425523996353149
Val loss: 0.7396846016248068
New best model for test task CHEMBL1909192 at epoch 5 with val loss 0.7372369964917501
Val loss: 0.7419821421305338
Val loss: 0.7434718410174052
New best model for test task CHEMBL1909192 at epoch 8 with val loss 0.7344343463579813
New best model for test task CHEMBL1909192 at epoch 9 with val loss 0.7316248615582784
Val loss: 0.736512819925944
Val loss: 0.7336705724398295
Val loss: 0.7323557337125143
Val loss: 0.7334595719973246
New best model for test task CHEMBL1909192 at epoch 14 with val loss 0.7296211322148641
Val loss: 0.7326121131579081
Val loss: 0.7304232716560364
New best model for test task CHEMBL1909192 at epoch 17 with val loss 0.7292700409889221
Val loss: 0.7296200195948283
New best model for test task CHEMBL1909192 at epoch 19 with val loss 0.7291430830955505
New best model for test task CHEMBL1909192 at epoch 20 with val loss 0.726805051167806
New best model for test task CHEMBL1909192 at epoch 21 with val loss 0.7263008753458658
Val loss: 0.7308927575747172
New best model for test task CHEMBL1909192 at epoch 23 with val loss 0.7258304357528687
Val loss: 0.7270959615707397
New best model for test task CHEMBL1909192 at epoch 25 with val loss 0.7249388694763184
New best model for test task CHEMBL1909192 at epoch 26 with val loss 0.7239130536715189
Val loss: 0.726430356502533
Val loss: 0.7255253791809082
New best model for test task CHEMBL1909192 at epoch 29 with val loss 0.723786453406016
Val loss: 0.7257604598999023
Finished early stopping for task CHEMBL1909192, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1614359
New best model for test task CHEMBL1614359 at epoch 1 with val loss 0.6720042824745178
New best model for test task CHEMBL1614359 at epoch 2 with val loss 0.6679218113422394
Val loss: 0.7013307511806488
New best model for test task CHEMBL1614359 at epoch 4 with val loss 0.6414148211479187
Val loss: 0.6596570611000061
Val loss: 0.6555308103561401
Val loss: 0.7161389291286469
Val loss: 0.6673532426357269
Val loss: 0.7075898051261902
Val loss: 0.6830341815948486
Val loss: 0.6713414490222931
Val loss: 0.6770092844963074
New best model for test task CHEMBL1614359 at epoch 13 with val loss 0.640081912279129
New best model for test task CHEMBL1614359 at epoch 14 with val loss 0.6344470679759979
Val loss: 0.6499557793140411
Val loss: 0.6582732498645782
New best model for test task CHEMBL1614359 at epoch 17 with val loss 0.6182486414909363
Val loss: 0.6725025475025177
Val loss: 0.6589375734329224
Val loss: 0.67380291223526
Val loss: 0.6459717452526093
New best model for test task CHEMBL1614359 at epoch 22 with val loss 0.6015087962150574
Val loss: 0.6622776091098785
Val loss: 0.6737600564956665
Val loss: 0.6247894465923309
Val loss: 0.6429208517074585
Val loss: 0.6345392465591431
Val loss: 0.6310814619064331
Val loss: 0.6450537145137787
Val loss: 0.64980149269104
Finished early stopping for task CHEMBL1614359, beginning testing
Meta testing on task: CHEMBL2028077
New best model for test task CHEMBL2028077 at epoch 1 with val loss 0.7882291674613953
New best model for test task CHEMBL2028077 at epoch 2 with val loss 0.7879900336265564
New best model for test task CHEMBL2028077 at epoch 3 with val loss 0.7876978516578674
New best model for test task CHEMBL2028077 at epoch 4 with val loss 0.7874315977096558
New best model for test task CHEMBL2028077 at epoch 5 with val loss 0.7871459722518921
New best model for test task CHEMBL2028077 at epoch 6 with val loss 0.7869023084640503
New best model for test task CHEMBL2028077 at epoch 7 with val loss 0.7866335511207581
New best model for test task CHEMBL2028077 at epoch 8 with val loss 0.7863544225692749
New best model for test task CHEMBL2028077 at epoch 9 with val loss 0.7860978841781616
New best model for test task CHEMBL2028077 at epoch 10 with val loss 0.7858774662017822
New best model for test task CHEMBL2028077 at epoch 11 with val loss 0.7856772541999817
New best model for test task CHEMBL2028077 at epoch 12 with val loss 0.785506546497345
New best model for test task CHEMBL2028077 at epoch 13 with val loss 0.7852328419685364
New best model for test task CHEMBL2028077 at epoch 14 with val loss 0.78499835729599
New best model for test task CHEMBL2028077 at epoch 15 with val loss 0.7847773432731628
New best model for test task CHEMBL2028077 at epoch 16 with val loss 0.7844249606132507
New best model for test task CHEMBL2028077 at epoch 17 with val loss 0.7843039631843567
New best model for test task CHEMBL2028077 at epoch 18 with val loss 0.7840383052825928
New best model for test task CHEMBL2028077 at epoch 19 with val loss 0.7837956547737122
New best model for test task CHEMBL2028077 at epoch 20 with val loss 0.783586859703064
New best model for test task CHEMBL2028077 at epoch 21 with val loss 0.7833799123764038
New best model for test task CHEMBL2028077 at epoch 22 with val loss 0.7830988168716431
New best model for test task CHEMBL2028077 at epoch 23 with val loss 0.7828463912010193
New best model for test task CHEMBL2028077 at epoch 24 with val loss 0.7826092839241028
New best model for test task CHEMBL2028077 at epoch 25 with val loss 0.7824093103408813
New best model for test task CHEMBL2028077 at epoch 26 with val loss 0.7822638154029846
New best model for test task CHEMBL2028077 at epoch 27 with val loss 0.7820425629615784
New best model for test task CHEMBL2028077 at epoch 28 with val loss 0.7818671464920044
New best model for test task CHEMBL2028077 at epoch 29 with val loss 0.7816702127456665
New best model for test task CHEMBL2028077 at epoch 30 with val loss 0.7814410328865051
Finished early stopping for task CHEMBL2028077, beginning testing
Meta testing on task: CHEMBL1794358
New best model for test task CHEMBL1794358 at epoch 1 with val loss 0.7064006924629211
New best model for test task CHEMBL1794358 at epoch 2 with val loss 0.7062726020812988
New best model for test task CHEMBL1794358 at epoch 3 with val loss 0.7061988115310669
New best model for test task CHEMBL1794358 at epoch 4 with val loss 0.7061219215393066
New best model for test task CHEMBL1794358 at epoch 5 with val loss 0.7060480117797852
New best model for test task CHEMBL1794358 at epoch 6 with val loss 0.7059630155563354
New best model for test task CHEMBL1794358 at epoch 7 with val loss 0.705883264541626
New best model for test task CHEMBL1794358 at epoch 8 with val loss 0.7057768702507019
New best model for test task CHEMBL1794358 at epoch 9 with val loss 0.7056859135627747
New best model for test task CHEMBL1794358 at epoch 10 with val loss 0.705596923828125
New best model for test task CHEMBL1794358 at epoch 11 with val loss 0.7055066227912903
New best model for test task CHEMBL1794358 at epoch 12 with val loss 0.7054299712181091
New best model for test task CHEMBL1794358 at epoch 13 with val loss 0.7053610682487488
New best model for test task CHEMBL1794358 at epoch 14 with val loss 0.7052914500236511
New best model for test task CHEMBL1794358 at epoch 15 with val loss 0.7051977515220642
New best model for test task CHEMBL1794358 at epoch 16 with val loss 0.705080509185791
New best model for test task CHEMBL1794358 at epoch 17 with val loss 0.7049765586853027
New best model for test task CHEMBL1794358 at epoch 18 with val loss 0.704911470413208
New best model for test task CHEMBL1794358 at epoch 19 with val loss 0.7048555612564087
New best model for test task CHEMBL1794358 at epoch 20 with val loss 0.7047823071479797
New best model for test task CHEMBL1794358 at epoch 21 with val loss 0.7046872973442078
New best model for test task CHEMBL1794358 at epoch 22 with val loss 0.7046022415161133
New best model for test task CHEMBL1794358 at epoch 23 with val loss 0.7045556306838989
New best model for test task CHEMBL1794358 at epoch 24 with val loss 0.704479992389679
New best model for test task CHEMBL1794358 at epoch 25 with val loss 0.7043692469596863
New best model for test task CHEMBL1794358 at epoch 26 with val loss 0.704302966594696
New best model for test task CHEMBL1794358 at epoch 27 with val loss 0.7042102813720703
New best model for test task CHEMBL1794358 at epoch 28 with val loss 0.7041065096855164
New best model for test task CHEMBL1794358 at epoch 29 with val loss 0.7040290236473083
New best model for test task CHEMBL1794358 at epoch 30 with val loss 0.703927755355835
Finished early stopping for task CHEMBL1794358, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1738021
New best model for test task CHEMBL1738021 at epoch 1 with val loss 0.6109349131584167
New best model for test task CHEMBL1738021 at epoch 2 with val loss 0.6103222966194153
New best model for test task CHEMBL1738021 at epoch 3 with val loss 0.6096300482749939
New best model for test task CHEMBL1738021 at epoch 4 with val loss 0.6089718341827393
New best model for test task CHEMBL1738021 at epoch 5 with val loss 0.6083335876464844
New best model for test task CHEMBL1738021 at epoch 6 with val loss 0.6077048182487488
New best model for test task CHEMBL1738021 at epoch 7 with val loss 0.6070631742477417
New best model for test task CHEMBL1738021 at epoch 8 with val loss 0.6064334511756897
New best model for test task CHEMBL1738021 at epoch 9 with val loss 0.6057998538017273
New best model for test task CHEMBL1738021 at epoch 10 with val loss 0.6051013469696045
New best model for test task CHEMBL1738021 at epoch 11 with val loss 0.6044630408287048
New best model for test task CHEMBL1738021 at epoch 12 with val loss 0.6037819981575012
New best model for test task CHEMBL1738021 at epoch 13 with val loss 0.6031614542007446
New best model for test task CHEMBL1738021 at epoch 14 with val loss 0.6024445295333862
New best model for test task CHEMBL1738021 at epoch 15 with val loss 0.6017866134643555
New best model for test task CHEMBL1738021 at epoch 16 with val loss 0.6011676788330078
New best model for test task CHEMBL1738021 at epoch 17 with val loss 0.6005293130874634
New best model for test task CHEMBL1738021 at epoch 18 with val loss 0.599842369556427
New best model for test task CHEMBL1738021 at epoch 19 with val loss 0.5991405844688416
New best model for test task CHEMBL1738021 at epoch 20 with val loss 0.5984557867050171
New best model for test task CHEMBL1738021 at epoch 21 with val loss 0.5977980494499207
New best model for test task CHEMBL1738021 at epoch 22 with val loss 0.5971981287002563
New best model for test task CHEMBL1738021 at epoch 23 with val loss 0.5965297818183899
New best model for test task CHEMBL1738021 at epoch 24 with val loss 0.595920205116272
New best model for test task CHEMBL1738021 at epoch 25 with val loss 0.5952529907226562
New best model for test task CHEMBL1738021 at epoch 26 with val loss 0.5945561528205872
New best model for test task CHEMBL1738021 at epoch 27 with val loss 0.5939590930938721
New best model for test task CHEMBL1738021 at epoch 28 with val loss 0.5932679772377014
New best model for test task CHEMBL1738021 at epoch 29 with val loss 0.592627227306366
New best model for test task CHEMBL1738021 at epoch 30 with val loss 0.5919951796531677
Finished early stopping for task CHEMBL1738021, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL3215116
New best model for test task CHEMBL3215116 at epoch 1 with val loss 0.6915186643600464
Val loss: 0.6915596723556519
Val loss: 0.6915954351425171
Val loss: 0.6916677355766296
Val loss: 0.6917147040367126
Val loss: 0.6917481422424316
Val loss: 0.6917784214019775
Val loss: 0.6918487548828125
Val loss: 0.6919094920158386
Val loss: 0.6919666528701782
Val loss: 0.6920177340507507
Val loss: 0.6920897364616394
Val loss: 0.6921184062957764
Val loss: 0.6921660304069519
Val loss: 0.692217230796814
Val loss: 0.692267656326294
Val loss: 0.6922945976257324
Val loss: 0.692332923412323
Val loss: 0.6924084424972534
Val loss: 0.6924411058425903
Val loss: 0.6924934387207031
Val loss: 0.6925517916679382
Val loss: 0.6926074028015137
Val loss: 0.692665696144104
Val loss: 0.6927121877670288
Val loss: 0.6927423477172852
Val loss: 0.6927890777587891
Val loss: 0.6928302049636841
Val loss: 0.6928755640983582
Val loss: 0.6929315328598022
Finished early stopping for task CHEMBL3215116, beginning testing
Meta testing on task: CHEMBL1614202
New best model for test task CHEMBL1614202 at epoch 1 with val loss 0.594567060470581
New best model for test task CHEMBL1614202 at epoch 2 with val loss 0.5902363657951355
New best model for test task CHEMBL1614202 at epoch 3 with val loss 0.5856970548629761
New best model for test task CHEMBL1614202 at epoch 4 with val loss 0.5813884139060974
New best model for test task CHEMBL1614202 at epoch 5 with val loss 0.5771923661231995
New best model for test task CHEMBL1614202 at epoch 6 with val loss 0.5726460814476013
New best model for test task CHEMBL1614202 at epoch 7 with val loss 0.5681979656219482
New best model for test task CHEMBL1614202 at epoch 8 with val loss 0.5640746355056763
New best model for test task CHEMBL1614202 at epoch 9 with val loss 0.5598300695419312
New best model for test task CHEMBL1614202 at epoch 10 with val loss 0.5556485652923584
New best model for test task CHEMBL1614202 at epoch 11 with val loss 0.5512819886207581
New best model for test task CHEMBL1614202 at epoch 12 with val loss 0.5471724271774292
New best model for test task CHEMBL1614202 at epoch 13 with val loss 0.5430627465248108
New best model for test task CHEMBL1614202 at epoch 14 with val loss 0.5391238331794739
New best model for test task CHEMBL1614202 at epoch 15 with val loss 0.5351887345314026
New best model for test task CHEMBL1614202 at epoch 16 with val loss 0.5312719941139221
New best model for test task CHEMBL1614202 at epoch 17 with val loss 0.527287483215332
New best model for test task CHEMBL1614202 at epoch 18 with val loss 0.5235539078712463
New best model for test task CHEMBL1614202 at epoch 19 with val loss 0.5197260975837708
New best model for test task CHEMBL1614202 at epoch 20 with val loss 0.5158321857452393
New best model for test task CHEMBL1614202 at epoch 21 with val loss 0.5120503902435303
New best model for test task CHEMBL1614202 at epoch 22 with val loss 0.5082058906555176
New best model for test task CHEMBL1614202 at epoch 23 with val loss 0.5045289993286133
New best model for test task CHEMBL1614202 at epoch 24 with val loss 0.5009678602218628
New best model for test task CHEMBL1614202 at epoch 25 with val loss 0.4973929226398468
New best model for test task CHEMBL1614202 at epoch 26 with val loss 0.4937838613986969
New best model for test task CHEMBL1614202 at epoch 27 with val loss 0.4902266561985016
New best model for test task CHEMBL1614202 at epoch 28 with val loss 0.4868694245815277
New best model for test task CHEMBL1614202 at epoch 29 with val loss 0.48352542519569397
New best model for test task CHEMBL1614202 at epoch 30 with val loss 0.48010414838790894
Finished early stopping for task CHEMBL1614202, beginning testing
Meta testing on task: CHEMBL1794567
New best model for test task CHEMBL1794567 at epoch 1 with val loss 0.6579912602901459
Val loss: 0.6652122139930725
New best model for test task CHEMBL1794567 at epoch 3 with val loss 0.6388672292232513
Val loss: 0.6468597054481506
Val loss: 0.6549190580844879
New best model for test task CHEMBL1794567 at epoch 6 with val loss 0.6388298571109772
New best model for test task CHEMBL1794567 at epoch 7 with val loss 0.6079007387161255
Val loss: 0.635367214679718
Val loss: 0.6148940622806549
Val loss: 0.6309865713119507
Val loss: 0.6139624118804932
New best model for test task CHEMBL1794567 at epoch 12 with val loss 0.5966009497642517
Val loss: 0.6064129769802094
Val loss: 0.6146500706672668
New best model for test task CHEMBL1794567 at epoch 15 with val loss 0.5838425159454346
Val loss: 0.5961587727069855
Val loss: 0.5850132703781128
Val loss: 0.6089670956134796
Val loss: 0.5949179530143738
New best model for test task CHEMBL1794567 at epoch 20 with val loss 0.5600179433822632
Val loss: 0.5653533637523651
New best model for test task CHEMBL1794567 at epoch 22 with val loss 0.5284151583909988
Val loss: 0.5700550675392151
Val loss: 0.5364098846912384
Val loss: 0.5533084571361542
Val loss: 0.5675446391105652
Val loss: 0.5298765897750854
New best model for test task CHEMBL1794567 at epoch 28 with val loss 0.5163233578205109
Val loss: 0.5271058678627014
New best model for test task CHEMBL1794567 at epoch 30 with val loss 0.5147524774074554
Finished early stopping for task CHEMBL1794567, beginning testing
Meta testing on task: CHEMBL3215176
New best model for test task CHEMBL3215176 at epoch 1 with val loss 0.7305707931518555
New best model for test task CHEMBL3215176 at epoch 2 with val loss 0.7305047512054443
New best model for test task CHEMBL3215176 at epoch 3 with val loss 0.7304892539978027
New best model for test task CHEMBL3215176 at epoch 4 with val loss 0.7304239273071289
New best model for test task CHEMBL3215176 at epoch 5 with val loss 0.7303923964500427
New best model for test task CHEMBL3215176 at epoch 6 with val loss 0.7303705215454102
New best model for test task CHEMBL3215176 at epoch 7 with val loss 0.7303102016448975
New best model for test task CHEMBL3215176 at epoch 8 with val loss 0.7302682399749756
New best model for test task CHEMBL3215176 at epoch 9 with val loss 0.7302568554878235
New best model for test task CHEMBL3215176 at epoch 10 with val loss 0.7301805019378662
New best model for test task CHEMBL3215176 at epoch 11 with val loss 0.7301106452941895
New best model for test task CHEMBL3215176 at epoch 12 with val loss 0.7300746440887451
New best model for test task CHEMBL3215176 at epoch 13 with val loss 0.7300734519958496
New best model for test task CHEMBL3215176 at epoch 14 with val loss 0.7300469875335693
New best model for test task CHEMBL3215176 at epoch 15 with val loss 0.7300121784210205
New best model for test task CHEMBL3215176 at epoch 16 with val loss 0.7299405932426453
New best model for test task CHEMBL3215176 at epoch 17 with val loss 0.7298967242240906
New best model for test task CHEMBL3215176 at epoch 18 with val loss 0.7298556566238403
New best model for test task CHEMBL3215176 at epoch 19 with val loss 0.7297704219818115
New best model for test task CHEMBL3215176 at epoch 20 with val loss 0.7296253442764282
New best model for test task CHEMBL3215176 at epoch 21 with val loss 0.7295663356781006
New best model for test task CHEMBL3215176 at epoch 22 with val loss 0.7295145988464355
New best model for test task CHEMBL3215176 at epoch 23 with val loss 0.7294117212295532
New best model for test task CHEMBL3215176 at epoch 24 with val loss 0.7294065952301025
New best model for test task CHEMBL3215176 at epoch 25 with val loss 0.7293367385864258
New best model for test task CHEMBL3215176 at epoch 26 with val loss 0.729280948638916
New best model for test task CHEMBL3215176 at epoch 27 with val loss 0.7292764782905579
New best model for test task CHEMBL3215176 at epoch 28 with val loss 0.7292605042457581
New best model for test task CHEMBL3215176 at epoch 29 with val loss 0.7292481660842896
New best model for test task CHEMBL3215176 at epoch 30 with val loss 0.7292171120643616
Finished early stopping for task CHEMBL3215176, beginning testing
Warning: Found a task with targets all 0s or all 1s
Meta testing on task: CHEMBL1963741
New best model for test task CHEMBL1963741 at epoch 1 with val loss 0.6718704303105673
New best model for test task CHEMBL1963741 at epoch 2 with val loss 0.6707837184270223
Val loss: 0.6746079325675964
Val loss: 0.6756935914357504
Val loss: 0.6724101503690084
Val loss: 0.6744475364685059
Val loss: 0.6717567841211954
Val loss: 0.6738226016362509
Val loss: 0.6760489741961161
Val loss: 0.6749831040700277
Val loss: 0.6780877709388733
Val loss: 0.6777056852976481
Val loss: 0.6775917410850525
Val loss: 0.6775695284207662
Val loss: 0.6782824397087097
Val loss: 0.6768509348233541
Val loss: 0.6785570383071899
Val loss: 0.6791967352231344
Val loss: 0.6790354053179423
Val loss: 0.6815927426020304
Val loss: 0.6834776004155477
Val loss: 0.6827690402666727
Val loss: 0.682575543721517
Val loss: 0.6823189854621887
Val loss: 0.6795375148455302
Val loss: 0.6813340783119202
Val loss: 0.6860398252805074
Val loss: 0.679935077826182
Val loss: 0.6829894383748373
Val loss: 0.6839919288953146
Finished early stopping for task CHEMBL1963741, beginning testing
Took 508.62706232070923 seconds to complete meta testing
Model test prc-auc = 0.576474
1-fold cross validation
Seed 0 ==> test prc-auc = 0.576474
Overall test prc-auc = 0.576474 +/- 0.000000
Total running time was 124558.35170269012 seconds
