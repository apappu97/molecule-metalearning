wandb: Tracking run with wandb version 0.9.3
wandb: Wandb version 0.9.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Run data is saved locally in wandb/run-20200725_000156-1d9tj9ig
wandb: Syncing run maml_test
wandb: â­ï¸ View project at https://app.wandb.ai/apappu97/molecule-metalearning-chemprop
wandb: ğŸš€ View run at https://app.wandb.ai/apappu97/molecule-metalearning-chemprop/runs/1d9tj9ig
wandb: Run `wandb off` to turn off syncing.
Setting args.meta_learning to True as we are meta learning

Fold 0
Command line
python chemprop/meta_train.py --meta_learning --dummy --data_path filtered_chembl/chembl_less_1024_more_128_645_tasks.csv --dataset_type classification --split_type scaffold_balanced --chembl_assay_metadata_pickle_path filtered_chembl/ --save_dir checkpoints/ --results_save_dir results/maml/ --experiment_name maml_test
Args
{'ANIL': False,
 'FO_MAML': False,
 'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'chembl_assay_metadata_pickle_path': 'filtered_chembl/',
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'data_path': 'filtered_chembl/chembl_less_1024_more_128_645_tasks.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'dropout': 0.0,
 'dummy': True,
 'ensemble_size': 1,
 'epochs': 30,
 'experiment_name': 'maml_test',
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'inner_loop_lr': 0.05,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'meta_batch_size': 3,
 'meta_learning': True,
 'meta_test_epochs': 30,
 'meta_test_lr': 0.001,
 'meta_test_split_sizes': (0.8, 0.1, 0.1),
 'meta_train_split_sizes': (0.8, 0.2, 0),
 'metric': 'auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 1,
 'num_inner_gradient_steps': 2,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'outer_loop_lr': 0.003,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results/maml/',
 'save_dir': 'checkpoints/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': None,
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
0it [00:00, ?it/s]158it [00:00, 1575.39it/s]322it [00:00, 1591.73it/s]495it [00:00, 1630.26it/s]659it [00:00, 1632.08it/s]830it [00:00, 1654.01it/s]990it [00:00, 1636.93it/s]1143it [00:00, 1600.16it/s]1307it [00:00, 1611.20it/s]1472it [00:00, 1620.27it/s]1644it [00:01, 1646.62it/s]1806it [00:01, 1638.50it/s]1968it [00:01, 1631.63it/s]2130it [00:01, 1594.50it/s]2301it [00:01, 1625.63it/s]2463it [00:01, 1510.76it/s]2635it [00:01, 1566.75it/s]2797it [00:01, 1581.91it/s]2959it [00:01, 1592.05it/s]3119it [00:01, 1570.76it/s]3289it [00:02, 1605.09it/s]3451it [00:02, 1595.80it/s]3624it [00:02, 1633.66it/s]3792it [00:02, 1646.41it/s]3958it [00:02, 1619.42it/s]4121it [00:02, 1612.05it/s]4283it [00:02, 1592.19it/s]4454it [00:02, 1623.67it/s]4617it [00:02, 1624.96it/s]4783it [00:02, 1633.19it/s]4947it [00:03, 1619.03it/s]5110it [00:03, 1612.92it/s]5272it [00:03, 1602.39it/s]5444it [00:03, 1635.89it/s]5608it [00:03, 1635.59it/s]5780it [00:03, 1658.02it/s]5946it [00:03, 1636.83it/s]6110it [00:03, 1627.38it/s]6273it [00:03, 1610.09it/s]6446it [00:03, 1641.78it/s]6611it [00:04, 1415.24it/s]6782it [00:04, 1491.69it/s]6937it [00:04, 1494.15it/s]7098it [00:04, 1524.84it/s]7267it [00:04, 1570.75it/s]7433it [00:04, 1594.21it/s]7603it [00:04, 1624.35it/s]7767it [00:04, 1617.30it/s]7935it [00:04, 1633.97it/s]8100it [00:05, 1553.34it/s]8263it [00:05, 1575.38it/s]8428it [00:05, 1596.37it/s]8601it [00:05, 1632.33it/s]8765it [00:05, 1634.07it/s]8933it [00:05, 1645.52it/s]9098it [00:05, 1610.61it/s]9264it [00:05, 1624.31it/s]9427it [00:05, 1619.46it/s]9599it [00:05, 1648.25it/s]9765it [00:06, 1646.24it/s]9930it [00:06, 1645.87it/s]10095it [00:06, 1610.97it/s]10260it [00:06, 1621.97it/s]10424it [00:06, 1622.72it/s]10594it [00:06, 1644.19it/s]10764it [00:06, 1660.42it/s]10931it [00:06, 1411.74it/s]11092it [00:06, 1465.27it/s]11256it [00:07, 1512.85it/s]11429it [00:07, 1570.34it/s]11593it [00:07, 1589.33it/s]11759it [00:07, 1609.70it/s]11922it [00:07, 1601.55it/s]12084it [00:07, 1602.11it/s]12246it [00:07, 1595.71it/s]12418it [00:07, 1628.72it/s]12582it [00:07, 1612.33it/s]12752it [00:07, 1637.43it/s]12917it [00:08, 1629.66it/s]13081it [00:08, 1581.38it/s]13240it [00:08, 1510.58it/s]13413it [00:08, 1567.94it/s]13580it [00:08, 1597.20it/s]13747it [00:08, 1617.75it/s]13914it [00:08, 1616.70it/s]14077it [00:08, 1612.74it/s]14243it [00:08, 1625.24it/s]14408it [00:08, 1632.22it/s]14581it [00:09, 1660.20it/s]14748it [00:09, 1660.62it/s]14921it [00:09, 1679.41it/s]15090it [00:09, 1417.23it/s]15256it [00:09, 1480.80it/s]15410it [00:10, 711.25it/s] 15572it [00:10, 854.79it/s]15737it [00:10, 998.30it/s]15910it [00:10, 1142.73it/s]16076it [00:10, 1259.26it/s]16249it [00:10, 1370.91it/s]16410it [00:10, 1422.74it/s]16571it [00:10, 1474.05it/s]16734it [00:10, 1517.58it/s]16908it [00:10, 1575.78it/s]17074it [00:11, 1598.01it/s]17246it [00:11, 1631.75it/s]17413it [00:11, 1620.20it/s]17578it [00:11, 1614.82it/s]17742it [00:11, 1585.46it/s]17915it [00:11, 1624.82it/s]18082it [00:11, 1638.06it/s]18252it [00:11, 1655.48it/s]18419it [00:11, 1656.72it/s]18586it [00:11, 1620.19it/s]18753it [00:12, 1632.98it/s]18918it [00:12, 1635.99it/s]19091it [00:12, 1662.37it/s]19258it [00:12, 1656.84it/s]19424it [00:12, 1644.44it/s]19589it [00:12, 1367.85it/s]19764it [00:12, 1462.67it/s]19929it [00:12, 1514.19it/s]20104it [00:12, 1577.77it/s]20271it [00:13, 1602.85it/s]20435it [00:13, 1608.25it/s]20599it [00:13, 1586.13it/s]20769it [00:13, 1618.36it/s]20933it [00:13, 1619.70it/s]21107it [00:13, 1651.39it/s]21273it [00:13, 1651.86it/s]21439it [00:13, 1645.82it/s]21604it [00:13, 1614.13it/s]21777it [00:13, 1645.81it/s]21944it [00:14, 1651.05it/s]22119it [00:14, 1677.99it/s]22288it [00:14, 1673.72it/s]22456it [00:14, 1666.78it/s]22623it [00:14, 1626.03it/s]22795it [00:14, 1651.52it/s]22964it [00:14, 1660.94it/s]23140it [00:14, 1687.24it/s]23309it [00:14, 1675.49it/s]23477it [00:14, 1655.81it/s]23643it [00:15, 1411.05it/s]23815it [00:15, 1490.48it/s]23986it [00:15, 1535.32it/s]24159it [00:15, 1588.65it/s]24328it [00:15, 1617.12it/s]24493it [00:15, 1624.87it/s]24668it [00:15, 1660.43it/s]24836it [00:15, 1663.11it/s]25012it [00:15, 1689.05it/s]25182it [00:16, 1687.74it/s]25357it [00:16, 1703.91it/s]25528it [00:16, 1692.58it/s]25701it [00:16, 1702.74it/s]25872it [00:16, 1699.09it/s]26055it [00:16, 1733.79it/s]26229it [00:16, 1735.03it/s]26409it [00:16, 1753.43it/s]26585it [00:16, 1741.51it/s]26761it [00:16, 1744.06it/s]26936it [00:17, 1725.42it/s]27109it [00:17, 1691.79it/s]27279it [00:17, 1652.24it/s]27453it [00:17, 1676.58it/s]27622it [00:17, 1677.58it/s]27798it [00:17, 1699.20it/s]27969it [00:17, 1492.18it/s]28146it [00:17, 1565.13it/s]28316it [00:17, 1601.03it/s]28502it [00:18, 1669.12it/s]28675it [00:18, 1685.96it/s]28851it [00:18, 1706.49it/s]29024it [00:18, 1685.87it/s]29196it [00:18, 1695.78it/s]29367it [00:18, 1672.94it/s]29543it [00:18, 1697.16it/s]29714it [00:18, 1686.64it/s]29890it [00:18, 1706.70it/s]30061it [00:18, 1693.01it/s]30238it [00:19, 1714.35it/s]30410it [00:19, 1701.36it/s]30585it [00:19, 1713.64it/s]30757it [00:19, 1706.18it/s]30934it [00:19, 1723.39it/s]31107it [00:19, 1711.31it/s]31282it [00:19, 1721.51it/s]31455it [00:19, 1704.36it/s]31629it [00:19, 1713.86it/s]31801it [00:19, 1703.31it/s]31986it [00:20, 1742.29it/s]32161it [00:20, 1515.26it/s]32343it [00:20, 1594.66it/s]32511it [00:20, 1618.48it/s]32686it [00:20, 1655.33it/s]32855it [00:20, 1663.31it/s]33031it [00:20, 1689.91it/s]33202it [00:20, 1692.73it/s]33380it [00:20, 1715.59it/s]33553it [00:21, 1708.11it/s]33731it [00:21, 1727.60it/s]33905it [00:21, 1717.34it/s]34082it [00:21, 1731.83it/s]34256it [00:21, 1720.33it/s]34436it [00:21, 1743.22it/s]34611it [00:21, 1723.17it/s]34790it [00:21, 1740.93it/s]34966it [00:21, 1743.85it/s]35141it [00:21, 1740.64it/s]35316it [00:22, 1708.19it/s]35491it [00:22, 1718.42it/s]35663it [00:22, 1697.90it/s]35836it [00:22, 1707.33it/s]36007it [00:22, 1695.63it/s]36181it [00:22, 1707.54it/s]36352it [00:22, 1454.72it/s]36513it [00:22, 1497.94it/s]36676it [00:22, 1534.33it/s]36850it [00:22, 1589.10it/s]37017it [00:23, 1610.95it/s]37190it [00:23, 1643.68it/s]37357it [00:23, 1643.56it/s]37523it [00:23, 1632.81it/s]37688it [00:23, 1606.90it/s]37860it [00:23, 1637.48it/s]38025it [00:23, 1641.10it/s]38197it [00:23, 1661.56it/s]38364it [00:23, 1641.78it/s]38529it [00:24, 1634.99it/s]38693it [00:24, 1603.68it/s]38860it [00:24, 1622.38it/s]39023it [00:24, 1624.51it/s]39195it [00:24, 1651.94it/s]39361it [00:24, 1651.84it/s]39529it [00:24, 1658.69it/s]39695it [00:24, 1620.95it/s]39863it [00:24, 1636.66it/s]40037it [00:24, 1665.99it/s]40205it [00:25, 1669.10it/s]40379it [00:25, 1687.19it/s]40548it [00:25, 1442.65it/s]40710it [00:25, 1489.21it/s]40868it [00:25, 1512.79it/s]41041it [00:25, 1571.02it/s]41207it [00:25, 1594.12it/s]41369it [00:25, 1574.14it/s]41528it [00:25, 1538.71it/s]41689it [00:25, 1559.27it/s]41852it [00:26, 1579.77it/s]42017it [00:26, 1598.47it/s]42182it [00:26, 1612.62it/s]42344it [00:26, 1613.56it/s]42506it [00:26, 1590.74it/s]42666it [00:26, 1335.84it/s]42829it [00:26, 1411.81it/s]42995it [00:26, 1477.33it/s]43168it [00:26, 1542.94it/s]43332it [00:27, 1569.93it/s]43495it [00:27, 1587.14it/s]43656it [00:27, 1572.90it/s]43821it [00:27, 1593.56it/s]43982it [00:27, 1522.09it/s]44154it [00:27, 1574.17it/s]44313it [00:27, 1565.39it/s]44471it [00:27, 1557.62it/s]44628it [00:27, 1304.17it/s]44798it [00:28, 1402.00it/s]44966it [00:28, 1474.56it/s]45120it [00:28, 1479.01it/s]45281it [00:28, 1515.23it/s]45436it [00:28, 1275.81it/s]45573it [00:28, 1232.78it/s]45738it [00:28, 1333.81it/s]45913it [00:28, 1434.86it/s]46080it [00:28, 1498.04it/s]46253it [00:29, 1560.72it/s]46414it [00:29, 1510.66it/s]46569it [00:29, 1502.46it/s]46732it [00:29, 1537.09it/s]46905it [00:29, 1588.61it/s]47071it [00:29, 1608.90it/s]47244it [00:29, 1642.07it/s]47410it [00:29, 1615.73it/s]47573it [00:29, 1612.14it/s]47735it [00:29, 1613.86it/s]47909it [00:30, 1648.30it/s]48075it [00:30, 1649.94it/s]48248it [00:30, 1671.93it/s]48416it [00:30, 1640.85it/s]48581it [00:30, 1631.19it/s]48754it [00:30, 1657.80it/s]48921it [00:30, 1442.85it/s]49094it [00:30, 1518.44it/s]49258it [00:30, 1552.15it/s]49420it [00:31, 1569.39it/s]49580it [00:31, 1564.91it/s]49754it [00:31, 1611.14it/s]49919it [00:31, 1621.12it/s]50091it [00:31, 1647.56it/s]50257it [00:31, 1633.90it/s]50422it [00:31, 1626.50it/s]50586it [00:31, 1607.08it/s]50758it [00:31, 1637.98it/s]50923it [00:31, 1637.59it/s]51097it [00:32, 1664.85it/s]51266it [00:32, 1671.83it/s]51434it [00:32, 1652.08it/s]51600it [00:32, 1623.00it/s]51773it [00:32, 1652.88it/s]51940it [00:32, 1655.25it/s]52113it [00:32, 1676.12it/s]52281it [00:32, 1657.96it/s]52447it [00:32, 1642.84it/s]52612it [00:32, 1615.43it/s]52785it [00:33, 1646.84it/s]52952it [00:33, 1420.38it/s]53120it [00:33, 1488.63it/s]53281it [00:33, 1521.24it/s]53437it [00:33, 1521.10it/s]53602it [00:33, 1556.71it/s]53765it [00:33, 1577.11it/s]53936it [00:33, 1614.71it/s]54099it [00:33, 1613.38it/s]54271it [00:34, 1642.51it/s]54436it [00:34, 1605.53it/s]54598it [00:34, 1601.77it/s]54762it [00:34, 1611.85it/s]54933it [00:34, 1640.02it/s]55098it [00:34, 1636.66it/s]55265it [00:34, 1645.85it/s]55430it [00:34, 1613.22it/s]55592it [00:34, 1605.48it/s]55753it [00:34, 1594.15it/s]55923it [00:35, 1622.29it/s]56093it [00:35, 1627.95it/s]56266it [00:35, 1655.36it/s]56432it [00:35, 1647.53it/s]56597it [00:35, 1611.05it/s]56767it [00:35, 1636.22it/s]56936it [00:35, 1649.84it/s]57115it [00:35, 1686.99it/s]57285it [00:35, 1453.81it/s]57437it [00:36, 1389.48it/s]57581it [00:36, 1333.17it/s]57757it [00:36, 1436.47it/s]57911it [00:36, 1465.07it/s]58082it [00:36, 1528.51it/s]58243it [00:36, 1551.33it/s]58405it [00:36, 1568.56it/s]58564it [00:36, 1572.74it/s]58737it [00:36, 1616.69it/s]58902it [00:36, 1624.55it/s]59075it [00:37, 1654.58it/s]59242it [00:37, 1648.46it/s]59408it [00:37, 1637.02it/s]59573it [00:37, 1636.91it/s]59739it [00:37, 1643.72it/s]59910it [00:37, 1660.51it/s]60077it [00:37, 1660.69it/s]60250it [00:37, 1680.55it/s]60419it [00:37, 1637.17it/s]60584it [00:38, 1634.84it/s]60751it [00:38, 1644.87it/s]60926it [00:38, 1673.93it/s]61094it [00:38, 1670.21it/s]61267it [00:38, 1686.84it/s]61436it [00:38, 1423.93it/s]61604it [00:38, 1491.29it/s]61759it [00:40, 320.35it/s] 61865it [00:40, 1542.83it/s]
  0% 0/61865 [00:00<?, ?it/s]  6% 3704/61865 [00:00<00:01, 37039.24it/s] 13% 7838/61865 [00:00<00:01, 38231.20it/s] 19% 11986/61865 [00:00<00:01, 39148.30it/s] 26% 16237/61865 [00:00<00:01, 40098.10it/s] 34% 20804/61865 [00:00<00:00, 41619.14it/s] 41% 25454/61865 [00:00<00:00, 42971.36it/s] 49% 30161/61865 [00:00<00:00, 44121.90it/s] 56% 34926/61865 [00:00<00:00, 45121.64it/s] 64% 39616/61865 [00:00<00:00, 45638.93it/s] 71% 44122/61865 [00:01<00:00, 45462.66it/s] 79% 48577/61865 [00:02<00:01, 11443.38it/s] 86% 53181/61865 [00:02<00:00, 14773.91it/s] 94% 58023/61865 [00:02<00:00, 18664.53it/s]100% 61865/61865 [00:02<00:00, 26400.96it/s]
  0% 0/61865 [00:00<?, ?it/s]  1% 467/61865 [00:00<00:13, 4667.44it/s]  2% 937/61865 [00:00<00:13, 4676.11it/s]  2% 1370/61865 [00:00<00:13, 4566.27it/s]  3% 1771/61865 [00:00<00:13, 4382.84it/s]  3% 2155/61865 [00:00<00:14, 4199.65it/s]  4% 2535/61865 [00:00<00:14, 4068.59it/s]  5% 2953/61865 [00:00<00:14, 4098.25it/s]  6% 3421/61865 [00:00<00:13, 4255.33it/s]  6% 3849/61865 [00:00<00:13, 4262.21it/s]  7% 4259/61865 [00:01<00:13, 4160.48it/s]  8% 4664/61865 [00:01<00:16, 3558.74it/s]  8% 5028/61865 [00:01<00:16, 3391.25it/s]  9% 5399/61865 [00:01<00:16, 3479.72it/s]  9% 5770/61865 [00:01<00:15, 3545.49it/s] 10% 6150/61865 [00:01<00:15, 3617.87it/s] 11% 6515/61865 [00:01<00:15, 3613.10it/s] 11% 6879/61865 [00:01<00:15, 3549.15it/s] 12% 7238/61865 [00:01<00:15, 3560.96it/s] 12% 7596/61865 [00:01<00:15, 3537.88it/s] 13% 8000/61865 [00:02<00:14, 3673.37it/s] 14% 8370/61865 [00:02<00:14, 3638.11it/s] 14% 8763/61865 [00:02<00:14, 3720.92it/s] 15% 9137/61865 [00:02<00:14, 3670.65it/s] 15% 9506/61865 [00:02<00:14, 3613.18it/s] 16% 9878/61865 [00:02<00:14, 3642.53it/s] 17% 10252/61865 [00:02<00:14, 3670.73it/s] 17% 10620/61865 [00:02<00:13, 3669.80it/s] 18% 10988/61865 [00:02<00:13, 3639.98it/s] 18% 11354/61865 [00:03<00:13, 3642.82it/s] 19% 11719/61865 [00:03<00:13, 3620.18it/s] 20% 12103/61865 [00:03<00:13, 3683.08it/s] 20% 12496/61865 [00:03<00:13, 3751.23it/s] 21% 12887/61865 [00:03<00:12, 3795.71it/s] 21% 13283/61865 [00:03<00:12, 3842.14it/s] 22% 13668/61865 [00:03<00:12, 3834.67it/s] 23% 14052/61865 [00:03<00:12, 3736.06it/s] 23% 14427/61865 [00:03<00:12, 3697.83it/s] 24% 14800/61865 [00:03<00:12, 3706.24it/s] 25% 15200/61865 [00:04<00:12, 3788.58it/s] 25% 15589/61865 [00:04<00:12, 3817.77it/s] 26% 15985/61865 [00:04<00:11, 3858.63it/s] 26% 16380/61865 [00:04<00:11, 3883.41it/s] 27% 16778/61865 [00:04<00:11, 3911.45it/s] 28% 17202/61865 [00:04<00:11, 4002.36it/s] 28% 17619/61865 [00:04<00:10, 4050.25it/s] 29% 18044/61865 [00:04<00:10, 4105.19it/s] 30% 18456/61865 [00:04<00:10, 4081.38it/s] 30% 18865/61865 [00:04<00:10, 3990.55it/s] 31% 19265/61865 [00:05<00:10, 3936.27it/s] 32% 19672/61865 [00:05<00:10, 3972.58it/s] 32% 20070/61865 [00:05<00:10, 3922.95it/s] 33% 20463/61865 [00:05<00:10, 3889.77it/s] 34% 20860/61865 [00:05<00:10, 3909.00it/s] 34% 21252/61865 [00:05<00:10, 3902.53it/s] 35% 21646/61865 [00:05<00:10, 3910.69it/s] 36% 22059/61865 [00:05<00:10, 3972.90it/s] 36% 22459/61865 [00:05<00:09, 3980.45it/s] 37% 22864/61865 [00:05<00:09, 3998.28it/s] 38% 23271/61865 [00:06<00:09, 4018.66it/s] 38% 23674/61865 [00:06<00:09, 4016.89it/s] 39% 24079/61865 [00:06<00:09, 4026.17it/s] 40% 24482/61865 [00:06<00:09, 4019.41it/s] 40% 24885/61865 [00:06<00:09, 3976.46it/s] 41% 25286/61865 [00:06<00:09, 3986.12it/s] 42% 25685/61865 [00:06<00:09, 3947.02it/s] 42% 26080/61865 [00:06<00:09, 3919.67it/s] 43% 26473/61865 [00:06<00:09, 3898.40it/s] 43% 26870/61865 [00:06<00:08, 3919.36it/s] 44% 27263/61865 [00:07<00:08, 3910.72it/s] 45% 27662/61865 [00:07<00:08, 3933.26it/s] 45% 28056/61865 [00:07<00:08, 3928.32it/s] 46% 28449/61865 [00:07<00:08, 3889.71it/s] 47% 28839/61865 [00:07<00:08, 3873.73it/s] 47% 29230/61865 [00:07<00:08, 3882.89it/s] 48% 29628/61865 [00:07<00:08, 3911.22it/s] 49% 30021/61865 [00:07<00:08, 3914.76it/s] 49% 30426/61865 [00:07<00:07, 3952.67it/s] 50% 30822/61865 [00:07<00:07, 3946.13it/s] 50% 31218/61865 [00:08<00:07, 3948.10it/s] 51% 31624/61865 [00:08<00:07, 3979.95it/s] 52% 32025/61865 [00:08<00:07, 3987.09it/s] 52% 32424/61865 [00:08<00:07, 3929.89it/s] 53% 32818/61865 [00:08<00:07, 3900.13it/s] 54% 33218/61865 [00:08<00:07, 3928.96it/s] 54% 33618/61865 [00:08<00:07, 3948.68it/s] 55% 34014/61865 [00:08<00:07, 3923.50it/s] 56% 34416/61865 [00:08<00:06, 3948.17it/s] 56% 34817/61865 [00:08<00:06, 3965.45it/s] 57% 35214/61865 [00:09<00:06, 3946.52it/s] 58% 35609/61865 [00:09<00:06, 3880.42it/s] 58% 35998/61865 [00:09<00:06, 3880.59it/s] 59% 36387/61865 [00:09<00:06, 3862.60it/s] 59% 36795/61865 [00:09<00:06, 3925.26it/s] 60% 37203/61865 [00:09<00:06, 3969.59it/s] 61% 37616/61865 [00:09<00:06, 4013.96it/s] 61% 38032/61865 [00:09<00:05, 4054.95it/s] 62% 38438/61865 [00:09<00:05, 4051.05it/s] 63% 38844/61865 [00:09<00:05, 3992.26it/s] 63% 39244/61865 [00:10<00:05, 3984.46it/s] 64% 39643/61865 [00:10<00:05, 3978.32it/s] 65% 40042/61865 [00:10<00:05, 3977.62it/s] 65% 40444/61865 [00:10<00:05, 3990.09it/s] 66% 40845/61865 [00:10<00:05, 3994.98it/s] 67% 41245/61865 [00:10<00:05, 3974.05it/s] 67% 41644/61865 [00:10<00:05, 3976.68it/s] 68% 42042/61865 [00:10<00:04, 3970.53it/s] 69% 42440/61865 [00:10<00:04, 3953.08it/s] 69% 42836/61865 [00:11<00:04, 3946.00it/s] 70% 43259/61865 [00:11<00:04, 4024.98it/s] 71% 43669/61865 [00:11<00:04, 4044.96it/s] 71% 44074/61865 [00:11<00:04, 4041.41it/s] 72% 44479/61865 [00:11<00:04, 4021.11it/s] 73% 44882/61865 [00:11<00:04, 3998.20it/s] 73% 45282/61865 [00:11<00:04, 3925.29it/s] 74% 45675/61865 [00:11<00:04, 3770.60it/s] 74% 46059/61865 [00:11<00:04, 3789.60it/s] 75% 46440/61865 [00:11<00:04, 3785.77it/s] 76% 46821/61865 [00:12<00:03, 3790.08it/s] 76% 47212/61865 [00:12<00:03, 3821.69it/s] 77% 47595/61865 [00:12<00:03, 3820.69it/s] 78% 47978/61865 [00:12<00:03, 3719.53it/s] 78% 48351/61865 [00:12<00:03, 3671.66it/s] 79% 48727/61865 [00:12<00:03, 3697.55it/s] 79% 49118/61865 [00:12<00:03, 3757.42it/s] 80% 49500/61865 [00:12<00:03, 3775.95it/s] 81% 49879/61865 [00:12<00:03, 3733.86it/s] 81% 50253/61865 [00:12<00:03, 3717.36it/s] 82% 50626/61865 [00:13<00:03, 3653.31it/s] 82% 50992/61865 [00:13<00:03, 3597.05it/s] 83% 51353/61865 [00:13<00:03, 3502.48it/s] 84% 51705/61865 [00:13<00:02, 3465.33it/s] 84% 52063/61865 [00:13<00:02, 3497.24it/s] 85% 52421/61865 [00:13<00:02, 3521.04it/s] 85% 52784/61865 [00:13<00:02, 3549.86it/s] 86% 53140/61865 [00:13<00:02, 3501.59it/s] 86% 53491/61865 [00:13<00:02, 3496.31it/s] 87% 53855/61865 [00:13<00:02, 3537.27it/s] 88% 54218/61865 [00:14<00:02, 3563.07it/s] 88% 54603/61865 [00:14<00:01, 3643.88it/s] 89% 54975/61865 [00:14<00:01, 3665.81it/s] 89% 55343/61865 [00:14<00:01, 3609.92it/s] 90% 55705/61865 [00:14<00:01, 3414.17it/s] 91% 56059/61865 [00:14<00:01, 3449.35it/s] 91% 56451/61865 [00:14<00:01, 3576.20it/s] 92% 56812/61865 [00:14<00:01, 3477.51it/s] 92% 57163/61865 [00:14<00:01, 3321.79it/s] 93% 57499/61865 [00:15<00:01, 3040.87it/s] 93% 57810/61865 [00:15<00:01, 2842.68it/s] 94% 58102/61865 [00:15<00:01, 2742.85it/s] 94% 58383/61865 [00:15<00:01, 2694.16it/s] 95% 58687/61865 [00:15<00:01, 2787.91it/s] 95% 59070/61865 [00:15<00:00, 3034.33it/s] 96% 59449/61865 [00:15<00:00, 3226.57it/s] 97% 59814/61865 [00:15<00:00, 3342.83it/s] 97% 60157/61865 [00:15<00:00, 3291.17it/s] 98% 60543/61865 [00:16<00:00, 3441.37it/s] 99% 60956/61865 [00:16<00:00, 3622.24it/s] 99% 61344/61865 [00:16<00:00, 3694.73it/s]100% 61725/61865 [00:16<00:00, 3725.63it/s]100% 61865/61865 [00:16<00:00, 3780.31it/s]
Number of tasks = 645
Class sizes
CHEMBL1033994 0: 41.98%, 1: 58.02%
CHEMBL1119333 0: 9.59%, 1: 90.41%
CHEMBL1217000 0: 52.41%, 1: 47.59%
CHEMBL1243965 0: 17.11%, 1: 82.89%
CHEMBL1243966 0: 41.96%, 1: 58.04%
CHEMBL1243967 0: 10.17%, 1: 89.83%
CHEMBL1243968 0: 15.06%, 1: 84.94%
CHEMBL1243970 0: 6.91%, 1: 93.09%
CHEMBL1243972 0: 9.92%, 1: 90.08%
CHEMBL1243976 0: 28.99%, 1: 71.01%
CHEMBL1246087 0: 33.88%, 1: 66.12%
CHEMBL1246088 0: 48.37%, 1: 51.63%
CHEMBL1613762 0: 2.45%, 1: 97.55%
CHEMBL1613779 0: 58.47%, 1: 41.53%
CHEMBL1613785 0: 43.22%, 1: 56.78%
CHEMBL1613787 0: 69.08%, 1: 30.92%
CHEMBL1613807 0: 95.99%, 1: 4.01%
CHEMBL1613813 0: 44.20%, 1: 55.80%
CHEMBL1613814 0: 53.85%, 1: 46.15%
CHEMBL1613817 0: 84.86%, 1: 15.14%
CHEMBL1613853 0: 41.86%, 1: 58.14%
CHEMBL1613861 0: 14.47%, 1: 85.53%
CHEMBL1613864 0: 5.19%, 1: 94.81%
CHEMBL1613867 0: 97.13%, 1: 2.87%
CHEMBL1613870 0: 15.80%, 1: 84.20%
CHEMBL1613871 0: 6.15%, 1: 93.85%
CHEMBL1613874 0: 65.78%, 1: 34.22%
CHEMBL1613876 0: 54.35%, 1: 45.65%
CHEMBL1613884 0: 21.51%, 1: 78.49%
CHEMBL1613890 0: 34.85%, 1: 65.15%
CHEMBL1613897 0: 46.64%, 1: 53.36%
CHEMBL1613898 0: 75.45%, 1: 24.55%
CHEMBL1613904 0: 50.77%, 1: 49.23%
CHEMBL1613907 0: 5.34%, 1: 94.66%
CHEMBL1613926 0: 30.15%, 1: 69.85%
CHEMBL1613928 0: 45.90%, 1: 54.10%
CHEMBL1613929 0: 81.25%, 1: 18.75%
CHEMBL1613941 0: 32.58%, 1: 67.42%
CHEMBL1613942 0: 41.43%, 1: 58.57%
CHEMBL1613947 0: 46.01%, 1: 53.99%
CHEMBL1613949 0: 60.13%, 1: 39.87%
CHEMBL1613950 0: 43.05%, 1: 56.95%
CHEMBL1613955 0: 54.04%, 1: 45.96%
CHEMBL1613962 0: 24.26%, 1: 75.74%
CHEMBL1613967 0: 44.59%, 1: 55.41%
CHEMBL1613981 0: 58.85%, 1: 41.15%
CHEMBL1613991 0: 14.98%, 1: 85.02%
CHEMBL1613997 0: 7.73%, 1: 92.27%
CHEMBL1614001 0: 98.28%, 1: 1.72%
CHEMBL1614004 0: 14.66%, 1: 85.34%
CHEMBL1614016 0: 12.22%, 1: 87.78%
CHEMBL1614030 0: 19.96%, 1: 80.04%
CHEMBL1614034 0: 2.63%, 1: 97.37%
CHEMBL1614035 0: 77.14%, 1: 22.86%
CHEMBL1614049 0: 39.63%, 1: 60.37%
CHEMBL1614053 0: 42.48%, 1: 57.52%
CHEMBL1614063 0: 95.18%, 1: 4.82%
CHEMBL1614065 0: 9.71%, 1: 90.29%
CHEMBL1614066 0: 94.74%, 1: 5.26%
CHEMBL1614069 0: 56.82%, 1: 43.18%
CHEMBL1614072 0: 50.36%, 1: 49.64%
CHEMBL1614084 0: 13.26%, 1: 86.74%
CHEMBL1614091 0: 4.43%, 1: 95.57%
CHEMBL1614092 0: 47.50%, 1: 52.50%
CHEMBL1614097 0: 46.99%, 1: 53.01%
CHEMBL1614098 0: 97.25%, 1: 2.75%
CHEMBL1614104 0: 7.74%, 1: 92.26%
CHEMBL1614105 0: 6.12%, 1: 93.88%
CHEMBL1614109 0: 59.87%, 1: 40.13%
CHEMBL1614128 0: 10.35%, 1: 89.65%
CHEMBL1614131 0: 97.14%, 1: 2.86%
CHEMBL1614132 0: 42.07%, 1: 57.93%
CHEMBL1614138 0: 67.83%, 1: 32.17%
CHEMBL1614155 0: 25.40%, 1: 74.60%
CHEMBL1614158 0: 74.52%, 1: 25.48%
CHEMBL1614167 0: 16.17%, 1: 83.83%
CHEMBL1614170 0: 68.86%, 1: 31.14%
CHEMBL1614171 0: 93.77%, 1: 6.23%
CHEMBL1614175 0: 80.66%, 1: 19.34%
CHEMBL1614185 0: 26.17%, 1: 73.83%
CHEMBL1614197 0: 5.26%, 1: 94.74%
CHEMBL1614199 0: 49.66%, 1: 50.34%
CHEMBL1614202 0: 7.32%, 1: 92.68%
CHEMBL1614215 0: 52.76%, 1: 47.24%
CHEMBL1614216 0: 48.52%, 1: 51.48%
CHEMBL1614218 0: 22.36%, 1: 77.64%
CHEMBL1614225 0: 62.50%, 1: 37.50%
CHEMBL1614244 0: 38.85%, 1: 61.15%
CHEMBL1614247 0: 44.34%, 1: 55.66%
CHEMBL1614252 0: 60.11%, 1: 39.89%
CHEMBL1614255 0: 61.26%, 1: 38.74%
CHEMBL1614259 0: 32.82%, 1: 67.18%
CHEMBL1614272 0: 67.71%, 1: 32.29%
CHEMBL1614276 0: 51.74%, 1: 48.26%
CHEMBL1614287 0: 85.53%, 1: 14.47%
CHEMBL1614288 0: 72.51%, 1: 27.49%
CHEMBL1614290 0: 4.51%, 1: 95.49%
CHEMBL1614295 0: 2.29%, 1: 97.71%
CHEMBL1614301 0: 19.74%, 1: 80.26%
CHEMBL1614304 0: 63.76%, 1: 36.24%
CHEMBL1614309 0: 27.66%, 1: 72.34%
CHEMBL1614311 0: 53.76%, 1: 46.24%
CHEMBL1614314 0: 94.70%, 1: 5.30%
CHEMBL1614319 0: 72.16%, 1: 27.84%
CHEMBL1614320 0: 29.79%, 1: 70.21%
CHEMBL1614321 0: 60.16%, 1: 39.84%
CHEMBL1614328 0: 46.64%, 1: 53.36%
CHEMBL1614329 0: 40.69%, 1: 59.31%
CHEMBL1614336 0: 47.01%, 1: 52.99%
CHEMBL1614344 0: 67.72%, 1: 32.28%
CHEMBL1614356 0: 43.90%, 1: 56.10%
CHEMBL1614359 0: 50.26%, 1: 49.74%
CHEMBL1614363 0: 53.19%, 1: 46.81%
CHEMBL1614385 0: 93.27%, 1: 6.73%
CHEMBL1614388 0: 56.19%, 1: 43.81%
CHEMBL1614393 0: 45.00%, 1: 55.00%
CHEMBL1614395 0: 11.32%, 1: 88.68%
CHEMBL1614403 0: 88.15%, 1: 11.85%
CHEMBL1614423 0: 57.84%, 1: 42.16%
CHEMBL1614425 0: 38.24%, 1: 61.76%
CHEMBL1614433 0: 60.77%, 1: 39.23%
CHEMBL1614434 0: 80.25%, 1: 19.75%
CHEMBL1614456 0: 3.36%, 1: 96.64%
CHEMBL1614466 0: 10.32%, 1: 89.68%
CHEMBL1614469 0: 24.90%, 1: 75.10%
CHEMBL1614477 0: 43.75%, 1: 56.25%
CHEMBL1614478 0: 12.50%, 1: 87.50%
CHEMBL1614480 0: 85.71%, 1: 14.29%
CHEMBL1614484 0: 9.01%, 1: 90.99%
CHEMBL1614492 0: 40.54%, 1: 59.46%
CHEMBL1614499 0: 44.47%, 1: 55.53%
CHEMBL1614503 0: 40.30%, 1: 59.70%
CHEMBL1614504 0: 17.80%, 1: 82.20%
CHEMBL1614509 0: 79.43%, 1: 20.57%
CHEMBL1614512 0: 62.79%, 1: 37.21%
CHEMBL1614514 0: 83.07%, 1: 16.93%
CHEMBL1614515 0: 19.61%, 1: 80.39%
CHEMBL1614516 0: 96.81%, 1: 3.19%
CHEMBL1614522 0: 28.55%, 1: 71.45%
CHEMBL1614524 0: 12.98%, 1: 87.02%
CHEMBL1614528 0: 40.91%, 1: 59.09%
CHEMBL1614547 0: 72.51%, 1: 27.49%
CHEMBL1614548 0: 17.95%, 1: 82.05%
CHEMBL1614549 0: 84.25%, 1: 15.75%
CHEMBL1614550 0: 9.30%, 1: 90.70%
CHEMBL1614554 0: 71.64%, 1: 28.36%
CHEMBL1676103 0: 39.85%, 1: 60.15%
CHEMBL1737860 0: 3.60%, 1: 96.40%
CHEMBL1737863 0: 36.27%, 1: 63.73%
CHEMBL1737865 0: 61.01%, 1: 38.99%
CHEMBL1737868 0: 79.41%, 1: 20.59%
CHEMBL1737910 0: 45.71%, 1: 54.29%
CHEMBL1737912 0: 26.51%, 1: 73.49%
CHEMBL1737942 0: 44.52%, 1: 55.48%
CHEMBL1737951 0: 51.83%, 1: 48.17%
CHEMBL1737961 0: 93.34%, 1: 6.66%
CHEMBL1737966 0: 21.36%, 1: 78.64%
CHEMBL1737967 0: 89.17%, 1: 10.83%
CHEMBL1737977 0: 13.87%, 1: 86.13%
CHEMBL1737978 0: 18.18%, 1: 81.82%
CHEMBL1737979 0: 8.94%, 1: 91.06%
CHEMBL1738019 0: 24.85%, 1: 75.15%
CHEMBL1738021 0: 10.87%, 1: 89.13%
CHEMBL1738025 0: 33.80%, 1: 66.20%
CHEMBL1738040 0: 73.48%, 1: 26.52%
CHEMBL1738043 0: 46.43%, 1: 53.57%
CHEMBL1738079 0: 3.88%, 1: 96.12%
CHEMBL1738080 0: 76.43%, 1: 23.57%
CHEMBL1738091 0: 89.77%, 1: 10.23%
CHEMBL1738097 0: 72.41%, 1: 27.59%
CHEMBL1738131 0: 68.59%, 1: 31.41%
CHEMBL1738164 0: 79.79%, 1: 20.21%
CHEMBL1738171 0: 85.45%, 1: 14.55%
CHEMBL1738183 0: 59.62%, 1: 40.38%
CHEMBL1738197 0: 36.90%, 1: 63.10%
CHEMBL1738202 0: 3.47%, 1: 96.53%
CHEMBL1738242 0: 16.55%, 1: 83.45%
CHEMBL1738249 0: 86.53%, 1: 13.47%
CHEMBL1738253 0: 7.33%, 1: 92.67%
CHEMBL1738319 0: 59.17%, 1: 40.83%
CHEMBL1738325 0: 96.86%, 1: 3.14%
CHEMBL1738362 0: 21.43%, 1: 78.57%
CHEMBL1738369 0: 56.34%, 1: 43.66%
CHEMBL1738371 0: 81.97%, 1: 18.03%
CHEMBL1738391 0: 68.38%, 1: 31.62%
CHEMBL1738400 0: 77.86%, 1: 22.14%
CHEMBL1738402 0: 54.19%, 1: 45.81%
CHEMBL1738407 0: 85.58%, 1: 14.42%
CHEMBL1738408 0: 2.18%, 1: 97.82%
CHEMBL1738414 0: 5.11%, 1: 94.89%
CHEMBL1738418 0: 68.85%, 1: 31.15%
CHEMBL1738422 0: 4.29%, 1: 95.71%
CHEMBL1738424 0: 77.50%, 1: 22.50%
CHEMBL1738430 0: 25.54%, 1: 74.46%
CHEMBL1738438 0: 2.76%, 1: 97.24%
CHEMBL1738482 0: 7.69%, 1: 92.31%
CHEMBL1738485 0: 17.65%, 1: 82.35%
CHEMBL1738494 0: 97.26%, 1: 2.74%
CHEMBL1738495 0: 1.70%, 1: 98.30%
CHEMBL1738497 0: 64.50%, 1: 35.50%
CHEMBL1738502 0: 15.20%, 1: 84.80%
CHEMBL1738510 0: 54.41%, 1: 45.59%
CHEMBL1738512 0: 3.05%, 1: 96.95%
CHEMBL1738513 0: 28.64%, 1: 71.36%
CHEMBL1738552 0: 64.70%, 1: 35.30%
CHEMBL1738575 0: 65.65%, 1: 34.35%
CHEMBL1738578 0: 23.30%, 1: 76.70%
CHEMBL1738579 0: 22.79%, 1: 77.21%
CHEMBL1738593 0: 17.65%, 1: 82.35%
CHEMBL1738599 0: 16.07%, 1: 83.93%
CHEMBL1738602 0: 42.70%, 1: 57.30%
CHEMBL1738610 0: 42.07%, 1: 57.93%
CHEMBL1738611 0: 2.23%, 1: 97.77%
CHEMBL1738632 0: 86.32%, 1: 13.68%
CHEMBL1738633 0: 4.05%, 1: 95.95%
CHEMBL1738639 0: 64.47%, 1: 35.53%
CHEMBL1738642 0: 34.82%, 1: 65.18%
CHEMBL1738670 0: 9.18%, 1: 90.82%
CHEMBL1738673 0: 12.98%, 1: 87.02%
CHEMBL1738679 0: 63.95%, 1: 36.05%
CHEMBL1738682 0: 62.45%, 1: 37.55%
CHEMBL1794296 0: 44.02%, 1: 55.98%
CHEMBL1794303 0: 2.71%, 1: 97.29%
CHEMBL1794320 0: 96.68%, 1: 3.32%
CHEMBL1794327 0: 91.59%, 1: 8.41%
CHEMBL1794336 0: 53.07%, 1: 46.93%
CHEMBL1794350 0: 3.44%, 1: 96.56%
CHEMBL1794355 0: 5.26%, 1: 94.74%
CHEMBL1794356 0: 13.92%, 1: 86.08%
CHEMBL1794358 0: 94.14%, 1: 5.86%
CHEMBL1794365 0: 7.12%, 1: 92.88%
CHEMBL1794383 0: 6.90%, 1: 93.10%
CHEMBL1794387 0: 65.04%, 1: 34.96%
CHEMBL1794393 0: 67.55%, 1: 32.45%
CHEMBL1794396 0: 3.96%, 1: 96.04%
CHEMBL1794410 0: 23.98%, 1: 76.02%
CHEMBL1794413 0: 6.02%, 1: 93.98%
CHEMBL1794438 0: 40.17%, 1: 59.83%
CHEMBL1794445 0: 87.84%, 1: 12.16%
CHEMBL1794452 0: 94.30%, 1: 5.70%
CHEMBL1794457 0: 92.98%, 1: 7.02%
CHEMBL1794460 0: 28.63%, 1: 71.37%
CHEMBL1794467 0: 18.39%, 1: 81.61%
CHEMBL1794475 0: 56.02%, 1: 43.98%
CHEMBL1794484 0: 44.93%, 1: 55.07%
CHEMBL1794494 0: 5.33%, 1: 94.67%
CHEMBL1794497 0: 8.47%, 1: 91.53%
CHEMBL1794499 0: 73.90%, 1: 26.10%
CHEMBL1794508 0: 94.63%, 1: 5.37%
CHEMBL1794516 0: 3.21%, 1: 96.79%
CHEMBL1794522 0: 58.65%, 1: 41.35%
CHEMBL1794528 0: 79.86%, 1: 20.14%
CHEMBL1794531 0: 92.03%, 1: 7.97%
CHEMBL1794548 0: 88.74%, 1: 11.26%
CHEMBL1794566 0: 6.05%, 1: 93.95%
CHEMBL1794567 0: 9.61%, 1: 90.39%
CHEMBL1794570 0: 92.79%, 1: 7.21%
CHEMBL1794571 0: 53.55%, 1: 46.45%
CHEMBL1794573 0: 10.54%, 1: 89.46%
CHEMBL1794574 0: 1.09%, 1: 98.91%
CHEMBL1794578 0: 46.99%, 1: 53.01%
CHEMBL1794581 0: 19.65%, 1: 80.35%
CHEMBL1863510 0: 2.14%, 1: 97.86%
CHEMBL1863512 0: 2.79%, 1: 97.21%
CHEMBL1909084 0: 99.28%, 1: 0.72%
CHEMBL1909085 0: 92.10%, 1: 7.90%
CHEMBL1909086 0: 91.52%, 1: 8.48%
CHEMBL1909087 0: 91.86%, 1: 8.14%
CHEMBL1909088 0: 87.97%, 1: 12.03%
CHEMBL1909089 0: 88.69%, 1: 11.31%
CHEMBL1909090 0: 90.81%, 1: 9.19%
CHEMBL1909091 0: 97.24%, 1: 2.76%
CHEMBL1909092 0: 97.14%, 1: 2.86%
CHEMBL1909093 0: 98.42%, 1: 1.58%
CHEMBL1909094 0: 90.85%, 1: 9.15%
CHEMBL1909095 0: 98.56%, 1: 1.44%
CHEMBL1909097 0: 99.28%, 1: 0.72%
CHEMBL1909102 0: 92.94%, 1: 7.06%
CHEMBL1909103 0: 97.23%, 1: 2.77%
CHEMBL1909104 0: 87.06%, 1: 12.94%
CHEMBL1909105 0: 88.70%, 1: 11.30%
CHEMBL1909106 0: 98.57%, 1: 1.43%
CHEMBL1909107 0: 97.86%, 1: 2.14%
CHEMBL1909108 0: 93.04%, 1: 6.96%
CHEMBL1909109 0: 90.35%, 1: 9.65%
CHEMBL1909110 0: 92.47%, 1: 7.53%
CHEMBL1909111 0: 94.03%, 1: 5.97%
CHEMBL1909112 0: 90.91%, 1: 9.09%
CHEMBL1909114 0: 96.50%, 1: 3.50%
CHEMBL1909115 0: 96.00%, 1: 4.00%
CHEMBL1909116 0: 96.98%, 1: 3.02%
CHEMBL1909121 0: 92.80%, 1: 7.20%
CHEMBL1909123 0: 99.16%, 1: 0.84%
CHEMBL1909124 0: 99.40%, 1: 0.60%
CHEMBL1909130 0: 95.56%, 1: 4.44%
CHEMBL1909131 0: 97.01%, 1: 2.99%
CHEMBL1909132 0: 96.15%, 1: 3.85%
CHEMBL1909134 0: 95.19%, 1: 4.81%
CHEMBL1909135 0: 95.76%, 1: 4.24%
CHEMBL1909136 0: 93.20%, 1: 6.80%
CHEMBL1909138 0: 96.16%, 1: 3.84%
CHEMBL1909139 0: 93.84%, 1: 6.16%
CHEMBL1909140 0: 93.87%, 1: 6.13%
CHEMBL1909141 0: 89.73%, 1: 10.27%
CHEMBL1909142 0: 99.04%, 1: 0.96%
CHEMBL1909143 0: 93.01%, 1: 6.99%
CHEMBL1909145 0: 97.97%, 1: 2.03%
CHEMBL1909148 0: 98.57%, 1: 1.43%
CHEMBL1909150 0: 95.10%, 1: 4.90%
CHEMBL1909156 0: 94.28%, 1: 5.72%
CHEMBL1909157 0: 96.38%, 1: 3.62%
CHEMBL1909158 0: 99.05%, 1: 0.95%
CHEMBL1909159 0: 93.18%, 1: 6.82%
CHEMBL1909165 0: 97.79%, 1: 2.21%
CHEMBL1909169 0: 98.32%, 1: 1.68%
CHEMBL1909170 0: 92.35%, 1: 7.65%
CHEMBL1909171 0: 93.29%, 1: 6.71%
CHEMBL1909172 0: 92.82%, 1: 7.18%
CHEMBL1909173 0: 92.38%, 1: 7.62%
CHEMBL1909174 0: 92.71%, 1: 7.29%
CHEMBL1909180 0: 98.20%, 1: 1.80%
CHEMBL1909181 0: 97.71%, 1: 2.29%
CHEMBL1909182 0: 96.99%, 1: 3.01%
CHEMBL1909184 0: 98.93%, 1: 1.07%
CHEMBL1909186 0: 99.52%, 1: 0.48%
CHEMBL1909190 0: 98.66%, 1: 1.34%
CHEMBL1909191 0: 94.99%, 1: 5.01%
CHEMBL1909192 0: 99.52%, 1: 0.48%
CHEMBL1909200 0: 97.97%, 1: 2.03%
CHEMBL1909201 0: 97.72%, 1: 2.28%
CHEMBL1909203 0: 98.05%, 1: 1.95%
CHEMBL1909204 0: 97.17%, 1: 2.83%
CHEMBL1909205 0: 98.67%, 1: 1.33%
CHEMBL1909206 0: 99.16%, 1: 0.84%
CHEMBL1909209 0: 92.34%, 1: 7.66%
CHEMBL1909210 0: 96.05%, 1: 3.95%
CHEMBL1909211 0: 88.80%, 1: 11.20%
CHEMBL1909212 0: 98.30%, 1: 1.70%
CHEMBL1909213 0: 99.28%, 1: 0.72%
CHEMBL1909214 0: 98.79%, 1: 1.21%
CHEMBL1909215 0: 95.61%, 1: 4.39%
CHEMBL1963686 0: 61.95%, 1: 38.05%
CHEMBL1963687 0: 64.42%, 1: 35.58%
CHEMBL1963688 0: 62.52%, 1: 37.48%
CHEMBL1963689 0: 89.69%, 1: 10.31%
CHEMBL1963690 0: 57.36%, 1: 42.64%
CHEMBL1963691 0: 49.33%, 1: 50.67%
CHEMBL1963692 0: 53.73%, 1: 46.27%
CHEMBL1963693 0: 71.00%, 1: 29.00%
CHEMBL1963694 0: 91.05%, 1: 8.95%
CHEMBL1963695 0: 74.90%, 1: 25.10%
CHEMBL1963696 0: 77.85%, 1: 22.15%
CHEMBL1963697 0: 74.06%, 1: 25.94%
CHEMBL1963698 0: 74.60%, 1: 25.40%
CHEMBL1963699 0: 83.18%, 1: 16.82%
CHEMBL1963701 0: 67.32%, 1: 32.68%
CHEMBL1963702 0: 83.82%, 1: 16.18%
CHEMBL1963703 0: 68.65%, 1: 31.35%
CHEMBL1963704 0: 65.28%, 1: 34.72%
CHEMBL1963705 0: 55.92%, 1: 44.08%
CHEMBL1963706 0: 53.38%, 1: 46.62%
CHEMBL1963707 0: 53.25%, 1: 46.75%
CHEMBL1963708 0: 48.77%, 1: 51.23%
CHEMBL1963710 0: 63.21%, 1: 36.79%
CHEMBL1963711 0: 94.72%, 1: 5.28%
CHEMBL1963712 0: 75.76%, 1: 24.24%
CHEMBL1963714 0: 74.21%, 1: 25.79%
CHEMBL1963715 0: 50.28%, 1: 49.72%
CHEMBL1963716 0: 91.91%, 1: 8.09%
CHEMBL1963717 0: 55.50%, 1: 44.50%
CHEMBL1963718 0: 65.45%, 1: 34.55%
CHEMBL1963719 0: 63.39%, 1: 36.61%
CHEMBL1963720 0: 69.73%, 1: 30.27%
CHEMBL1963721 0: 56.94%, 1: 43.06%
CHEMBL1963722 0: 49.40%, 1: 50.60%
CHEMBL1963723 0: 53.82%, 1: 46.18%
CHEMBL1963724 0: 67.32%, 1: 32.68%
CHEMBL1963725 0: 69.97%, 1: 30.03%
CHEMBL1963727 0: 56.59%, 1: 43.41%
CHEMBL1963728 0: 92.02%, 1: 7.98%
CHEMBL1963729 0: 84.64%, 1: 15.36%
CHEMBL1963731 0: 61.35%, 1: 38.65%
CHEMBL1963733 0: 79.77%, 1: 20.23%
CHEMBL1963734 0: 79.51%, 1: 20.49%
CHEMBL1963735 0: 71.66%, 1: 28.34%
CHEMBL1963736 0: 78.32%, 1: 21.68%
CHEMBL1963737 0: 86.79%, 1: 13.21%
CHEMBL1963738 0: 61.18%, 1: 38.82%
CHEMBL1963739 0: 84.32%, 1: 15.68%
CHEMBL1963740 0: 64.30%, 1: 35.70%
CHEMBL1963741 0: 67.03%, 1: 32.97%
CHEMBL1963742 0: 73.47%, 1: 26.53%
CHEMBL1963743 0: 66.24%, 1: 33.76%
CHEMBL1963744 0: 71.97%, 1: 28.03%
CHEMBL1963745 0: 67.16%, 1: 32.84%
CHEMBL1963746 0: 66.34%, 1: 33.66%
CHEMBL1963747 0: 81.33%, 1: 18.67%
CHEMBL1963748 0: 58.75%, 1: 41.25%
CHEMBL1963749 0: 55.02%, 1: 44.98%
CHEMBL1963750 0: 58.61%, 1: 41.39%
CHEMBL1963751 0: 70.90%, 1: 29.10%
CHEMBL1963752 0: 64.78%, 1: 35.22%
CHEMBL1963753 0: 88.02%, 1: 11.98%
CHEMBL1963754 0: 59.12%, 1: 40.88%
CHEMBL1963756 0: 61.66%, 1: 38.34%
CHEMBL1963757 0: 73.28%, 1: 26.72%
CHEMBL1963758 0: 82.56%, 1: 17.44%
CHEMBL1963759 0: 81.38%, 1: 18.62%
CHEMBL1963760 0: 86.36%, 1: 13.64%
CHEMBL1963761 0: 83.63%, 1: 16.37%
CHEMBL1963763 0: 72.14%, 1: 27.86%
CHEMBL1963764 0: 62.50%, 1: 37.50%
CHEMBL1963765 0: 86.41%, 1: 13.59%
CHEMBL1963766 0: 81.38%, 1: 18.62%
CHEMBL1963767 0: 77.76%, 1: 22.24%
CHEMBL1963768 0: 77.63%, 1: 22.37%
CHEMBL1963770 0: 89.44%, 1: 10.56%
CHEMBL1963771 0: 53.53%, 1: 46.47%
CHEMBL1963772 0: 48.99%, 1: 51.01%
CHEMBL1963773 0: 59.00%, 1: 41.00%
CHEMBL1963775 0: 76.92%, 1: 23.08%
CHEMBL1963776 0: 87.71%, 1: 12.29%
CHEMBL1963777 0: 63.92%, 1: 36.08%
CHEMBL1963778 0: 51.70%, 1: 48.30%
CHEMBL1963779 0: 54.69%, 1: 45.31%
CHEMBL1963780 0: 89.06%, 1: 10.94%
CHEMBL1963781 0: 94.31%, 1: 5.69%
CHEMBL1963782 0: 75.86%, 1: 24.14%
CHEMBL1963783 0: 57.36%, 1: 42.64%
CHEMBL1963785 0: 70.10%, 1: 29.90%
CHEMBL1963786 0: 50.08%, 1: 49.92%
CHEMBL1963787 0: 66.39%, 1: 33.61%
CHEMBL1963789 0: 67.34%, 1: 32.66%
CHEMBL1963790 0: 57.93%, 1: 42.07%
CHEMBL1963791 0: 82.08%, 1: 17.92%
CHEMBL1963792 0: 85.73%, 1: 14.27%
CHEMBL1963793 0: 65.48%, 1: 34.52%
CHEMBL1963794 0: 77.53%, 1: 22.47%
CHEMBL1963795 0: 46.98%, 1: 53.02%
CHEMBL1963796 0: 69.76%, 1: 30.24%
CHEMBL1963797 0: 83.64%, 1: 16.36%
CHEMBL1963798 0: 89.45%, 1: 10.55%
CHEMBL1963799 0: 43.54%, 1: 56.46%
CHEMBL1963800 0: 64.88%, 1: 35.12%
CHEMBL1963801 0: 75.90%, 1: 24.10%
CHEMBL1963802 0: 61.41%, 1: 38.59%
CHEMBL1963803 0: 87.44%, 1: 12.56%
CHEMBL1963804 0: 65.90%, 1: 34.10%
CHEMBL1963805 0: 53.77%, 1: 46.23%
CHEMBL1963806 0: 41.19%, 1: 58.81%
CHEMBL1963807 0: 36.83%, 1: 63.17%
CHEMBL1963808 0: 68.82%, 1: 31.18%
CHEMBL1963809 0: 79.97%, 1: 20.03%
CHEMBL1963810 0: 43.69%, 1: 56.31%
CHEMBL1963811 0: 73.98%, 1: 26.02%
CHEMBL1963812 0: 53.08%, 1: 46.92%
CHEMBL1963813 0: 79.19%, 1: 20.81%
CHEMBL1963814 0: 51.10%, 1: 48.90%
CHEMBL1963815 0: 71.80%, 1: 28.20%
CHEMBL1963816 0: 89.64%, 1: 10.36%
CHEMBL1963817 0: 71.90%, 1: 28.10%
CHEMBL1963818 0: 55.13%, 1: 44.87%
CHEMBL1963819 0: 62.64%, 1: 37.36%
CHEMBL1963820 0: 84.46%, 1: 15.54%
CHEMBL1963821 0: 79.31%, 1: 20.69%
CHEMBL1963822 0: 90.91%, 1: 9.09%
CHEMBL1963823 0: 76.61%, 1: 23.39%
CHEMBL1963824 0: 39.57%, 1: 60.43%
CHEMBL1963825 0: 51.12%, 1: 48.88%
CHEMBL1963826 0: 59.25%, 1: 40.75%
CHEMBL1963827 0: 50.74%, 1: 49.26%
CHEMBL1963828 0: 80.86%, 1: 19.14%
CHEMBL1963829 0: 86.41%, 1: 13.59%
CHEMBL1963831 0: 52.56%, 1: 47.44%
CHEMBL1963832 0: 72.98%, 1: 27.02%
CHEMBL1963833 0: 87.13%, 1: 12.87%
CHEMBL1963834 0: 59.69%, 1: 40.31%
CHEMBL1963836 0: 85.18%, 1: 14.82%
CHEMBL1963837 0: 26.01%, 1: 73.99%
CHEMBL1963838 0: 75.83%, 1: 24.17%
CHEMBL1963846 0: 85.07%, 1: 14.93%
CHEMBL1963867 0: 73.13%, 1: 26.87%
CHEMBL1963893 0: 85.61%, 1: 14.39%
CHEMBL1963898 0: 75.62%, 1: 24.38%
CHEMBL1963907 0: 64.35%, 1: 35.65%
CHEMBL1963910 0: 10.50%, 1: 89.50%
CHEMBL1963915 0: 15.00%, 1: 85.00%
CHEMBL1963916 0: 40.48%, 1: 59.52%
CHEMBL1963918 0: 73.35%, 1: 26.65%
CHEMBL1963930 0: 3.64%, 1: 96.36%
CHEMBL1963933 0: 93.91%, 1: 6.09%
CHEMBL1963934 0: 3.64%, 1: 96.36%
CHEMBL1963937 0: 39.27%, 1: 60.73%
CHEMBL1963938 0: 65.85%, 1: 34.15%
CHEMBL1963940 0: 33.79%, 1: 66.21%
CHEMBL1963947 0: 6.25%, 1: 93.75%
CHEMBL1963966 0: 3.29%, 1: 96.71%
CHEMBL1963968 0: 51.52%, 1: 48.48%
CHEMBL1963969 0: 25.83%, 1: 74.17%
CHEMBL1963971 0: 10.20%, 1: 89.80%
CHEMBL1963974 0: 88.26%, 1: 11.74%
CHEMBL1963983 0: 76.39%, 1: 23.61%
CHEMBL1964000 0: 76.77%, 1: 23.23%
CHEMBL1964005 0: 6.67%, 1: 93.33%
CHEMBL1964010 0: 76.06%, 1: 23.94%
CHEMBL1964015 0: 73.77%, 1: 26.23%
CHEMBL1964022 0: 76.61%, 1: 23.39%
CHEMBL1964023 0: 73.75%, 1: 26.25%
CHEMBL1964081 0: 47.98%, 1: 52.02%
CHEMBL1964095 0: 65.41%, 1: 34.59%
CHEMBL1964096 0: 11.36%, 1: 88.64%
CHEMBL1964100 0: 79.62%, 1: 20.38%
CHEMBL1964101 0: 34.62%, 1: 65.38%
CHEMBL1964102 0: 71.54%, 1: 28.46%
CHEMBL1964103 0: 51.58%, 1: 48.42%
CHEMBL1964104 0: 63.88%, 1: 36.12%
CHEMBL1964105 0: 64.53%, 1: 35.47%
CHEMBL1964106 0: 57.96%, 1: 42.04%
CHEMBL1964108 0: 69.62%, 1: 30.38%
CHEMBL1964111 0: 73.41%, 1: 26.59%
CHEMBL1964112 0: 87.59%, 1: 12.41%
CHEMBL1964114 0: 61.11%, 1: 38.89%
CHEMBL1964115 0: 43.36%, 1: 56.64%
CHEMBL1964116 0: 70.55%, 1: 29.45%
CHEMBL1964117 0: 63.66%, 1: 36.34%
CHEMBL1964118 0: 60.33%, 1: 39.67%
CHEMBL1964119 0: 57.23%, 1: 42.77%
CHEMBL2028073 0: 76.99%, 1: 23.01%
CHEMBL2028074 0: 74.88%, 1: 25.12%
CHEMBL2028075 0: 56.46%, 1: 43.54%
CHEMBL2028076 0: 85.04%, 1: 14.96%
CHEMBL2028077 0: 96.19%, 1: 3.81%
CHEMBL2095143 0: 93.77%, 1: 6.23%
CHEMBL2098499 0: 74.45%, 1: 25.55%
CHEMBL2114715 0: 60.31%, 1: 39.69%
CHEMBL2114716 0: 12.27%, 1: 87.73%
CHEMBL2114719 0: 77.41%, 1: 22.59%
CHEMBL2114725 0: 51.74%, 1: 48.26%
CHEMBL2114727 0: 16.96%, 1: 83.04%
CHEMBL2114728 0: 9.82%, 1: 90.18%
CHEMBL2114737 0: 87.93%, 1: 12.07%
CHEMBL2114742 0: 33.90%, 1: 66.10%
CHEMBL2114748 0: 59.44%, 1: 40.56%
CHEMBL2114752 0: 15.02%, 1: 84.98%
CHEMBL2114753 0: 8.36%, 1: 91.64%
CHEMBL2114761 0: 78.40%, 1: 21.60%
CHEMBL2114764 0: 42.92%, 1: 57.08%
CHEMBL2114771 0: 64.02%, 1: 35.98%
CHEMBL2114791 0: 1.99%, 1: 98.01%
CHEMBL2114797 0: 42.41%, 1: 57.59%
CHEMBL2114811 0: 10.16%, 1: 89.84%
CHEMBL2114814 0: 44.98%, 1: 55.02%
CHEMBL2114816 0: 37.94%, 1: 62.06%
CHEMBL2114818 0: 4.15%, 1: 95.85%
CHEMBL2114820 0: 16.31%, 1: 83.69%
CHEMBL2114821 0: 18.77%, 1: 81.23%
CHEMBL2114823 0: 20.90%, 1: 79.10%
CHEMBL2114825 0: 18.62%, 1: 81.38%
CHEMBL2114827 0: 93.80%, 1: 6.20%
CHEMBL2114829 0: 25.17%, 1: 74.83%
CHEMBL2114830 0: 20.17%, 1: 79.83%
CHEMBL2114839 0: 28.69%, 1: 71.31%
CHEMBL2114842 0: 62.00%, 1: 38.00%
CHEMBL2114844 0: 32.59%, 1: 67.41%
CHEMBL2114847 0: 13.84%, 1: 86.16%
CHEMBL2114850 0: 8.62%, 1: 91.38%
CHEMBL2114852 0: 3.84%, 1: 96.16%
CHEMBL2114857 0: 50.00%, 1: 50.00%
CHEMBL2114858 0: 32.36%, 1: 67.64%
CHEMBL2114863 0: 33.55%, 1: 66.45%
CHEMBL2114865 0: 3.43%, 1: 96.57%
CHEMBL2114872 0: 9.51%, 1: 90.49%
CHEMBL2114874 0: 28.03%, 1: 71.97%
CHEMBL2114882 0: 92.25%, 1: 7.75%
CHEMBL2114896 0: 51.22%, 1: 48.78%
CHEMBL2114899 0: 87.76%, 1: 12.24%
CHEMBL2114909 0: 1.94%, 1: 98.06%
CHEMBL2114916 0: 42.50%, 1: 57.50%
CHEMBL2114926 0: 41.57%, 1: 58.43%
CHEMBL2114928 0: 86.06%, 1: 13.94%
CHEMBL2114930 0: 4.68%, 1: 95.32%
CHEMBL2114931 0: 97.44%, 1: 2.56%
CHEMBL2114932 0: 4.35%, 1: 95.65%
CHEMBL2354206 0: 93.86%, 1: 6.14%
CHEMBL2354207 0: 15.46%, 1: 84.54%
CHEMBL2354217 0: 20.69%, 1: 79.31%
CHEMBL2354227 0: 21.70%, 1: 78.30%
CHEMBL2354228 0: 59.88%, 1: 40.12%
CHEMBL2354248 0: 96.55%, 1: 3.45%
CHEMBL2354256 0: 7.85%, 1: 92.15%
CHEMBL2354269 0: 9.42%, 1: 90.58%
CHEMBL2354274 0: 5.26%, 1: 94.74%
CHEMBL2354276 0: 75.30%, 1: 24.70%
CHEMBL2354289 0: 34.38%, 1: 65.62%
CHEMBL2354292 0: 77.91%, 1: 22.09%
CHEMBL2354303 0: 86.76%, 1: 13.24%
CHEMBL2354305 0: 14.37%, 1: 85.63%
CHEMBL2354308 0: 9.09%, 1: 90.91%
CHEMBL2378059 0: 2.84%, 1: 97.16%
CHEMBL2449559 0: 17.37%, 1: 82.63%
CHEMBL3214794 0: 64.60%, 1: 35.40%
CHEMBL3214801 0: 28.46%, 1: 71.54%
CHEMBL3214812 0: 33.19%, 1: 66.81%
CHEMBL3214816 0: 25.43%, 1: 74.57%
CHEMBL3214851 0: 77.02%, 1: 22.98%
CHEMBL3214906 0: 61.72%, 1: 38.28%
CHEMBL3214907 0: 48.12%, 1: 51.88%
CHEMBL3214929 0: 44.25%, 1: 55.75%
CHEMBL3214930 0: 79.15%, 1: 20.85%
CHEMBL3214944 0: 32.70%, 1: 67.30%
CHEMBL3214958 0: 69.31%, 1: 30.69%
CHEMBL3214959 0: 31.09%, 1: 68.91%
CHEMBL3214970 0: 20.09%, 1: 79.91%
CHEMBL3214992 0: 27.76%, 1: 72.24%
CHEMBL3214993 0: 32.48%, 1: 67.52%
CHEMBL3214997 0: 27.27%, 1: 72.73%
CHEMBL3215006 0: 73.18%, 1: 26.82%
CHEMBL3215013 0: 10.05%, 1: 89.95%
CHEMBL3215025 0: 7.49%, 1: 92.51%
CHEMBL3215034 0: 39.87%, 1: 60.13%
CHEMBL3215078 0: 41.84%, 1: 58.16%
CHEMBL3215092 0: 57.98%, 1: 42.02%
CHEMBL3215096 0: 30.30%, 1: 69.70%
CHEMBL3215112 0: 7.48%, 1: 92.52%
CHEMBL3215116 0: 83.87%, 1: 16.13%
CHEMBL3215128 0: 14.05%, 1: 85.95%
CHEMBL3215154 0: 44.44%, 1: 55.56%
CHEMBL3215157 0: 5.96%, 1: 94.04%
CHEMBL3215158 0: 12.59%, 1: 87.41%
CHEMBL3215171 0: 97.98%, 1: 2.02%
CHEMBL3215176 0: 91.72%, 1: 8.28%
CHEMBL3215185 0: 91.95%, 1: 8.05%
CHEMBL3215187 0: 7.98%, 1: 92.02%
CHEMBL3215216 0: 15.79%, 1: 84.21%
CHEMBL3215220 0: 91.18%, 1: 8.82%
CHEMBL3215227 0: 32.66%, 1: 67.34%
CHEMBL3215228 0: 88.71%, 1: 11.29%
CHEMBL3215276 0: 7.46%, 1: 92.54%
CHEMBL3215277 0: 90.53%, 1: 9.47%
CHEMBL3215288 0: 27.05%, 1: 72.95%
CHEMBL829401 0: 10.53%, 1: 89.47%
CHEMBL830839 0: 3.07%, 1: 96.93%
CHEMBL830842 0: 10.53%, 1: 89.47%
CHEMBL914418 0: 14.93%, 1: 85.07%
CHEMBL918058 0: 93.33%, 1: 6.67%
  0% 0/3 [00:00<?, ?it/s]
  0% 0/614 [00:00<?, ?it/s][A
 48% 293/614 [00:00<00:00, 2928.94it/s][A
 94% 578/614 [00:00<00:00, 2904.18it/s][A100% 614/614 [00:00<00:00, 2885.61it/s]
Total scaffolds = 409 | train scaffolds = 339 | val scaffolds = 69 | test scaffolds = 1
/home/apappu/thesis/molecule-metalearning/chemprop/chemprop/data/scaffold.py:152: RuntimeWarning: Mean of empty slice
  target_avgs.append(np.nanmean(targets, axis=0))
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]))]
 33% 1/3 [00:00<00:01,  1.20it/s]
  0% 0/892 [00:00<?, ?it/s][A
 34% 300/892 [00:00<00:00, 2996.45it/s][A
 70% 622/892 [00:00<00:00, 3059.96it/s][A100% 892/892 [00:00<00:00, 3168.22it/s]
Total scaffolds = 726 | train scaffolds = 579 | val scaffolds = 146 | test scaffolds = 1
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
 67% 2/3 [00:01<00:00,  1.15it/s]
  0% 0/791 [00:00<?, ?it/s][A
 40% 319/791 [00:00<00:00, 3189.74it/s][A
 80% 634/791 [00:00<00:00, 3176.12it/s][A100% 791/791 [00:00<00:00, 3169.60it/s]
Total scaffolds = 630 | train scaffolds = 498 | val scaffolds = 131 | test scaffolds = 1
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
100% 3/3 [00:02<00:00,  1.11it/s]100% 3/3 [00:02<00:00,  1.08it/s]
  0% 0/3 [00:00<?, ?it/s]
  0% 0/175 [00:00<?, ?it/s][A100% 175/175 [00:00<00:00, 2816.84it/s]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
 33% 1/3 [00:00<00:01,  1.99it/s]
  0% 0/342 [00:00<?, ?it/s][A
 92% 315/342 [00:00<00:00, 3148.58it/s][A100% 342/342 [00:00<00:00, 3118.19it/s]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 56 | test scaffolds = 1
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
 67% 2/3 [00:01<00:00,  1.87it/s]
  0% 0/645 [00:00<?, ?it/s][A
 44% 282/645 [00:00<00:00, 2815.57it/s][A
 88% 568/645 [00:00<00:00, 2827.12it/s][A100% 645/645 [00:00<00:00, 2853.97it/s]
Total scaffolds = 376 | train scaffolds = 301 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
100% 3/3 [00:01<00:00,  1.65it/s]100% 3/3 [00:01<00:00,  1.59it/s]
  0% 0/3 [00:00<?, ?it/s]
  0% 0/132 [00:00<?, ?it/s][A100% 132/132 [00:00<00:00, 2645.44it/s]
Total scaffolds = 103 | train scaffolds = 84 | val scaffolds = 7 | test scaffolds = 12
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
 33% 1/3 [00:00<00:01,  1.99it/s]
  0% 0/892 [00:00<?, ?it/s][A
 32% 286/892 [00:00<00:00, 2858.93it/s][A
 66% 588/892 [00:00<00:00, 2904.17it/s][A100% 892/892 [00:00<00:00, 2973.06it/s]
Total scaffolds = 526 | train scaffolds = 421 | val scaffolds = 50 | test scaffolds = 55
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
 67% 2/3 [00:01<00:00,  1.61it/s]
  0% 0/509 [00:00<?, ?it/s][A
 59% 299/509 [00:00<00:00, 2985.26it/s][A100% 509/509 [00:00<00:00, 3033.02it/s]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 29 | test scaffolds = 40
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
100% 3/3 [00:02<00:00,  1.57it/s]100% 3/3 [00:02<00:00,  1.44it/s]
Building model 
MoleculeModel(
  (sigmoid): Sigmoid()
  (encoder): MPN(
    (encoder): MPNEncoder(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (act_func): ReLU()
      (W_i): Linear(in_features=147, out_features=300, bias=False)
      (W_h): Linear(in_features=300, out_features=300, bias=False)
      (W_o): Linear(in_features=433, out_features=300, bias=True)
    )
  )
  (ffn): Sequential(
    (0): Dropout(p=0.0, inplace=False)
    (1): Linear(in_features=300, out_features=300, bias=True)
    (2): ReLU()
    (3): Dropout(p=0.0, inplace=False)
    (4): Linear(in_features=300, out_features=1, bias=True)
  )
)
Number of parameters = 354,901
Moving maml model to cuda
  0% 0/30 [00:00<?, ?it/s]Epoch 0

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/3 [00:00<?, ?it/s][A[A/home/apappu/miniconda3/envs/chemprop/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


 33% 1/3 [00:01<00:03,  1.54s/it][A[A

 67% 2/3 [00:02<00:01,  1.42s/it][A[A

100% 3/3 [00:03<00:00,  1.32s/it][A[A100% 3/3 [00:03<00:00,  1.26s/it]
Meta loss = 4.7668e-01, PNorm = 34.0314, GNorm = 0.3010

100% 1/1 [00:03<00:00,  3.88s/it][A100% 1/1 [00:03<00:00,  3.88s/it]
Took 3.9092464447021484 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/3 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A


100% 1/1 [00:00<00:00,  4.32it/s][A[A[A100% 1/1 [00:00<00:00,  4.31it/s]
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with predictions all 0s or all 1s


 33% 1/3 [00:01<00:02,  1.07s/it][A[A


  0% 0/2 [00:00<?, ?it/s][A[A[A


 50% 1/2 [00:02<00:02,  2.09s/it][A[A[A


100% 2/2 [00:02<00:00,  1.51s/it][A[A[A100% 2/2 [00:02<00:00,  1.12s/it]
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with predictions all 0s or all 1s


 67% 2/3 [00:04<00:01,  1.64s/it][A[A


  0% 0/3 [00:00<?, ?it/s][A[A[A


 33% 1/3 [00:00<00:00,  2.84it/s][A[A[A


 67% 2/3 [00:00<00:00,  2.72it/s][A[A[A


100% 3/3 [00:00<00:00,  3.17it/s][A[A[A100% 3/3 [00:00<00:00,  3.16it/s]
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with predictions all 0s or all 1s


100% 3/3 [00:05<00:00,  1.67s/it][A[A100% 3/3 [00:05<00:00,  1.93s/it]

100% 1/1 [00:05<00:00,  5.79s/it][A100% 1/1 [00:05<00:00,  5.79s/it]
Took 9.727905035018921 seconds to complete one epoch of meta training and testing
  0% 0/30 [00:09<?, ?it/s]

wandb: Waiting for W&B process to finish, PID 46206
Traceback (most recent call last):
  File "chemprop/meta_train.py", line 18, in <module>
    meta_cross_validate(args, logger)
  File "/home/apappu/thesis/molecule-metalearning/chemprop/chemprop/train/meta_cross_validate.py", line 30, in meta_cross_validate
    model_scores, meta_best_epochs = run_meta_training(args, logger)
TypeError: 'NoneType' object is not iterable
Running in dummy mode
Meta learning, so output of FFN is 1
Filename: /home/apappu/thesis/molecule-metalearning/chemprop/chemprop/train/meta_train.py

Line #    Mem usage    Increment   Line Contents
================================================
    62   6134.2 MiB   6134.2 MiB   @profile
    63                             def meta_train(maml_model,
    64                                       meta_task_data_loader: MetaTaskDataLoader,
    65                                       epoch: int,
    66                                       loss_func: Callable,
    67                                       meta_optimizer: Optimizer,
    68                                       args: TrainArgs,
    69                                       logger: logging.Logger = None) -> int:
    70                                 """
    71                                 Trains a model for an epoch on TASKS.
    72                                 We define an epoch as a loop over all batches of tasks.
    73                                 This means we cycle through all batches of tasks, and on each task, we take a certain number of fixed inner gradient steps.
    74                             
    75                                 The number of inner gradient steps represents the number of batches per task the model fast adapts on.
    76                             
    77                                 After each batch of inner task fast adaptations, an outer loop update will be performed.
    78                             
    79                                 The epoch concludes once all task batches have been cycled through.
    80                             
    81                                 :param maml_model: l2L maml Model.
    82                                 :param meta_task_data_loader: A MetaTaskDataLoader.
    83                                 :param epoch: The current epoch -- used to skip batches of data for each task we've already seen while preserving the iterator interface
    84                                 :param loss_func: Loss function.
    85                                 :param meta_optimizer: An Optimizer.
    86                                 :param args: Arguments.
    87                                 :param logger: A logger for printing intermediate results.
    88                                 :return: The total number of iterations (training examples) trained on so far.
    89                                 """
    90   6134.2 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    91   6134.2 MiB      0.0 MiB       maml_model.train()
    92                                 # meta_train_error refers to the fast adaptation error on each task
    93                                 # meta_val_error refers to the error on the evaluation sets from each task
    94   6318.8 MiB      0.0 MiB       for meta_train_batch in tqdm(meta_task_data_loader.tasks(), total = len(meta_task_data_loader)):
    95                                     
    96   6134.2 MiB      0.wandb: Program failed with code 1. Press ctrl-c to abort syncing.
0 MiB           task_evaluation_loss = 0.0
    97                                     
    98   6318.4 MiB      0.0 MiB           for task in tqdm(meta_train_batch):
    99   6317.9 MiB      0.1 MiB               learner = maml_model.clone()
   100   6317.9 MiB      0.0 MiB               curr_task_mask = torch.Tensor(task.get_task_mask()) # Bit vector for current task
   101   6317.9 MiB      0.1 MiB               curr_task_target_idx = torch.argmax(curr_task_mask)
   102                                         
   103   6318.4 MiB    181.4 MiB               fast_adapt(learner, task, curr_task_target_idx, loss_func, args.num_inner_gradient_steps)
   104   6318.4 MiB      0.1 MiB               eval_loss = calculate_meta_loss(learner, task, curr_task_target_idx, loss_func)
   105   6318.4 MiB      0.0 MiB               task_evaluation_loss += eval_loss
   106                             
   107                                     # Should we average over the task evaluation losses?
   108   6318.4 MiB      0.0 MiB           task_evaluation_loss = task_evaluation_loss / len(meta_train_batch)
   109                             
   110                                     # Now that we are done with meta batch of tasks, perform meta update.
   111                                     # Zero out the meta opt gradient for new meta batch
   112                                     # pdb.set_trace()
   113   6318.4 MiB      0.0 MiB           meta_optimizer.zero_grad()
   114   6318.6 MiB      0.2 MiB           task_evaluation_loss.backward()
   115   6318.7 MiB      0.1 MiB           meta_optimizer.step()
   116                             
   117                                     # Compute stats and log to wandb 
   118   6318.7 MiB      0.0 MiB           with torch.no_grad():
   119   6318.7 MiB      0.0 MiB               avg_meta_loss = task_evaluation_loss.item()
   120   6318.8 MiB      0.0 MiB           pnorm = compute_pnorm(maml_model)
   121   6318.8 MiB      0.0 MiB           gnorm = compute_gnorm(maml_model)
   122   6318.8 MiB      0.1 MiB           debug(f'Meta loss = {avg_meta_loss:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}')
   123   6318.8 MiB      0.0 MiB           wandb.log({'batch_meta_loss': avg_meta_loss, 'PNorm': pnorm, 'GNorm': gnorm})


Filename: /home/apappu/thesis/molecule-metalearning/chemprop/chemprop/train/meta_evaluate.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   6318.9 MiB   6318.9 MiB   @profile
    18                             def meta_evaluate(maml_model,
    19                                          meta_task_data_loader: MetaTaskDataLoader,
    20                                          num_inner_gradient_steps: int,
    21                                          metric_func: Callable,
    22                                          loss_func: Callable,
    23                                          dataset_type: str,
    24                                          logger: logging.Logger = None) -> List[float]:
    25                                 """
    26                                 Evaluates a MAML model on a set of meta validation tasks. 
    27                             
    28                                 For each task, fast adapts, records the metric, and averages the metric over all validation tasks.
    29                             
    30                                 Returns average validation score.
    31                             
    32                                 :param maml_model: An l2l wrapped model.
    33                                 :param data_loader: A MoleculeDataLoader.
    34                                 :param metric_func: Metric function which takes in a list of targets and a list of predictions.
    35                                 :param dataset_type: Dataset type.
    36                                 :param logger: Logger.
    37                                 :return: A list with the score for each task based on `metric_func`.
    38                                 """
    39                                 ## TODO Finish this function -- after we fast adapt, we should score the predictions and append the AUC for the task
    40                             
    41   6318.9 MiB      0.0 MiB       val_task_results = []
    42   6319.2 MiB      0.0 MiB       for meta_val_batch in tqdm(meta_task_data_loader.tasks(), total = len(meta_task_data_loader)):
    43   6318.9 MiB      0.0 MiB           meta_task_losses = 0.0
    44   6319.2 MiB      0.0 MiB           for task in tqdm(meta_val_batch):
    45   6319.1 MiB      0.0 MiB               learner = maml_model.clone()
    46   6319.1 MiB      0.0 MiB               learner.train()
    47   6319.1 MiB      0.0 MiB               curr_task_mask = torch.Tensor(task.get_task_mask()) # Bit vector for current task
    48   6319.1 MiB      0.0 MiB               curr_task_target_idx = torch.argmax(curr_task_mask)
    49   6319.2 MiB      0.2 MiB               fast_adapt(learner, task, curr_task_target_idx, loss_func, num_inner_gradient_steps)
    50                             
    51                                         # Evaluate predictions now 
    52   6319.2 MiB      0.0 MiB               learner.eval()
    53   6319.2 MiB      0.0 MiB               preds = predict(
    54   6319.2 MiB      0.0 MiB                   model=learner,
    55   6319.2 MiB      0.0 MiB                   data_loader=task.val_data_loader,
    56   6319.2 MiB      0.1 MiB                   scaler=None
    57                                         )
    58                             
    59   6319.2 MiB      0.0 MiB               targets = task.get_targets('val')
    60                             
    61   6319.2 MiB      0.0 MiB               results = evaluate_predictions(
    62   6319.2 MiB      0.0 MiB                   preds=preds,
    63   6319.2 MiB      0.0 MiB                   targets=targets,
    64   6319.2 MiB      0.0 MiB                   num_tasks=1,
    65   6319.2 MiB      0.0 MiB                   metric_func=metric_func,
    66   6319.2 MiB      0.0 MiB                   dataset_type=dataset_type,
    67   6319.2 MiB      0.0 MiB                   logger=logger
    68                                         )
    69                             
    70   6319.2 MiB      0.0 MiB               val_task_results.append(results)
    71                             
    72   6319.2 MiB      0.0 MiB       return val_task_results


Filename: /home/apappu/thesis/molecule-metalearning/chemprop/chemprop/train/run_meta_training.py

Line #    Mem usage    Increment   Line Contents
================================================
    30    243.2 MiB    243.2 MiB   @profile
    31                             def run_meta_training(args: TrainArgs, logger: Logger = None) -> List[float]:
    32                                 """
    33                                 Trains a model and returns test scores on the model checkpoint with the highest validation score.
    34                             
    35                                 :param args: Arguments.
    36                                 :param logger: Logger.
    37                                 :return: A list of ensemble scores for each task.
    38                                 """
    39    243.2 MiB      0.0 MiB       if logger is not None:
    40    243.2 MiB      0.0 MiB           debug, info = logger.debug, logger.info
    41                                 else:
    42                                     debug = info = print
    43                             
    44                                 # Print command line
    45    243.2 MiB      0.0 MiB       debug('Command line')
    46    243.2 MiB      0.0 MiB       debug(f'python {" ".join(sys.argv)}')
    47                             
    48                                 # Print args
    49    243.2 MiB      0.0 MiB       debug('Args')
    50    244.0 MiB      0.8 MiB       debug(args)
    51                             
    52                                 # Save args
    53    244.3 MiB      0.3 MiB       args.save(os.path.join(args.save_dir, 'args.json'))
    54                             
    55                                 # Set pytorch seed for random initial weights
    56    244.3 MiB      0.0 MiB       torch.manual_seed(args.pytorch_seed)
    57                             
    58                                 # Get data
    59    244.3 MiB      0.0 MiB       debug('Loading data')
    60    244.3 MiB      0.0 MiB       args.task_names = args.target_columns or get_task_names(args.data_path)
    61   4535.5 MiB   4291.2 MiB       data = get_data(path=args.data_path, args=args, logger=logger)
    62   4535.5 MiB      0.0 MiB       args.num_tasks = data.num_tasks()
    63   4535.5 MiB      0.0 MiB       args.features_size = data.features_size()
    64   4535.5 MiB      0.0 MiB       debug(f'Number of tasks = {args.num_tasks}')
    65   4535.5 MiB      0.0 MiB       if args.dataset_type == 'classification':
    66   4537.1 MiB      1.6 MiB           class_sizes = get_class_sizes(data)
    67   4537.1 MiB      0.0 MiB           debug('Class sizes')
    68   4537.1 MiB      0.0 MiB           for i, task_class_sizes in enumerate(class_sizes):
    69   4537.1 MiB      0.0 MiB               debug(f'{args.task_names[i]} '
    70                                               f'{", ".join(f"{cls}: {size * 100:.2f}%" for cls, size in enumerate(task_class_sizes))}')
    71                             
    72                                 # Get loss and metric functions
    73   4537.1 MiB      0.0 MiB       loss_func = get_loss_func(args)
    74   4537.1 MiB      0.0 MiB       metric_func = get_metric_func(metric=args.metric)
    75                             
    76                                 # Automatically determine whether to cache
    77   4537.1 MiB      0.0 MiB       if len(data) <= args.cache_cutoff:
    78                                     cache = True
    79                                     num_workers = 0
    80                                 else:
    81   4537.1 MiB      0.0 MiB           cache = False
    82   4537.1 MiB      0.0 MiB           num_workers = args.num_workers
    83                             
    84                                 # Set up MetaTaskDataLoaders, which takes care of task splits under the hood 
    85                                 # Set up task splits into T_tr, T_val, T_test
    86                             
    87   4537.1 MiB      0.0 MiB       if not args.dummy:
    88                                     """ 
    89                                     Load ChEMBL task splits. Same in spirit as GSK implementation of task splits.
    90                                     We have 5 Task types remaining
    91                                     ADME (A)
    92                                     Toxicity (T)
    93                                     Unassigned (U) 
    94                                     Binding (B)
    95                                     Functional (F)
    96                                     resulting in 902 tasks.
    97                                     """
    98                             
    99                                     assert args.chembl_assay_metadata_pickle_path is not None
   100                                     with open(args.chembl_assay_metadata_pickle_path + 'chembl_1024_assay_type_to_names.pickle', 'rb') as handle:
   101                                         chembl_1024_assay_type_to_names = pickle.load(handle)
   102                                     with open(args.chembl_assay_metadata_pickle_path + 'chembl_1024_assay_name_to_type.pickle', 'rb') as handle:
   103                                         chembl_1024_assay_name_to_type = pickle.load(handle)
   104                             
   105                                     chembl_id_to_idx = {chembl_id: idx for idx, chembl_id in enumerate(args.task_names)}
   106                                     with open(args.chembl_assay_metadata_pickle_path + 'chembl_1024_meta_train_task_split.pickle', 'rb') as handle:
   107                                         T_tr = pickle.load(handle)
   108                                     with open(args.chembl_assay_metadata_pickle_path + 'chembl_1024_meta_val_task_split.pickle', 'rb') as handle:
   109                                         T_val = pickle.load(handle)
   110                                     with open(args.chembl_assay_metadata_pickle_path + 'chembl_1024_meta_test_task_split.pickle', 'rb') as handle:
   111                                         T_test = pickle.load(handle)
   112                             
   113                                 else:
   114                                     """
   115                                     Random task split for testing of *REDUCED* size 
   116                                     """
   117   4537.1 MiB      0.0 MiB           print("Running in dummy mode")
   118   4537.1 MiB      0.0 MiB           task_indices = list(range(len(args.task_names)))
   119   4537.1 MiB      0.0 MiB           np.random.shuffle(task_indices)
   120   4537.1 MiB      0.0 MiB           train_task_split, val_task_split, test_task_split = 0.005, 0.005, 0.005
   121   4537.1 MiB      0.0 MiB           train_task_cutoff = int(len(task_indices) * train_task_split)
   122   4537.1 MiB      0.0 MiB           val_task_cutoff = train_task_cutoff + int(len(task_indices)*val_task_split)
   123   4537.1 MiB      0.0 MiB           test_task_cutoff = val_task_cutoff + int(len(task_indices) * test_task_split)
   124   4537.1 MiB      0.0 MiB           T_tr, T_val, T_test = [0] * len(task_indices), [0] * len(task_indices), [0] * len(task_indices)
   125   4537.1 MiB      0.0 MiB           for idx in task_indices[:train_task_cutoff]:
   126   4537.1 MiB      0.0 MiB               T_tr[idx] = 1
   127   4537.1 MiB      0.0 MiB           for idx in task_indices[train_task_cutoff:val_task_cutoff]:
   128   4537.1 MiB      0.0 MiB               T_val[idx] = 1
   129   4537.1 MiB      0.0 MiB           for idx in task_indices[val_task_cutoff:test_task_cutoff]:
   130   4537.1 MiB      0.0 MiB               T_test[idx] = 1
   131                             
   132   4537.1 MiB      0.0 MiB       train_meta_task_data_loader = MetaTaskDataLoader(
   133   4537.1 MiB      0.0 MiB               dataset=data,
   134   4537.1 MiB      0.0 MiB               tasks=T_tr,
   135   4537.1 MiB      0.0 MiB               task_names=args.task_names,
   136   4537.1 MiB      0.0 MiB               meta_batch_size=args.meta_batch_size,
   137   4537.1 MiB      0.0 MiB               num_workers=args.num_workers,
   138   4537.1 MiB      0.0 MiB               sizes=args.meta_train_split_sizes,
   139   4537.1 MiB      0.0 MiB               args=args,
   140   4562.3 MiB     25.2 MiB               logger=logger)
   141   4562.3 MiB      0.0 MiB       val_meta_task_data_loader = MetaTaskDataLoader(
   142   4562.3 MiB      0.0 MiB               dataset=data,
   143   4562.3 MiB      0.0 MiB               tasks=T_val,
   144   4562.3 MiB      0.0 MiB               task_names=args.task_names,
   145   4562.3 MiB      0.0 MiB               meta_batch_size=args.meta_batch_size,
   146   4562.3 MiB      0.0 MiB               num_workers=args.num_workers,
   147   4562.3 MiB      0.0 MiB               sizes=args.meta_train_split_sizes,
   148   4562.3 MiB      0.0 MiB               args=args,
   149   4568.1 MiB      5.9 MiB               logger=logger)
   150   4568.1 MiB      0.0 MiB       test_meta_task_data_loader = MetaTaskDataLoader(
   151   4568.1 MiB      0.0 MiB               dataset=data,
   152   4568.1 MiB      0.0 MiB               tasks=T_test,
   153   4568.1 MiB      0.0 MiB               task_names=args.task_names,
   154   4568.1 MiB      0.0 MiB               meta_batch_size=1,
   155   4568.1 MiB      0.0 MiB               num_workers=args.num_workers,
   156   4568.1 MiB      0.0 MiB               sizes=args.meta_test_split_sizes,
   157   4568.1 MiB      0.0 MiB               args=args,
   158   4571.9 MiB      3.8 MiB               logger=logger)
   159                             
   160                                 # Initialize scaler and scale training targets by subtracting mean and dividing standard deviation (regression only)
   161   4571.9 MiB      0.0 MiB       if args.dataset_type == 'regression':
   162                                     raise ValueError("This script can only be run on ChEMBL classification")
   163                                 else:
   164   4571.9 MiB      0.0 MiB           scaler = None
   165                             
   166                                 # Set up save dir
   167   4571.9 MiB      0.0 MiB       save_dir = os.path.join(args.save_dir, f'maml_model')
   168   4571.9 MiB      0.0 MiB       makedirs(save_dir)
   169                             
   170                                 # Load/build model
   171                                 # Only set up for one model, no ensembling
   172   4571.9 MiB      0.0 MiB       if args.checkpoint_paths is not None:
   173                                     debug(f'Loading model')
   174                                     model = load_checkpoint(args.checkpoint_paths[0], logger=logger)
   175                                 else:
   176   4571.9 MiB      0.0 MiB           debug(f'Building model ')
   177   4573.0 MiB      1.1 MiB           model = MoleculeModel(args)
   178                             
   179   4573.0 MiB      0.0 MiB       debug(model)
   180   4573.0 MiB      0.0 MiB       debug(f'Number of parameters = {param_count(model):,}')
   181                                 # if args.cuda:
   182                                 #     debug('Moving model to cuda')
   183                                 # model = model.to(args.device)
   184                             
   185                                 # Ensure that model is saved in correct location for evaluation if 0 epochs
   186   4573.0 MiB      0.0 MiB       maml_model_name = 'maml_model.pt'
   187   4573.5 MiB      0.5 MiB       save_checkpoint(os.path.join(save_dir, maml_model_name), model, scaler=scaler, features_scaler=None, args=args)
   188                             
   189                                 # Optimizers
   190                                 # optimizer = build_optimizer(model, args)
   191                             
   192                                 # Learning rate schedulers
   193                                 # scheduler = build_lr_scheduler(optimizer, args)
   194                             
   195                                 # Keep it simple, we are using fixed outer and inner loop LRs and ADAM optimizer. 
   196   4573.5 MiB      0.0 MiB       maml_model = l2l.algorithms.MAML(model, lr=args.inner_loop_lr, first_order=args.FO_MAML, allow_nograd=True)
   197   4573.5 MiB      0.0 MiB       if args.cuda:
   198   4573.5 MiB      0.0 MiB           debug('Moving maml model to cuda')
   199   6134.2 MiB   1560.8 MiB       maml_model = maml_model.to(args.device)
   200   6134.2 MiB      0.0 MiB       meta_opt = optim.Adam(maml_model.parameters(), args.outer_loop_lr)
   201                                 # Run training
   202   6134.2 MiB      0.0 MiB       best_score = float('inf') if args.minimize_score else -float('inf')
   203   6134.2 MiB      0.0 MiB       best_epoch = 0 
   204   6134.2 MiB      0.0 MiB       for epoch in trange(args.epochs):
   205   6134.2 MiB      0.0 MiB           debug(f'Epoch {epoch}')
   206   6134.2 MiB      0.0 MiB           wandb.log({'Epoch': epoch})
   207   6134.2 MiB      0.0 MiB           start_time = time.time()
   208   6134.2 MiB      0.0 MiB           meta_train(
   209   6134.2 MiB      0.0 MiB               maml_model=maml_model,
   210   6134.2 MiB      0.0 MiB               meta_task_data_loader=train_meta_task_data_loader,
   211   6134.2 MiB      0.0 MiB               epoch=epoch,
   212   6134.2 MiB      0.0 MiB               loss_func=loss_func,
   213   6134.2 MiB      0.0 MiB               meta_optimizer=meta_opt,
   214   6134.2 MiB      0.0 MiB               args=args,
   215   6318.9 MiB    184.6 MiB               logger=logger
   216                                     )
   217   6318.9 MiB      0.0 MiB           info('Took {} seconds to complete one epoch of meta training'.format(time.time() - start_time))
   218                                     # No annealing / stepping as we are using a fixed learning rate for inner and outer loop
   219                                     # if isinstance(scheduler, ExponentialLR):
   220                                     #     scheduler.step()
   221                             
   222                                     # meta validation to determine whether to save a new checkpoint 
   223                                     
   224   6318.9 MiB      0.0 MiB           val_task_scores = meta_evaluate(
   225   6318.9 MiB      0.0 MiB               maml_model=maml_model,
   226   6318.9 MiB      0.0 MiB               meta_task_data_loader=val_meta_task_data_loader,
   227   6318.9 MiB      0.0 MiB               num_inner_gradient_steps=args.num_inner_gradient_steps,
   228   6318.9 MiB      0.0 MiB               metric_func=metric_func,
   229   6318.9 MiB      0.0 MiB               loss_func=loss_func,
   230   6318.9 MiB      0.0 MiB               dataset_type=args.dataset_type,
   231   6319.2 MiB      0.4 MiB               logger=logger
   232                                     )
   233   6319.2 MiB      0.0 MiB           info('Took {} seconds to complete one epoch of meta training and testing'.format(time.time() - start_time))
   234                             
   235                                     """ TODO DEBUG CODE REMOVE """
   236   6319.3 MiB      0.0 MiB           return
   237                                     # Average validation score
   238                                     avg_val_score = np.nanmean(val_task_scores)
   239                                     debug(f'Meta Validation Score {args.metric} = {avg_val_score:.6f}')
   240                                     wandb.log({'meta_val_score': avg_val_score})
   241                             
   242                                     if args.show_individual_scores:
   243                                         # Individual validation scores
   244                                         for task_name, val_score in zip(val_meta_task_data_loader.meta_task_names, val_task_scores):
   245                                             debug(f'Meta validation {task_name} {args.metric} = {val_score:.6f}')
   246                                             wandb.log({'Meta validation {} {}'.format(task_name, args.metric): val_score})
   247                             
   248                                     # Save model checkpoint if improved validation score
   249                                     if args.minimize_score and avg_val_score < best_score or \
   250                                             not args.minimize_score and avg_val_score > best_score:
   251                                         best_score, best_epoch = avg_val_score, epoch
   252                                         save_checkpoint(os.path.join(save_dir, maml_model_name), model, scaler=scaler, args=args)        
   253                                         wandb.save(os.path.join(save_dir, maml_model_name))
   254                                 
   255                                 # Evaluate on test set using model with best validation score
   256                                 info(f'Best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')
   257                                 model = load_checkpoint(os.path.join(save_dir, maml_model_name), device=args.device, logger=logger)
   258                                 maml_model = l2l.algorithms.MAML(model, lr=args.meta_test_lr, first_order=args.FO_MAML, allow_nograd=True)
   259                                 
   260                                 # Meta test time -- evaluate with early stopping
   261                                 start_time = time.time()
   262                                 test_scores, best_epochs = meta_test(
   263                                         maml_model, 
   264                                         test_meta_task_data_loader, 
   265                                         metric_func=metric_func, 
   266                                         loss_func=loss_func,
   267                                         dataset_type=args.dataset_type, 
   268                                         meta_test_epochs = args.meta_test_epochs,
   269                                         save_dir=save_dir,
   270                                         args=args,
   271                                         logger=logger)
   272                                 info('Took {} seconds to complete meta testing'.format(time.time() - start_time))
   273                                 
   274                                 # Average test score
   275                                 avg_test_score = np.nanmean(test_scores)
   276                                 info(f'Model test {args.metric} = {avg_test_score:.6f}')
   277                             
   278                                 return test_scores, best_epochs


wandb: Run summary:
wandb:                      _timestamp 1595631890.5937314
wandb:                        _runtime 175.80077505111694
wandb:                           _step 13
wandb:                           Epoch 0
wandb:   CHEMBL1613779_adaptation_loss 0.6668760776519775
wandb:   CHEMBL1614554_adaptation_loss 0.6518193483352661
wandb:   CHEMBL1963966_adaptation_loss 0.11338113248348236
wandb:                 batch_meta_loss 0.47668182849884033
wandb:                           PNorm 34.03142160964908
wandb:                           GNorm 0.30097603490120367
wandb:   CHEMBL1614065_adaptation_loss 0.4151487350463867
wandb:   CHEMBL1614288_adaptation_loss 5.441404819488525
wandb:   CHEMBL1963834_adaptation_loss 3.985596179962158
wandb: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: - 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: \ 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb: | 0.07MB of 0.07MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced maml_test: https://app.wandb.ai/apappu97/molecule-metalearning-chemprop/runs/1d9tj9ig
