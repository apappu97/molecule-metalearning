wandb: Currently logged in as: apappu97 (use `wandb login --relogin` to force relogin)
wandb: Tracking run with wandb version 0.10.1
wandb: Run data is saved locally in wandb/run-20200917_125929-38ac9e75
wandb: Syncing run finetuning_finetuning_on_CHEMBL1963934_test_task
wandb: ‚≠êÔ∏è View project at https://app.wandb.ai/apappu97/molecule-metalearning
wandb: üöÄ View run at https://app.wandb.ai/apappu97/molecule-metalearning/runs/38ac9e75
wandb: Run `wandb off` to turn off syncing.
Fold 0
Command line
python chemprop/finetune.py --seeds 0 4 5 6 12 --data_path /home/azureuser/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv --target_columns CHEMBL1963934 --dataset_type classification --split_type scaffold_balanced --checkpoint_paths /home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/outputs/pretraining/fold_0/model_0/pretraining_0.pt --num_folds 5 --epochs 30 --results_save_dir results/finetuning/pretraining_0/ --experiment_name finetuning_on_CHEMBL1963934_test_task
Args
{'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 32,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': ['/home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/outputs/pretraining/fold_0/model_0/pretraining_0.pt'],
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': '/home/azureuser/molecule-metalearning/filtered_chembl/chembl_less_1024_more_128_645_tasks.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'device': device(type='cuda'),
 'dropout': 0.2,
 'ensemble_size': 1,
 'epochs': 30,
 'experiment_name': 'finetuning_on_CHEMBL1963934_test_task',
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'features_size': None,
 'ffn_hidden_size': 400,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'metric': 'prc-auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 5,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results/finetuning/pretraining_0/',
 'save_dir': '/tmp/tmpc4fn6tnu/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'seeds': [0, 4, 5, 6, 12],
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': ['CHEMBL1963934'],
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'test_time_lr': 0.0001,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
0it [00:00, ?it/s]940it [00:00, 9399.99it/s]1860it [00:00, 9336.74it/s]2806it [00:00, 9372.72it/s]3750it [00:00, 9389.98it/s]4687it [00:00, 9382.22it/s]5634it [00:00, 9407.29it/s]6586it [00:00, 9433.72it/s]7523it [00:00, 9413.37it/s]8470it [00:00, 9428.53it/s]9421it [00:01, 9450.54it/s]10359it [00:01, 9426.42it/s]11284it [00:01, 9361.09it/s]12218it [00:01, 9354.14it/s]13161it [00:01, 9376.45it/s]14093it [00:01, 9347.41it/s]15024it [00:01, 9326.88it/s]15954it [00:01, 9313.04it/s]16884it [00:01, 9269.66it/s]17814it [00:01, 9275.26it/s]18741it [00:02, 9272.97it/s]19691it [00:02, 9339.70it/s]20625it [00:02, 9317.18it/s]21557it [00:02, 9298.95it/s]22487it [00:02, 9272.14it/s]23424it [00:02, 9299.94it/s]24368it [00:02, 9339.04it/s]25314it [00:02, 9372.20it/s]26252it [00:02, 9293.75it/s]27182it [00:02, 9283.58it/s]28127it [00:03, 9331.72it/s]29061it [00:03, 9187.61it/s]29981it [00:03, 9182.07it/s]30900it [00:03, 9067.16it/s]31818it [00:03, 9100.60it/s]32760it [00:03, 9192.37it/s]33680it [00:03, 9072.93it/s]34609it [00:03, 9136.54it/s]35524it [00:03, 9116.11it/s]36452it [00:03, 9161.99it/s]37392it [00:04, 9231.96it/s]38316it [00:04, 8977.50it/s]39264it [00:04, 9120.57it/s]40178it [00:04, 8803.58it/s]41129it [00:04, 9003.93it/s]42086it [00:04, 9164.88it/s]43006it [00:04, 9123.46it/s]43921it [00:04, 8983.25it/s]44864it [00:04, 9111.95it/s]45818it [00:04, 9235.50it/s]46779it [00:05, 9343.18it/s]47715it [00:05, 9310.36it/s]48657it [00:05, 9340.85it/s]49592it [00:05, 9310.15it/s]50540it [00:05, 9360.32it/s]51494it [00:05, 9411.03it/s]52447it [00:05, 9444.30it/s]53395it [00:05, 9453.83it/s]54341it [00:05, 9356.97it/s]55278it [00:05, 9348.28it/s]56217it [00:06, 9360.14it/s]57172it [00:06, 9415.15it/s]58133it [00:06, 9471.65it/s]59081it [00:06, 9461.42it/s]60040it [00:06, 9499.39it/s]61003it [00:06, 9536.59it/s]61865it [00:06, 9298.84it/s]
Number of examples remaining: 165
  0%|          | 0/165 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:00<00:00, 68971.51it/s]
  0%|          | 0/165 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:00<00:00, 3757.19it/s]
Number of tasks = 1
Splitting data with seed 0
  0%|          | 0/165 [00:00<?, ?it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:00<00:00, 4280.78it/s]
Total scaffolds = 134 | train scaffolds = 111 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([3])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1])), (array([1.]), array([1]))]
Class sizes
CHEMBL1963934 0: 3.64%, 1: 96.36%
Total size = 165 | train size = 132 | val size = 16 | test size = 17
Loading model 0 from /home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/outputs/pretraining/fold_0/model_0/pretraining_0.pt
Traceback (most recent call last):
  File "chemprop/finetune.py", line 12, in <module>
    finetune_cross_validate(args, logger)
  File "/home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/chemprop/chemprop/train/finetune_cross_validate.py", line 35, in finetune_cross_validate
    model_scores, best_epoch = run_finetuning(args, logger) # best VALIDATION scores
  File "/home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/chemprop/chemprop/train/run_finetuning.py", line 158, in run_finetuning
    model = _setup_pretrained_model(args, logger)
  File "/home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/chemprop/chemprop/train/run_finetuning.py", line 141, in _setup_pretrained_model
    model = load_checkpoint(args.checkpoint_paths[model_idx], logger=logger)
  File "/home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/chemprop/chemprop/utils.py", line 110, in load_checkpoint
    state = torch.load(path, map_location=lambda storage, loc: storage)
  File "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/serialization.py", line 525, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/serialization.py", line 212, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/serialization.py", line 193, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/home/azureuser/molecule-metalearning-finetuning/molecule-metalearning/outputs/pretraining/fold_0/model_0/pretraining_0.pt'
wandb: Waiting for W&B process to finish, PID 15351
wandb: Program failed with code 1.  Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.02MB of 0.02MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: wandb/run-20200917_125929-38ac9e75/logs/debug.log
wandb: Find internal logs for this run at: wandb/run-20200917_125929-38ac9e75/logs/debug-internal.log
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced finetuning_finetuning_on_CHEMBL1963934_test_task: https://app.wandb.ai/apappu97/molecule-metalearning/runs/38ac9e75

